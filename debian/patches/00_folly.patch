Description: Embed the folly source tree
 Upstream uses git submodules for this purpose and dpkg-source's multiple
 tarballs feature doesn't work well in combination with git-buildpackage. Hence,
 using debian/patches for this is a reasonable compromise.
 .
 This is fetched from folly upstream: https://github.com/facebook/folly/
 .
 This embeds revision d9c79af, as pointed by the HHVM 3.0.1 git tree.
Author: Faidon Liambotis <paravoid@debian.org>
Origin: upstream
Forwarded: not-needed
Last-Update: 2014-04-30

--- /dev/null
+++ b/hphp/submodules/folly/folly/ApplyTuple.h
@@ -0,0 +1,111 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+ * Defines a function folly::applyTuple, which takes a function and a
+ * std::tuple of arguments and calls the function with those
+ * arguments.
+ *
+ * Example:
+ *
+ *    int x = folly::applyTuple(std::plus<int>(), std::make_tuple(12, 12));
+ *    ASSERT(x == 24);
+ */
+
+#ifndef FOLLY_APPLYTUPLE_H_
+#define FOLLY_APPLYTUPLE_H_
+
+#include <tuple>
+#include <functional>
+#include <type_traits>
+
+namespace folly {
+
+//////////////////////////////////////////////////////////////////////
+
+namespace detail {
+
+// This is to allow using this with pointers to member functions,
+// where the first argument in the tuple will be the this pointer.
+template<class F> F& makeCallable(F& f) { return f; }
+template<class R, class C, class ...A>
+auto makeCallable(R (C::*d)(A...)) -> decltype(std::mem_fn(d)) {
+  return std::mem_fn(d);
+}
+
+template<class Tuple>
+struct DerefSize
+  : std::tuple_size<typename std::remove_reference<Tuple>::type>
+{};
+
+// CallTuple recursively unpacks tuple arguments so we can forward
+// them into the function.
+template<class Ret>
+struct CallTuple {
+  template<class F, class Tuple, class ...Unpacked>
+  static typename std::enable_if<
+    (sizeof...(Unpacked) < DerefSize<Tuple>::value),
+    Ret
+  >::type call(const F& f, Tuple&& t, Unpacked&&... unp) {
+    typedef typename std::tuple_element<
+      sizeof...(Unpacked),
+      typename std::remove_reference<Tuple>::type
+    >::type ElementType;
+    return CallTuple<Ret>::call(f, std::forward<Tuple>(t),
+      std::forward<Unpacked>(unp)...,
+      std::forward<ElementType>(std::get<sizeof...(Unpacked)>(t))
+    );
+  }
+
+  template<class F, class Tuple, class ...Unpacked>
+  static typename std::enable_if<
+    (sizeof...(Unpacked) == DerefSize<Tuple>::value),
+    Ret
+  >::type call(const F& f, Tuple&& t, Unpacked&&... unp) {
+    return makeCallable(f)(std::forward<Unpacked>(unp)...);
+  }
+};
+
+// The point of this meta function is to extract the contents of the
+// tuple as a parameter pack so we can pass it into std::result_of<>.
+template<class F, class Args> struct ReturnValue {};
+template<class F, class ...Args>
+struct ReturnValue<F,std::tuple<Args...>> {
+  typedef typename std::result_of<F (Args...)>::type type;
+};
+
+}
+
+//////////////////////////////////////////////////////////////////////
+
+template<class Callable, class Tuple>
+typename detail::ReturnValue<
+  typename std::decay<Callable>::type,
+  typename std::remove_reference<Tuple>::type
+>::type
+applyTuple(const Callable& c, Tuple&& t) {
+  typedef typename detail::ReturnValue<
+    typename std::decay<Callable>::type,
+    typename std::remove_reference<Tuple>::type
+  >::type RetT;
+  return detail::CallTuple<RetT>::call(c, std::forward<Tuple>(t));
+}
+
+//////////////////////////////////////////////////////////////////////
+
+}
+
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/Arena.h
@@ -0,0 +1,248 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_ARENA_H_
+#define FOLLY_ARENA_H_
+
+#include <cassert>
+#include <utility>
+#include <limits>
+#include <boost/intrusive/slist.hpp>
+
+#include "folly/Likely.h"
+#include "folly/Malloc.h"
+
+namespace folly {
+
+/**
+ * Simple arena: allocate memory which gets freed when the arena gets
+ * destroyed.
+ *
+ * The arena itself allocates memory using a custom allocator which provides
+ * the following interface (same as required by StlAllocator in StlAllocator.h)
+ *
+ *   void* allocate(size_t size);
+ *      Allocate a block of size bytes, properly aligned to the maximum
+ *      alignment required on your system; throw std::bad_alloc if the
+ *      allocation can't be satisfied.
+ *
+ *   void deallocate(void* ptr);
+ *      Deallocate a previously allocated block.
+ *
+ * You may also specialize ArenaAllocatorTraits for your allocator type to
+ * provide:
+ *
+ *   size_t goodSize(const Allocator& alloc, size_t size) const;
+ *      Return a size (>= the provided size) that is considered "good" for your
+ *      allocator (for example, if your allocator allocates memory in 4MB
+ *      chunks, size should be rounded up to 4MB).  The provided value is
+ *      guaranteed to be rounded up to a multiple of the maximum alignment
+ *      required on your system; the returned value must be also.
+ *
+ * An implementation that uses malloc() / free() is defined below, see
+ * SysAlloc / SysArena.
+ */
+template <class Alloc> struct ArenaAllocatorTraits;
+template <class Alloc>
+class Arena {
+ public:
+  explicit Arena(const Alloc& alloc,
+                 size_t minBlockSize = kDefaultMinBlockSize,
+                 size_t sizeLimit = 0)
+    : allocAndSize_(alloc, minBlockSize)
+    , ptr_(nullptr)
+    , end_(nullptr)
+    , totalAllocatedSize_(0)
+    , bytesUsed_(0)
+    , sizeLimit_(sizeLimit) {
+  }
+
+  ~Arena();
+
+  void* allocate(size_t size) {
+    size = roundUp(size);
+    bytesUsed_ += size;
+
+    if (LIKELY(end_ - ptr_ >= size)) {
+      // Fast path: there's enough room in the current block
+      char* r = ptr_;
+      ptr_ += size;
+      assert(isAligned(r));
+      return r;
+    }
+
+    // Not enough room in the current block
+    void* r = allocateSlow(size);
+    assert(isAligned(r));
+    return r;
+  }
+
+  void deallocate(void* p) {
+    // Deallocate? Never!
+  }
+
+  // Transfer ownership of all memory allocated from "other" to "this".
+  void merge(Arena&& other);
+
+  // Gets the total memory used by the arena
+  size_t totalSize() const {
+    return totalAllocatedSize_ + sizeof(Arena);
+  }
+
+  // Gets the total number of "used" bytes, i.e. bytes that the arena users
+  // allocated via the calls to `allocate`. Doesn't include fragmentation, e.g.
+  // if block size is 4KB and you allocate 2 objects of 3KB in size,
+  // `bytesUsed()` will be 6KB, while `totalSize()` will be 8KB+.
+  size_t bytesUsed() const {
+    return bytesUsed_;
+  }
+
+ private:
+  // not copyable
+  Arena(const Arena&) = delete;
+  Arena& operator=(const Arena&) = delete;
+
+  // movable
+  Arena(Arena&&) = default;
+  Arena& operator=(Arena&&) = default;
+
+  struct Block;
+  typedef boost::intrusive::slist_member_hook<
+    boost::intrusive::tag<Arena>> BlockLink;
+
+  struct Block {
+    BlockLink link;
+
+    // Allocate a block with at least size bytes of storage.
+    // If allowSlack is true, allocate more than size bytes if convenient
+    // (via ArenaAllocatorTraits::goodSize()) as we'll try to pack small
+    // allocations in this block.
+    static std::pair<Block*, size_t> allocate(
+        Alloc& alloc, size_t size, bool allowSlack);
+    void deallocate(Alloc& alloc);
+
+    char* start() {
+      return reinterpret_cast<char*>(this + 1);
+    }
+
+   private:
+    Block() { }
+    ~Block() { }
+  } __attribute__((aligned));
+  // This should be alignas(std::max_align_t) but neither alignas nor
+  // max_align_t are supported by gcc 4.6.2.
+
+ public:
+  static constexpr size_t kDefaultMinBlockSize = 4096 - sizeof(Block);
+
+ private:
+  static constexpr size_t maxAlign = alignof(Block);
+  static constexpr bool isAligned(uintptr_t address) {
+    return (address & (maxAlign - 1)) == 0;
+  }
+  static bool isAligned(void* p) {
+    return isAligned(reinterpret_cast<uintptr_t>(p));
+  }
+
+  // Round up size so it's properly aligned
+  static constexpr size_t roundUp(size_t size) {
+    return (size + maxAlign - 1) & ~(maxAlign - 1);
+  }
+
+  // cache_last<true> makes the list keep a pointer to the last element, so we
+  // have push_back() and constant time splice_after()
+  typedef boost::intrusive::slist<
+    Block,
+    boost::intrusive::member_hook<Block, BlockLink, &Block::link>,
+    boost::intrusive::constant_time_size<false>,
+    boost::intrusive::cache_last<true>> BlockList;
+
+  void* allocateSlow(size_t size);
+
+  // Empty member optimization: package Alloc with a non-empty member
+  // in case Alloc is empty (as it is in the case of SysAlloc).
+  struct AllocAndSize : public Alloc {
+    explicit AllocAndSize(const Alloc& a, size_t s)
+      : Alloc(a), minBlockSize(s) {
+    }
+
+    size_t minBlockSize;
+  };
+
+  size_t minBlockSize() const {
+    return allocAndSize_.minBlockSize;
+  }
+  Alloc& alloc() { return allocAndSize_; }
+  const Alloc& alloc() const { return allocAndSize_; }
+
+  AllocAndSize allocAndSize_;
+  BlockList blocks_;
+  char* ptr_;
+  char* end_;
+  size_t totalAllocatedSize_;
+  size_t bytesUsed_;
+  size_t sizeLimit_;
+};
+
+/**
+ * By default, don't pad the given size.
+ */
+template <class Alloc>
+struct ArenaAllocatorTraits {
+  static size_t goodSize(const Alloc& alloc, size_t size) {
+    return size;
+  }
+};
+
+/**
+ * Arena-compatible allocator that calls malloc() and free(); see
+ * goodMallocSize() in Malloc.h for goodSize().
+ */
+class SysAlloc {
+ public:
+  void* allocate(size_t size) {
+    return checkedMalloc(size);
+  }
+
+  void deallocate(void* p) {
+    free(p);
+  }
+};
+
+template <>
+struct ArenaAllocatorTraits<SysAlloc> {
+  static size_t goodSize(const SysAlloc& alloc, size_t size) {
+    return goodMallocSize(size);
+  }
+};
+
+/**
+ * Arena that uses the system allocator (malloc / free)
+ */
+class SysArena : public Arena<SysAlloc> {
+ public:
+  explicit SysArena(
+    size_t minBlockSize = kDefaultMinBlockSize,
+    size_t sizeLimit = 0)
+      : Arena<SysAlloc>(SysAlloc(), minBlockSize, sizeLimit) {
+  }
+};
+
+}  // namespace folly
+
+#include "folly/Arena-inl.h"
+
+#endif /* FOLLY_ARENA_H_ */
--- /dev/null
+++ b/hphp/submodules/folly/folly/Arena-inl.h
@@ -0,0 +1,94 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_ARENA_H_
+#error This file may only be included from Arena.h
+#endif
+
+// Implementation of Arena.h functions
+
+namespace folly {
+
+template <class Alloc>
+std::pair<typename Arena<Alloc>::Block*, size_t>
+Arena<Alloc>::Block::allocate(Alloc& alloc, size_t size, bool allowSlack) {
+  size_t allocSize = sizeof(Block) + size;
+  if (allowSlack) {
+    allocSize = ArenaAllocatorTraits<Alloc>::goodSize(alloc, allocSize);
+  }
+
+  void* mem = alloc.allocate(allocSize);
+  assert(isAligned(mem));
+  return std::make_pair(new (mem) Block(), allocSize - sizeof(Block));
+}
+
+template <class Alloc>
+void Arena<Alloc>::Block::deallocate(Alloc& alloc) {
+  this->~Block();
+  alloc.deallocate(this);
+}
+
+template <class Alloc>
+void* Arena<Alloc>::allocateSlow(size_t size) {
+  std::pair<Block*, size_t> p;
+  char* start;
+
+
+  size_t allocSize = std::max(size, minBlockSize()) + sizeof(Block);
+  if(sizeLimit_ && allocSize > sizeLimit_ - totalAllocatedSize_) {
+    throw std::bad_alloc();
+  }
+
+  if (size > minBlockSize()) {
+    // Allocate a large block for this chunk only, put it at the back of the
+    // list so it doesn't get used for small allocations; don't change ptr_
+    // and end_, let them point into a normal block (or none, if they're
+    // null)
+    p = Block::allocate(alloc(), size, false);
+    start = p.first->start();
+    blocks_.push_back(*p.first);
+  } else {
+    // Allocate a normal sized block and carve out size bytes from it
+    p = Block::allocate(alloc(), minBlockSize(), true);
+    start = p.first->start();
+    blocks_.push_front(*p.first);
+    ptr_ = start + size;
+    end_ = start + p.second;
+  }
+
+  assert(p.second >= size);
+  totalAllocatedSize_ += p.second + sizeof(Block);
+  return start;
+}
+
+template <class Alloc>
+void Arena<Alloc>::merge(Arena<Alloc>&& other) {
+  blocks_.splice_after(blocks_.before_begin(), other.blocks_);
+  other.blocks_.clear();
+  other.ptr_ = other.end_ = nullptr;
+  totalAllocatedSize_ += other.totalAllocatedSize_;
+  other.totalAllocatedSize_ = 0;
+}
+
+template <class Alloc>
+Arena<Alloc>::~Arena() {
+  auto disposer = [this] (Block* b) { b->deallocate(this->alloc()); };
+  while (!blocks_.empty()) {
+    blocks_.pop_front_and_dispose(disposer);
+  }
+}
+
+}  // namespace folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/AtomicBitSet.h
@@ -0,0 +1,161 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_ATOMICBITSET_H_
+#define FOLLY_ATOMICBITSET_H_
+
+#include <array>
+#include <atomic>
+#include <cassert>
+#include <cstddef>
+#include <limits>
+
+#include <boost/noncopyable.hpp>
+
+namespace folly {
+
+/**
+ * An atomic bitset of fixed size (specified at compile time).
+ */
+template <size_t N>
+class AtomicBitSet : private boost::noncopyable {
+ public:
+  /**
+   * Construct an AtomicBitSet; all bits are initially false.
+   */
+  AtomicBitSet();
+
+  /**
+   * Set bit idx to true, using the given memory order. Returns the
+   * previous value of the bit.
+   *
+   * Note that the operation is a read-modify-write operation due to the use
+   * of fetch_or.
+   */
+  bool set(size_t idx, std::memory_order order = std::memory_order_seq_cst);
+
+  /**
+   * Set bit idx to false, using the given memory order. Returns the
+   * previous value of the bit.
+   *
+   * Note that the operation is a read-modify-write operation due to the use
+   * of fetch_and.
+   */
+  bool reset(size_t idx, std::memory_order order = std::memory_order_seq_cst);
+
+  /**
+   * Set bit idx to the given value, using the given memory order. Returns
+   * the previous value of the bit.
+   *
+   * Note that the operation is a read-modify-write operation due to the use
+   * of fetch_and or fetch_or.
+   *
+   * Yes, this is an overload of set(), to keep as close to std::bitset's
+   * interface as possible.
+   */
+  bool set(size_t idx,
+           bool value,
+           std::memory_order order = std::memory_order_seq_cst);
+
+  /**
+   * Read bit idx.
+   */
+  bool test(size_t idx,
+            std::memory_order order = std::memory_order_seq_cst) const;
+
+  /**
+   * Same as test() with the default memory order.
+   */
+  bool operator[](size_t idx) const;
+
+  /**
+   * Return the size of the bitset.
+   */
+  constexpr size_t size() const {
+    return N;
+  }
+
+ private:
+  // Pick the largest lock-free type available
+#if (ATOMIC_LLONG_LOCK_FREE == 2)
+  typedef unsigned long long BlockType;
+#elif (ATOMIC_LONG_LOCK_FREE == 2)
+  typedef unsigned long BlockType;
+#else
+  // Even if not lock free, what can we do?
+  typedef unsigned int BlockType;
+#endif
+  typedef std::atomic<BlockType> AtomicBlockType;
+
+  static constexpr size_t kBitsPerBlock =
+    std::numeric_limits<BlockType>::digits;
+
+  static constexpr size_t blockIndex(size_t bit) {
+    return bit / kBitsPerBlock;
+  }
+
+  static constexpr size_t bitOffset(size_t bit) {
+    return bit % kBitsPerBlock;
+  }
+
+  // avoid casts
+  static constexpr BlockType kOne = 1;
+
+  std::array<AtomicBlockType, N> data_;
+};
+
+// value-initialize to zero
+template <size_t N>
+inline AtomicBitSet<N>::AtomicBitSet() : data_() {
+}
+
+template <size_t N>
+inline bool AtomicBitSet<N>::set(size_t idx, std::memory_order order) {
+  assert(idx < N * kBitsPerBlock);
+  BlockType mask = kOne << bitOffset(idx);
+  return data_[blockIndex(idx)].fetch_or(mask, order) & mask;
+}
+
+template <size_t N>
+inline bool AtomicBitSet<N>::reset(size_t idx, std::memory_order order) {
+  assert(idx < N * kBitsPerBlock);
+  BlockType mask = kOne << bitOffset(idx);
+  return data_[blockIndex(idx)].fetch_and(~mask, order) & mask;
+}
+
+template <size_t N>
+inline bool AtomicBitSet<N>::set(size_t idx,
+                                 bool value,
+                                 std::memory_order order) {
+  return value ? set(idx, order) : reset(idx, order);
+}
+
+template <size_t N>
+inline bool AtomicBitSet<N>::test(size_t idx, std::memory_order order) const {
+  assert(idx < N * kBitsPerBlock);
+  BlockType mask = kOne << bitOffset(idx);
+  return data_[blockIndex(idx)].load(order) & mask;
+}
+
+template <size_t N>
+inline bool AtomicBitSet<N>::operator[](size_t idx) const {
+  return test(idx);
+}
+
+}  // namespaces
+
+#endif /* FOLLY_ATOMICBITSET_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/AtomicHashArray.h
@@ -0,0 +1,301 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ *  AtomicHashArray is the building block for AtomicHashMap.  It provides the
+ *  core lock-free functionality, but is limitted by the fact that it cannot
+ *  grow past it's initialization size and is a little more awkward (no public
+ *  constructor, for example).  If you're confident that you won't run out of
+ *  space, don't mind the awkardness, and really need bare-metal performance,
+ *  feel free to use AHA directly.
+ *
+ *  Check out AtomicHashMap.h for more thorough documentation on perf and
+ *  general pros and cons relative to other hash maps.
+ *
+ *  @author Spencer Ahrens <sahrens@fb.com>
+ *  @author Jordan DeLong <delong.j@fb.com>
+ */
+
+#ifndef FOLLY_ATOMICHASHARRAY_H_
+#define FOLLY_ATOMICHASHARRAY_H_
+
+#include <atomic>
+
+#include <boost/iterator/iterator_facade.hpp>
+#include <boost/noncopyable.hpp>
+
+#include "folly/Hash.h"
+#include "folly/ThreadCachedInt.h"
+
+namespace folly {
+
+template <class KeyT, class ValueT,
+          class HashFcn = std::hash<KeyT>,
+          class EqualFcn = std::equal_to<KeyT>,
+          class Allocator = std::allocator<char>>
+class AtomicHashMap;
+
+template <class KeyT, class ValueT,
+          class HashFcn = std::hash<KeyT>,
+          class EqualFcn = std::equal_to<KeyT>,
+          class Allocator = std::allocator<char>>
+class AtomicHashArray : boost::noncopyable {
+  static_assert((std::is_convertible<KeyT,int32_t>::value ||
+                 std::is_convertible<KeyT,int64_t>::value ||
+                 std::is_convertible<KeyT,const void*>::value),
+             "You are trying to use AtomicHashArray with disallowed key "
+             "types.  You must use atomically compare-and-swappable integer "
+             "keys, or a different container class.");
+ public:
+  typedef KeyT                key_type;
+  typedef ValueT              mapped_type;
+  typedef std::pair<const KeyT, ValueT> value_type;
+  typedef std::size_t         size_type;
+  typedef std::ptrdiff_t      difference_type;
+  typedef value_type&         reference;
+  typedef const value_type&   const_reference;
+  typedef value_type*         pointer;
+  typedef const value_type*   const_pointer;
+
+  const size_t  capacity_;
+  const size_t  maxEntries_;
+  const KeyT    kEmptyKey_;
+  const KeyT    kLockedKey_;
+  const KeyT    kErasedKey_;
+
+  template<class ContT, class IterVal>
+  struct aha_iterator;
+
+  typedef aha_iterator<const AtomicHashArray,const value_type> const_iterator;
+  typedef aha_iterator<AtomicHashArray,value_type> iterator;
+
+  // You really shouldn't need this if you use the SmartPtr provided by create,
+  // but if you really want to do something crazy like stick the released
+  // pointer into a DescriminatedPtr or something, you'll need this to clean up
+  // after yourself.
+  static void destroy(AtomicHashArray*);
+
+ private:
+  const size_t  kAnchorMask_;
+
+  struct Deleter {
+    void operator()(AtomicHashArray* ptr) {
+      AtomicHashArray::destroy(ptr);
+    }
+  };
+
+ public:
+  typedef std::unique_ptr<AtomicHashArray, Deleter> SmartPtr;
+
+  /*
+   * create --
+   *
+   *   Creates AtomicHashArray objects.  Use instead of constructor/destructor.
+   *
+   *   We do things this way in order to avoid the perf penalty of a second
+   *   pointer indirection when composing these into AtomicHashMap, which needs
+   *   to store an array of pointers so that it can perform atomic operations on
+   *   them when growing.
+   *
+   *   Instead of a mess of arguments, we take a max size and a Config struct to
+   *   simulate named ctor parameters.  The Config struct has sensible defaults
+   *   for everything, but is overloaded - if you specify a positive capacity,
+   *   that will be used directly instead of computing it based on
+   *   maxLoadFactor.
+   *
+   *   Create returns an AHA::SmartPtr which is a unique_ptr with a custom
+   *   deleter to make sure everything is cleaned up properly.
+   */
+  struct Config {
+    KeyT   emptyKey;
+    KeyT   lockedKey;
+    KeyT   erasedKey;
+    double maxLoadFactor;
+    double growthFactor;
+    int    entryCountThreadCacheSize;
+    size_t capacity; // if positive, overrides maxLoadFactor
+
+    constexpr Config() : emptyKey((KeyT)-1),
+                         lockedKey((KeyT)-2),
+                         erasedKey((KeyT)-3),
+                         maxLoadFactor(0.8),
+                         growthFactor(-1),
+                         entryCountThreadCacheSize(1000),
+                         capacity(0) {}
+  };
+
+  static const Config defaultConfig;
+  static SmartPtr create(size_t maxSize, const Config& = defaultConfig);
+
+  iterator find(KeyT k) {
+    return iterator(this, findInternal(k).idx);
+  }
+  const_iterator find(KeyT k) const {
+    return const_cast<AtomicHashArray*>(this)->find(k);
+  }
+
+  /*
+   * insert --
+   *
+   *   Returns a pair with iterator to the element at r.first and bool success.
+   *   Retrieve the index with ret.first.getIndex().
+   *
+   *   Fails on key collision (does not overwrite) or if map becomes
+   *   full, at which point no element is inserted, iterator is set to end(),
+   *   and success is set false.  On collisions, success is set false, but the
+   *   iterator is set to the existing entry.
+   */
+  std::pair<iterator,bool> insert(const value_type& r) {
+    SimpleRetT ret = insertInternal(r.first, r.second);
+    return std::make_pair(iterator(this, ret.idx), ret.success);
+  }
+  std::pair<iterator,bool> insert(value_type&& r) {
+    SimpleRetT ret = insertInternal(r.first, std::move(r.second));
+    return std::make_pair(iterator(this, ret.idx), ret.success);
+  }
+
+  // returns the number of elements erased - should never exceed 1
+  size_t erase(KeyT k);
+
+  // clears all keys and values in the map and resets all counters.  Not thread
+  // safe.
+  void clear();
+
+  // Exact number of elements in the map - note that readFull() acquires a
+  // mutex.  See folly/ThreadCachedInt.h for more details.
+  size_t size() const {
+    return numEntries_.readFull() -
+      numErases_.load(std::memory_order_relaxed);
+  }
+
+  bool empty() const { return size() == 0; }
+
+  iterator begin()             { return iterator(this, 0); }
+  iterator end()               { return iterator(this, capacity_); }
+  const_iterator begin() const { return const_iterator(this, 0); }
+  const_iterator end() const   { return const_iterator(this, capacity_); }
+
+  // See AtomicHashMap::findAt - access elements directly
+  // WARNING: The following 2 functions will fail silently for hashtable
+  // with capacity > 2^32
+  iterator findAt(uint32_t idx) {
+    DCHECK_LT(idx, capacity_);
+    return iterator(this, idx);
+  }
+  const_iterator findAt(uint32_t idx) const {
+    return const_cast<AtomicHashArray*>(this)->findAt(idx);
+  }
+
+  iterator makeIter(size_t idx) { return iterator(this, idx); }
+  const_iterator makeIter(size_t idx) const {
+    return const_iterator(this, idx);
+  }
+
+  // The max load factor allowed for this map
+  double maxLoadFactor() const { return ((double) maxEntries_) / capacity_; }
+
+  void setEntryCountThreadCacheSize(uint32_t newSize) {
+    numEntries_.setCacheSize(newSize);
+    numPendingEntries_.setCacheSize(newSize);
+  }
+
+  int getEntryCountThreadCacheSize() const {
+    return numEntries_.getCacheSize();
+  }
+
+  /* Private data and helper functions... */
+
+ private:
+  friend class AtomicHashMap<KeyT, ValueT, HashFcn, EqualFcn, Allocator>;
+
+  struct SimpleRetT { size_t idx; bool success;
+    SimpleRetT(size_t i, bool s) : idx(i), success(s) {}
+    SimpleRetT() {}
+  };
+
+  template <class T>
+  SimpleRetT insertInternal(KeyT key, T&& value);
+
+  SimpleRetT findInternal(const KeyT key);
+
+  static std::atomic<KeyT>* cellKeyPtr(const value_type& r) {
+    // We need some illegal casting here in order to actually store
+    // our value_type as a std::pair<const,>.  But a little bit of
+    // undefined behavior never hurt anyone ...
+    static_assert(sizeof(std::atomic<KeyT>) == sizeof(KeyT),
+                  "std::atomic is implemented in an unexpected way for AHM");
+    return
+      const_cast<std::atomic<KeyT>*>(
+        reinterpret_cast<std::atomic<KeyT> const*>(&r.first));
+  }
+
+  static KeyT relaxedLoadKey(const value_type& r) {
+    return cellKeyPtr(r)->load(std::memory_order_relaxed);
+  }
+
+  static KeyT acquireLoadKey(const value_type& r) {
+    return cellKeyPtr(r)->load(std::memory_order_acquire);
+  }
+
+  // Fun with thread local storage - atomic increment is expensive
+  // (relatively), so we accumulate in the thread cache and periodically
+  // flush to the actual variable, and walk through the unflushed counts when
+  // reading the value, so be careful of calling size() too frequently.  This
+  // increases insertion throughput several times over while keeping the count
+  // accurate.
+  ThreadCachedInt<int64_t> numEntries_;  // Successful key inserts
+  ThreadCachedInt<int64_t> numPendingEntries_; // Used by insertInternal
+  std::atomic<int64_t> isFull_; // Used by insertInternal
+  std::atomic<int64_t> numErases_;   // Successful key erases
+
+  value_type cells_[0];  // This must be the last field of this class
+
+  // Force constructor/destructor private since create/destroy should be
+  // used externally instead
+  AtomicHashArray(size_t capacity, KeyT emptyKey, KeyT lockedKey,
+                  KeyT erasedKey, double maxLoadFactor, size_t cacheSize);
+
+  ~AtomicHashArray() {}
+
+  inline void unlockCell(value_type* const cell, KeyT newKey) {
+    cellKeyPtr(*cell)->store(newKey, std::memory_order_release);
+  }
+
+  inline bool tryLockCell(value_type* const cell) {
+    KeyT expect = kEmptyKey_;
+    return cellKeyPtr(*cell)->compare_exchange_strong(expect, kLockedKey_,
+      std::memory_order_acq_rel);
+  }
+
+  inline size_t keyToAnchorIdx(const KeyT k) const {
+    const size_t hashVal = HashFcn()(k);
+    const size_t probe = hashVal & kAnchorMask_;
+    return LIKELY(probe < capacity_) ? probe : hashVal % capacity_;
+  }
+
+  inline size_t probeNext(size_t idx, size_t numProbes) {
+    //idx += numProbes; // quadratic probing
+    idx += 1; // linear probing
+    // Avoid modulus because it's slow
+    return LIKELY(idx < capacity_) ? idx : (idx - capacity_);
+  }
+}; // AtomicHashArray
+
+} // namespace folly
+
+#include "AtomicHashArray-inl.h"
+
+#endif // FOLLY_ATOMICHASHARRAY_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/AtomicHashArray-inl.h
@@ -0,0 +1,402 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_ATOMICHASHARRAY_H_
+#error "This should only be included by AtomicHashArray.h"
+#endif
+
+#include "folly/Bits.h"
+#include "folly/detail/AtomicHashUtils.h"
+
+namespace folly {
+
+// AtomicHashArray private constructor --
+template <class KeyT, class ValueT,
+          class HashFcn, class EqualFcn, class Allocator>
+AtomicHashArray<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::
+AtomicHashArray(size_t capacity, KeyT emptyKey, KeyT lockedKey,
+                KeyT erasedKey, double maxLoadFactor, size_t cacheSize)
+    : capacity_(capacity), maxEntries_(size_t(maxLoadFactor * capacity_ + 0.5)),
+      kEmptyKey_(emptyKey), kLockedKey_(lockedKey), kErasedKey_(erasedKey),
+      kAnchorMask_(nextPowTwo(capacity_) - 1), numEntries_(0, cacheSize),
+      numPendingEntries_(0, cacheSize), isFull_(0), numErases_(0) {
+}
+
+/*
+ * findInternal --
+ *
+ *   Sets ret.second to value found and ret.index to index
+ *   of key and returns true, or if key does not exist returns false and
+ *   ret.index is set to capacity_.
+ */
+template <class KeyT, class ValueT,
+          class HashFcn, class EqualFcn, class Allocator>
+typename AtomicHashArray<KeyT, ValueT,
+         HashFcn, EqualFcn, Allocator>::SimpleRetT
+AtomicHashArray<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::
+findInternal(const KeyT key_in) {
+  DCHECK_NE(key_in, kEmptyKey_);
+  DCHECK_NE(key_in, kLockedKey_);
+  DCHECK_NE(key_in, kErasedKey_);
+  for (size_t idx = keyToAnchorIdx(key_in), numProbes = 0;
+       ;
+       idx = probeNext(idx, numProbes)) {
+    const KeyT key = acquireLoadKey(cells_[idx]);
+    if (LIKELY(EqualFcn()(key, key_in))) {
+      return SimpleRetT(idx, true);
+    }
+    if (UNLIKELY(key == kEmptyKey_)) {
+      // if we hit an empty element, this key does not exist
+      return SimpleRetT(capacity_, false);
+    }
+    ++numProbes;
+    if (UNLIKELY(numProbes >= capacity_)) {
+      // probed every cell...fail
+      return SimpleRetT(capacity_, false);
+    }
+  }
+}
+
+/*
+ * insertInternal --
+ *
+ *   Returns false on failure due to key collision or full.
+ *   Also sets ret.index to the index of the key.  If the map is full, sets
+ *   ret.index = capacity_.  Also sets ret.second to cell value, thus if insert
+ *   successful this will be what we just inserted, if there is a key collision
+ *   this will be the previously inserted value, and if the map is full it is
+ *   default.
+ */
+template <class KeyT, class ValueT,
+          class HashFcn, class EqualFcn, class Allocator>
+template <class T>
+typename AtomicHashArray<KeyT, ValueT,
+         HashFcn, EqualFcn, Allocator>::SimpleRetT
+AtomicHashArray<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::
+insertInternal(KeyT key_in, T&& value) {
+  const short NO_NEW_INSERTS = 1;
+  const short NO_PENDING_INSERTS = 2;
+  CHECK_NE(key_in, kEmptyKey_);
+  CHECK_NE(key_in, kLockedKey_);
+  CHECK_NE(key_in, kErasedKey_);
+
+  size_t idx = keyToAnchorIdx(key_in);
+  size_t numProbes = 0;
+  for (;;) {
+    DCHECK_LT(idx, capacity_);
+    value_type* cell = &cells_[idx];
+    if (relaxedLoadKey(*cell) == kEmptyKey_) {
+      // NOTE: isFull_ is set based on numEntries_.readFast(), so it's
+      // possible to insert more than maxEntries_ entries. However, it's not
+      // possible to insert past capacity_.
+      ++numPendingEntries_;
+      if (isFull_.load(std::memory_order_acquire)) {
+        --numPendingEntries_;
+
+        // Before deciding whether this insert succeeded, this thread needs to
+        // wait until no other thread can add a new entry.
+
+        // Correctness assumes isFull_ is true at this point. If
+        // another thread now does ++numPendingEntries_, we expect it
+        // to pass the isFull_.load() test above. (It shouldn't insert
+        // a new entry.)
+        FOLLY_SPIN_WAIT(
+          isFull_.load(std::memory_order_acquire) != NO_PENDING_INSERTS
+            && numPendingEntries_.readFull() != 0
+        );
+        isFull_.store(NO_PENDING_INSERTS, std::memory_order_release);
+
+        if (relaxedLoadKey(*cell) == kEmptyKey_) {
+          // Don't insert past max load factor
+          return SimpleRetT(capacity_, false);
+        }
+      } else {
+        // An unallocated cell. Try once to lock it. If we succeed, insert here.
+        // If we fail, fall through to comparison below; maybe the insert that
+        // just beat us was for this very key....
+        if (tryLockCell(cell)) {
+          // Write the value - done before unlocking
+          try {
+            DCHECK(relaxedLoadKey(*cell) == kLockedKey_);
+            /*
+             * This happens using the copy constructor because we won't have
+             * constructed a lhs to use an assignment operator on when
+             * values are being set.
+             */
+            new (&cell->second) ValueT(std::forward<T>(value));
+            unlockCell(cell, key_in); // Sets the new key
+          } catch (...) {
+            // Transition back to empty key---requires handling
+            // locked->empty below.
+            unlockCell(cell, kEmptyKey_);
+            --numPendingEntries_;
+            throw;
+          }
+          // Direct comparison rather than EqualFcn ok here
+          // (we just inserted it)
+          DCHECK(relaxedLoadKey(*cell) == key_in);
+          --numPendingEntries_;
+          ++numEntries_;  // This is a thread cached atomic increment :)
+          if (numEntries_.readFast() >= maxEntries_) {
+            isFull_.store(NO_NEW_INSERTS, std::memory_order_relaxed);
+          }
+          return SimpleRetT(idx, true);
+        }
+        --numPendingEntries_;
+      }
+    }
+    DCHECK(relaxedLoadKey(*cell) != kEmptyKey_);
+    if (kLockedKey_ == acquireLoadKey(*cell)) {
+      FOLLY_SPIN_WAIT(
+        kLockedKey_ == acquireLoadKey(*cell)
+      );
+    }
+
+    const KeyT thisKey = acquireLoadKey(*cell);
+    if (EqualFcn()(thisKey, key_in)) {
+      // Found an existing entry for our key, but we don't overwrite the
+      // previous value.
+      return SimpleRetT(idx, false);
+    } else if (thisKey == kEmptyKey_ || thisKey == kLockedKey_) {
+      // We need to try again (i.e., don't increment numProbes or
+      // advance idx): this case can happen if the constructor for
+      // ValueT threw for this very cell (the rethrow block above).
+      continue;
+    }
+
+    ++numProbes;
+    if (UNLIKELY(numProbes >= capacity_)) {
+      // probed every cell...fail
+      return SimpleRetT(capacity_, false);
+    }
+
+    idx = probeNext(idx, numProbes);
+  }
+}
+
+
+/*
+ * erase --
+ *
+ *   This will attempt to erase the given key key_in if the key is found. It
+ *   returns 1 iff the key was located and marked as erased, and 0 otherwise.
+ *
+ *   Memory is not freed or reclaimed by erase, i.e. the cell containing the
+ *   erased key will never be reused. If there's an associated value, we won't
+ *   touch it either.
+ */
+template <class KeyT, class ValueT,
+          class HashFcn, class EqualFcn, class Allocator>
+size_t AtomicHashArray<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::
+erase(KeyT key_in) {
+  CHECK_NE(key_in, kEmptyKey_);
+  CHECK_NE(key_in, kLockedKey_);
+  CHECK_NE(key_in, kErasedKey_);
+  for (size_t idx = keyToAnchorIdx(key_in), numProbes = 0;
+       ;
+       idx = probeNext(idx, numProbes)) {
+    DCHECK_LT(idx, capacity_);
+    value_type* cell = &cells_[idx];
+    KeyT currentKey = acquireLoadKey(*cell);
+    if (currentKey == kEmptyKey_ || currentKey == kLockedKey_) {
+      // If we hit an empty (or locked) element, this key does not exist. This
+      // is similar to how it's handled in find().
+      return 0;
+    }
+    if (EqualFcn()(currentKey, key_in)) {
+      // Found an existing entry for our key, attempt to mark it erased.
+      // Some other thread may have erased our key, but this is ok.
+      KeyT expect = currentKey;
+      if (cellKeyPtr(*cell)->compare_exchange_strong(expect, kErasedKey_)) {
+        numErases_.fetch_add(1, std::memory_order_relaxed);
+
+        // Even if there's a value in the cell, we won't delete (or even
+        // default construct) it because some other thread may be accessing it.
+        // Locking it meanwhile won't work either since another thread may be
+        // holding a pointer to it.
+
+        // We found the key and successfully erased it.
+        return 1;
+      }
+      // If another thread succeeds in erasing our key, we'll stop our search.
+      return 0;
+    }
+    ++numProbes;
+    if (UNLIKELY(numProbes >= capacity_)) {
+      // probed every cell...fail
+      return 0;
+    }
+  }
+}
+
+template <class KeyT, class ValueT,
+          class HashFcn, class EqualFcn, class Allocator>
+const typename AtomicHashArray<KeyT, ValueT,
+      HashFcn, EqualFcn, Allocator>::Config
+AtomicHashArray<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::defaultConfig;
+
+template <class KeyT, class ValueT,
+         class HashFcn, class EqualFcn, class Allocator>
+typename AtomicHashArray<KeyT, ValueT,
+         HashFcn, EqualFcn, Allocator>::SmartPtr
+AtomicHashArray<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::
+create(size_t maxSize, const Config& c) {
+  CHECK_LE(c.maxLoadFactor, 1.0);
+  CHECK_GT(c.maxLoadFactor, 0.0);
+  CHECK_NE(c.emptyKey, c.lockedKey);
+  size_t capacity = size_t(maxSize / c.maxLoadFactor);
+  size_t sz = sizeof(AtomicHashArray) + sizeof(value_type) * capacity;
+
+  auto const mem = Allocator().allocate(sz);
+  try {
+    new (mem) AtomicHashArray(capacity, c.emptyKey, c.lockedKey, c.erasedKey,
+                              c.maxLoadFactor, c.entryCountThreadCacheSize);
+  } catch (...) {
+    Allocator().deallocate(mem, sz);
+    throw;
+  }
+
+  SmartPtr map(static_cast<AtomicHashArray*>((void *)mem));
+
+  /*
+   * Mark all cells as empty.
+   *
+   * Note: we're bending the rules a little here accessing the key
+   * element in our cells even though the cell object has not been
+   * constructed, and casting them to atomic objects (see cellKeyPtr).
+   * (Also, in fact we never actually invoke the value_type
+   * constructor.)  This is in order to avoid needing to default
+   * construct a bunch of value_type when we first start up: if you
+   * have an expensive default constructor for the value type this can
+   * noticeably speed construction time for an AHA.
+   */
+  FOR_EACH_RANGE(i, 0, map->capacity_) {
+    cellKeyPtr(map->cells_[i])->store(map->kEmptyKey_,
+      std::memory_order_relaxed);
+  }
+  return map;
+}
+
+template <class KeyT, class ValueT,
+          class HashFcn, class EqualFcn, class Allocator>
+void AtomicHashArray<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::
+destroy(AtomicHashArray* p) {
+  assert(p);
+
+  size_t sz = sizeof(AtomicHashArray) + sizeof(value_type) * p->capacity_;
+
+  FOR_EACH_RANGE(i, 0, p->capacity_) {
+    if (p->cells_[i].first != p->kEmptyKey_) {
+      p->cells_[i].~value_type();
+    }
+  }
+  p->~AtomicHashArray();
+
+  Allocator().deallocate((char *)p, sz);
+}
+
+// clear -- clears all keys and values in the map and resets all counters
+template <class KeyT, class ValueT,
+          class HashFcn, class EqualFcn, class Allocator>
+void AtomicHashArray<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::
+clear() {
+  FOR_EACH_RANGE(i, 0, capacity_) {
+    if (cells_[i].first != kEmptyKey_) {
+      cells_[i].~value_type();
+      *const_cast<KeyT*>(&cells_[i].first) = kEmptyKey_;
+    }
+    CHECK(cells_[i].first == kEmptyKey_);
+  }
+  numEntries_.set(0);
+  numPendingEntries_.set(0);
+  isFull_.store(0, std::memory_order_relaxed);
+  numErases_.store(0, std::memory_order_relaxed);
+}
+
+
+// Iterator implementation
+
+template <class KeyT, class ValueT,
+          class HashFcn, class EqualFcn, class Allocator>
+template <class ContT, class IterVal>
+struct AtomicHashArray<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::aha_iterator
+    : boost::iterator_facade<aha_iterator<ContT,IterVal>,
+                             IterVal,
+                             boost::forward_traversal_tag>
+{
+  explicit aha_iterator() : aha_(0) {}
+
+  // Conversion ctor for interoperability between const_iterator and
+  // iterator.  The enable_if<> magic keeps us well-behaved for
+  // is_convertible<> (v. the iterator_facade documentation).
+  template<class OtherContT, class OtherVal>
+  aha_iterator(const aha_iterator<OtherContT,OtherVal>& o,
+               typename std::enable_if<
+               std::is_convertible<OtherVal*,IterVal*>::value >::type* = 0)
+      : aha_(o.aha_)
+      , offset_(o.offset_)
+  {}
+
+  explicit aha_iterator(ContT* array, size_t offset)
+      : aha_(array)
+      , offset_(offset)
+  {
+    advancePastEmpty();
+  }
+
+  // Returns unique index that can be used with findAt().
+  // WARNING: The following function will fail silently for hashtable
+  // with capacity > 2^32
+  uint32_t getIndex() const { return offset_; }
+
+ private:
+  friend class AtomicHashArray;
+  friend class boost::iterator_core_access;
+
+  void increment() {
+    ++offset_;
+    advancePastEmpty();
+  }
+
+  bool equal(const aha_iterator& o) const {
+    return aha_ == o.aha_ && offset_ == o.offset_;
+  }
+
+  IterVal& dereference() const {
+    return aha_->cells_[offset_];
+  }
+
+  void advancePastEmpty() {
+    while (offset_ < aha_->capacity_ && !isValid()) {
+      ++offset_;
+    }
+  }
+
+  bool isValid() const {
+    KeyT key = acquireLoadKey(aha_->cells_[offset_]);
+    return key != aha_->kEmptyKey_  &&
+      key != aha_->kLockedKey_ &&
+      key != aha_->kErasedKey_;
+  }
+
+ private:
+  ContT* aha_;
+  size_t offset_;
+}; // aha_iterator
+
+} // namespace folly
+
+#undef FOLLY_SPIN_WAIT
--- /dev/null
+++ b/hphp/submodules/folly/folly/AtomicHashMap.h
@@ -0,0 +1,415 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+ * AtomicHashMap --
+ *
+ * A high performance concurrent hash map with int32 or int64 keys. Supports
+ * insert, find(key), findAt(index), erase(key), size, and more.  Memory cannot
+ * be freed or reclaimed by erase.  Can grow to a maximum of about 18 times the
+ * initial capacity, but performance degrades linearly with growth. Can also be
+ * used as an object store with unique 32-bit references directly into the
+ * internal storage (retrieved with iterator::getIndex()).
+ *
+ * Advantages:
+ *    - High performance (~2-4x tbb::concurrent_hash_map in heavily
+ *      multi-threaded environments).
+ *    - Efficient memory usage if initial capacity is not over estimated
+ *      (especially for small keys and values).
+ *    - Good fragmentation properties (only allocates in large slabs which can
+ *      be reused with clear() and never move).
+ *    - Can generate unique, long-lived 32-bit references for efficient lookup
+ *      (see findAt()).
+ *
+ * Disadvantages:
+ *    - Keys must be native int32 or int64, or explicitly converted.
+ *    - Must be able to specify unique empty, locked, and erased keys
+ *    - Performance degrades linearly as size grows beyond initialization
+ *      capacity.
+ *    - Max size limit of ~18x initial size (dependent on max load factor).
+ *    - Memory is not freed or reclaimed by erase.
+ *
+ * Usage and Operation Details:
+ *   Simple performance/memory tradeoff with maxLoadFactor.  Higher load factors
+ *   give better memory utilization but probe lengths increase, reducing
+ *   performance.
+ *
+ * Implementation and Performance Details:
+ *   AHArray is a fixed size contiguous block of value_type cells.  When
+ *   writing a cell, the key is locked while the rest of the record is
+ *   written.  Once done, the cell is unlocked by setting the key.  find()
+ *   is completely wait-free and doesn't require any non-relaxed atomic
+ *   operations.  AHA cannot grow beyond initialization capacity, but is
+ *   faster because of reduced data indirection.
+ *
+ *   AHMap is a wrapper around AHArray sub-maps that allows growth and provides
+ *   an interface closer to the stl UnorderedAssociativeContainer concept. These
+ *   sub-maps are allocated on the fly and are processed in series, so the more
+ *   there are (from growing past initial capacity), the worse the performance.
+ *
+ *   Insert returns false if there is a key collision and throws if the max size
+ *   of the map is exceeded.
+ *
+ *   Benchmark performance with 8 simultaneous threads processing 1 million
+ *   unique <int64, int64> entries on a 4-core, 2.5 GHz machine:
+ *
+ *     Load Factor   Mem Efficiency   usec/Insert   usec/Find
+ *         50%             50%           0.19         0.05
+ *         85%             85%           0.20         0.06
+ *         90%             90%           0.23         0.08
+ *         95%             95%           0.27         0.10
+ *
+ *   See folly/tests/AtomicHashMapTest.cpp for more benchmarks.
+ *
+ * @author Spencer Ahrens <sahrens@fb.com>
+ * @author Jordan DeLong <delong.j@fb.com>
+ *
+ */
+
+#ifndef FOLLY_ATOMICHASHMAP_H_
+#define FOLLY_ATOMICHASHMAP_H_
+
+#include <boost/iterator/iterator_facade.hpp>
+#include <boost/noncopyable.hpp>
+#include <boost/type_traits/is_convertible.hpp>
+
+#include <stdexcept>
+#include <functional>
+#include <atomic>
+
+#include "folly/AtomicHashArray.h"
+#include "folly/Foreach.h"
+#include "folly/Hash.h"
+#include "folly/Likely.h"
+#include "folly/ThreadCachedInt.h"
+
+namespace folly {
+
+/*
+ * AtomicHashMap provides an interface somewhat similar to the
+ * UnorderedAssociativeContainer concept in C++.  This does not
+ * exactly match this concept (or even the basic Container concept),
+ * because of some restrictions imposed by our datastructure.
+ *
+ * Specific differences (there are quite a few):
+ *
+ * - Efficiently thread safe for inserts (main point of this stuff),
+ *   wait-free for lookups.
+ *
+ * - You can erase from this container, but the cell containing the key will
+ *   not be free or reclaimed.
+ *
+ * - You can erase everything by calling clear() (and you must guarantee only
+ *   one thread can be using the container to do that).
+ *
+ * - We aren't DefaultConstructible, CopyConstructible, Assignable, or
+ *   EqualityComparable.  (Most of these are probably not something
+ *   you actually want to do with this anyway.)
+ *
+ * - We don't support the various bucket functions, rehash(),
+ *   reserve(), or equal_range().  Also no constructors taking
+ *   iterators, although this could change.
+ *
+ * - Several insertion functions, notably operator[], are not
+ *   implemented.  It is a little too easy to misuse these functions
+ *   with this container, where part of the point is that when an
+ *   insertion happens for a new key, it will atomically have the
+ *   desired value.
+ *
+ * - The map has no templated insert() taking an iterator range, but
+ *   we do provide an insert(key, value).  The latter seems more
+ *   frequently useful for this container (to avoid sprinkling
+ *   make_pair everywhere), and providing both can lead to some gross
+ *   template error messages.
+ *
+ * - The Allocator must not be stateful (a new instance will be spun up for
+ *   each allocation), and its allocate() method must take a raw number of
+ *   bytes.
+ *
+ * - KeyT must be a 32 bit or 64 bit atomic integer type, and you must
+ *   define special 'locked' and 'empty' key values in the ctor
+ *
+ * - We don't take the Hash function object as an instance in the
+ *   constructor.
+ *
+ */
+
+// Thrown when insertion fails due to running out of space for
+// submaps.
+struct AtomicHashMapFullError : std::runtime_error {
+  explicit AtomicHashMapFullError()
+    : std::runtime_error("AtomicHashMap is full")
+  {}
+};
+
+template<class KeyT, class ValueT,
+         class HashFcn, class EqualFcn, class Allocator>
+class AtomicHashMap : boost::noncopyable {
+  typedef AtomicHashArray<KeyT, ValueT, HashFcn, EqualFcn, Allocator> SubMap;
+
+ public:
+  typedef KeyT                key_type;
+  typedef ValueT              mapped_type;
+  typedef std::pair<const KeyT, ValueT> value_type;
+  typedef HashFcn             hasher;
+  typedef EqualFcn            key_equal;
+  typedef value_type*         pointer;
+  typedef value_type&         reference;
+  typedef const value_type&   const_reference;
+  typedef std::ptrdiff_t      difference_type;
+  typedef std::size_t         size_type;
+  typedef typename SubMap::Config Config;
+
+  template<class ContT, class IterVal, class SubIt>
+  struct ahm_iterator;
+
+  typedef ahm_iterator<const AtomicHashMap,
+                       const value_type,
+                       typename SubMap::const_iterator>
+    const_iterator;
+  typedef ahm_iterator<AtomicHashMap,
+                       value_type,
+                       typename SubMap::iterator>
+    iterator;
+
+ public:
+  const float kGrowthFrac_;  // How much to grow when we run out of capacity.
+
+  // The constructor takes a finalSizeEst which is the optimal
+  // number of elements to maximize space utilization and performance,
+  // and a Config object to specify more advanced options.
+  static const Config defaultConfig;
+  explicit AtomicHashMap(size_t finalSizeEst, const Config& = defaultConfig);
+
+  ~AtomicHashMap() {
+    const int numMaps = numMapsAllocated_.load(std::memory_order_relaxed);
+    FOR_EACH_RANGE (i, 0, numMaps) {
+      SubMap* thisMap = subMaps_[i].load(std::memory_order_relaxed);
+      DCHECK(thisMap);
+      SubMap::destroy(thisMap);
+    }
+  }
+
+  key_equal key_eq() const { return key_equal(); }
+  hasher hash_function() const { return hasher(); }
+
+  // TODO: emplace() support would be nice.
+
+  /*
+   * insert --
+   *
+   *   Returns a pair with iterator to the element at r.first and
+   *   success.  Retrieve the index with ret.first.getIndex().
+   *
+   *   Does not overwrite on key collision, but returns an iterator to
+   *   the existing element (since this could due to a race with
+   *   another thread, it is often important to check this return
+   *   value).
+   *
+   *   Allocates new sub maps as the existing ones become full.  If
+   *   all sub maps are full, no element is inserted, and
+   *   AtomicHashMapFullError is thrown.
+   */
+  std::pair<iterator,bool> insert(const value_type& r) {
+    return insert(r.first, r.second);
+  }
+  std::pair<iterator,bool> insert(key_type k, const mapped_type& v);
+  std::pair<iterator,bool> insert(value_type&& r) {
+    return insert(r.first, std::move(r.second));
+  }
+  std::pair<iterator,bool> insert(key_type k, mapped_type&& v);
+
+  /*
+   * find --
+   *
+   *   Returns an iterator into the map.
+   *
+   *   If the key is not found, returns end().
+   */
+  iterator find(key_type k);
+  const_iterator find(key_type k) const;
+
+  /*
+   * erase --
+   *
+   *   Erases key k from the map
+   *
+   *   Returns 1 iff the key is found and erased, and 0 otherwise.
+   */
+  size_type erase(key_type k);
+
+  /*
+   * clear --
+   *
+   *   Wipes all keys and values from primary map and destroys all secondary
+   *   maps.  Primary map remains allocated and thus the memory can be reused
+   *   in place.  Not thread safe.
+   *
+   */
+  void clear();
+
+  /*
+   * size --
+   *
+   *  Returns the exact size of the map.  Note this is not as cheap as typical
+   *  size() implementations because, for each AtomicHashArray in this AHM, we
+   *  need to grab a lock and accumulate the values from all the thread local
+   *  counters.  See folly/ThreadCachedInt.h for more details.
+   */
+  size_t size() const;
+
+  bool empty() const { return size() == 0; }
+
+  size_type count(key_type k) const {
+    return find(k) == end() ? 0 : 1;
+  }
+
+
+  /*
+   * findAt --
+   *
+   *   Returns an iterator into the map.
+   *
+   *   idx should only be an unmodified value returned by calling getIndex() on
+   *   a valid iterator returned by find() or insert(). If idx is invalid you
+   *   have a bug and the process aborts.
+   */
+  iterator findAt(uint32_t idx) {
+    SimpleRetT ret = findAtInternal(idx);
+    DCHECK_LT(ret.i, numSubMaps());
+    return iterator(this, ret.i,
+      subMaps_[ret.i].load(std::memory_order_relaxed)->makeIter(ret.j));
+  }
+  const_iterator findAt(uint32_t idx) const {
+    return const_cast<AtomicHashMap*>(this)->findAt(idx);
+  }
+
+  // Total capacity - summation of capacities of all submaps.
+  size_t capacity() const;
+
+  // Number of new insertions until current submaps are all at max load factor.
+  size_t spaceRemaining() const;
+
+  void setEntryCountThreadCacheSize(int32_t newSize) {
+    const int numMaps = numMapsAllocated_.load(std::memory_order_acquire);
+    for (int i = 0; i < numMaps; ++i) {
+      SubMap* map = subMaps_[i].load(std::memory_order_relaxed);
+      map->setEntryCountThreadCacheSize(newSize);
+    }
+  }
+
+  // Number of sub maps allocated so far to implement this map.  The more there
+  // are, the worse the performance.
+  int numSubMaps() const {
+    return numMapsAllocated_.load(std::memory_order_acquire);
+  }
+
+  iterator begin() {
+    return iterator(this, 0,
+      subMaps_[0].load(std::memory_order_relaxed)->begin());
+  }
+
+  iterator end() {
+    return iterator();
+  }
+
+  const_iterator begin() const {
+    return const_iterator(this, 0,
+      subMaps_[0].load(std::memory_order_relaxed)->begin());
+  }
+
+  const_iterator end() const {
+    return const_iterator();
+  }
+
+  /* Advanced functions for direct access: */
+
+  inline uint32_t recToIdx(const value_type& r, bool mayInsert = true) {
+    SimpleRetT ret = mayInsert ?
+      insertInternal(r.first, r.second) : findInternal(r.first);
+    return encodeIndex(ret.i, ret.j);
+  }
+
+  inline uint32_t recToIdx(value_type&& r, bool mayInsert = true) {
+    SimpleRetT ret = mayInsert ?
+      insertInternal(r.first, std::move(r.second)) : findInternal(r.first);
+    return encodeIndex(ret.i, ret.j);
+  }
+
+  inline uint32_t recToIdx(key_type k, const mapped_type& v,
+    bool mayInsert = true) {
+    SimpleRetT ret = mayInsert ? insertInternal(k, v) : findInternal(k);
+    return encodeIndex(ret.i, ret.j);
+  }
+
+  inline uint32_t recToIdx(key_type k, mapped_type&& v, bool mayInsert = true) {
+    SimpleRetT ret = mayInsert ?
+      insertInternal(k, std::move(v)) : findInternal(k);
+    return encodeIndex(ret.i, ret.j);
+  }
+
+  inline uint32_t keyToIdx(const KeyT k, bool mayInsert = false) {
+    return recToIdx(value_type(k), mayInsert);
+  }
+
+  inline const value_type& idxToRec(uint32_t idx) const {
+    SimpleRetT ret = findAtInternal(idx);
+    return subMaps_[ret.i].load(std::memory_order_relaxed)->idxToRec(ret.j);
+  }
+
+  /* Private data and helper functions... */
+
+ private:
+  // This limits primary submap size to 2^31 ~= 2 billion, secondary submap
+  // size to 2^(32 - kNumSubMapBits_ - 1) = 2^27 ~= 130 million, and num subMaps
+  // to 2^kNumSubMapBits_ = 16.
+  static const uint32_t  kNumSubMapBits_     = 4;
+  static const uint32_t  kSecondaryMapBit_   = 1u << 31; // Highest bit
+  static const uint32_t  kSubMapIndexShift_  = 32 - kNumSubMapBits_ - 1;
+  static const uint32_t  kSubMapIndexMask_   = (1 << kSubMapIndexShift_) - 1;
+  static const uint32_t  kNumSubMaps_        = 1 << kNumSubMapBits_;
+  static const uintptr_t kLockedPtr_         = 0x88ul << 48; // invalid pointer
+
+  struct SimpleRetT { uint32_t i; size_t j; bool success;
+    SimpleRetT(uint32_t ii, size_t jj, bool s) : i(ii), j(jj), success(s) {}
+    SimpleRetT() {}
+  };
+
+  template <class T>
+  SimpleRetT insertInternal(KeyT key, T&& value);
+
+  SimpleRetT findInternal(const KeyT k) const;
+
+  SimpleRetT findAtInternal(uint32_t idx) const;
+
+  std::atomic<SubMap*> subMaps_[kNumSubMaps_];
+  std::atomic<uint32_t> numMapsAllocated_;
+
+  inline bool tryLockMap(int idx) {
+    SubMap* val = nullptr;
+    return subMaps_[idx].compare_exchange_strong(val, (SubMap*)kLockedPtr_,
+      std::memory_order_acquire);
+  }
+
+  static inline uint32_t encodeIndex(uint32_t subMap, uint32_t subMapIdx);
+
+}; // AtomicHashMap
+
+} // namespace folly
+
+#include "AtomicHashMap-inl.h"
+
+#endif // FOLLY_ATOMICHASHMAP_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/AtomicHashMap-inl.h
@@ -0,0 +1,432 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_ATOMICHASHMAP_H_
+#error "This should only be included by AtomicHashMap.h"
+#endif
+
+#include "folly/detail/AtomicHashUtils.h"
+
+namespace folly {
+
+template <class KeyT, class ValueT,
+          class HashFcn, class EqualFcn, class Allocator>
+const typename AtomicHashMap<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::Config
+AtomicHashMap<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::defaultConfig;
+
+// AtomicHashMap constructor -- Atomic wrapper that allows growth
+// This class has a lot of overhead (184 Bytes) so only use for big maps
+template <typename KeyT, typename ValueT,
+          typename HashFcn, typename EqualFcn, typename Allocator>
+AtomicHashMap<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::
+AtomicHashMap(size_t size, const Config& config)
+  : kGrowthFrac_(config.growthFactor < 0 ?
+                 1.0 - config.maxLoadFactor : config.growthFactor) {
+  CHECK(config.maxLoadFactor > 0.0 && config.maxLoadFactor < 1.0);
+  subMaps_[0].store(SubMap::create(size, config).release(),
+    std::memory_order_relaxed);
+  auto numSubMaps = kNumSubMaps_;
+  FOR_EACH_RANGE(i, 1, numSubMaps) {
+    subMaps_[i].store(nullptr, std::memory_order_relaxed);
+  }
+  numMapsAllocated_.store(1, std::memory_order_relaxed);
+}
+
+// insert --
+template <typename KeyT, typename ValueT,
+          typename HashFcn, typename EqualFcn, typename Allocator>
+std::pair<typename AtomicHashMap<KeyT, ValueT, HashFcn,
+                                 EqualFcn, Allocator>::iterator, bool>
+AtomicHashMap<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::
+insert(key_type k, const mapped_type& v) {
+  SimpleRetT ret = insertInternal(k,v);
+  SubMap* subMap = subMaps_[ret.i].load(std::memory_order_relaxed);
+  return std::make_pair(iterator(this, ret.i, subMap->makeIter(ret.j)),
+                        ret.success);
+}
+
+template <typename KeyT, typename ValueT,
+          typename HashFcn, typename EqualFcn, typename Allocator>
+std::pair<typename AtomicHashMap<KeyT, ValueT, HashFcn,
+                                 EqualFcn, Allocator>::iterator, bool>
+AtomicHashMap<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::
+insert(key_type k, mapped_type&& v) {
+  SimpleRetT ret = insertInternal(k, std::move(v));
+  SubMap* subMap = subMaps_[ret.i].load(std::memory_order_relaxed);
+  return std::make_pair(iterator(this, ret.i, subMap->makeIter(ret.j)),
+                        ret.success);
+}
+
+// insertInternal -- Allocates new sub maps as existing ones fill up.
+template <typename KeyT, typename ValueT,
+          typename HashFcn, typename EqualFcn, typename Allocator>
+template <class T>
+typename AtomicHashMap<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::SimpleRetT
+AtomicHashMap<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::
+insertInternal(key_type key, T&& value) {
+ beginInsertInternal:
+  int nextMapIdx = // this maintains our state
+    numMapsAllocated_.load(std::memory_order_acquire);
+  typename SubMap::SimpleRetT ret;
+  FOR_EACH_RANGE(i, 0, nextMapIdx) {
+    // insert in each map successively.  If one succeeds, we're done!
+    SubMap* subMap = subMaps_[i].load(std::memory_order_relaxed);
+    ret = subMap->insertInternal(key, std::forward<T>(value));
+    if (ret.idx == subMap->capacity_) {
+      continue;  //map is full, so try the next one
+    }
+    // Either collision or success - insert in either case
+    return SimpleRetT(i, ret.idx, ret.success);
+  }
+
+  // If we made it this far, all maps are full and we need to try to allocate
+  // the next one.
+
+  SubMap* primarySubMap = subMaps_[0].load(std::memory_order_relaxed);
+  if (nextMapIdx >= kNumSubMaps_ ||
+      primarySubMap->capacity_ * kGrowthFrac_ < 1.0) {
+    // Can't allocate any more sub maps.
+    throw AtomicHashMapFullError();
+  }
+
+  if (tryLockMap(nextMapIdx)) {
+    // Alloc a new map and shove it in.  We can change whatever
+    // we want because other threads are waiting on us...
+    size_t numCellsAllocated = (size_t)
+      (primarySubMap->capacity_ *
+       std::pow(1.0 + kGrowthFrac_, nextMapIdx - 1));
+    size_t newSize = (int) (numCellsAllocated * kGrowthFrac_);
+    DCHECK(subMaps_[nextMapIdx].load(std::memory_order_relaxed) ==
+      (SubMap*)kLockedPtr_);
+    // create a new map using the settings stored in the first map
+
+    Config config;
+    config.emptyKey = primarySubMap->kEmptyKey_;
+    config.lockedKey = primarySubMap->kLockedKey_;
+    config.erasedKey = primarySubMap->kErasedKey_;
+    config.maxLoadFactor = primarySubMap->maxLoadFactor();
+    config.entryCountThreadCacheSize =
+      primarySubMap->getEntryCountThreadCacheSize();
+    subMaps_[nextMapIdx].store(SubMap::create(newSize, config).release(),
+      std::memory_order_relaxed);
+
+    // Publish the map to other threads.
+    numMapsAllocated_.fetch_add(1, std::memory_order_release);
+    DCHECK_EQ(nextMapIdx + 1,
+      numMapsAllocated_.load(std::memory_order_relaxed));
+  } else {
+    // If we lost the race, we'll have to wait for the next map to get
+    // allocated before doing any insertion here.
+    FOLLY_SPIN_WAIT(
+      nextMapIdx >= numMapsAllocated_.load(std::memory_order_acquire)
+    );
+  }
+
+  // Relaxed is ok here because either we just created this map, or we
+  // just did a spin wait with an acquire load on numMapsAllocated_.
+  SubMap* loadedMap = subMaps_[nextMapIdx].load(std::memory_order_relaxed);
+  DCHECK(loadedMap && loadedMap != (SubMap*)kLockedPtr_);
+  ret = loadedMap->insertInternal(key, std::forward<T>(value));
+  if (ret.idx != loadedMap->capacity_) {
+    return SimpleRetT(nextMapIdx, ret.idx, ret.success);
+  }
+  // We took way too long and the new map is already full...try again from
+  // the top (this should pretty much never happen).
+  goto beginInsertInternal;
+}
+
+// find --
+template <typename KeyT, typename ValueT,
+          typename HashFcn, typename EqualFcn, typename Allocator>
+typename AtomicHashMap<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::iterator
+AtomicHashMap<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::
+find(KeyT k) {
+  SimpleRetT ret = findInternal(k);
+  if (!ret.success) {
+    return end();
+  }
+  SubMap* subMap = subMaps_[ret.i].load(std::memory_order_relaxed);
+  return iterator(this, ret.i, subMap->makeIter(ret.j));
+}
+
+template <typename KeyT, typename ValueT,
+          typename HashFcn, typename EqualFcn, typename Allocator>
+typename AtomicHashMap<KeyT, ValueT,
+         HashFcn, EqualFcn, Allocator>::const_iterator
+AtomicHashMap<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::
+find(KeyT k) const {
+  return const_cast<AtomicHashMap*>(this)->find(k);
+}
+
+// findInternal --
+template <typename KeyT, typename ValueT,
+          typename HashFcn, typename EqualFcn, typename Allocator>
+typename AtomicHashMap<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::SimpleRetT
+AtomicHashMap<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::
+findInternal(const KeyT k) const {
+  SubMap* const primaryMap = subMaps_[0].load(std::memory_order_relaxed);
+  typename SubMap::SimpleRetT ret = primaryMap->findInternal(k);
+  if (LIKELY(ret.idx != primaryMap->capacity_)) {
+    return SimpleRetT(0, ret.idx, ret.success);
+  }
+  int const numMaps = numMapsAllocated_.load(std::memory_order_acquire);
+  FOR_EACH_RANGE(i, 1, numMaps) {
+    // Check each map successively.  If one succeeds, we're done!
+    SubMap* thisMap = subMaps_[i].load(std::memory_order_relaxed);
+    ret = thisMap->findInternal(k);
+    if (LIKELY(ret.idx != thisMap->capacity_)) {
+      return SimpleRetT(i, ret.idx, ret.success);
+    }
+  }
+  // Didn't find our key...
+  return SimpleRetT(numMaps, 0, false);
+}
+
+// findAtInternal -- see encodeIndex() for details.
+template <typename KeyT, typename ValueT,
+          typename HashFcn, typename EqualFcn, typename Allocator>
+typename AtomicHashMap<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::SimpleRetT
+AtomicHashMap<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::
+findAtInternal(uint32_t idx) const {
+  uint32_t subMapIdx, subMapOffset;
+  if (idx & kSecondaryMapBit_) {
+    // idx falls in a secondary map
+    idx &= ~kSecondaryMapBit_;  // unset secondary bit
+    subMapIdx = idx >> kSubMapIndexShift_;
+    DCHECK_LT(subMapIdx, numMapsAllocated_.load(std::memory_order_relaxed));
+    subMapOffset = idx & kSubMapIndexMask_;
+  } else {
+    // idx falls in primary map
+    subMapIdx = 0;
+    subMapOffset = idx;
+  }
+  return SimpleRetT(subMapIdx, subMapOffset, true);
+}
+
+// erase --
+template <typename KeyT, typename ValueT,
+          typename HashFcn, typename EqualFcn, typename Allocator>
+typename AtomicHashMap<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::size_type
+AtomicHashMap<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::
+erase(const KeyT k) {
+  int const numMaps = numMapsAllocated_.load(std::memory_order_acquire);
+  FOR_EACH_RANGE(i, 0, numMaps) {
+    // Check each map successively.  If one succeeds, we're done!
+    if (subMaps_[i].load(std::memory_order_relaxed)->erase(k)) {
+      return 1;
+    }
+  }
+  // Didn't find our key...
+  return 0;
+}
+
+// capacity -- summation of capacities of all submaps
+template <typename KeyT, typename ValueT,
+          typename HashFcn, typename EqualFcn, typename Allocator>
+size_t AtomicHashMap<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::
+capacity() const {
+  size_t totalCap(0);
+  int const numMaps = numMapsAllocated_.load(std::memory_order_acquire);
+  FOR_EACH_RANGE(i, 0, numMaps) {
+    totalCap += subMaps_[i].load(std::memory_order_relaxed)->capacity_;
+  }
+  return totalCap;
+}
+
+// spaceRemaining --
+// number of new insertions until current submaps are all at max load
+template <typename KeyT, typename ValueT,
+          typename HashFcn, typename EqualFcn, typename Allocator>
+size_t AtomicHashMap<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::
+spaceRemaining() const {
+  size_t spaceRem(0);
+  int const numMaps = numMapsAllocated_.load(std::memory_order_acquire);
+  FOR_EACH_RANGE(i, 0, numMaps) {
+    SubMap* thisMap = subMaps_[i].load(std::memory_order_relaxed);
+    spaceRem += std::max(
+      0,
+      thisMap->maxEntries_ - &thisMap->numEntries_.readFull()
+    );
+  }
+  return spaceRem;
+}
+
+// clear -- Wipes all keys and values from primary map and destroys
+// all secondary maps.  Not thread safe.
+template <typename KeyT, typename ValueT,
+          typename HashFcn, typename EqualFcn, typename Allocator>
+void AtomicHashMap<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::
+clear() {
+  subMaps_[0].load(std::memory_order_relaxed)->clear();
+  int const numMaps = numMapsAllocated_
+    .load(std::memory_order_relaxed);
+  FOR_EACH_RANGE(i, 1, numMaps) {
+    SubMap* thisMap = subMaps_[i].load(std::memory_order_relaxed);
+    DCHECK(thisMap);
+    SubMap::destroy(thisMap);
+    subMaps_[i].store(nullptr, std::memory_order_relaxed);
+  }
+  numMapsAllocated_.store(1, std::memory_order_relaxed);
+}
+
+// size --
+template <typename KeyT, typename ValueT,
+          typename HashFcn, typename EqualFcn, typename Allocator>
+size_t AtomicHashMap<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::
+size() const {
+  size_t totalSize(0);
+  int const numMaps = numMapsAllocated_.load(std::memory_order_acquire);
+  FOR_EACH_RANGE(i, 0, numMaps) {
+    totalSize += subMaps_[i].load(std::memory_order_relaxed)->size();
+  }
+  return totalSize;
+}
+
+// encodeIndex -- Encode the submap index and offset into return.
+// index_ret must be pre-populated with the submap offset.
+//
+// We leave index_ret untouched when referring to the primary map
+// so it can be as large as possible (31 data bits).  Max size of
+// secondary maps is limited by what can fit in the low 27 bits.
+//
+// Returns the following bit-encoded data in index_ret:
+//   if subMap == 0 (primary map) =>
+//     bit(s)          value
+//         31              0
+//       0-30  submap offset (index_ret input)
+//
+//   if subMap > 0 (secondary maps) =>
+//     bit(s)          value
+//         31              1
+//      27-30   which subMap
+//       0-26  subMap offset (index_ret input)
+template <typename KeyT, typename ValueT,
+          typename HashFcn, typename EqualFcn, typename Allocator>
+inline uint32_t AtomicHashMap<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::
+encodeIndex(uint32_t subMap, uint32_t offset) {
+  DCHECK_EQ(offset & kSecondaryMapBit_, 0);  // offset can't be too big
+  if (subMap == 0) return offset;
+  // Make sure subMap isn't too big
+  DCHECK_EQ(subMap >> kNumSubMapBits_, 0);
+  // Make sure subMap bits of offset are clear
+  DCHECK_EQ(offset & (~kSubMapIndexMask_ | kSecondaryMapBit_), 0);
+
+  // Set high-order bits to encode which submap this index belongs to
+  return offset | (subMap << kSubMapIndexShift_) | kSecondaryMapBit_;
+}
+
+
+// Iterator implementation
+
+template <typename KeyT, typename ValueT,
+          typename HashFcn, typename EqualFcn, typename Allocator>
+template<class ContT, class IterVal, class SubIt>
+struct AtomicHashMap<KeyT, ValueT, HashFcn, EqualFcn, Allocator>::ahm_iterator
+    : boost::iterator_facade<ahm_iterator<ContT,IterVal,SubIt>,
+                             IterVal,
+                             boost::forward_traversal_tag>
+{
+  explicit ahm_iterator() : ahm_(0) {}
+
+  // Conversion ctor for interoperability between const_iterator and
+  // iterator.  The enable_if<> magic keeps us well-behaved for
+  // is_convertible<> (v. the iterator_facade documentation).
+  template<class OtherContT, class OtherVal, class OtherSubIt>
+  ahm_iterator(const ahm_iterator<OtherContT,OtherVal,OtherSubIt>& o,
+               typename std::enable_if<
+               std::is_convertible<OtherSubIt,SubIt>::value >::type* = 0)
+      : ahm_(o.ahm_)
+      , subMap_(o.subMap_)
+      , subIt_(o.subIt_)
+  {}
+
+  /*
+   * Returns the unique index that can be used for access directly
+   * into the data storage.
+   */
+  uint32_t getIndex() const {
+    CHECK(!isEnd());
+    return ahm_->encodeIndex(subMap_, subIt_.getIndex());
+  }
+
+ private:
+  friend class AtomicHashMap;
+  explicit ahm_iterator(ContT* ahm,
+                        uint32_t subMap,
+                        const SubIt& subIt)
+      : ahm_(ahm)
+      , subMap_(subMap)
+      , subIt_(subIt)
+  {
+    checkAdvanceToNextSubmap();
+  }
+
+  friend class boost::iterator_core_access;
+
+  void increment() {
+    CHECK(!isEnd());
+    ++subIt_;
+    checkAdvanceToNextSubmap();
+  }
+
+  bool equal(const ahm_iterator& other) const {
+    if (ahm_ != other.ahm_) {
+      return false;
+    }
+
+    if (isEnd() || other.isEnd()) {
+      return isEnd() == other.isEnd();
+    }
+
+    return subMap_ == other.subMap_ &&
+      subIt_ == other.subIt_;
+  }
+
+  IterVal& dereference() const {
+    return *subIt_;
+  }
+
+  bool isEnd() const { return ahm_ == nullptr; }
+
+  void checkAdvanceToNextSubmap() {
+    if (isEnd()) {
+      return;
+    }
+
+    SubMap* thisMap = ahm_->subMaps_[subMap_].
+      load(std::memory_order_relaxed);
+    if (subIt_ == thisMap->end()) {
+      // This sub iterator is done, advance to next one
+      if (subMap_ + 1 <
+          ahm_->numMapsAllocated_.load(std::memory_order_acquire)) {
+        ++subMap_;
+        thisMap = ahm_->subMaps_[subMap_].load(std::memory_order_relaxed);
+        subIt_ = thisMap->begin();
+      } else {
+        ahm_ = nullptr;
+      }
+    }
+  }
+
+ private:
+  ContT* ahm_;
+  uint32_t subMap_;
+  SubIt subIt_;
+}; // ahm_iterator
+
+} // namespace folly
+
+#undef FOLLY_SPIN_WAIT
--- /dev/null
+++ b/hphp/submodules/folly/folly/AtomicStruct.h
@@ -0,0 +1,139 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_ATOMIC_STRUCT_H_
+#define FOLLY_ATOMIC_STRUCT_H_
+
+#include <atomic>
+#include <type_traits>
+#include <folly/Traits.h>
+#include <string.h>
+#include <stdint.h>
+
+namespace folly {
+
+namespace detail {
+template <int N> struct AtomicStructIntPick {};
+}
+
+/// AtomicStruct<T> work like C++ atomics, but can be used on any POD
+/// type <= 8 bytes.
+template <
+    typename T,
+    template<typename> class Atom = std::atomic,
+    typename Raw = typename detail::AtomicStructIntPick<sizeof(T)>::type>
+class AtomicStruct {
+  static_assert(alignof(T) <= alignof(Raw),
+      "target type can't have stricter alignment than matching int");
+  static_assert(sizeof(T) <= sizeof(Raw),
+      "underlying type isn't big enough");
+  static_assert(std::is_trivial<T>::value ||
+                folly::IsTriviallyCopyable<T>::value,
+      "target type must be trivially copyable");
+
+  Atom<Raw> data;
+
+  static Raw encode(T v) noexcept {
+    // we expect the compiler to optimize away the memcpy, but without
+    // it we would violate strict aliasing rules
+    Raw d = 0;
+    memcpy(&d, &v, sizeof(T));
+    return d;
+  }
+
+  static T decode(Raw d) noexcept {
+    T v;
+    memcpy(&v, &d, sizeof(T));
+    return v;
+  }
+
+ public:
+  AtomicStruct() = default;
+  ~AtomicStruct() = default;
+  AtomicStruct(AtomicStruct<T> const &) = delete;
+  AtomicStruct<T>& operator= (AtomicStruct<T> const &) = delete;
+
+  constexpr /* implicit */ AtomicStruct(T v) noexcept : data(encode(v)) {}
+
+  bool is_lock_free() const noexcept {
+    return data.is_lock_free();
+  }
+
+  bool compare_exchange_strong(
+          T& v0, T v1,
+          std::memory_order mo = std::memory_order_seq_cst) noexcept {
+    Raw d0 = encode(v0);
+    bool rv = data.compare_exchange_strong(d0, encode(v1), mo);
+    if (!rv) {
+      v0 = decode(d0);
+    }
+    return rv;
+  }
+
+  bool compare_exchange_weak(
+          T& v0, T v1,
+          std::memory_order mo = std::memory_order_seq_cst) noexcept {
+    Raw d0 = encode(v0);
+    bool rv = data.compare_exchange_weak(d0, encode(v1), mo);
+    if (!rv) {
+      v0 = decode(d0);
+    }
+    return rv;
+  }
+
+  T exchange(T v, std::memory_order mo = std::memory_order_seq_cst) noexcept {
+    return decode(data.exchange(encode(v), mo));
+  }
+
+  /* implicit */ operator T () const noexcept {
+    return decode(data);
+  }
+
+  T load(std::memory_order mo = std::memory_order_seq_cst) const noexcept {
+    return decode(data.load(mo));
+  }
+
+  T operator= (T v) noexcept {
+    return decode(data = encode(v));
+  }
+
+  void store(T v, std::memory_order mo = std::memory_order_seq_cst) noexcept {
+    data.store(encode(v), mo);
+  }
+
+  // std::atomic also provides volatile versions of all of the access
+  // methods.  These are callable on volatile objects, and also can
+  // theoretically have different implementations than their non-volatile
+  // counterpart.  If someone wants them here they can easily be added
+  // by duplicating the above code and the corresponding unit tests.
+};
+
+namespace detail {
+
+template <> struct AtomicStructIntPick<1> { typedef uint8_t type; };
+template <> struct AtomicStructIntPick<2> { typedef uint16_t type; };
+template <> struct AtomicStructIntPick<3> { typedef uint32_t type; };
+template <> struct AtomicStructIntPick<4> { typedef uint32_t type; };
+template <> struct AtomicStructIntPick<5> { typedef uint64_t type; };
+template <> struct AtomicStructIntPick<6> { typedef uint64_t type; };
+template <> struct AtomicStructIntPick<7> { typedef uint64_t type; };
+template <> struct AtomicStructIntPick<8> { typedef uint64_t type; };
+
+} // namespace detail
+
+} // namespace folly
+
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/Baton.h
@@ -0,0 +1,214 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_BATON_H
+#define FOLLY_BATON_H
+
+#include <stdint.h>
+#include <atomic>
+#include <boost/noncopyable.hpp>
+#include <errno.h>
+#include <assert.h>
+
+#include <folly/detail/Futex.h>
+#include <folly/detail/MemoryIdler.h>
+
+namespace folly {
+
+/// A Baton allows a thread to block once and be awoken: it captures
+/// a single handoff.  During its lifecycle (from construction/reset to
+/// destruction/reset) a baton must either be post()ed and wait()ed exactly
+/// once each, or not at all.
+///
+/// Baton includes no internal padding, and is only 4 bytes in size.
+/// Any alignment or padding to avoid false sharing is up to the user.
+///
+/// This is basically a stripped-down semaphore that supports only a
+/// single call to sem_post and a single call to sem_wait.  The current
+/// posix semaphore sem_t isn't too bad, but this provides more a bit more
+/// speed, inlining, smaller size, a guarantee that the implementation
+/// won't change, and compatibility with DeterministicSchedule.  By having
+/// a much more restrictive lifecycle we can also add a bunch of assertions
+/// that can help to catch race conditions ahead of time.
+template <template<typename> class Atom = std::atomic>
+struct Baton : boost::noncopyable {
+  Baton() : state_(INIT) {}
+
+  /// It is an error to destroy a Baton on which a thread is currently
+  /// wait()ing.  In practice this means that the waiter usually takes
+  /// responsibility for destroying the Baton.
+  ~Baton() {
+    // The docblock for this function says that is can't be called when
+    // there is a concurrent waiter.  We assume a strong version of this
+    // requirement in which the caller must _know_ that this is true, they
+    // are not allowed to be merely lucky.  If two threads are involved,
+    // the destroying thread must actually have synchronized with the
+    // waiting thread after wait() returned.  To convey causality the the
+    // waiting thread must have used release semantics and the destroying
+    // thread must have used acquire semantics for that communication,
+    // so we are guaranteed to see the post-wait() value of state_,
+    // which cannot be WAITING.
+    //
+    // Note that since we only care about a single memory location,
+    // the only two plausible memory orders here are relaxed and seq_cst.
+    assert(state_.load(std::memory_order_relaxed) != WAITING);
+  }
+
+  /// Equivalent to destroying the Baton and creating a new one.  It is
+  /// a bug to call this while there is a waiting thread, so in practice
+  /// the waiter will be the one that resets the baton.
+  void reset() {
+    // See ~Baton for a discussion about why relaxed is okay here
+    assert(state_.load(std::memory_order_relaxed) != WAITING);
+
+    // We use a similar argument to justify the use of a relaxed store
+    // here.  Since both wait() and post() are required to be called
+    // only once per lifetime, no thread can actually call those methods
+    // correctly after a reset() unless it synchronizes with the thread
+    // that performed the reset().  If a post() or wait() on another thread
+    // didn't synchronize, then regardless of what operation we performed
+    // here there would be a race on proper use of the Baton's spec
+    // (although not on any particular load and store).  Put another way,
+    // we don't need to synchronize here because anybody that might rely
+    // on such synchronization is required by the baton rules to perform
+    // an additional synchronization that has the desired effect anyway.
+    //
+    // There is actually a similar argument to be made about the
+    // constructor, in which the fenceless constructor initialization
+    // of state_ is piggybacked on whatever synchronization mechanism
+    // distributes knowledge of the Baton's existence
+    state_.store(INIT, std::memory_order_relaxed);
+  }
+
+  /// Causes wait() to wake up.  For each lifetime of a Baton (where a
+  /// lifetime starts at construction or reset() and ends at destruction
+  /// or reset()) there can be at most one call to post().  Any thread
+  /// may call post().
+  ///
+  /// Although we could implement a more generic semaphore semantics
+  /// without any extra size or CPU overhead, the single-call limitation
+  /// allows us to have better assert-ions during debug builds.
+  void post() {
+    uint32_t before = state_.load(std::memory_order_acquire);
+    assert(before == INIT || before == WAITING);
+    if (before != INIT ||
+        !state_.compare_exchange_strong(before, EARLY_DELIVERY)) {
+      // we didn't get to state_ before wait(), so we need to call futex()
+      assert(before == WAITING);
+
+      state_.store(LATE_DELIVERY, std::memory_order_release);
+      state_.futexWake(1);
+    }
+  }
+
+  /// Waits until post() has been called in the current Baton lifetime.
+  /// May be called at most once during a Baton lifetime (construction
+  /// |reset until destruction|reset).  If post is called before wait in
+  /// the current lifetime then this method returns immediately.
+  ///
+  /// The restriction that there can be at most one wait() per lifetime
+  /// could be relaxed somewhat without any perf or size regressions,
+  /// but by making this condition very restrictive we can provide better
+  /// checking in debug builds.
+  void wait() {
+    uint32_t before;
+
+    static_assert(PreBlockAttempts > 0,
+        "isn't this assert clearer than an uninitialized variable warning?");
+    for (int i = 0; i < PreBlockAttempts; ++i) {
+      before = state_.load(std::memory_order_acquire);
+      if (before == EARLY_DELIVERY) {
+        // hooray!
+        return;
+      }
+      assert(before == INIT);
+#ifdef __x86_64__
+      // The pause instruction is the polite way to spin, but it doesn't
+      // actually affect correctness to omit it if we don't have it.
+      // Pausing donates the full capabilities of the current core to
+      // its other hyperthreads for a dozen cycles or so
+      asm volatile ("pause");
+#endif
+    }
+
+    // guess we have to block :(
+    if (!state_.compare_exchange_strong(before, WAITING)) {
+      // CAS failed, last minute reprieve
+      assert(before == EARLY_DELIVERY);
+      return;
+    }
+
+    while (true) {
+      detail::MemoryIdler::futexWait(state_, WAITING);
+
+      // state_ is the truth even if FUTEX_WAIT reported a matching
+      // FUTEX_WAKE, since we aren't using type-stable storage and we
+      // don't guarantee reuse.  The scenario goes like this: thread
+      // A's last touch of a Baton is a call to wake(), which stores
+      // LATE_DELIVERY and gets an unlucky context switch before delivering
+      // the corresponding futexWake.  Thread B sees LATE_DELIVERY
+      // without consuming a futex event, because it calls futexWait
+      // with an expected value of WAITING and hence doesn't go to sleep.
+      // B returns, so the Baton's memory is reused and becomes another
+      // Baton (or a reuse of this one).  B calls futexWait on the new
+      // Baton lifetime, then A wakes up and delivers a spurious futexWake
+      // to the same memory location.  B's futexWait will then report a
+      // consumed wake event even though state_ is still WAITING.
+      //
+      // It would be possible to add an extra state_ dance to communicate
+      // that the futexWake has been sent so that we can be sure to consume
+      // it before returning, but that would be a perf and complexity hit.
+      uint32_t s = state_.load(std::memory_order_acquire);
+      assert(s == WAITING || s == LATE_DELIVERY);
+
+      if (s == LATE_DELIVERY) {
+        return;
+      }
+      // retry
+    }
+  }
+
+ private:
+  enum State : uint32_t {
+    INIT = 0,
+    EARLY_DELIVERY = 1,
+    WAITING = 2,
+    LATE_DELIVERY = 3,
+  };
+
+  enum {
+    // Must be positive.  If multiple threads are actively using a
+    // higher-level data structure that uses batons internally, it is
+    // likely that the post() and wait() calls happen almost at the same
+    // time.  In this state, we lose big 50% of the time if the wait goes
+    // to sleep immediately.  On circa-2013 devbox hardware it costs about
+    // 7 usec to FUTEX_WAIT and then be awoken (half the t/iter as the
+    // posix_sem_pingpong test in BatonTests).  We can improve our chances
+    // of EARLY_DELIVERY by spinning for a bit, although we have to balance
+    // this against the loss if we end up sleeping any way.  Spins on this
+    // hw take about 7 nanos (all but 0.5 nanos is the pause instruction).
+    // We give ourself 300 spins, which is about 2 usec of waiting.  As a
+    // partial consolation, since we are using the pause instruction we
+    // are giving a speed boost to the colocated hyperthread.
+    PreBlockAttempts = 300,
+  };
+
+  detail::Futex<Atom> state_;
+};
+
+} // namespace folly
+
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/Benchmark.cpp
@@ -0,0 +1,443 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// @author Andrei Alexandrescu (andrei.alexandrescu@fb.com)
+
+#include "Benchmark.h"
+#include "Foreach.h"
+#include "json.h"
+#include "String.h"
+
+#include <algorithm>
+#include <boost/regex.hpp>
+#include <cmath>
+#include <iostream>
+#include <limits>
+#include <utility>
+#include <vector>
+
+using namespace std;
+
+DEFINE_bool(benchmark, false, "Run benchmarks.");
+DEFINE_bool(json, false, "Output in JSON format.");
+
+DEFINE_string(bm_regex, "",
+              "Only benchmarks whose names match this regex will be run.");
+
+DEFINE_int64(bm_min_usec, 100,
+             "Minimum # of microseconds we'll accept for each benchmark.");
+
+DEFINE_int64(bm_min_iters, 1,
+             "Minimum # of iterations we'll try for each benchmark.");
+
+DEFINE_int32(bm_max_secs, 1,
+             "Maximum # of seconds we'll spend on each benchmark.");
+
+
+namespace folly {
+
+BenchmarkSuspender::NanosecondsSpent BenchmarkSuspender::nsSpent;
+
+typedef function<uint64_t(unsigned int)> BenchmarkFun;
+static vector<tuple<const char*, const char*, BenchmarkFun>> benchmarks;
+
+// Add the global baseline
+BENCHMARK(globalBenchmarkBaseline) {
+  asm volatile("");
+}
+
+void detail::addBenchmarkImpl(const char* file, const char* name,
+                              BenchmarkFun fun) {
+  benchmarks.emplace_back(file, name, std::move(fun));
+}
+
+/**
+ * Given a point, gives density at that point as a number 0.0 < x <=
+ * 1.0. The result is 1.0 if all samples are equal to where, and
+ * decreases near 0 if all points are far away from it. The density is
+ * computed with the help of a radial basis function.
+ */
+static double density(const double * begin, const double *const end,
+                      const double where, const double bandwidth) {
+  assert(begin < end);
+  assert(bandwidth > 0.0);
+  double sum = 0.0;
+  FOR_EACH_RANGE (i, begin, end) {
+    auto d = (*i - where) / bandwidth;
+    sum += exp(- d * d);
+  }
+  return sum / (end - begin);
+}
+
+/**
+ * Computes mean and variance for a bunch of data points. Note that
+ * mean is currently not being used.
+ */
+static pair<double, double>
+meanVariance(const double * begin, const double *const end) {
+  assert(begin < end);
+  double sum = 0.0, sum2 = 0.0;
+  FOR_EACH_RANGE (i, begin, end) {
+    sum += *i;
+    sum2 += *i * *i;
+  }
+  auto const n = end - begin;
+  return make_pair(sum / n, sqrt((sum2 - sum * sum / n) / n));
+}
+
+/**
+ * Computes the mode of a sample set through brute force. Assumes
+ * input is sorted.
+ */
+static double mode(const double * begin, const double *const end) {
+  assert(begin < end);
+  // Lower bound and upper bound for result and their respective
+  // densities.
+  auto
+    result = 0.0,
+    bestDensity = 0.0;
+
+  // Get the variance so we pass it down to density()
+  auto const sigma = meanVariance(begin, end).second;
+  if (!sigma) {
+    // No variance means constant signal
+    return *begin;
+  }
+
+  FOR_EACH_RANGE (i, begin, end) {
+    assert(i == begin || *i >= i[-1]);
+    auto candidate = density(begin, end, *i, sigma * sqrt(2.0));
+    if (candidate > bestDensity) {
+      // Found a new best
+      bestDensity = candidate;
+      result = *i;
+    } else {
+      // Density is decreasing... we could break here if we definitely
+      // knew this is unimodal.
+    }
+  }
+
+  return result;
+}
+
+/**
+ * Given a bunch of benchmark samples, estimate the actual run time.
+ */
+static double estimateTime(double * begin, double * end) {
+  assert(begin < end);
+
+  // Current state of the art: get the minimum. After some
+  // experimentation, it seems taking the minimum is the best.
+
+  return *min_element(begin, end);
+
+  // What follows after estimates the time as the mode of the
+  // distribution.
+
+  // Select the awesomest (i.e. most frequent) result. We do this by
+  // sorting and then computing the longest run length.
+  sort(begin, end);
+
+  // Eliminate outliers. A time much larger than the minimum time is
+  // considered an outlier.
+  while (end[-1] > 2.0 * *begin) {
+    --end;
+    if (begin == end) {
+      LOG(INFO) << *begin;
+    }
+    assert(begin < end);
+  }
+
+  double result = 0;
+
+  /* Code used just for comparison purposes */ {
+    unsigned bestFrequency = 0;
+    unsigned candidateFrequency = 1;
+    double candidateValue = *begin;
+    for (auto current = begin + 1; ; ++current) {
+      if (current == end || *current != candidateValue) {
+        // Done with the current run, see if it was best
+        if (candidateFrequency > bestFrequency) {
+          bestFrequency = candidateFrequency;
+          result = candidateValue;
+        }
+        if (current == end) {
+          break;
+        }
+        // Start a new run
+        candidateValue = *current;
+        candidateFrequency = 1;
+      } else {
+        // Cool, inside a run, increase the frequency
+        ++candidateFrequency;
+      }
+    }
+  }
+
+  result = mode(begin, end);
+
+  return result;
+}
+
+static double runBenchmarkGetNSPerIteration(const BenchmarkFun& fun,
+                                            const double globalBaseline) {
+  // They key here is accuracy; too low numbers means the accuracy was
+  // coarse. We up the ante until we get to at least minNanoseconds
+  // timings.
+  static uint64_t resolutionInNs = 0, coarseResolutionInNs = 0;
+  if (!resolutionInNs) {
+    timespec ts;
+    CHECK_EQ(0, clock_getres(detail::DEFAULT_CLOCK_ID, &ts));
+    CHECK_EQ(0, ts.tv_sec) << "Clock sucks.";
+    CHECK_LT(0, ts.tv_nsec) << "Clock too fast for its own good.";
+    CHECK_EQ(1, ts.tv_nsec) << "Clock too coarse, upgrade your kernel.";
+    resolutionInNs = ts.tv_nsec;
+  }
+  // We choose a minimum minimum (sic) of 100,000 nanoseconds, but if
+  // the clock resolution is worse than that, it will be larger. In
+  // essence we're aiming at making the quantization noise 0.01%.
+  static const auto minNanoseconds =
+    max(FLAGS_bm_min_usec * 1000UL,
+        min<uint64_t>(resolutionInNs * 100000, 1000000000ULL));
+
+  // We do measurements in several epochs and take the minimum, to
+  // account for jitter.
+  static const unsigned int epochs = 1000;
+  // We establish a total time budget as we don't want a measurement
+  // to take too long. This will curtail the number of actual epochs.
+  const uint64_t timeBudgetInNs = FLAGS_bm_max_secs * 1000000000;
+  timespec global;
+  CHECK_EQ(0, clock_gettime(CLOCK_REALTIME, &global));
+
+  double epochResults[epochs] = { 0 };
+  size_t actualEpochs = 0;
+
+  for (; actualEpochs < epochs; ++actualEpochs) {
+    for (unsigned int n = FLAGS_bm_min_iters; n < (1UL << 30); n *= 2) {
+      auto const nsecs = fun(n);
+      if (nsecs < minNanoseconds) {
+        continue;
+      }
+      // We got an accurate enough timing, done. But only save if
+      // smaller than the current result.
+      epochResults[actualEpochs] = max(0.0, double(nsecs) / n - globalBaseline);
+      // Done with the current epoch, we got a meaningful timing.
+      break;
+    }
+    timespec now;
+    CHECK_EQ(0, clock_gettime(CLOCK_REALTIME, &now));
+    if (detail::timespecDiff(now, global) >= timeBudgetInNs) {
+      // No more time budget available.
+      ++actualEpochs;
+      break;
+    }
+  }
+
+  // If the benchmark was basically drowned in baseline noise, it's
+  // possible it became negative.
+  return max(0.0, estimateTime(epochResults, epochResults + actualEpochs));
+}
+
+struct ScaleInfo {
+  double boundary;
+  const char* suffix;
+};
+
+static const ScaleInfo kTimeSuffixes[] {
+  { 365.25 * 24 * 3600, "years" },
+  { 24 * 3600, "days" },
+  { 3600, "hr" },
+  { 60, "min" },
+  { 1, "s" },
+  { 1E-3, "ms" },
+  { 1E-6, "us" },
+  { 1E-9, "ns" },
+  { 1E-12, "ps" },
+  { 1E-15, "fs" },
+  { 0, NULL },
+};
+
+static const ScaleInfo kMetricSuffixes[] {
+  { 1E24, "Y" },  // yotta
+  { 1E21, "Z" },  // zetta
+  { 1E18, "X" },  // "exa" written with suffix 'X' so as to not create
+                  //   confusion with scientific notation
+  { 1E15, "P" },  // peta
+  { 1E12, "T" },  // terra
+  { 1E9, "G" },   // giga
+  { 1E6, "M" },   // mega
+  { 1E3, "K" },   // kilo
+  { 1, "" },
+  { 1E-3, "m" },  // milli
+  { 1E-6, "u" },  // micro
+  { 1E-9, "n" },  // nano
+  { 1E-12, "p" }, // pico
+  { 1E-15, "f" }, // femto
+  { 1E-18, "a" }, // atto
+  { 1E-21, "z" }, // zepto
+  { 1E-24, "y" }, // yocto
+  { 0, NULL },
+};
+
+static string humanReadable(double n, unsigned int decimals,
+                            const ScaleInfo* scales) {
+  if (std::isinf(n) || std::isnan(n)) {
+    return folly::to<string>(n);
+  }
+
+  const double absValue = fabs(n);
+  const ScaleInfo* scale = scales;
+  while (absValue < scale[0].boundary && scale[1].suffix != NULL) {
+    ++scale;
+  }
+
+  const double scaledValue = n / scale->boundary;
+  return stringPrintf("%.*f%s", decimals, scaledValue, scale->suffix);
+}
+
+static string readableTime(double n, unsigned int decimals) {
+  return humanReadable(n, decimals, kTimeSuffixes);
+}
+
+static string metricReadable(double n, unsigned int decimals) {
+  return humanReadable(n, decimals, kMetricSuffixes);
+}
+
+static void printBenchmarkResultsAsTable(
+  const vector<tuple<const char*, const char*, double> >& data) {
+  // Width available
+  static const uint columns = 76;
+
+  // Compute the longest benchmark name
+  size_t longestName = 0;
+  FOR_EACH_RANGE (i, 1, benchmarks.size()) {
+    longestName = max(longestName, strlen(get<1>(benchmarks[i])));
+  }
+
+  // Print a horizontal rule
+  auto separator = [&](char pad) {
+    puts(string(columns, pad).c_str());
+  };
+
+  // Print header for a file
+  auto header = [&](const char* file) {
+    separator('=');
+    printf("%-*srelative  time/iter  iters/s\n",
+           columns - 28, file);
+    separator('=');
+  };
+
+  double baselineNsPerIter = numeric_limits<double>::max();
+  const char* lastFile = "";
+
+  for (auto& datum : data) {
+    auto file = get<0>(datum);
+    if (strcmp(file, lastFile)) {
+      // New file starting
+      header(file);
+      lastFile = file;
+    }
+
+    string s = get<1>(datum);
+    if (s == "-") {
+      separator('-');
+      continue;
+    }
+    bool useBaseline /* = void */;
+    if (s[0] == '%') {
+      s.erase(0, 1);
+      useBaseline = true;
+    } else {
+      baselineNsPerIter = get<2>(datum);
+      useBaseline = false;
+    }
+    s.resize(columns - 29, ' ');
+    auto nsPerIter = get<2>(datum);
+    auto secPerIter = nsPerIter / 1E9;
+    auto itersPerSec = 1 / secPerIter;
+    if (!useBaseline) {
+      // Print without baseline
+      printf("%*s           %9s  %7s\n",
+             static_cast<int>(s.size()), s.c_str(),
+             readableTime(secPerIter, 2).c_str(),
+             metricReadable(itersPerSec, 2).c_str());
+    } else {
+      // Print with baseline
+      auto rel = baselineNsPerIter / nsPerIter * 100.0;
+      printf("%*s %7.2f%%  %9s  %7s\n",
+             static_cast<int>(s.size()), s.c_str(),
+             rel,
+             readableTime(secPerIter, 2).c_str(),
+             metricReadable(itersPerSec, 2).c_str());
+    }
+  }
+  separator('=');
+}
+
+static void printBenchmarkResultsAsJson(
+  const vector<tuple<const char*, const char*, double> >& data) {
+  dynamic d = dynamic::object;
+  for (auto& datum: data) {
+    d[std::get<1>(datum)] = std::get<2>(datum) * 1000.;
+  }
+
+  printf("%s\n", toPrettyJson(d).c_str());
+}
+
+static void printBenchmarkResults(
+  const vector<tuple<const char*, const char*, double> >& data) {
+
+  if (FLAGS_json) {
+    printBenchmarkResultsAsJson(data);
+  } else {
+    printBenchmarkResultsAsTable(data);
+  }
+}
+
+void runBenchmarks() {
+  CHECK(!benchmarks.empty());
+
+  vector<tuple<const char*, const char*, double>> results;
+  results.reserve(benchmarks.size() - 1);
+
+  std::unique_ptr<boost::regex> bmRegex;
+  if (!FLAGS_bm_regex.empty()) {
+    bmRegex.reset(new boost::regex(FLAGS_bm_regex));
+  }
+
+  // PLEASE KEEP QUIET. MEASUREMENTS IN PROGRESS.
+
+  auto const globalBaseline = runBenchmarkGetNSPerIteration(
+    get<2>(benchmarks.front()), 0);
+  FOR_EACH_RANGE (i, 1, benchmarks.size()) {
+    double elapsed = 0.0;
+    if (strcmp(get<1>(benchmarks[i]), "-") != 0) { // skip separators
+      if (bmRegex && !boost::regex_search(get<1>(benchmarks[i]), *bmRegex)) {
+        continue;
+      }
+      elapsed = runBenchmarkGetNSPerIteration(get<2>(benchmarks[i]),
+                                              globalBaseline);
+    }
+    results.emplace_back(get<0>(benchmarks[i]),
+                         get<1>(benchmarks[i]), elapsed);
+  }
+
+  // PLEASE MAKE NOISE. MEASUREMENTS DONE.
+
+  printBenchmarkResults(results);
+}
+
+} // namespace folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/Benchmark.h
@@ -0,0 +1,411 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_BENCHMARK_H_
+#define FOLLY_BENCHMARK_H_
+
+#include "folly/Portability.h"
+#include "folly/Preprocessor.h" // for FB_ANONYMOUS_VARIABLE
+#include <cassert>
+#include <ctime>
+#include <boost/function_types/function_arity.hpp>
+#include <functional>
+#include <glog/logging.h>
+#include <gflags/gflags.h>
+#include <limits>
+
+DECLARE_bool(benchmark);
+
+namespace folly {
+
+/**
+ * Runs all benchmarks defined. Usually put in main().
+ */
+void runBenchmarks();
+
+/**
+ * Runs all benchmarks defined if and only if the --benchmark flag has
+ * been passed to the program. Usually put in main().
+ */
+inline bool runBenchmarksOnFlag() {
+  if (FLAGS_benchmark) {
+    runBenchmarks();
+  }
+  return FLAGS_benchmark;
+}
+
+namespace detail {
+
+/**
+ * This is the clock ID used for measuring time. On older kernels, the
+ * resolution of this clock will be very coarse, which will cause the
+ * benchmarks to fail.
+ */
+enum Clock { DEFAULT_CLOCK_ID = CLOCK_REALTIME };
+
+/**
+ * Adds a benchmark wrapped in a std::function. Only used
+ * internally. Pass by value is intentional.
+ */
+void addBenchmarkImpl(const char* file,
+                      const char* name,
+                      std::function<uint64_t(unsigned int)>);
+
+/**
+ * Takes the difference between two timespec values. end is assumed to
+ * occur after start.
+ */
+inline uint64_t timespecDiff(timespec end, timespec start) {
+  if (end.tv_sec == start.tv_sec) {
+    assert(end.tv_nsec >= start.tv_nsec);
+    return end.tv_nsec - start.tv_nsec;
+  }
+  assert(end.tv_sec > start.tv_sec &&
+         end.tv_sec - start.tv_sec <
+         std::numeric_limits<uint64_t>::max() / 1000000000UL);
+  return (end.tv_sec - start.tv_sec) * 1000000000UL
+    + end.tv_nsec - start.tv_nsec;
+}
+
+/**
+ * Takes the difference between two sets of timespec values. The first
+ * two come from a high-resolution clock whereas the other two come
+ * from a low-resolution clock. The crux of the matter is that
+ * high-res values may be bogus as documented in
+ * http://linux.die.net/man/3/clock_gettime. The trouble is when the
+ * running process migrates from one CPU to another, which is more
+ * likely for long-running processes. Therefore we watch for high
+ * differences between the two timings.
+ *
+ * This function is subject to further improvements.
+ */
+inline uint64_t timespecDiff(timespec end, timespec start,
+                             timespec endCoarse, timespec startCoarse) {
+  auto fine = timespecDiff(end, start);
+  auto coarse = timespecDiff(endCoarse, startCoarse);
+  if (coarse - fine >= 1000000) {
+    // The fine time is in all likelihood bogus
+    return coarse;
+  }
+  return fine;
+}
+
+} // namespace detail
+
+/**
+ * Supporting type for BENCHMARK_SUSPEND defined below.
+ */
+struct BenchmarkSuspender {
+  BenchmarkSuspender() {
+    CHECK_EQ(0, clock_gettime(detail::DEFAULT_CLOCK_ID, &start));
+  }
+
+  BenchmarkSuspender(const BenchmarkSuspender &) = delete;
+  BenchmarkSuspender(BenchmarkSuspender && rhs) {
+    start = rhs.start;
+    rhs.start.tv_nsec = rhs.start.tv_sec = 0;
+  }
+
+  BenchmarkSuspender& operator=(const BenchmarkSuspender &) = delete;
+  BenchmarkSuspender& operator=(BenchmarkSuspender && rhs) {
+    if (start.tv_nsec > 0 || start.tv_sec > 0) {
+      tally();
+    }
+    start = rhs.start;
+    rhs.start.tv_nsec = rhs.start.tv_sec = 0;
+    return *this;
+  }
+
+  ~BenchmarkSuspender() {
+    if (start.tv_nsec > 0 || start.tv_sec > 0) {
+      tally();
+    }
+  }
+
+  void dismiss() {
+    assert(start.tv_nsec > 0 || start.tv_sec > 0);
+    tally();
+    start.tv_nsec = start.tv_sec = 0;
+  }
+
+  void rehire() {
+    assert(start.tv_nsec == 0 || start.tv_sec == 0);
+    CHECK_EQ(0, clock_gettime(detail::DEFAULT_CLOCK_ID, &start));
+  }
+
+  /**
+   * This helps the macro definition. To get around the dangers of
+   * operator bool, returns a pointer to member (which allows no
+   * arithmetic).
+   */
+  operator int BenchmarkSuspender::*() const {
+    return nullptr;
+  }
+
+  /**
+   * Accumulates nanoseconds spent outside benchmark.
+   */
+  typedef uint64_t NanosecondsSpent;
+  static NanosecondsSpent nsSpent;
+
+private:
+  void tally() {
+    timespec end;
+    CHECK_EQ(0, clock_gettime(detail::DEFAULT_CLOCK_ID, &end));
+    nsSpent += detail::timespecDiff(end, start);
+    start = end;
+  }
+
+  timespec start;
+};
+
+/**
+ * Adds a benchmark. Usually not called directly but instead through
+ * the macro BENCHMARK defined below. The lambda function involved
+ * must take exactly one parameter of type unsigned, and the benchmark
+ * uses it with counter semantics (iteration occurs inside the
+ * function).
+ */
+template <typename Lambda>
+typename std::enable_if<
+  boost::function_types::function_arity<decltype(&Lambda::operator())>::value
+  == 2
+>::type
+addBenchmark(const char* file, const char* name, Lambda&& lambda) {
+  auto execute = [=](unsigned int times) -> uint64_t {
+    BenchmarkSuspender::nsSpent = 0;
+    timespec start, end;
+
+    // CORE MEASUREMENT STARTS
+    auto const r1 = clock_gettime(detail::DEFAULT_CLOCK_ID, &start);
+    lambda(times);
+    auto const r2 = clock_gettime(detail::DEFAULT_CLOCK_ID, &end);
+    // CORE MEASUREMENT ENDS
+
+    CHECK_EQ(0, r1);
+    CHECK_EQ(0, r2);
+
+    return detail::timespecDiff(end, start) - BenchmarkSuspender::nsSpent;
+  };
+
+  detail::addBenchmarkImpl(file, name,
+                           std::function<uint64_t(unsigned int)>(execute));
+}
+
+/**
+ * Adds a benchmark. Usually not called directly but instead through
+ * the macro BENCHMARK defined below. The lambda function involved
+ * must take zero parameters, and the benchmark calls it repeatedly
+ * (iteration occurs outside the function).
+ */
+template <typename Lambda>
+typename std::enable_if<
+  boost::function_types::function_arity<decltype(&Lambda::operator())>::value
+  == 1
+>::type
+addBenchmark(const char* file, const char* name, Lambda&& lambda) {
+  addBenchmark(file, name, [=](unsigned int times) {
+      while (times-- > 0) {
+        lambda();
+      }
+    });
+}
+
+/**
+ * Call doNotOptimizeAway(var) against variables that you use for
+ * benchmarking but otherwise are useless. The compiler tends to do a
+ * good job at eliminating unused variables, and this function fools
+ * it into thinking var is in fact needed.
+ */
+template <class T>
+void doNotOptimizeAway(T&& datum) {
+  asm volatile("" : "+r" (datum));
+}
+
+} // namespace folly
+
+/**
+ * Introduces a benchmark function. Used internally, see BENCHMARK and
+ * friends below.
+ */
+#define BENCHMARK_IMPL(funName, stringName, paramType, paramName)       \
+  static void funName(paramType);                                       \
+  static bool FB_ANONYMOUS_VARIABLE(follyBenchmarkUnused) = (           \
+    ::folly::addBenchmark(__FILE__, stringName,                         \
+      [](paramType paramName) { funName(paramName); }),                 \
+    true);                                                              \
+  static void funName(paramType paramName)
+
+/**
+ * Introduces a benchmark function. Use with either one one or two
+ * arguments. The first is the name of the benchmark. Use something
+ * descriptive, such as insertVectorBegin. The second argument may be
+ * missing, or could be a symbolic counter. The counter dictates how
+ * many internal iteration the benchmark does. Example:
+ *
+ * BENCHMARK(vectorPushBack) {
+ *   vector<int> v;
+ *   v.push_back(42);
+ * }
+ *
+ * BENCHMARK(insertVectorBegin, n) {
+ *   vector<int> v;
+ *   FOR_EACH_RANGE (i, 0, n) {
+ *     v.insert(v.begin(), 42);
+ *   }
+ * }
+ */
+#define BENCHMARK(name, ...)                                    \
+  BENCHMARK_IMPL(                                               \
+    name,                                                       \
+    FB_STRINGIZE(name),                                         \
+    FB_ONE_OR_NONE(unsigned, ## __VA_ARGS__),                   \
+    __VA_ARGS__)
+
+/**
+ * Defines a benchmark that passes a parameter to another one. This is
+ * common for benchmarks that need a "problem size" in addition to
+ * "number of iterations". Consider:
+ *
+ * void pushBack(uint n, size_t initialSize) {
+ *   vector<int> v;
+ *   BENCHMARK_SUSPEND {
+ *     v.resize(initialSize);
+ *   }
+ *   FOR_EACH_RANGE (i, 0, n) {
+ *    v.push_back(i);
+ *   }
+ * }
+ * BENCHMARK_PARAM(pushBack, 0)
+ * BENCHMARK_PARAM(pushBack, 1000)
+ * BENCHMARK_PARAM(pushBack, 1000000)
+ *
+ * The benchmark above estimates the speed of push_back at different
+ * initial sizes of the vector. The framework will pass 0, 1000, and
+ * 1000000 for initialSize, and the iteration count for n.
+ */
+#define BENCHMARK_PARAM(name, param)                                    \
+  BENCHMARK_NAMED_PARAM(name, param, param)
+
+/*
+ * Like BENCHMARK_PARAM(), but allows a custom name to be specified for each
+ * parameter, rather than using the parameter value.
+ *
+ * Useful when the parameter value is not a valid token for string pasting,
+ * of when you want to specify multiple parameter arguments.
+ *
+ * For example:
+ *
+ * void addValue(uint n, int64_t bucketSize, int64_t min, int64_t max) {
+ *   Histogram<int64_t> hist(bucketSize, min, max);
+ *   int64_t num = min;
+ *   FOR_EACH_RANGE (i, 0, n) {
+ *     hist.addValue(num);
+ *     ++num;
+ *     if (num > max) { num = min; }
+ *   }
+ * }
+ *
+ * BENCHMARK_NAMED_PARAM(addValue, 0_to_100, 1, 0, 100)
+ * BENCHMARK_NAMED_PARAM(addValue, 0_to_1000, 10, 0, 1000)
+ * BENCHMARK_NAMED_PARAM(addValue, 5k_to_20k, 250, 5000, 20000)
+ */
+#define BENCHMARK_NAMED_PARAM(name, param_name, ...)                    \
+  BENCHMARK_IMPL(                                                       \
+      FB_CONCATENATE(name, FB_CONCATENATE(_, param_name)),              \
+      FB_STRINGIZE(name) "(" FB_STRINGIZE(param_name) ")",              \
+      unsigned,                                                         \
+      iters) {                                                          \
+    name(iters, ## __VA_ARGS__);                                        \
+  }
+
+/**
+ * Just like BENCHMARK, but prints the time relative to a
+ * baseline. The baseline is the most recent BENCHMARK() seen in
+ * lexical order. Example:
+ *
+ * // This is the baseline
+ * BENCHMARK(insertVectorBegin, n) {
+ *   vector<int> v;
+ *   FOR_EACH_RANGE (i, 0, n) {
+ *     v.insert(v.begin(), 42);
+ *   }
+ * }
+ *
+ * BENCHMARK_RELATIVE(insertListBegin, n) {
+ *   list<int> s;
+ *   FOR_EACH_RANGE (i, 0, n) {
+ *     s.insert(s.begin(), 42);
+ *   }
+ * }
+ *
+ * Any number of relative benchmark can be associated with a
+ * baseline. Another BENCHMARK() occurrence effectively establishes a
+ * new baseline.
+ */
+#define BENCHMARK_RELATIVE(name, ...)                           \
+  BENCHMARK_IMPL(                                               \
+    name,                                                       \
+    "%" FB_STRINGIZE(name),                                     \
+    FB_ONE_OR_NONE(unsigned, ## __VA_ARGS__),                   \
+    __VA_ARGS__)
+
+/**
+ * A combination of BENCHMARK_RELATIVE and BENCHMARK_PARAM.
+ */
+#define BENCHMARK_RELATIVE_PARAM(name, param)                           \
+  BENCHMARK_RELATIVE_NAMED_PARAM(name, param, param)
+
+/**
+ * A combination of BENCHMARK_RELATIVE and BENCHMARK_NAMED_PARAM.
+ */
+#define BENCHMARK_RELATIVE_NAMED_PARAM(name, param_name, ...)           \
+  BENCHMARK_IMPL(                                                       \
+      FB_CONCATENATE(name, FB_CONCATENATE(_, param_name)),              \
+      "%" FB_STRINGIZE(name) "(" FB_STRINGIZE(param_name) ")",          \
+      unsigned,                                                         \
+      iters) {                                                          \
+    name(iters, ## __VA_ARGS__);                                        \
+  }
+
+/**
+ * Draws a line of dashes.
+ */
+#define BENCHMARK_DRAW_LINE()                                   \
+  static bool FB_ANONYMOUS_VARIABLE(follyBenchmarkUnused) = (   \
+    ::folly::addBenchmark(__FILE__, "-", []() { }),             \
+    true);
+
+/**
+ * Allows execution of code that doesn't count torward the benchmark's
+ * time budget. Example:
+ *
+ * BENCHMARK_START_GROUP(insertVectorBegin, n) {
+ *   vector<int> v;
+ *   BENCHMARK_SUSPEND {
+ *     v.reserve(n);
+ *   }
+ *   FOR_EACH_RANGE (i, 0, n) {
+ *     v.insert(v.begin(), 42);
+ *   }
+ * }
+ */
+#define BENCHMARK_SUSPEND                               \
+  if (auto FB_ANONYMOUS_VARIABLE(BENCHMARK_SUSPEND) =   \
+      ::folly::BenchmarkSuspender()) {}                 \
+  else
+
+#endif // FOLLY_BENCHMARK_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/Bits.cpp
@@ -0,0 +1,94 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Bits.h"
+
+#include "folly/CpuId.h"
+#include "folly/Portability.h"
+
+// None of this is necessary if we're compiling for a target that supports
+// popcnt
+#ifndef __POPCNT__
+
+namespace {
+
+int popcount_builtin(unsigned int x) {
+  return __builtin_popcount(x);
+}
+
+int popcountll_builtin(unsigned long long x) {
+  return __builtin_popcountll(x);
+}
+
+#if FOLLY_HAVE_IFUNC && !defined(FOLLY_SANITIZE_ADDRESS)
+
+// Strictly speaking, these versions of popcount are usable without ifunc
+// support. However, we would have to check, via CpuId, if the processor
+// implements the popcnt instruction first, which is what we use ifunc for.
+int popcount_inst(unsigned int x) {
+  int n;
+  asm ("popcntl %1, %0" : "=r" (n) : "r" (x));
+  return n;
+}
+
+int popcountll_inst(unsigned long long x) {
+  unsigned long long n;
+  asm ("popcntq %1, %0" : "=r" (n) : "r" (x));
+  return n;
+}
+
+typedef decltype(popcount_builtin) Type_popcount;
+typedef decltype(popcountll_builtin) Type_popcountll;
+
+// This function is called on startup to resolve folly::detail::popcount
+extern "C" Type_popcount* folly_popcount_ifunc() {
+  return folly::CpuId().popcnt() ?  popcount_inst : popcount_builtin;
+}
+
+// This function is called on startup to resolve folly::detail::popcountll
+extern "C" Type_popcountll* folly_popcountll_ifunc() {
+  return folly::CpuId().popcnt() ?  popcountll_inst : popcountll_builtin;
+}
+
+#endif  // FOLLY_HAVE_IFUNC && !defined(FOLLY_SANITIZE_ADDRESS)
+
+}  // namespace
+
+namespace folly {
+namespace detail {
+
+// Call folly_popcount_ifunc on startup to resolve to either popcount_inst
+// or popcount_builtin
+int popcount(unsigned int x)
+#if FOLLY_HAVE_IFUNC && !defined(FOLLY_SANITIZE_ADDRESS)
+  __attribute__((ifunc("folly_popcount_ifunc")));
+#else
+{  return popcount_builtin(x); }
+#endif
+
+// Call folly_popcount_ifunc on startup to resolve to either popcountll_inst
+// or popcountll_builtin
+int popcountll(unsigned long long x)
+#if FOLLY_HAVE_IFUNC && !defined(FOLLY_SANITIZE_ADDRESS)
+  __attribute__((ifunc("folly_popcountll_ifunc")));
+#else
+{  return popcountll_builtin(x); }
+#endif
+
+}  // namespace detail
+}  // namespace folly
+
+#endif  /* !__POPCNT__ */
--- /dev/null
+++ b/hphp/submodules/folly/folly/Bits.h
@@ -0,0 +1,558 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Various low-level, bit-manipulation routines.
+ *
+ * findFirstSet(x)  [constexpr]
+ *    find first (least significant) bit set in a value of an integral type,
+ *    1-based (like ffs()).  0 = no bits are set (x == 0)
+ *
+ * findLastSet(x)  [constexpr]
+ *    find last (most significant) bit set in a value of an integral type,
+ *    1-based.  0 = no bits are set (x == 0)
+ *    for x != 0, findLastSet(x) == 1 + floor(log2(x))
+ *
+ * nextPowTwo(x)  [constexpr]
+ *    Finds the next power of two >= x.
+ *
+ * isPowTwo(x)  [constexpr]
+ *    return true iff x is a power of two
+ *
+ * popcount(x)
+ *    return the number of 1 bits in x
+ *
+ * Endian
+ *    convert between native, big, and little endian representation
+ *    Endian::big(x)      big <-> native
+ *    Endian::little(x)   little <-> native
+ *    Endian::swap(x)     big <-> little
+ *
+ * BitIterator
+ *    Wrapper around an iterator over an integral type that iterates
+ *    over its underlying bits in MSb to LSb order
+ *
+ * findFirstSet(BitIterator begin, BitIterator end)
+ *    return a BitIterator pointing to the first 1 bit in [begin, end), or
+ *    end if all bits in [begin, end) are 0
+ *
+ * @author Tudor Bosman (tudorb@fb.com)
+ */
+
+#ifndef FOLLY_BITS_H_
+#define FOLLY_BITS_H_
+
+#include "folly/Portability.h"
+
+#ifndef __GNUC__
+#error GCC required
+#endif
+
+#ifndef __clang__
+#define FOLLY_INTRINSIC_CONSTEXPR constexpr
+#else
+// Unlike GCC, in Clang (as of 3.2) intrinsics aren't constexpr.
+#define FOLLY_INTRINSIC_CONSTEXPR const
+#endif
+
+#ifndef FOLLY_NO_CONFIG
+#include "folly/folly-config.h"
+#endif
+
+#include "folly/detail/BitsDetail.h"
+#include "folly/detail/BitIteratorDetail.h"
+#include "folly/Likely.h"
+
+#if FOLLY_HAVE_BYTESWAP_H
+# include <byteswap.h>
+#endif
+
+#include <cassert>
+#include <cinttypes>
+#include <iterator>
+#include <limits>
+#include <type_traits>
+#include <boost/iterator/iterator_adaptor.hpp>
+#include <stdint.h>
+
+namespace folly {
+
+// Generate overloads for findFirstSet as wrappers around
+// appropriate ffs, ffsl, ffsll gcc builtins
+template <class T>
+inline FOLLY_INTRINSIC_CONSTEXPR
+typename std::enable_if<
+  (std::is_integral<T>::value &&
+   std::is_unsigned<T>::value &&
+   sizeof(T) <= sizeof(unsigned int)),
+  unsigned int>::type
+  findFirstSet(T x) {
+  return __builtin_ffs(x);
+}
+
+template <class T>
+inline FOLLY_INTRINSIC_CONSTEXPR
+typename std::enable_if<
+  (std::is_integral<T>::value &&
+   std::is_unsigned<T>::value &&
+   sizeof(T) > sizeof(unsigned int) &&
+   sizeof(T) <= sizeof(unsigned long)),
+  unsigned int>::type
+  findFirstSet(T x) {
+  return __builtin_ffsl(x);
+}
+
+template <class T>
+inline FOLLY_INTRINSIC_CONSTEXPR
+typename std::enable_if<
+  (std::is_integral<T>::value &&
+   std::is_unsigned<T>::value &&
+   sizeof(T) > sizeof(unsigned long) &&
+   sizeof(T) <= sizeof(unsigned long long)),
+  unsigned int>::type
+  findFirstSet(T x) {
+  return __builtin_ffsll(x);
+}
+
+template <class T>
+inline FOLLY_INTRINSIC_CONSTEXPR
+typename std::enable_if<
+  (std::is_integral<T>::value && std::is_signed<T>::value),
+  unsigned int>::type
+  findFirstSet(T x) {
+  // Note that conversion from a signed type to the corresponding unsigned
+  // type is technically implementation-defined, but will likely work
+  // on any impementation that uses two's complement.
+  return findFirstSet(static_cast<typename std::make_unsigned<T>::type>(x));
+}
+
+// findLastSet: return the 1-based index of the highest bit set
+// for x > 0, findLastSet(x) == 1 + floor(log2(x))
+template <class T>
+inline FOLLY_INTRINSIC_CONSTEXPR
+typename std::enable_if<
+  (std::is_integral<T>::value &&
+   std::is_unsigned<T>::value &&
+   sizeof(T) <= sizeof(unsigned int)),
+  unsigned int>::type
+  findLastSet(T x) {
+  return x ? 8 * sizeof(unsigned int) - __builtin_clz(x) : 0;
+}
+
+template <class T>
+inline FOLLY_INTRINSIC_CONSTEXPR
+typename std::enable_if<
+  (std::is_integral<T>::value &&
+   std::is_unsigned<T>::value &&
+   sizeof(T) > sizeof(unsigned int) &&
+   sizeof(T) <= sizeof(unsigned long)),
+  unsigned int>::type
+  findLastSet(T x) {
+  return x ? 8 * sizeof(unsigned long) - __builtin_clzl(x) : 0;
+}
+
+template <class T>
+inline FOLLY_INTRINSIC_CONSTEXPR
+typename std::enable_if<
+  (std::is_integral<T>::value &&
+   std::is_unsigned<T>::value &&
+   sizeof(T) > sizeof(unsigned long) &&
+   sizeof(T) <= sizeof(unsigned long long)),
+  unsigned int>::type
+  findLastSet(T x) {
+  return x ? 8 * sizeof(unsigned long long) - __builtin_clzll(x) : 0;
+}
+
+template <class T>
+inline FOLLY_INTRINSIC_CONSTEXPR
+typename std::enable_if<
+  (std::is_integral<T>::value &&
+   std::is_signed<T>::value),
+  unsigned int>::type
+  findLastSet(T x) {
+  return findLastSet(static_cast<typename std::make_unsigned<T>::type>(x));
+}
+
+template <class T>
+inline FOLLY_INTRINSIC_CONSTEXPR
+typename std::enable_if<
+  std::is_integral<T>::value && std::is_unsigned<T>::value,
+  T>::type
+nextPowTwo(T v) {
+  return v ? (1ul << findLastSet(v - 1)) : 1;
+}
+
+template <class T>
+inline constexpr
+typename std::enable_if<
+  std::is_integral<T>::value && std::is_unsigned<T>::value,
+  bool>::type
+isPowTwo(T v) {
+  return (v != 0) && !(v & (v - 1));
+}
+
+/**
+ * Population count
+ */
+template <class T>
+inline typename std::enable_if<
+  (std::is_integral<T>::value &&
+   std::is_unsigned<T>::value &&
+   sizeof(T) <= sizeof(unsigned int)),
+  size_t>::type
+  popcount(T x) {
+  return detail::popcount(x);
+}
+
+template <class T>
+inline typename std::enable_if<
+  (std::is_integral<T>::value &&
+   std::is_unsigned<T>::value &&
+   sizeof(T) > sizeof(unsigned int) &&
+   sizeof(T) <= sizeof(unsigned long long)),
+  size_t>::type
+  popcount(T x) {
+  return detail::popcountll(x);
+}
+
+/**
+ * Endianness detection and manipulation primitives.
+ */
+namespace detail {
+
+template <class T>
+struct EndianIntBase {
+ public:
+  static T swap(T x);
+};
+
+/**
+ * If we have the bswap_16 macro from byteswap.h, use it; otherwise, provide our
+ * own definition.
+ */
+#ifdef bswap_16
+# define our_bswap16 bswap_16
+#else
+
+template<class Int16>
+inline constexpr typename std::enable_if<
+  sizeof(Int16) == 2,
+  Int16>::type
+our_bswap16(Int16 x) {
+  return ((x >> 8) & 0xff) | ((x & 0xff) << 8);
+}
+#endif
+
+#define FB_GEN(t, fn) \
+template<> inline t EndianIntBase<t>::swap(t x) { return fn(x); }
+
+// fn(x) expands to (x) if the second argument is empty, which is exactly
+// what we want for [u]int8_t. Also, gcc 4.7 on Intel doesn't have
+// __builtin_bswap16 for some reason, so we have to provide our own.
+FB_GEN( int8_t,)
+FB_GEN(uint8_t,)
+FB_GEN( int64_t, __builtin_bswap64)
+FB_GEN(uint64_t, __builtin_bswap64)
+FB_GEN( int32_t, __builtin_bswap32)
+FB_GEN(uint32_t, __builtin_bswap32)
+FB_GEN( int16_t, our_bswap16)
+FB_GEN(uint16_t, our_bswap16)
+
+#undef FB_GEN
+
+#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__
+
+template <class T>
+struct EndianInt : public detail::EndianIntBase<T> {
+ public:
+  static T big(T x) { return EndianInt::swap(x); }
+  static T little(T x) { return x; }
+};
+
+#elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__
+
+template <class T>
+struct EndianInt : public detail::EndianIntBase<T> {
+ public:
+  static T big(T x) { return x; }
+  static T little(T x) { return EndianInt::swap(x); }
+};
+
+#else
+# error Your machine uses a weird endianness!
+#endif  /* __BYTE_ORDER__ */
+
+}  // namespace detail
+
+// big* convert between native and big-endian representations
+// little* convert between native and little-endian representations
+// swap* convert between big-endian and little-endian representations
+//
+// ntohs, htons == big16
+// ntohl, htonl == big32
+#define FB_GEN1(fn, t, sz) \
+  static t fn##sz(t x) { return fn<t>(x); } \
+
+#define FB_GEN2(t, sz) \
+  FB_GEN1(swap, t, sz) \
+  FB_GEN1(big, t, sz) \
+  FB_GEN1(little, t, sz)
+
+#define FB_GEN(sz) \
+  FB_GEN2(uint##sz##_t, sz) \
+  FB_GEN2(int##sz##_t, sz)
+
+class Endian {
+ public:
+  enum class Order : uint8_t {
+    LITTLE,
+    BIG
+  };
+
+  static constexpr Order order =
+#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__
+    Order::LITTLE;
+#elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__
+    Order::BIG;
+#else
+# error Your machine uses a weird endianness!
+#endif  /* __BYTE_ORDER__ */
+
+  template <class T> static T swap(T x) {
+    return detail::EndianInt<T>::swap(x);
+  }
+  template <class T> static T big(T x) {
+    return detail::EndianInt<T>::big(x);
+  }
+  template <class T> static T little(T x) {
+    return detail::EndianInt<T>::little(x);
+  }
+
+  FB_GEN(64)
+  FB_GEN(32)
+  FB_GEN(16)
+  FB_GEN(8)
+};
+
+#undef FB_GEN
+#undef FB_GEN2
+#undef FB_GEN1
+
+/**
+ * Fast bit iteration facility.
+ */
+
+
+template <class BaseIter> class BitIterator;
+template <class BaseIter>
+BitIterator<BaseIter> findFirstSet(BitIterator<BaseIter>,
+                                   BitIterator<BaseIter>);
+/**
+ * Wrapper around an iterator over an integer type that iterates
+ * over its underlying bits in LSb to MSb order.
+ *
+ * BitIterator models the same iterator concepts as the base iterator.
+ */
+template <class BaseIter>
+class BitIterator
+  : public bititerator_detail::BitIteratorBase<BaseIter>::type {
+ public:
+  /**
+   * Return the number of bits in an element of the underlying iterator.
+   */
+  static size_t bitsPerBlock() {
+    return std::numeric_limits<
+      typename std::make_unsigned<
+        typename std::iterator_traits<BaseIter>::value_type
+      >::type
+    >::digits;
+  }
+
+  /**
+   * Construct a BitIterator that points at a given bit offset (default 0)
+   * in iter.
+   */
+  #pragma GCC diagnostic push // bitOffset shadows a member
+  #pragma GCC diagnostic ignored "-Wshadow"
+  explicit BitIterator(const BaseIter& iter, size_t bitOffset=0)
+    : bititerator_detail::BitIteratorBase<BaseIter>::type(iter),
+      bitOffset_(bitOffset) {
+    assert(bitOffset_ < bitsPerBlock());
+  }
+  #pragma GCC diagnostic pop
+
+  size_t bitOffset() const {
+    return bitOffset_;
+  }
+
+  void advanceToNextBlock() {
+    bitOffset_ = 0;
+    ++this->base_reference();
+  }
+
+  BitIterator& operator=(const BaseIter& other) {
+    this->~BitIterator();
+    new (this) BitIterator(other);
+    return *this;
+  }
+
+ private:
+  friend class boost::iterator_core_access;
+  friend BitIterator findFirstSet<>(BitIterator, BitIterator);
+
+  typedef bititerator_detail::BitReference<
+      typename std::iterator_traits<BaseIter>::reference,
+      typename std::iterator_traits<BaseIter>::value_type
+    > BitRef;
+
+  void advanceInBlock(size_t n) {
+    bitOffset_ += n;
+    assert(bitOffset_ < bitsPerBlock());
+  }
+
+  BitRef dereference() const {
+    return BitRef(*this->base_reference(), bitOffset_);
+  }
+
+  void advance(ssize_t n) {
+    size_t bpb = bitsPerBlock();
+    ssize_t blocks = n / bpb;
+    bitOffset_ += n % bpb;
+    if (bitOffset_ >= bpb) {
+      bitOffset_ -= bpb;
+      ++blocks;
+    }
+    this->base_reference() += blocks;
+  }
+
+  void increment() {
+    if (++bitOffset_ == bitsPerBlock()) {
+      advanceToNextBlock();
+    }
+  }
+
+  void decrement() {
+    if (bitOffset_-- == 0) {
+      bitOffset_ = bitsPerBlock() - 1;
+      --this->base_reference();
+    }
+  }
+
+  bool equal(const BitIterator& other) const {
+    return (bitOffset_ == other.bitOffset_ &&
+            this->base_reference() == other.base_reference());
+  }
+
+  ssize_t distance_to(const BitIterator& other) const {
+    return
+      (other.base_reference() - this->base_reference()) * bitsPerBlock() +
+      (other.bitOffset_ - bitOffset_);
+  }
+
+  ssize_t bitOffset_;
+};
+
+/**
+ * Helper function, so you can write
+ * auto bi = makeBitIterator(container.begin());
+ */
+template <class BaseIter>
+BitIterator<BaseIter> makeBitIterator(const BaseIter& iter) {
+  return BitIterator<BaseIter>(iter);
+}
+
+
+/**
+ * Find first bit set in a range of bit iterators.
+ * 4.5x faster than the obvious std::find(begin, end, true);
+ */
+template <class BaseIter>
+BitIterator<BaseIter> findFirstSet(BitIterator<BaseIter> begin,
+                                   BitIterator<BaseIter> end) {
+  // shortcut to avoid ugly static_cast<>
+  static const typename BaseIter::value_type one = 1;
+
+  while (begin.base() != end.base()) {
+    typename BaseIter::value_type v = *begin.base();
+    // mask out the bits that don't matter (< begin.bitOffset)
+    v &= ~((one << begin.bitOffset()) - 1);
+    size_t firstSet = findFirstSet(v);
+    if (firstSet) {
+      --firstSet;  // now it's 0-based
+      assert(firstSet >= begin.bitOffset());
+      begin.advanceInBlock(firstSet - begin.bitOffset());
+      return begin;
+    }
+    begin.advanceToNextBlock();
+  }
+
+  // now begin points to the same block as end
+  if (end.bitOffset() != 0) {  // assume end is dereferenceable
+    typename BaseIter::value_type v = *begin.base();
+    // mask out the bits that don't matter (< begin.bitOffset)
+    v &= ~((one << begin.bitOffset()) - 1);
+    // mask out the bits that don't matter (>= end.bitOffset)
+    v &= (one << end.bitOffset()) - 1;
+    size_t firstSet = findFirstSet(v);
+    if (firstSet) {
+      --firstSet;  // now it's 0-based
+      assert(firstSet >= begin.bitOffset());
+      begin.advanceInBlock(firstSet - begin.bitOffset());
+      return begin;
+    }
+  }
+
+  return end;
+}
+
+
+template <class T, class Enable=void> struct Unaligned;
+
+/**
+ * Representation of an unaligned value of a POD type.
+ */
+template <class T>
+struct Unaligned<
+    T,
+    typename std::enable_if<std::is_pod<T>::value>::type> {
+  Unaligned() = default;  // uninitialized
+  /* implicit */ Unaligned(T v) : value(v) { }
+  T value;
+} __attribute__((packed));
+
+/**
+ * Read an unaligned value of type T and return it.
+ */
+template <class T>
+inline T loadUnaligned(const void* p) {
+  static_assert(sizeof(Unaligned<T>) == sizeof(T), "Invalid unaligned size");
+  static_assert(alignof(Unaligned<T>) == 1, "Invalid alignment");
+  return static_cast<const Unaligned<T>*>(p)->value;
+}
+
+/**
+ * Write an unaligned value of type T.
+ */
+template <class T>
+inline void storeUnaligned(void* p, T value) {
+  static_assert(sizeof(Unaligned<T>) == sizeof(T), "Invalid unaligned size");
+  static_assert(alignof(Unaligned<T>) == 1, "Invalid alignment");
+  new (p) Unaligned<T>(value);
+}
+
+}  // namespace folly
+
+#endif /* FOLLY_BITS_H_ */
--- /dev/null
+++ b/hphp/submodules/folly/folly/build/generate_escape_tables.py
@@ -0,0 +1,113 @@
+#!/usr/bin/env python
+#
+# Generate Escape tables.
+# Copyright 2011 Facebook
+#
+# @author Tudor Bosman (tudorb@fb.com)
+#
+import os
+from optparse import OptionParser
+
+OUTPUT_FILE = "EscapeTables.cpp"
+
+def generate(f):
+    f.write("namespace folly {\n"
+            "namespace detail {\n"
+            "\n")
+
+    f.write("extern const char cEscapeTable[] =\n")
+    escapes = dict((
+        ('"', '\\"'),
+        ('\\', '\\\\'),
+        ('?', '?'),
+        ('\n', 'n'),
+        ('\r', 'r'),
+        ('\t', 't'),
+    ))
+    for i in range(0, 256):
+        if i % 64 == 0:
+            if i != 0:
+                f.write("\"\n")
+            f.write("  \"")
+        c = chr(i)
+        if c in escapes:
+            c = escapes[c]
+        elif i < 32 or i > 126:
+            c = 'O'  # octal
+        else:
+            c = 'P'  # printable
+        f.write(c)
+    f.write("\";\n\n")
+
+    f.write("extern const char cUnescapeTable[] =\n")
+    for i in range(0, 256):
+        if i % 64 == 0:
+            if i != 0:
+                f.write("\"\n")
+            f.write("  \"")
+        c = chr(i)
+        if c in '\'?':
+            f.write(c)
+        elif c in '"\\abfnrtv':
+            f.write("\\" + c)
+        elif i >= ord('0') and i <= ord('7'):
+            f.write("O")  # octal
+        elif c == "x":
+            f.write("X")  # hex
+        else:
+            f.write("I")  # invalid
+    f.write("\";\n\n")
+
+    f.write("extern const unsigned char hexTable[] = {")
+    for i in range(0, 256):
+        if i % 16 == 0:
+            f.write("\n  ")
+        if i >= ord('0') and i <= ord('9'):
+            f.write("{0:2d}, ".format(i - ord('0')))
+        elif i >= ord('a') and i <= ord('f'):
+            f.write("{0:2d}, ".format(i - ord('a') + 10))
+        elif i >= ord('A') and i <= ord('F'):
+            f.write("{0:2d}, ".format(i - ord('A') + 10))
+        else:
+            f.write("16, ")
+    f.write("\n};\n\n")
+
+    # 0 = passthrough
+    # 1 = unused
+    # 2 = safe in path (/)
+    # 3 = space (replace with '+' in query)
+    # 4 = always percent-encode
+    f.write("extern const unsigned char uriEscapeTable[] = {")
+    passthrough = (
+        range(ord('0'), ord('9')) +
+        range(ord('A'), ord('Z')) +
+        range(ord('a'), ord('z')) +
+        map(ord, '-_.~'))
+    for i in range(0, 256):
+        if i % 16 == 0:
+            f.write("\n  ")
+        if i in passthrough:
+            f.write("0, ")
+        elif i == ord('/'):
+            f.write("2, ")
+        elif i == ord(' '):
+            f.write("3, ")
+        else:
+            f.write("4, ")
+    f.write("\n};\n\n")
+
+    f.write("}  // namespace detail\n"
+            "}  // namespace folly\n")
+
+def main():
+    parser = OptionParser()
+    parser.add_option("--install_dir", dest="install_dir", default=".",
+                      help="write output to DIR", metavar="DIR")
+    parser.add_option("--fbcode_dir")
+    (options, args) = parser.parse_args()
+    f = open(os.path.join(options.install_dir, OUTPUT_FILE), "w")
+    generate(f)
+    f.close()
+
+if __name__ == "__main__":
+    main()
--- /dev/null
+++ b/hphp/submodules/folly/folly/build/GenerateFingerprintTables.cpp
@@ -0,0 +1,164 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef __STDC_FORMAT_MACROS
+#define __STDC_FORMAT_MACROS 1
+#endif
+
+#include <cstdio>
+#include <cinttypes>
+
+#include <string>
+
+#include <glog/logging.h>
+#include <gflags/gflags.h>
+
+#include "folly/Format.h"
+
+#include "folly/detail/FingerprintPolynomial.h"
+
+using namespace folly;
+using namespace folly::detail;
+
+// The defaults were generated by a separate program that requires the
+// NTL (Number Theory Library) from http://www.shoup.net/ntl/
+//
+// Briefly: randomly generate a polynomial of degree D, test for
+// irreducibility, repeat until you find an irreducible polynomial
+// (roughly 1/D of all polynomials of degree D are irreducible, so
+// this will succeed in D/2 tries on average; D is small (64..128) so
+// this simple method works well)
+//
+// DO NOT REPLACE THE POLYNOMIALS USED, EVER, as that would change the value
+// of every single fingerprint in existence.
+DEFINE_int64(poly64, 0xbf3736b51869e9b7,
+             "Generate 64-bit tables using this polynomial");
+DEFINE_int64(poly96_m, 0x51555cb0aa8d39c3,
+             "Generate 96-bit tables using this polynomial "
+             "(most significant 64 bits)");
+DEFINE_int32(poly96_l, 0xb679ec37,
+             "Generate 96-bit tables using this polynomial "
+             "(least significant 32 bits)");
+DEFINE_int64(poly128_m, 0xc91bff9b8768b51b,
+             "Generate 128-bit tables using this polynomial "
+             "(most significant 64 bits)");
+DEFINE_int64(poly128_l, 0x8c5d5853bd77b0d3,
+             "Generate 128-bit tables using this polynomial "
+             "(least significant 64 bits)");
+DEFINE_string(install_dir, ".",
+              "Direectory to place output files in");
+DEFINE_string(fbcode_dir, "", "fbcode directory (ignored)");
+
+namespace {
+
+template <int DEG>
+void computeTables(FILE* file, const FingerprintPolynomial<DEG>& poly) {
+  uint64_t table[8][256][FingerprintPolynomial<DEG>::size()];
+  // table[i][q] is Q(X) * X^(k+8*i) mod P(X),
+  // where k is the number of bits in the fingerprint (and deg(P)) and
+  // Q(X) = q7*X^7 + q6*X^6 + ... + q1*X + q0 is a degree-7 polyonomial
+  // whose coefficients are the bits of q.
+  for (int x = 0; x < 256; x++) {
+    FingerprintPolynomial<DEG> t;
+    t.setHigh8Bits(x);
+    for (int i = 0; i < 8; i++) {
+      t.mulXkmod(8, poly);
+      t.write(&(table[i][x][0]));
+    }
+  }
+
+  // Write the actual polynomial used; this isn't needed during fast
+  // fingerprint calculation, but it's useful for reference and unittesting.
+  uint64_t poly_val[FingerprintPolynomial<DEG>::size()];
+  poly.write(poly_val);
+  CHECK_ERR(fprintf(file,
+      "template <>\n"
+      "const uint64_t FingerprintTable<%d>::poly[%d] = {",
+      DEG+1, FingerprintPolynomial<DEG>::size()));
+  for (int j = 0; j < FingerprintPolynomial<DEG>::size(); j++) {
+    CHECK_ERR(fprintf(file, "%s%" PRIu64 "LU", j ? ", " : "", poly_val[j]));
+  }
+  CHECK_ERR(fprintf(file, "};\n\n"));
+
+  // Write the tables.
+  CHECK_ERR(fprintf(file,
+      "template <>\n"
+      "const uint64_t FingerprintTable<%d>::table[8][256][%d] = {\n",
+      DEG+1, FingerprintPolynomial<DEG>::size()));
+  for (int i = 0; i < 8; i++) {
+    CHECK_ERR(fprintf(file,
+        "  // Table %d"
+        "\n"
+        "  {\n", i));
+    for (int x = 0; x < 256; x++) {
+      CHECK_ERR(fprintf(file, "    {"));
+      for (int j = 0; j < FingerprintPolynomial<DEG>::size(); j++) {
+        CHECK_ERR(fprintf(
+          file, "%s%" PRIu64 "LU", (j ? ", " : ""), table[i][x][j]));
+      }
+      CHECK_ERR(fprintf(file, "},\n"));
+    }
+    CHECK_ERR(fprintf(file, "  },\n"));
+  }
+  CHECK_ERR(fprintf(file, "\n};\n\n"));
+}
+
+}  // namespace
+
+int main(int argc, char *argv[]) {
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  google::InitGoogleLogging(argv[0]);
+
+  std::string name = folly::format("{}/{}", FLAGS_install_dir,
+                                   "FingerprintTables.cpp").str();
+  FILE* file = fopen(name.c_str(), "w");
+  PCHECK(file);
+
+  CHECK_ERR(fprintf(file,
+      "/**\n"
+      " * Fingerprint tables for 64-, 96-, and 128-bit Rabin fingerprints.\n"
+      " *\n"
+      " * AUTOMATICALLY GENERATED.  DO NOT EDIT.\n"
+      " */\n"
+      "\n"
+      "#include \"folly/Fingerprint.h\"\n"
+      "\n"
+      "namespace folly {\n"
+      "namespace detail {\n"
+      "\n"));
+
+  FingerprintPolynomial<63> poly64((const uint64_t*)&FLAGS_poly64);
+  computeTables(file, poly64);
+
+  uint64_t poly96_val[2];
+  poly96_val[0] = (uint64_t)FLAGS_poly96_m;
+  poly96_val[1] = (uint64_t)FLAGS_poly96_l << 32;
+  FingerprintPolynomial<95> poly96(poly96_val);
+  computeTables(file, poly96);
+
+  uint64_t poly128_val[2];
+  poly128_val[0] = (uint64_t)FLAGS_poly128_m;
+  poly128_val[1] = (uint64_t)FLAGS_poly128_l;
+  FingerprintPolynomial<127> poly128(poly128_val);
+  computeTables(file, poly128);
+
+  CHECK_ERR(fprintf(file,
+      "}  // namespace detail\n"
+      "}  // namespace folly\n"));
+  CHECK_ERR(fclose(file));
+
+  return 0;
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/build/generate_format_tables.py
@@ -0,0 +1,79 @@
+#!/usr/bin/env python
+#
+# Generate Format tables
+
+
+import os
+from optparse import OptionParser
+
+OUTPUT_FILE = "FormatTables.cpp"
+
+def generate_table(f, type_name, name, map):
+    f.write("extern const {0} {1}[] = {{".format(type_name, name))
+    for i in range(0, 256):
+        if i % 2 == 0:
+            f.write("\n  ")
+        f.write("{0}::{1}, ".format(type_name, map.get(chr(i), "INVALID")))
+    f.write("\n};\n\n")
+
+def generate_conv_table(f, name, values):
+    values = list(values)
+    line = ''
+    for i, v in enumerate(values):
+        if i == 0:
+            f.write("extern const char {0}[{1}][{2}] = {{\n".format(
+                name, len(values), len(v)))
+        row = "{{{0}}}, ".format(", ".join("'{0}'".format(x) for x in v))
+        if len(line) + len(row) > 79:
+            f.write(line + "\n")
+            line = ''
+        line += row
+    if line:
+        f.write(line + "\n")
+    f.write("};\n\n")
+
+def octal_values():
+    return (tuple("{0:03o}".format(x)) for x in xrange(512))
+
+def hex_values(upper):
+    fmt = "{0:02X}" if upper else "{0:02x}"
+    return (tuple(fmt.format(x)) for x in xrange(256))
+
+def binary_values():
+    return (tuple("{0:08b}".format(x)) for x in xrange(256))
+
+def generate(f):
+    f.write("#include \"folly/FormatArg.h\"\n"
+            "\n"
+            "namespace folly {\n"
+            "namespace detail {\n"
+            "\n")
+
+    generate_table(
+        f, "FormatArg::Align", "formatAlignTable",
+        {"<": "LEFT", ">": "RIGHT", "=": "PAD_AFTER_SIGN", "^": "CENTER"})
+
+    generate_table(
+        f, "FormatArg::Sign", "formatSignTable",
+        {"+": "PLUS_OR_MINUS", "-": "MINUS", " ": "SPACE_OR_MINUS"})
+
+    generate_conv_table(f, "formatOctal", octal_values())
+    generate_conv_table(f, "formatHexLower", hex_values(False))
+    generate_conv_table(f, "formatHexUpper", hex_values(True))
+    generate_conv_table(f, "formatBinary", binary_values())
+
+    f.write("}  // namespace detail\n"
+            "}  // namespace folly\n")
+
+def main():
+    parser = OptionParser()
+    parser.add_option("--install_dir", dest="install_dir", default=".",
+                      help="write output to DIR", metavar="DIR")
+    parser.add_option("--fbcode_dir")
+    (options, args) = parser.parse_args()
+    f = open(os.path.join(options.install_dir, OUTPUT_FILE), "w")
+    generate(f)
+    f.close()
+
+if __name__ == "__main__":
+    main()
--- /dev/null
+++ b/hphp/submodules/folly/folly/build/generate_varint_tables.py
@@ -0,0 +1,113 @@
+#!/usr/bin/env python
+#
+# Generate tables for GroupVarint32
+# Copyright 2011 Facebook
+#
+# @author Tudor Bosman (tudorb@fb.com)
+#
+# Reference: http://www.stepanovpapers.com/CIKM_2011.pdf
+#
+# From 17 encoded bytes, we may use between 5 and 17 bytes to encode 4
+# integers.  The first byte is a key that indicates how many bytes each of
+# the 4 integers takes:
+#
+# bit 0..1: length-1 of first integer
+# bit 2..3: length-1 of second integer
+# bit 4..5: length-1 of third integer
+# bit 6..7: length-1 of fourth integer
+#
+# The value of the first byte is used as the index in a table which returns
+# a mask value for the SSSE3 PSHUFB instruction, which takes an XMM register
+# (16 bytes) and shuffles bytes from it into a destination XMM register
+# (optionally setting some of them to 0)
+#
+# For example, if the key has value 4, that means that the first integer
+# uses 1 byte, the second uses 2 bytes, the third and fourth use 1 byte each,
+# so we set the mask value so that
+#
+# r[0] = a[0]
+# r[1] = 0
+# r[2] = 0
+# r[3] = 0
+#
+# r[4] = a[1]
+# r[5] = a[2]
+# r[6] = 0
+# r[7] = 0
+#
+# r[8] = a[3]
+# r[9] = 0
+# r[10] = 0
+# r[11] = 0
+#
+# r[12] = a[4]
+# r[13] = 0
+# r[14] = 0
+# r[15] = 0
+
+import os
+from optparse import OptionParser
+
+OUTPUT_FILE = "GroupVarintTables.cpp"
+
+def generate(f):
+    f.write("""
+#if defined(__x86_64__) || defined(__i386__)
+#include <stdint.h>
+#include <x86intrin.h>
+
+namespace folly {
+namespace detail {
+
+extern const __m128i groupVarintSSEMasks[] = {
+""")
+
+    # Compute SSE masks
+    for i in range(0, 256):
+        offset = 0
+        vals = [0, 0, 0, 0]
+        for j in range(0, 4):
+            d = 1 + ((i >> (2 * j)) & 3)
+            # the j'th integer uses d bytes, consume them
+            for k in range(0, d):
+                vals[j] |= offset << (8 * k)
+                offset += 1
+            # set remaining bytes in result to 0
+            # 0xff: set corresponding byte in result to 0
+            for k in range(d, 4):
+                vals[j] |= 0xff << (8 * k)
+        f.write("  {{static_cast<int64_t>(0x{1:08x}{0:08x}), "
+            "static_cast<int64_t>(0x{3:08x}{2:08x})}},\n".format(*vals))
+
+    f.write("};\n"
+            "\n"
+            "extern const uint8_t groupVarintLengths[] = {\n")
+
+    # Also compute total encoded lengths, including key byte
+    for i in range(0, 256):
+        offset = 1  # include key byte
+        for j in range(0, 4):
+            d = 1 + ((i >> (2 * j)) & 3)
+            offset += d
+        f.write("  {0},\n".format(offset))
+
+    f.write("""
+};
+
+}  // namespace detail
+}  // namespace folly
+#endif /* defined(__x86_64__) || defined(__i386__) */
+""")
+
+def main():
+    parser = OptionParser()
+    parser.add_option("--install_dir", dest="install_dir", default=".",
+                      help="write output to DIR", metavar="DIR")
+    parser.add_option("--fbcode_dir")
+    (options, args) = parser.parse_args()
+    f = open(os.path.join(options.install_dir, OUTPUT_FILE), "w")
+    generate(f)
+    f.close()
+
+if __name__ == "__main__":
+    main()
--- /dev/null
+++ b/hphp/submodules/folly/folly/Checksum.cpp
@@ -0,0 +1,117 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Checksum.h"
+#include <algorithm>
+#include <stdexcept>
+#include <boost/crc.hpp>
+#include "folly/CpuId.h"
+
+namespace folly {
+
+namespace detail {
+
+#if defined(__x86_64__) && defined(__GNUC__) && defined(__GNUC_MINOR__) && \
+    (((__GNUC__ * 100) + __GNUC_MINOR__) >= 407)
+
+// Fast SIMD implementation of CRC-32C for x86 with SSE 4.2
+uint32_t crc32c_hw(const uint8_t *data, size_t nbytes,
+    uint32_t startingChecksum) {
+  uint32_t sum = startingChecksum;
+  size_t offset = 0;
+
+  // Process bytes one at a time until we reach an 8-byte boundary and can
+  // start doing aligned 64-bit reads.
+  static uintptr_t ALIGN_MASK = sizeof(uint64_t) - 1;
+  size_t mask = (size_t)((uintptr_t)data & ALIGN_MASK);
+  if (mask != 0) {
+    size_t limit = std::min(nbytes, sizeof(uint64_t) - mask);
+    while (offset < limit) {
+      sum = (uint32_t)__builtin_ia32_crc32qi(sum, data[offset]);
+      offset++;
+    }
+  }
+
+  // Process 8 bytes at a time until we have fewer than 8 bytes left.
+  while (offset + sizeof(uint64_t) <= nbytes) {
+    const uint64_t* src = (const uint64_t*)(data + offset);
+    sum = __builtin_ia32_crc32di(sum, *src);
+    offset += sizeof(uint64_t);
+  }
+
+  // Process any bytes remaining after the last aligned 8-byte block.
+  while (offset < nbytes) {
+    sum = (uint32_t)__builtin_ia32_crc32qi(sum, data[offset]);
+    offset++;
+  }
+  return sum;
+}
+
+bool crc32c_hw_supported() {
+  static folly::CpuId id;
+  return id.sse42();
+}
+
+#else
+
+uint32_t crc32c_hw(const uint8_t *data, size_t nbytes,
+    uint32_t startingChecksum) {
+  throw std::runtime_error("crc32_hw is not implemented on this platform");
+}
+
+bool crc32c_hw_supported() {
+  return false;
+}
+
+#endif
+
+uint32_t crc32c_sw(const uint8_t *data, size_t nbytes,
+    uint32_t startingChecksum) {
+
+  // Reverse the bits in the starting checksum so they'll be in the
+  // right internal format for Boost's CRC engine.
+  //     O(1)-time, branchless bit reversal algorithm from
+  //     http://graphics.stanford.edu/~seander/bithacks.html
+  startingChecksum = ((startingChecksum >> 1) & 0x55555555) |
+      ((startingChecksum & 0x55555555) << 1);
+  startingChecksum = ((startingChecksum >> 2) & 0x33333333) |
+      ((startingChecksum & 0x33333333) << 2);
+  startingChecksum = ((startingChecksum >> 4) & 0x0f0f0f0f) |
+      ((startingChecksum & 0x0f0f0f0f) << 4);
+  startingChecksum = ((startingChecksum >> 8) & 0x00ff00ff) |
+      ((startingChecksum & 0x00ff00ff) << 8);
+  startingChecksum = (startingChecksum >> 16) |
+      (startingChecksum << 16);
+
+  static const uint32_t CRC32C_POLYNOMIAL = 0x1EDC6F41;
+  boost::crc_optimal<32, CRC32C_POLYNOMIAL, ~0U, 0, true, true> sum(
+      startingChecksum);
+  sum.process_bytes(data, nbytes);
+  return sum.checksum();
+}
+
+} // folly::detail
+
+uint32_t crc32c(const uint8_t *data, size_t nbytes,
+    uint32_t startingChecksum) {
+  if (detail::crc32c_hw_supported()) {
+    return detail::crc32c_hw(data, nbytes, startingChecksum);
+  } else {
+    return detail::crc32c_sw(data, nbytes, startingChecksum);
+  }
+}
+
+} // folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/Checksum.h
@@ -0,0 +1,43 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_CHECKSUM_H_
+#define FOLLY_CHECKSUM_H_
+
+#include <stdint.h>
+#include <cstddef>
+
+/*
+ * Checksum functions
+ */
+
+namespace folly {
+
+/**
+ * Compute the CRC-32C checksum of a buffer, using a hardware-accelerated
+ * implementation if available or a portable software implementation as
+ * a default.
+ *
+ * @note CRC-32C is different from CRC-32; CRC-32C starts with a different
+ *       polynomial and thus yields different results for the same input
+ *       than a traditional CRC-32.
+ */
+uint32_t crc32c(const uint8_t* data, size_t nbytes,
+    uint32_t startingChecksum = ~0U);
+
+} // folly
+
+#endif /* FOLLY_CHECKSUM_H_ */
--- /dev/null
+++ b/hphp/submodules/folly/folly/Chrono.h
@@ -0,0 +1,33 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// Wrapper around <chrono> that hides away some gcc 4.6 issues
+#ifndef FOLLY_CHRONO_H_
+#define FOLLY_CHRONO_H_
+
+#include <chrono>
+#include "folly/Portability.h"
+
+// gcc 4.6 uses an obsolete name for steady_clock, although the implementation
+// is the same
+#if __GNUC_PREREQ(4, 6) && !__GNUC_PREREQ(4, 7)
+namespace std { namespace chrono {
+typedef monotonic_clock steady_clock;
+}}  // namespaces
+#endif
+
+#endif /* FOLLY_CHRONO_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/ConcurrentSkipList.h
@@ -0,0 +1,859 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// @author: Xin Liu <xliux@fb.com>
+//
+// A concurrent skip list (CSL) implementation.
+// Ref: http://www.cs.tau.ac.il/~shanir/nir-pubs-web/Papers/OPODIS2006-BA.pdf
+
+/*
+
+This implements a sorted associative container that supports only
+unique keys.  (Similar to std::set.)
+
+Features:
+
+  1. Small memory overhead: ~40% less memory overhead compared with
+     std::set (1.6 words per node versus 3). It has an minimum of 4
+     words (7 words if there nodes got deleted) per-list overhead
+     though.
+
+  2. Read accesses (count, find iterator, skipper) are lock-free and
+     mostly wait-free (the only wait a reader may need to do is when
+     the node it is visiting is in a pending stage, i.e. deleting,
+     adding and not fully linked).  Write accesses (remove, add) need
+     to acquire locks, but locks are local to the predecessor nodes
+     and/or successor nodes.
+
+  3. Good high contention performance, comparable single-thread
+     performance.  In the multithreaded case (12 workers), CSL tested
+     10x faster than a RWSpinLocked std::set for an averaged sized
+     list (1K - 1M nodes).
+
+     Comparable read performance to std::set when single threaded,
+     especially when the list size is large, and scales better to
+     larger lists: when the size is small, CSL can be 20-50% slower on
+     find()/contains().  As the size gets large (> 1M elements),
+     find()/contains() can be 30% faster.
+
+     Iterating through a skiplist is similar to iterating through a
+     linked list, thus is much (2-6x) faster than on a std::set
+     (tree-based).  This is especially true for short lists due to
+     better cache locality.  Based on that, it's also faster to
+     intersect two skiplists.
+
+  4. Lazy removal with GC support.  The removed nodes get deleted when
+     the last Accessor to the skiplist is destroyed.
+
+Caveats:
+
+  1. Write operations are usually 30% slower than std::set in a single
+     threaded environment.
+
+  2. Need to have a head node for each list, which has a 4 word
+     overhead.
+
+  3. When the list is quite small (< 1000 elements), single threaded
+     benchmarks show CSL can be 10x slower than std:set.
+
+  4. The interface requires using an Accessor to access the skiplist.
+    (See below.)
+
+  5. Currently x64 only, due to use of MicroSpinLock.
+
+  6. Freed nodes will not be reclaimed as long as there are ongoing
+     uses of the list.
+
+Sample usage:
+
+     typedef ConcurrentSkipList<int> SkipListT;
+     shared_ptr<SkipListT> sl(SkipListT::createInstance(init_head_height);
+     {
+       // It's usually good practice to hold an accessor only during
+       // its necessary life cycle (but not in a tight loop as
+       // Accessor creation incurs ref-counting overhead).
+       //
+       // Holding it longer delays garbage-collecting the deleted
+       // nodes in the list.
+       SkipListT::Accessor accessor(sl);
+       accessor.insert(23);
+       accessor.erase(2);
+       for (auto &elem : accessor) {
+         // use elem to access data
+       }
+       ... ...
+     }
+
+ Another useful type is the Skipper accessor.  This is useful if you
+ want to skip to locations in the way std::lower_bound() works,
+ i.e. it can be used for going through the list by skipping to the
+ node no less than a specified key.  The Skipper keeps its location as
+ state, which makes it convenient for things like implementing
+ intersection of two sets efficiently, as it can start from the last
+ visited position.
+
+     {
+       SkipListT::Accessor accessor(sl);
+       SkipListT::Skipper skipper(accessor);
+       skipper.to(30);
+       if (skipper) {
+         CHECK_LE(30, *skipper);
+       }
+       ...  ...
+       // GC may happen when the accessor gets destructed.
+     }
+*/
+
+#ifndef FOLLY_CONCURRENT_SKIP_LIST_H_
+#define FOLLY_CONCURRENT_SKIP_LIST_H_
+
+#include <algorithm>
+#include <climits>
+#include <type_traits>
+#include <utility>
+#include <vector>
+#include <atomic>
+#include <thread>
+#include <boost/iterator/iterator_facade.hpp>
+#include <boost/scoped_ptr.hpp>
+#include <memory>
+
+#include <glog/logging.h>
+#include "folly/ConcurrentSkipList-inl.h"
+#include "folly/Likely.h"
+#include "folly/SmallLocks.h"
+
+namespace folly {
+
+template<typename T, typename Comp=std::less<T>, int MAX_HEIGHT=24>
+class ConcurrentSkipList {
+  // MAX_HEIGHT needs to be at least 2 to suppress compiler
+  // warnings/errors (Werror=uninitialized tiggered due to preds_[1]
+  // being treated as a scalar in the compiler).
+  static_assert(MAX_HEIGHT >= 2 && MAX_HEIGHT < 64,
+      "MAX_HEIGHT can only be in the range of [2, 64)");
+  typedef std::unique_lock<folly::MicroSpinLock> ScopedLocker;
+  typedef ConcurrentSkipList<T, Comp, MAX_HEIGHT> SkipListType;
+
+ public:
+  typedef detail::SkipListNode<T> NodeType;
+  typedef T value_type;
+  typedef T key_type;
+
+
+  typedef detail::csl_iterator<value_type, NodeType> iterator;
+  typedef detail::csl_iterator<const value_type, const NodeType> const_iterator;
+
+  class Accessor;
+  class Skipper;
+
+  // convenient function to get an Accessor to a new instance.
+  static Accessor create(int height=1) {
+    return Accessor(createInstance(height));
+  }
+
+  // create a shared_ptr skiplist object with initial head height.
+  static std::shared_ptr<SkipListType> createInstance(int height=1) {
+    return std::shared_ptr<SkipListType>(new SkipListType(height));
+  }
+
+  // create a unique_ptr skiplist object with initial head height.
+  static std::unique_ptr<SkipListType> createRawInstance(int height=1) {
+    return std::unique_ptr<SkipListType>(new SkipListType(height));
+  }
+
+
+  //===================================================================
+  // Below are implementation details.
+  // Please see ConcurrentSkipList::Accessor for stdlib-like APIs.
+  //===================================================================
+
+  ~ConcurrentSkipList() {
+    CHECK_EQ(recycler_.refs(), 0);
+    while (NodeType* current = head_.load(std::memory_order_relaxed)) {
+      NodeType* tmp = current->skip(0);
+      NodeType::destroy(current);
+      head_.store(tmp, std::memory_order_relaxed);
+    }
+  }
+
+ private:
+
+  static bool greater(const value_type &data, const NodeType *node) {
+    return node && Comp()(node->data(), data);
+  }
+
+  static bool less(const value_type &data, const NodeType *node) {
+    return (node == nullptr) || Comp()(data, node->data());
+  }
+
+  static int findInsertionPoint(NodeType *cur, int cur_layer,
+      const value_type &data,
+      NodeType *preds[], NodeType *succs[]) {
+    int foundLayer = -1;
+    NodeType *pred = cur;
+    NodeType *foundNode = nullptr;
+    for (int layer = cur_layer; layer >= 0; --layer) {
+      NodeType *node = pred->skip(layer);
+      while (greater(data, node)) {
+        pred = node;
+        node = node->skip(layer);
+      }
+      if (foundLayer == -1 && !less(data, node)) { // the two keys equal
+        foundLayer = layer;
+        foundNode = node;
+      }
+      preds[layer] = pred;
+
+      // if found, succs[0..foundLayer] need to point to the cached foundNode,
+      // as foundNode might be deleted at the same time thus pred->skip() can
+      // return NULL or another node.
+      succs[layer] = foundNode ? foundNode : node;
+    }
+    return foundLayer;
+  }
+
+  struct Recycler : private boost::noncopyable {
+    Recycler() : refs_(0), dirty_(false) { lock_.init(); }
+
+    ~Recycler() {
+      if (nodes_) {
+        for (auto& node : *nodes_) {
+          NodeType::destroy(node);
+        }
+      }
+    }
+
+    void add(NodeType* node) {
+      std::lock_guard<MicroSpinLock> g(lock_);
+      if (nodes_.get() == nullptr) {
+        nodes_.reset(new std::vector<NodeType*>(1, node));
+      } else {
+        nodes_->push_back(node);
+      }
+      DCHECK_GT(refs(), 0);
+      dirty_.store(true, std::memory_order_relaxed);
+    }
+
+    int refs() const {
+      return refs_.load(std::memory_order_relaxed);
+    }
+
+    int addRef() {
+      return refs_.fetch_add(1, std::memory_order_relaxed);
+    }
+
+    int release() {
+      // We don't expect to clean the recycler immediately everytime it is OK
+      // to do so. Here, it is possible that multiple accessors all release at
+      // the same time but nobody would clean the recycler here. If this
+      // happens, the recycler will usually still get cleaned when
+      // such a race doesn't happen. The worst case is the recycler will
+      // eventually get deleted along with the skiplist.
+      if (LIKELY(!dirty_.load(std::memory_order_relaxed) || refs() > 1)) {
+        return refs_.fetch_add(-1, std::memory_order_relaxed);
+      }
+
+      boost::scoped_ptr<std::vector<NodeType*> > newNodes;
+      {
+        std::lock_guard<MicroSpinLock> g(lock_);
+        if (nodes_.get() == nullptr || refs() > 1) {
+          return refs_.fetch_add(-1, std::memory_order_relaxed);
+        }
+        // once refs_ reaches 1 and there is no other accessor, it is safe to
+        // remove all the current nodes in the recycler, as we already acquired
+        // the lock here so no more new nodes can be added, even though new
+        // accessors may be added after that.
+        newNodes.swap(nodes_);
+        dirty_.store(false, std::memory_order_relaxed);
+      }
+
+      // TODO(xliu) should we spawn a thread to do this when there are large
+      // number of nodes in the recycler?
+      for (auto& node : *newNodes) {
+        NodeType::destroy(node);
+      }
+
+      // decrease the ref count at the very end, to minimize the
+      // chance of other threads acquiring lock_ to clear the deleted
+      // nodes again.
+      return refs_.fetch_add(-1, std::memory_order_relaxed);
+    }
+
+   private:
+    boost::scoped_ptr<std::vector<NodeType*>> nodes_;
+    std::atomic<int32_t> refs_; // current number of visitors to the list
+    std::atomic<bool> dirty_; // whether *nodes_ is non-empty
+    MicroSpinLock lock_; // protects access to *nodes_
+  };  // class ConcurrentSkipList::Recycler
+
+  explicit ConcurrentSkipList(int height) :
+    head_(NodeType::create(height, value_type(), true)), size_(0) {}
+
+  size_t size() const { return size_.load(std::memory_order_relaxed); }
+  int height() const {
+    return head_.load(std::memory_order_consume)->height();
+  }
+  int maxLayer() const { return height() - 1; }
+
+  size_t incrementSize(int delta) {
+    return size_.fetch_add(delta, std::memory_order_relaxed) + delta;
+  }
+
+  // Returns the node if found, nullptr otherwise.
+  NodeType* find(const value_type &data) {
+    auto ret = findNode(data);
+    if (ret.second && !ret.first->markedForRemoval()) return ret.first;
+    return nullptr;
+  }
+
+  // lock all the necessary nodes for changing (adding or removing) the list.
+  // returns true if all the lock acquried successfully and the related nodes
+  // are all validate (not in certain pending states), false otherwise.
+  bool lockNodesForChange(int nodeHeight,
+      ScopedLocker guards[MAX_HEIGHT],
+      NodeType *preds[MAX_HEIGHT],
+      NodeType *succs[MAX_HEIGHT],
+      bool adding=true) {
+    NodeType *pred, *succ, *prevPred = nullptr;
+    bool valid = true;
+    for (int layer = 0; valid && layer < nodeHeight; ++layer) {
+      pred = preds[layer];
+      DCHECK(pred != nullptr) << "layer=" << layer << " height=" << height()
+        << " nodeheight=" << nodeHeight;
+      succ = succs[layer];
+      if (pred != prevPred) {
+        guards[layer] = pred->acquireGuard();
+        prevPred = pred;
+      }
+      valid = !pred->markedForRemoval() &&
+        pred->skip(layer) == succ;  // check again after locking
+
+      if (adding) {  // when adding a node, the succ shouldn't be going away
+        valid = valid && (succ == nullptr || !succ->markedForRemoval());
+      }
+    }
+
+    return valid;
+  }
+
+  // Returns a paired value:
+  //   pair.first always stores the pointer to the node with the same input key.
+  //     It could be either the newly added data, or the existed data in the
+  //     list with the same key.
+  //   pair.second stores whether the data is added successfully:
+  //     0 means not added, otherwise reutrns the new size.
+  template<typename U>
+  std::pair<NodeType*, size_t> addOrGetData(U &&data) {
+    NodeType *preds[MAX_HEIGHT], *succs[MAX_HEIGHT];
+    NodeType *newNode;
+    size_t newSize;
+    while (true) {
+      int max_layer = 0;
+      int layer = findInsertionPointGetMaxLayer(data, preds, succs, &max_layer);
+
+      if (layer >= 0) {
+        NodeType *nodeFound = succs[layer];
+        DCHECK(nodeFound != nullptr);
+        if (nodeFound->markedForRemoval()) {
+          continue;  // if it's getting deleted retry finding node.
+        }
+        // wait until fully linked.
+        while (UNLIKELY(!nodeFound->fullyLinked())) {}
+        return std::make_pair(nodeFound, 0);
+      }
+
+      // need to capped at the original height -- the real height may have grown
+      int nodeHeight = detail::SkipListRandomHeight::instance()->
+        getHeight(max_layer + 1);
+
+      ScopedLocker guards[MAX_HEIGHT];
+      if (!lockNodesForChange(nodeHeight, guards, preds, succs)) {
+        continue; // give up the locks and retry until all valid
+      }
+
+      // locks acquired and all valid, need to modify the links under the locks.
+      newNode = NodeType::create(nodeHeight, std::forward<U>(data));
+      for (int layer = 0; layer < nodeHeight; ++layer) {
+        newNode->setSkip(layer, succs[layer]);
+        preds[layer]->setSkip(layer, newNode);
+      }
+
+      newNode->setFullyLinked();
+      newSize = incrementSize(1);
+      break;
+    }
+
+    int hgt = height();
+    size_t sizeLimit =
+      detail::SkipListRandomHeight::instance()->getSizeLimit(hgt);
+
+    if (hgt < MAX_HEIGHT && newSize > sizeLimit) {
+      growHeight(hgt + 1);
+    }
+    CHECK_GT(newSize, 0);
+    return std::make_pair(newNode, newSize);
+  }
+
+  bool remove(const value_type &data) {
+    NodeType *nodeToDelete = nullptr;
+    ScopedLocker nodeGuard;
+    bool isMarked = false;
+    int nodeHeight = 0;
+    NodeType* preds[MAX_HEIGHT], *succs[MAX_HEIGHT];
+
+    while (true) {
+      int max_layer = 0;
+      int layer = findInsertionPointGetMaxLayer(data, preds, succs, &max_layer);
+      if (!isMarked && (layer < 0 || !okToDelete(succs[layer], layer))) {
+        return false;
+      }
+
+      if (!isMarked) {
+        nodeToDelete = succs[layer];
+        nodeHeight = nodeToDelete->height();
+        nodeGuard = nodeToDelete->acquireGuard();
+        if (nodeToDelete->markedForRemoval()) return false;
+        nodeToDelete->setMarkedForRemoval();
+        isMarked = true;
+      }
+
+      // acquire pred locks from bottom layer up
+      ScopedLocker guards[MAX_HEIGHT];
+      if (!lockNodesForChange(nodeHeight, guards, preds, succs, false)) {
+        continue;  // this will unlock all the locks
+      }
+
+      for (int layer = nodeHeight - 1; layer >= 0; --layer) {
+        preds[layer]->setSkip(layer, nodeToDelete->skip(layer));
+      }
+
+      incrementSize(-1);
+      break;
+    }
+    recycle(nodeToDelete);
+    return true;
+  }
+
+  const value_type *first() const {
+    auto node = head_.load(std::memory_order_consume)->skip(0);
+    return node ? &node->data() : nullptr;
+  }
+
+  const value_type *last() const {
+    NodeType *pred = head_.load(std::memory_order_consume);
+    NodeType *node = nullptr;
+    for (int layer = maxLayer(); layer >= 0; --layer) {
+      do {
+        node = pred->skip(layer);
+        if (node) pred = node;
+      } while (node != nullptr);
+    }
+    return pred == head_.load(std::memory_order_relaxed)
+      ? nullptr : &pred->data();
+  }
+
+  static bool okToDelete(NodeType *candidate, int layer) {
+    DCHECK(candidate != nullptr);
+    return candidate->fullyLinked() &&
+      candidate->maxLayer() == layer &&
+      !candidate->markedForRemoval();
+  }
+
+  // find node for insertion/deleting
+  int findInsertionPointGetMaxLayer(const value_type &data,
+      NodeType *preds[], NodeType *succs[], int *max_layer) const {
+    *max_layer = maxLayer();
+    return findInsertionPoint(head_.load(std::memory_order_consume),
+      *max_layer, data, preds, succs);
+  }
+
+  // Find node for access. Returns a paired values:
+  // pair.first = the first node that no-less than data value
+  // pair.second = 1 when the data value is founded, or 0 otherwise.
+  // This is like lower_bound, but not exact: we could have the node marked for
+  // removal so still need to check that.
+  std::pair<NodeType*, int> findNode(const value_type &data) const {
+    return findNodeDownRight(data);
+  }
+
+  // Find node by first stepping down then stepping right. Based on benchmark
+  // results, this is slightly faster than findNodeRightDown for better
+  // localality on the skipping pointers.
+  std::pair<NodeType*, int> findNodeDownRight(const value_type &data) const {
+    NodeType *pred = head_.load(std::memory_order_consume);
+    int ht = pred->height();
+    NodeType *node = nullptr;
+
+    bool found = false;
+    while (!found) {
+      // stepping down
+      for (; ht > 0 && less(data, pred->skip(ht - 1)); --ht) {}
+      if (ht == 0) return std::make_pair(pred->skip(0), 0);  // not found
+
+      node = pred->skip(--ht);  // node <= data now
+      // stepping right
+      while (greater(data, node)) {
+        pred = node;
+        node = node->skip(ht);
+      }
+      found = !less(data, node);
+    }
+    return std::make_pair(node, found);
+  }
+
+  // find node by first stepping right then stepping down.
+  // We still keep this for reference purposes.
+  std::pair<NodeType*, int> findNodeRightDown(const value_type &data) const {
+    NodeType *pred = head_.load(std::memory_order_consume);
+    NodeType *node = nullptr;
+    auto top = maxLayer();
+    int found = 0;
+    for (int layer = top; !found && layer >= 0; --layer) {
+      node = pred->skip(layer);
+      while (greater(data, node)) {
+        pred = node;
+        node = node->skip(layer);
+      }
+      found = !less(data, node);
+    }
+    return std::make_pair(node, found);
+  }
+
+  NodeType* lower_bound(const value_type &data) const {
+    auto node = findNode(data).first;
+    while (node != nullptr && node->markedForRemoval()) {
+      node = node->skip(0);
+    }
+    return node;
+  }
+
+  void growHeight(int height) {
+    NodeType* oldHead = head_.load(std::memory_order_consume);
+    if (oldHead->height() >= height) {  // someone else already did this
+      return;
+    }
+
+    NodeType* newHead = NodeType::create(height, value_type(), true);
+
+    { // need to guard the head node in case others are adding/removing
+      // nodes linked to the head.
+      ScopedLocker g = oldHead->acquireGuard();
+      newHead->copyHead(oldHead);
+      NodeType* expected = oldHead;
+      if (!head_.compare_exchange_strong(expected, newHead,
+          std::memory_order_release)) {
+        // if someone has already done the swap, just return.
+        NodeType::destroy(newHead);
+        return;
+      }
+      oldHead->setMarkedForRemoval();
+    }
+    recycle(oldHead);
+  }
+
+  void recycle(NodeType *node) {
+    recycler_.add(node);
+  }
+
+  std::atomic<NodeType*> head_;
+  Recycler recycler_;
+  std::atomic<size_t> size_;
+};
+
+template<typename T, typename Comp, int MAX_HEIGHT>
+class ConcurrentSkipList<T, Comp, MAX_HEIGHT>::Accessor {
+  typedef detail::SkipListNode<T> NodeType;
+  typedef ConcurrentSkipList<T, Comp, MAX_HEIGHT> SkipListType;
+ public:
+  typedef T value_type;
+  typedef T key_type;
+  typedef T& reference;
+  typedef T* pointer;
+  typedef const T& const_reference;
+  typedef const T* const_pointer;
+  typedef size_t size_type;
+  typedef Comp key_compare;
+  typedef Comp value_compare;
+
+  typedef typename SkipListType::iterator iterator;
+  typedef typename SkipListType::const_iterator const_iterator;
+  typedef typename SkipListType::Skipper Skipper;
+
+  explicit Accessor(std::shared_ptr<ConcurrentSkipList> skip_list)
+    : slHolder_(std::move(skip_list))
+  {
+    sl_ = slHolder_.get();
+    DCHECK(sl_ != nullptr);
+    sl_->recycler_.addRef();
+  }
+
+  // Unsafe initializer: the caller assumes the responsibility to keep
+  // skip_list valid during the whole life cycle of the Acessor.
+  explicit Accessor(ConcurrentSkipList *skip_list) : sl_(skip_list) {
+    DCHECK(sl_ != nullptr);
+    sl_->recycler_.addRef();
+  }
+
+  Accessor(const Accessor &accessor) :
+      sl_(accessor.sl_),
+      slHolder_(accessor.slHolder_) {
+    sl_->recycler_.addRef();
+  }
+
+  Accessor& operator=(const Accessor &accessor) {
+    if (this != &accessor) {
+      slHolder_ = accessor.slHolder_;
+      sl_->recycler_.release();
+      sl_ = accessor.sl_;
+      sl_->recycler_.addRef();
+    }
+    return *this;
+  }
+
+  ~Accessor() {
+    sl_->recycler_.release();
+  }
+
+  bool empty() const { return sl_->size() == 0; }
+  size_t size() const { return sl_->size(); }
+  size_type max_size() const { return std::numeric_limits<size_type>::max(); }
+
+  // returns end() if the value is not in the list, otherwise returns an
+  // iterator pointing to the data, and it's guaranteed that the data is valid
+  // as far as the Accessor is hold.
+  iterator find(const key_type &value) { return iterator(sl_->find(value)); }
+  const_iterator find(const key_type &value) const {
+    return iterator(sl_->find(value));
+  }
+  size_type count(const key_type &data) const { return contains(data); }
+
+  iterator begin() const {
+    NodeType* head = sl_->head_.load(std::memory_order_consume);
+    return iterator(head->next());
+  }
+  iterator end() const { return iterator(nullptr); }
+  const_iterator cbegin() const { return begin(); }
+  const_iterator cend() const { return end(); }
+
+  template<typename U,
+    typename=typename std::enable_if<std::is_convertible<U, T>::value>::type>
+  std::pair<iterator, bool> insert(U&& data) {
+    auto ret = sl_->addOrGetData(std::forward<U>(data));
+    return std::make_pair(iterator(ret.first), ret.second);
+  }
+  size_t erase(const key_type &data) { return remove(data); }
+
+  iterator lower_bound(const key_type &data) const {
+    return iterator(sl_->lower_bound(data));
+  }
+
+  size_t height() const { return sl_->height(); }
+
+  // first() returns pointer to the first element in the skiplist, or
+  // nullptr if empty.
+  //
+  // last() returns the pointer to the last element in the skiplist,
+  // nullptr if list is empty.
+  //
+  // Note: As concurrent writing can happen, first() is not
+  //   guaranteed to be the min_element() in the list. Similarly
+  //   last() is not guaranteed to be the max_element(), and both of them can
+  //   be invalid (i.e. nullptr), so we name them differently from front() and
+  //   tail() here.
+  const key_type *first() const { return sl_->first(); }
+  const key_type *last() const { return sl_->last(); }
+
+  // Try to remove the last element in the skip list.
+  //
+  // Returns true if we removed it, false if either the list is empty
+  // or a race condition happened (i.e. the used-to-be last element
+  // was already removed by another thread).
+  bool pop_back() {
+    auto last = sl_->last();
+    return last ? sl_->remove(*last) : false;
+  }
+
+  std::pair<key_type*, bool> addOrGetData(const key_type &data) {
+    auto ret = sl_->addOrGetData(data);
+    return std::make_pair(&ret.first->data(), ret.second);
+  }
+
+  SkipListType* skiplist() const { return sl_; }
+
+  // legacy interfaces
+  // TODO:(xliu) remove these.
+  // Returns true if the node is added successfully, false if not, i.e. the
+  // node with the same key already existed in the list.
+  bool contains(const key_type &data) const { return sl_->find(data); }
+  bool add(const key_type &data) { return sl_->addOrGetData(data).second; }
+  bool remove(const key_type &data) { return sl_->remove(data); }
+
+ private:
+  SkipListType *sl_;
+  std::shared_ptr<SkipListType> slHolder_;
+};
+
+// implements forward iterator concept.
+template<typename ValT, typename NodeT>
+class detail::csl_iterator :
+  public boost::iterator_facade<csl_iterator<ValT, NodeT>,
+    ValT, boost::forward_traversal_tag> {
+ public:
+  typedef ValT value_type;
+  typedef value_type& reference;
+  typedef value_type* pointer;
+  typedef ptrdiff_t difference_type;
+
+  explicit csl_iterator(NodeT* node = nullptr) : node_(node) {}
+
+  template<typename OtherVal, typename OtherNode>
+  csl_iterator(const csl_iterator<OtherVal, OtherNode> &other,
+      typename std::enable_if<std::is_convertible<OtherVal, ValT>::value>::type*
+      = 0) : node_(other.node_) {}
+
+  size_t nodeSize() const {
+    return node_ == nullptr ? 0 :
+      node_->height() * sizeof(NodeT*) + sizeof(*this);
+  }
+
+  bool good() const { return node_ != nullptr; }
+
+ private:
+  friend class boost::iterator_core_access;
+  template<class,class> friend class csl_iterator;
+
+  void increment() { node_ = node_->next(); };
+  bool equal(const csl_iterator& other) const { return node_ == other.node_; }
+  value_type& dereference() const { return node_->data(); }
+
+  NodeT* node_;
+};
+
+// Skipper interface
+template<typename T, typename Comp, int MAX_HEIGHT>
+class ConcurrentSkipList<T, Comp, MAX_HEIGHT>::Skipper {
+  typedef detail::SkipListNode<T> NodeType;
+  typedef ConcurrentSkipList<T, Comp, MAX_HEIGHT> SkipListType;
+  typedef typename SkipListType::Accessor Accessor;
+
+ public:
+  typedef T  value_type;
+  typedef T& reference;
+  typedef T* pointer;
+  typedef ptrdiff_t difference_type;
+
+  Skipper(const std::shared_ptr<SkipListType>& skipList) :
+    accessor_(skipList) {
+    init();
+  }
+
+  Skipper(const Accessor& accessor) : accessor_(accessor) {
+    init();
+  }
+
+  void init() {
+    // need to cache the head node
+    NodeType* head_node = head();
+    headHeight_ = head_node->height();
+    for (int i = 0; i < headHeight_; ++i) {
+      preds_[i] = head_node;
+      succs_[i] = head_node->skip(i);
+    }
+    int max_layer = maxLayer();
+    for (int i = 0; i < max_layer; ++i) {
+      hints_[i] = i + 1;
+    }
+    hints_[max_layer] = max_layer;
+  }
+
+  // advance to the next node in the list.
+  Skipper& operator ++() {
+    preds_[0] = succs_[0];
+    succs_[0] = preds_[0]->skip(0);
+    int height = curHeight();
+    for (int i = 1; i < height && preds_[0] == succs_[i]; ++i) {
+      preds_[i] = succs_[i];
+      succs_[i] = preds_[i]->skip(i);
+    }
+    return *this;
+  }
+
+  bool good() const { return succs_[0] != nullptr; }
+
+  int maxLayer() const { return headHeight_ - 1; }
+
+  int curHeight() const {
+    // need to cap the height to the cached head height, as the current node
+    // might be some newly inserted node and also during the time period the
+    // head height may have grown.
+    return succs_[0] ? std::min(headHeight_, succs_[0]->height()) : 0;
+  }
+
+  const value_type &data() const {
+    DCHECK(succs_[0] != NULL);
+    return succs_[0]->data();
+  }
+
+  value_type &operator *() const {
+    DCHECK(succs_[0] != NULL);
+    return succs_[0]->data();
+  }
+
+  value_type *operator->() {
+    DCHECK(succs_[0] != NULL);
+    return &succs_[0]->data();
+  }
+
+  /*
+   * Skip to the position whose data is no less than the parameter.
+   * (I.e. the lower_bound).
+   *
+   * Returns true if the data is found, false otherwise.
+   */
+  bool to(const value_type &data) {
+    int layer = curHeight() - 1;
+    if (layer < 0) return false;   // reaches the end of the list
+
+    int lyr = hints_[layer];
+    int max_layer = maxLayer();
+    while (SkipListType::greater(data, succs_[lyr]) && lyr < max_layer) {
+      ++lyr;
+    }
+    hints_[layer] = lyr;  // update the hint
+
+    int foundLayer = SkipListType::
+      findInsertionPoint(preds_[lyr], lyr, data, preds_, succs_);
+    if (foundLayer < 0) return false;
+
+    DCHECK(succs_[0] != NULL) << "lyr=" << lyr << "; max_layer=" << max_layer;
+    return !succs_[0]->markedForRemoval();
+  }
+
+ private:
+  NodeType* head() const {
+    return accessor_.skiplist()->head_.load(std::memory_order_consume);
+  }
+
+  Accessor accessor_;
+  int headHeight_;
+  NodeType *succs_[MAX_HEIGHT], *preds_[MAX_HEIGHT];
+  uint8_t hints_[MAX_HEIGHT];
+};
+
+} // namespace folly
+
+#endif  // FOLLY_CONCURRENT_SKIP_LIST_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/ConcurrentSkipList-inl.h
@@ -0,0 +1,216 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// @author: Xin Liu <xliux@fb.com>
+
+#ifndef FOLLY_CONCURRENTSKIPLIST_INL_H_
+#define FOLLY_CONCURRENTSKIPLIST_INL_H_
+
+#include <algorithm>
+#include <climits>
+#include <cmath>
+#include <boost/random.hpp>
+
+#include <glog/logging.h>
+#include "folly/SmallLocks.h"
+#include "folly/ThreadLocal.h"
+
+namespace folly { namespace detail {
+
+template<typename ValT, typename NodeT> class csl_iterator;
+
+template<typename T>
+class SkipListNode : boost::noncopyable {
+  enum {
+    IS_HEAD_NODE = 1,
+    MARKED_FOR_REMOVAL = (1 << 1),
+    FULLY_LINKED = (1 << 2),
+  };
+ public:
+  typedef T value_type;
+
+  template<typename U,
+    typename=typename std::enable_if<std::is_convertible<U, T>::value>::type>
+  static SkipListNode* create(int height, U&& data, bool isHead = false) {
+    DCHECK(height >= 1 && height < 64) << height;
+
+    size_t size = sizeof(SkipListNode) +
+      height * sizeof(std::atomic<SkipListNode*>);
+    auto* node = static_cast<SkipListNode*>(malloc(size));
+    // do placement new
+    new (node) SkipListNode(height, std::forward<U>(data), isHead);
+    return node;
+  }
+
+  static void destroy(SkipListNode* node) {
+    node->~SkipListNode();
+    free(node);
+  }
+
+  // copy the head node to a new head node assuming lock acquired
+  SkipListNode* copyHead(SkipListNode* node) {
+    DCHECK(node != nullptr && height_ > node->height_);
+    setFlags(node->getFlags());
+    for (int i = 0; i < node->height_; ++i) {
+      setSkip(i, node->skip(i));
+    }
+    return this;
+  }
+
+  inline SkipListNode* skip(int layer) const {
+    DCHECK_LT(layer, height_);
+    return skip_[layer].load(std::memory_order_consume);
+  }
+
+  // next valid node as in the linked list
+  SkipListNode* next() {
+    SkipListNode* node;
+    for (node = skip(0);
+        (node != nullptr && node->markedForRemoval());
+        node = node->skip(0)) {}
+    return node;
+  }
+
+  void setSkip(uint8_t h, SkipListNode* next) {
+    DCHECK_LT(h, height_);
+    skip_[h].store(next, std::memory_order_release);
+  }
+
+  value_type& data() { return data_; }
+  const value_type& data() const { return data_; }
+  int maxLayer() const { return height_ - 1; }
+  int height() const { return height_; }
+
+  std::unique_lock<MicroSpinLock> acquireGuard() {
+    return std::unique_lock<MicroSpinLock>(spinLock_);
+  }
+
+  bool fullyLinked() const      { return getFlags() & FULLY_LINKED; }
+  bool markedForRemoval() const { return getFlags() & MARKED_FOR_REMOVAL; }
+  bool isHeadNode() const       { return getFlags() & IS_HEAD_NODE; }
+
+  void setIsHeadNode() {
+    setFlags(getFlags() | IS_HEAD_NODE);
+  }
+  void setFullyLinked() {
+    setFlags(getFlags() | FULLY_LINKED);
+  }
+  void setMarkedForRemoval() {
+    setFlags(getFlags() | MARKED_FOR_REMOVAL);
+  }
+
+ private:
+  // Note! this can only be called from create() as a placement new.
+  template<typename U>
+  SkipListNode(uint8_t height, U&& data, bool isHead) :
+      height_(height), data_(std::forward<U>(data)) {
+    spinLock_.init();
+    setFlags(0);
+    if (isHead) setIsHeadNode();
+    // need to explicitly init the dynamic atomic pointer array
+    for (uint8_t i = 0; i < height_; ++i) {
+      new (&skip_[i]) std::atomic<SkipListNode*>(nullptr);
+    }
+  }
+
+  ~SkipListNode() {
+    for (uint8_t i = 0; i < height_; ++i) {
+      skip_[i].~atomic();
+    }
+  }
+
+  uint16_t getFlags() const {
+    return flags_.load(std::memory_order_consume);
+  }
+  void setFlags(uint16_t flags) {
+    flags_.store(flags, std::memory_order_release);
+  }
+
+  // TODO(xliu): on x86_64, it's possible to squeeze these into
+  // skip_[0] to maybe save 8 bytes depending on the data alignments.
+  // NOTE: currently this is x86_64 only anyway, due to the
+  // MicroSpinLock.
+  std::atomic<uint16_t> flags_;
+  const uint8_t height_;
+  MicroSpinLock spinLock_;
+
+  value_type data_;
+
+  std::atomic<SkipListNode*> skip_[0];
+};
+
+class SkipListRandomHeight {
+  enum { kMaxHeight = 64 };
+ public:
+  // make it a singleton.
+  static SkipListRandomHeight *instance() {
+    static SkipListRandomHeight instance_;
+    return &instance_;
+  }
+
+  int getHeight(int maxHeight) const {
+    DCHECK_LE(maxHeight, kMaxHeight) << "max height too big!";
+    double p = randomProb();
+    for (int i = 0; i < maxHeight; ++i) {
+      if (p < lookupTable_[i]) {
+        return i + 1;
+      }
+    }
+    return maxHeight;
+  }
+
+  size_t getSizeLimit(int height) const {
+    DCHECK_LT(height, kMaxHeight);
+    return sizeLimitTable_[height];
+  }
+
+ private:
+
+  SkipListRandomHeight() { initLookupTable(); }
+
+  void initLookupTable() {
+    // set skip prob = 1/E
+    static const double kProbInv = exp(1);
+    static const double kProb = 1.0 / kProbInv;
+    static const size_t kMaxSizeLimit = std::numeric_limits<size_t>::max();
+
+    double sizeLimit = 1;
+    double p = lookupTable_[0] = (1 - kProb);
+    sizeLimitTable_[0] = 1;
+    for (int i = 1; i < kMaxHeight - 1; ++i) {
+      p *= kProb;
+      sizeLimit *= kProbInv;
+      lookupTable_[i] = lookupTable_[i - 1] + p;
+      sizeLimitTable_[i] = sizeLimit > kMaxSizeLimit ?
+        kMaxSizeLimit :
+        static_cast<size_t>(sizeLimit);
+    }
+    lookupTable_[kMaxHeight - 1] = 1;
+    sizeLimitTable_[kMaxHeight - 1] = kMaxSizeLimit;
+  }
+
+  static double randomProb() {
+    static ThreadLocal<boost::lagged_fibonacci2281> rng_;
+    return (*rng_)();
+  }
+
+  double lookupTable_[kMaxHeight];
+  size_t sizeLimitTable_[kMaxHeight];
+};
+
+}}
+
+#endif  // FOLLY_CONCURRENTSKIPLIST_INL_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/configure.ac
@@ -0,0 +1,192 @@
+
+#                                               -*- Autoconf -*-
+# Process this file with autoconf to produce a configure script.
+
+AC_PREREQ(2.59)
+AC_INIT(folly, 0.1, folly@fb.com)
+AC_CONFIG_SRCDIR([Likely.h])
+AC_CONFIG_HEADERS([config.h])
+AX_PREFIX_CONFIG_H([folly-config.h], [folly], [config.h])
+AC_CONFIG_AUX_DIR([build-aux])
+
+AM_INIT_AUTOMAKE([foreign dist-bzip2 nostdinc])
+
+AC_CONFIG_MACRO_DIR([m4])
+
+AC_PROG_INSTALL
+AM_PROG_LIBTOOL
+
+AC_LANG([C++])
+
+# Checks for programs.
+AC_PROG_CXX
+AC_PROG_CC
+AC_CXX_COMPILE_STDCXX_0X
+
+# Be sure to add any -std option to CXXFLAGS before we invoke any
+# AC_COMPILE_IFELSE() or similar macros. Any such macros that are invoked
+# before we update CXXFLAGS will not be run with the same options that we use
+# during the real build.
+STD=""
+if test "x$ac_cv_cxx_compile_cxx0x_cxx" = xyes; then
+   STD="-std=c++0x"
+fi
+if test "x$ac_cv_cxx_compile_cxx0x_gxx" = xyes; then
+   STD="-std=gnu++0x"
+fi
+
+CXXFLAGS="$STD $CXXFLAGS"
+
+# Checks for libraries.
+AC_CHECK_LIB([glog],[openlog],[],[AC_MSG_ERROR(
+             [Please install google-glog library])])
+AC_CHECK_LIB([gflags],[getenv],[],[AC_MSG_ERROR(
+             [Please install google-gflags library])])
+
+# check for boost libs
+AX_BOOST_BASE([1.20.0], [], [AC_MSG_ERROR(
+              [Please install boost >= 1.20.0 (thread, regex, and system)])])
+AX_BOOST_THREAD
+AX_BOOST_REGEX
+AX_BOOST_SYSTEM
+
+# Checks for header files.
+AC_HEADER_STDC
+AC_CHECK_HEADERS([fcntl.h features.h inttypes.h limits.h stdint.h stdlib.h string.h sys/time.h unistd.h mutex.h malloc.h emmintrin.h byteswap.h bits/functexcept.h])
+
+AC_CHECK_HEADER(double-conversion/double-conversion.h, [], [AC_MSG_ERROR(
+                [Couldn't find double-conversion.h, please download from \
+                http://code.google.com/p/double-conversion/])], [])
+AC_CHECK_LIB([double-conversion],[ceil],[],[AC_MSG_ERROR(
+             [Please install double-conversion library])])
+
+AC_CHECK_LIB([event], [event_set], [], [AC_MSG_ERROR([Unable to find libevent])])
+
+# Checks for typedefs, structures, and compiler characteristics.
+AC_HEADER_STDBOOL
+AC_C_CONST
+AC_C_INLINE
+AC_TYPE_SIZE_T
+AC_HEADER_TIME
+AC_C_VOLATILE
+AC_CHECK_TYPE([__int128],
+  [AC_DEFINE([HAVE_INT128_T], [1], [Define if __int128 exists])],
+  [AC_DEFINE([HAVE_INT128_T], [0], [Define if __int128 does not exist])])
+AC_CHECK_TYPES([ptrdiff_t])
+AC_COMPILE_IFELSE(
+  [AC_LANG_SOURCE[
+    #pragma GCC diagnostic error "-Wattributes"
+    extern "C" void (*test_ifunc(void))() { return 0; }
+    void func() __attribute__((ifunc("test_ifunc")));]
+  ],
+  [AC_DEFINE([HAVE_IFUNC], [1], [Define to 1 if the compiler supports ifunc])],
+  [AC_DEFINE([HAVE_IFUNC], [0], [Define to 0 if the compiler doesn't support ifunc])]
+)
+AC_COMPILE_IFELSE(
+  [AC_LANG_SOURCE[class C { virtual void f() final {} virtual void g() {} };
+                  class D : public C { virtual void g() override {} };]],
+  [AC_DEFINE([FINAL], [final],
+             [Define to "final" if the compiler supports C++11 "final"])
+   AC_DEFINE([OVERRIDE], [override],
+             [Define to "override" if the compiler supports C++11 "override"])],
+  [AC_DEFINE([FINAL], [],
+             [Define to "final" if the compiler supports C++11 "final"])
+   AC_DEFINE([OVERRIDE], [],
+             [Define to "override" if the compiler supports C++11 "override"])]
+)
+
+AC_COMPILE_IFELSE(
+  [AC_LANG_SOURCE[
+    #include <thread>
+    #include <chrono>
+    void func() { std::this_thread::sleep_for(std::chrono::seconds(1)); }]],
+  [AC_DEFINE([HAVE_STD__THIS_THREAD__SLEEP_FOR], [1],
+             [Define to 1 if std::this_thread::sleep_for() is defined.])])
+
+AC_COMPILE_IFELSE(
+  [AC_LANG_SOURCE[
+    #include <cstring>
+    static constexpr int val = strlen("foo");]],
+  [AC_DEFINE([HAVE_CONSTEXPR_STRLEN], [1],
+             [Define to 1 if strlen(3) is constexpr.])])
+
+AC_COMPILE_IFELSE(
+  [AC_LANG_SOURCE[
+    #include <type_traits>
+    #if !_LIBCPP_VERSION
+    #error No libc++
+    #endif
+    void func() {}]
+  ],
+  [AC_DEFINE([USE_LIBCPP], [1], [Define to 1 if we're using libc++.])])
+
+AC_COMPILE_IFELSE(
+  [AC_LANG_SOURCE[
+    #include <type_traits>
+    const bool val = std::is_trivially_copyable<bool>::value;]
+  ],
+  [AC_DEFINE([HAVE_STD__IS_TRIVIALLY_COPYABLE], [1],
+             [Define to 1 if we have a usable std::is_trivially_copyable<T>
+              implementation.])])
+
+# Figure out if we support weak symbols. If not, we will link in some null
+# stubs for functions that would otherwise be weak.
+AC_LINK_IFELSE(
+  [AC_LANG_SOURCE[
+    extern "C" void configure_link_extern_weak_test() __attribute__((weak));
+    int main(int argc, char** argv) {
+        return configure_link_extern_weak_test == nullptr;
+    }]
+  ],
+  [
+    ac_have_weak_symbols="yes"
+    AC_DEFINE([HAVE_WEAK_SYMBOLS], [1],
+              [Define to 1 if the linker supports weak symbols.])])
+
+AC_SEARCH_LIBS([cplus_demangle_v3_callback], [iberty])
+if test "$ac_cv_search_cplus_demangle_v3_callback" != "no" ; then
+  AC_DEFINE([HAVE_CPLUS_DEMANGLE_V3_CALLBACK], [1],
+            [Define to 1 if we have cplus_demangle_v3_callback.])
+fi
+
+# Check for clock_gettime(2). This is not in an AC_CHECK_FUNCS() because we
+# want to link with librt if necessary.
+AC_SEARCH_LIBS([clock_gettime], [rt],
+  AC_DEFINE(
+    [HAVE_CLOCK_GETTIME],
+    [1],
+    [Define to 1 if we support clock_gettime(2).]),
+  [])
+
+# Checks for library functions.
+AC_CHECK_FUNCS([getdelim \
+                gettimeofday \
+                memmove \
+                memset \
+                pow \
+                strerror \
+                pthread_yield \
+                rallocm \
+                malloc_size \
+                malloc_usable_size \
+                memrchr])
+
+if test "$ac_cv_func_pthread_yield" = "no"; then
+   AC_CHECK_HEADERS([sched.h])
+   AC_CHECK_FUNCS([sched_yield])
+fi
+
+AC_SUBST(AM_CPPFLAGS, '-I../$(top_srcdir)'" "'-I$(top_srcdir)/io'" "'-I$(top_srcdir)/test'" $BOOST_CPPFLAGS")
+AC_SUBST(AM_LDFLAGS, "$BOOST_LDFLAGS $BOOST_THREAD_LIB $BOOST_SYSTEM_LIB $BOOST_REGEX_LIB -lpthread")
+
+AM_CONDITIONAL([HAVE_STD_THREAD], [test "$ac_cv_header_features" = "yes"])
+AM_CONDITIONAL([HAVE_X86_64], [test "$build_cpu" = "x86_64"])
+AM_CONDITIONAL([HAVE_LINUX], [test "$build_os" == "linux-gnu"])
+AM_CONDITIONAL([HAVE_WEAK_SYMBOLS], [test "$ac_have_weak_symbols" = "yes"])
+AM_CONDITIONAL([HAVE_BITS_FUNCTEXCEPT], [test "$ac_cv_header_bits_functexcept" = "yes"])
+
+# Output
+AC_CONFIG_FILES([Makefile
+                 test/Makefile
+                 test/function_benchmark/Makefile])
+AC_OUTPUT
--- /dev/null
+++ b/hphp/submodules/folly/folly/Conv.cpp
@@ -0,0 +1,142 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#define FOLLY_CONV_INTERNAL
+#include "folly/Conv.h"
+
+namespace folly {
+namespace detail {
+
+extern const char digit1[101] =
+  "00000000001111111111222222222233333333334444444444"
+  "55555555556666666666777777777788888888889999999999";
+extern const char digit2[101] =
+  "01234567890123456789012345678901234567890123456789"
+  "01234567890123456789012345678901234567890123456789";
+
+template <> const char *const MaxString<bool>::value = "true";
+template <> const char *const MaxString<uint8_t>::value = "255";
+template <> const char *const MaxString<uint16_t>::value = "65535";
+template <> const char *const MaxString<uint32_t>::value = "4294967295";
+#if __SIZEOF_LONG__ == 4
+template <> const char *const MaxString<unsigned long>::value =
+  "4294967295";
+#else
+template <> const char *const MaxString<unsigned long>::value =
+  "18446744073709551615";
+#endif
+static_assert(sizeof(unsigned long) >= 4,
+              "Wrong value for MaxString<unsigned long>::value,"
+              " please update.");
+template <> const char *const MaxString<unsigned long long>::value =
+  "18446744073709551615";
+static_assert(sizeof(unsigned long long) >= 8,
+              "Wrong value for MaxString<unsigned long long>::value"
+              ", please update.");
+
+/* Test for GCC >= 3.6.0 */
+#if __GNUC__ > 3 || (__GNUC__ == 3 && (__GNUC_MINOR__ >= 6))
+template <> const char *const MaxString<__uint128_t>::value =
+  "340282366920938463463374607431768211455";
+#endif
+
+inline bool bool_str_cmp(const char** b, size_t len, const char* value) {
+  // Can't use strncasecmp, since we want to ensure that the full value matches
+  const char* p = *b;
+  const char* e = *b + len;
+  const char* v = value;
+  while (*v != '\0') {
+    if (p == e || tolower(*p) != *v) { // value is already lowercase
+      return false;
+    }
+    ++p;
+    ++v;
+  }
+
+  *b = p;
+  return true;
+}
+
+bool str_to_bool(StringPiece* src) {
+  auto b = src->begin(), e = src->end();
+  for (;; ++b) {
+    FOLLY_RANGE_CHECK(b < e,
+                      "No non-whitespace characters found in input string");
+    if (!isspace(*b)) break;
+  }
+
+  bool result;
+  size_t len = e - b;
+  switch (*b) {
+    case '0':
+    case '1': {
+      // Attempt to parse the value as an integer
+      StringPiece tmp(*src);
+      uint8_t value = to<uint8_t>(&tmp);
+      // Only accept 0 or 1
+      FOLLY_RANGE_CHECK(value <= 1,
+                        "Integer overflow when parsing bool: must be 0 or 1");
+      b = tmp.begin();
+      result = (value == 1);
+      break;
+    }
+    case 'y':
+    case 'Y':
+      result = true;
+      if (!bool_str_cmp(&b, len, "yes")) {
+        ++b;  // accept the single 'y' character
+      }
+      break;
+    case 'n':
+    case 'N':
+      result = false;
+      if (!bool_str_cmp(&b, len, "no")) {
+        ++b;
+      }
+      break;
+    case 't':
+    case 'T':
+      result = true;
+      if (!bool_str_cmp(&b, len, "true")) {
+        ++b;
+      }
+      break;
+    case 'f':
+    case 'F':
+      result = false;
+      if (!bool_str_cmp(&b, len, "false")) {
+        ++b;
+      }
+      break;
+    case 'o':
+    case 'O':
+      if (bool_str_cmp(&b, len, "on")) {
+        result = true;
+      } else if (bool_str_cmp(&b, len, "off")) {
+        result = false;
+      } else {
+        FOLLY_RANGE_CHECK(false, "Invalid value for bool");
+      }
+      break;
+    default:
+      FOLLY_RANGE_CHECK(false, "Invalid value for bool");
+  }
+
+  src->assign(b, e);
+  return result;
+}
+
+} // namespace detail
+} // namespace folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/Conv.h
@@ -0,0 +1,1139 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Converts anything to anything, with an emphasis on performance and
+ * safety.
+ *
+ * @author Andrei Alexandrescu (andrei.alexandrescu@fb.com)
+ */
+
+#ifndef FOLLY_BASE_CONV_H_
+#define FOLLY_BASE_CONV_H_
+
+#include "folly/FBString.h"
+#include "folly/Likely.h"
+#include "folly/Preprocessor.h"
+#include "folly/Range.h"
+
+#include <boost/implicit_cast.hpp>
+#include <type_traits>
+#include <limits>
+#include <string>
+#include <tuple>
+#include <stdexcept>
+#include <typeinfo>
+
+#include <limits.h>
+
+// V8 JavaScript implementation
+#include <double-conversion/double-conversion.h>
+
+#define FOLLY_RANGE_CHECK(condition, message)                           \
+  ((condition) ? (void)0 : throw std::range_error(                      \
+    (__FILE__ "(" + std::to_string((long long int) __LINE__) + "): "    \
+     + (message)).c_str()))
+
+namespace folly {
+
+/*******************************************************************************
+ * Integral to integral
+ ******************************************************************************/
+
+/**
+ * Checked conversion from integral to integral. The checks are only
+ * performed when meaningful, e.g. conversion from int to long goes
+ * unchecked.
+ */
+template <class Tgt, class Src>
+typename std::enable_if<
+  std::is_integral<Src>::value && std::is_integral<Tgt>::value,
+  Tgt>::type
+to(const Src & value) {
+  /* static */ if (std::numeric_limits<Tgt>::max()
+                   < std::numeric_limits<Src>::max()) {
+    FOLLY_RANGE_CHECK(
+      (!greater_than<Tgt, std::numeric_limits<Tgt>::max()>(value)),
+      "Overflow"
+    );
+  }
+  /* static */ if (std::is_signed<Src>::value &&
+                   (!std::is_signed<Tgt>::value || sizeof(Src) > sizeof(Tgt))) {
+    FOLLY_RANGE_CHECK(
+      (!less_than<Tgt, std::numeric_limits<Tgt>::min()>(value)),
+      "Negative overflow"
+    );
+  }
+  return static_cast<Tgt>(value);
+}
+
+/*******************************************************************************
+ * Floating point to floating point
+ ******************************************************************************/
+
+template <class Tgt, class Src>
+typename std::enable_if<
+  std::is_floating_point<Tgt>::value && std::is_floating_point<Src>::value,
+  Tgt>::type
+to(const Src & value) {
+  /* static */ if (std::numeric_limits<Tgt>::max() <
+                   std::numeric_limits<Src>::max()) {
+    FOLLY_RANGE_CHECK(value <= std::numeric_limits<Tgt>::max(),
+                      "Overflow");
+    FOLLY_RANGE_CHECK(value >= -std::numeric_limits<Tgt>::max(),
+                      "Negative overflow");
+  }
+  return boost::implicit_cast<Tgt>(value);
+}
+
+/*******************************************************************************
+ * Anything to string
+ ******************************************************************************/
+
+namespace detail {
+
+template <class T>
+const T& getLastElement(const T & v) {
+  return v;
+}
+
+template <class T, class... Ts>
+typename std::tuple_element<
+  sizeof...(Ts),
+  std::tuple<T, Ts...> >::type const&
+  getLastElement(const T& v, const Ts&... vs) {
+  return getLastElement(vs...);
+}
+
+// This class exists to specialize away std::tuple_element in the case where we
+// have 0 template arguments. Without this, Clang/libc++ will blow a
+// static_assert even if tuple_element is protected by an enable_if.
+template <class... Ts>
+struct last_element {
+  typedef typename std::enable_if<
+    sizeof...(Ts) >= 1,
+    typename std::tuple_element<
+      sizeof...(Ts) - 1, std::tuple<Ts...>
+    >::type>::type type;
+};
+
+template <>
+struct last_element<> {
+  typedef void type;
+};
+
+} // namespace detail
+
+/*******************************************************************************
+ * Conversions from integral types to string types.
+ ******************************************************************************/
+
+#if FOLLY_HAVE_INT128_T
+namespace detail {
+
+template <typename IntegerType>
+constexpr unsigned int
+digitsEnough() {
+  return ceil((double(sizeof(IntegerType) * CHAR_BIT) * M_LN2) / M_LN10);
+}
+
+inline unsigned int
+unsafeTelescope128(char * buffer, unsigned int room, unsigned __int128 x) {
+  typedef unsigned __int128 Usrc;
+  unsigned int p = room - 1;
+
+  while (x >= (Usrc(1) << 64)) { // Using 128-bit division while needed
+    const auto y = x / 10;
+    const auto digit = x % 10;
+
+    buffer[p--] = '0' + digit;
+    x = y;
+  }
+
+  uint64_t xx = x; // Moving to faster 64-bit division thereafter
+
+  while (xx >= 10) {
+    const auto y = xx / 10ULL;
+    const auto digit = xx % 10ULL;
+
+    buffer[p--] = '0' + digit;
+    xx = y;
+  }
+
+  buffer[p] = '0' + xx;
+
+  return p;
+}
+
+}
+#endif
+
+/**
+ * Returns the number of digits in the base 10 representation of an
+ * uint64_t. Useful for preallocating buffers and such. It's also used
+ * internally, see below. Measurements suggest that defining a
+ * separate overload for 32-bit integers is not worthwhile.
+ */
+
+inline uint32_t digits10(uint64_t v) {
+  uint32_t result = 1;
+  for (;;) {
+    if (LIKELY(v < 10)) return result;
+    if (LIKELY(v < 100)) return result + 1;
+    if (LIKELY(v < 1000)) return result + 2;
+    if (LIKELY(v < 10000)) return result + 3;
+    // Skip ahead by 4 orders of magnitude
+    v /= 10000U;
+    result += 4;
+  }
+}
+
+/**
+ * Copies the ASCII base 10 representation of v into buffer and
+ * returns the number of bytes written. Does NOT append a \0. Assumes
+ * the buffer points to digits10(v) bytes of valid memory. Note that
+ * uint64 needs at most 20 bytes, uint32_t needs at most 10 bytes,
+ * uint16_t needs at most 5 bytes, and so on. Measurements suggest
+ * that defining a separate overload for 32-bit integers is not
+ * worthwhile.
+ *
+ * This primitive is unsafe because it makes the size assumption and
+ * because it does not add a terminating \0.
+ */
+
+inline uint32_t uint64ToBufferUnsafe(uint64_t v, char *const buffer) {
+  auto const result = digits10(v);
+  // WARNING: using size_t or pointer arithmetic for pos slows down
+  // the loop below 20x. This is because several 32-bit ops can be
+  // done in parallel, but only fewer 64-bit ones.
+  uint32_t pos = result - 1;
+  while (v >= 10) {
+    // Keep these together so a peephole optimization "sees" them and
+    // computes them in one shot.
+    auto const q = v / 10;
+    auto const r = static_cast<uint32_t>(v % 10);
+    buffer[pos--] = '0' + r;
+    v = q;
+  }
+  // Last digit is trivial to handle
+  buffer[pos] = static_cast<uint32_t>(v) + '0';
+  return result;
+}
+
+/**
+ * A single char gets appended.
+ */
+template <class Tgt>
+void toAppend(char value, Tgt * result) {
+  *result += value;
+}
+
+/**
+ * Ubiquitous helper template for writing string appenders
+ */
+template <class T> struct IsSomeString {
+  enum { value = std::is_same<T, std::string>::value
+         || std::is_same<T, fbstring>::value };
+};
+
+/**
+ * Everything implicitly convertible to const char* gets appended.
+ */
+template <class Tgt, class Src>
+typename std::enable_if<
+  std::is_convertible<Src, const char*>::value
+  && IsSomeString<Tgt>::value>::type
+toAppend(Src value, Tgt * result) {
+  // Treat null pointers like an empty string, as in:
+  // operator<<(std::ostream&, const char*).
+  const char* c = value;
+  if (c) {
+    result->append(value);
+  }
+}
+
+/**
+ * Strings get appended, too.
+ */
+template <class Tgt, class Src>
+typename std::enable_if<
+  IsSomeString<Src>::value && IsSomeString<Tgt>::value>::type
+toAppend(const Src& value, Tgt * result) {
+  result->append(value);
+}
+
+/**
+ * and StringPiece objects too
+ */
+template <class Tgt>
+typename std::enable_if<
+   IsSomeString<Tgt>::value>::type
+toAppend(StringPiece value, Tgt * result) {
+  result->append(value.data(), value.size());
+}
+
+/**
+ * There's no implicit conversion from fbstring to other string types,
+ * so make a specialization.
+ */
+template <class Tgt>
+typename std::enable_if<
+   IsSomeString<Tgt>::value>::type
+toAppend(const fbstring& value, Tgt * result) {
+  result->append(value.data(), value.size());
+}
+
+#if FOLLY_HAVE_INT128_T
+/**
+ * Special handling for 128 bit integers.
+ */
+
+template <class Tgt>
+void
+toAppend(__int128 value, Tgt * result) {
+  typedef unsigned __int128 Usrc;
+  char buffer[detail::digitsEnough<unsigned __int128>() + 1];
+  unsigned int p;
+
+  if (value < 0) {
+    p = detail::unsafeTelescope128(buffer, sizeof(buffer), Usrc(-value));
+    buffer[--p] = '-';
+  } else {
+    p = detail::unsafeTelescope128(buffer, sizeof(buffer), value);
+  }
+
+  result->append(buffer + p, buffer + sizeof(buffer));
+}
+
+template <class Tgt>
+void
+toAppend(unsigned __int128 value, Tgt * result) {
+  char buffer[detail::digitsEnough<unsigned __int128>()];
+  unsigned int p;
+
+  p = detail::unsafeTelescope128(buffer, sizeof(buffer), value);
+
+  result->append(buffer + p, buffer + sizeof(buffer));
+}
+
+#endif
+
+/**
+ * int32_t and int64_t to string (by appending) go through here. The
+ * result is APPENDED to a preexisting string passed as the second
+ * parameter. This should be efficient with fbstring because fbstring
+ * incurs no dynamic allocation below 23 bytes and no number has more
+ * than 22 bytes in its textual representation (20 for digits, one for
+ * sign, one for the terminating 0).
+ */
+template <class Tgt, class Src>
+typename std::enable_if<
+  std::is_integral<Src>::value && std::is_signed<Src>::value &&
+  IsSomeString<Tgt>::value && sizeof(Src) >= 4>::type
+toAppend(Src value, Tgt * result) {
+  char buffer[20];
+  if (value < 0) {
+    result->push_back('-');
+    result->append(buffer, uint64ToBufferUnsafe(-uint64_t(value), buffer));
+  } else {
+    result->append(buffer, uint64ToBufferUnsafe(value, buffer));
+  }
+}
+
+/**
+ * As above, but for uint32_t and uint64_t.
+ */
+template <class Tgt, class Src>
+typename std::enable_if<
+  std::is_integral<Src>::value && !std::is_signed<Src>::value
+  && IsSomeString<Tgt>::value && sizeof(Src) >= 4>::type
+toAppend(Src value, Tgt * result) {
+  char buffer[20];
+  result->append(buffer, buffer + uint64ToBufferUnsafe(value, buffer));
+}
+
+/**
+ * All small signed and unsigned integers to string go through 32-bit
+ * types int32_t and uint32_t, respectively.
+ */
+template <class Tgt, class Src>
+typename std::enable_if<
+  std::is_integral<Src>::value
+  && IsSomeString<Tgt>::value && sizeof(Src) < 4>::type
+toAppend(Src value, Tgt * result) {
+  typedef typename
+    std::conditional<std::is_signed<Src>::value, int64_t, uint64_t>::type
+    Intermediate;
+  toAppend<Tgt>(static_cast<Intermediate>(value), result);
+}
+
+#if defined(__clang__) || __GNUC_PREREQ(4, 7)
+// std::underlying_type became available by gcc 4.7.0
+
+/**
+ * Enumerated values get appended as integers.
+ */
+template <class Tgt, class Src>
+typename std::enable_if<
+  std::is_enum<Src>::value && IsSomeString<Tgt>::value>::type
+toAppend(Src value, Tgt * result) {
+  toAppend(
+      static_cast<typename std::underlying_type<Src>::type>(value), result);
+}
+
+#else
+
+/**
+ * Enumerated values get appended as integers.
+ */
+template <class Tgt, class Src>
+typename std::enable_if<
+  std::is_enum<Src>::value && IsSomeString<Tgt>::value>::type
+toAppend(Src value, Tgt * result) {
+  /* static */ if (Src(-1) < 0) {
+    /* static */ if (sizeof(Src) <= sizeof(int)) {
+      toAppend(static_cast<int>(value), result);
+    } else {
+      toAppend(static_cast<long>(value), result);
+    }
+  } else {
+    /* static */ if (sizeof(Src) <= sizeof(int)) {
+      toAppend(static_cast<unsigned int>(value), result);
+    } else {
+      toAppend(static_cast<unsigned long>(value), result);
+    }
+  }
+}
+
+#endif // gcc 4.7 onwards
+
+/*******************************************************************************
+ * Conversions from floating-point types to string types.
+ ******************************************************************************/
+
+/** Wrapper around DoubleToStringConverter **/
+template <class Tgt, class Src>
+typename std::enable_if<
+  std::is_floating_point<Src>::value
+  && IsSomeString<Tgt>::value>::type
+toAppend(
+  Src value,
+  Tgt * result,
+  double_conversion::DoubleToStringConverter::DtoaMode mode,
+  unsigned int numDigits) {
+  using namespace double_conversion;
+  DoubleToStringConverter
+    conv(DoubleToStringConverter::NO_FLAGS,
+         "infinity", "NaN", 'E',
+         -6,  // decimal in shortest low
+         21,  // decimal in shortest high
+         6,   // max leading padding zeros
+         1);  // max trailing padding zeros
+  char buffer[256];
+  StringBuilder builder(buffer, sizeof(buffer));
+  switch (mode) {
+    case DoubleToStringConverter::SHORTEST:
+      conv.ToShortest(value, &builder);
+      break;
+    case DoubleToStringConverter::FIXED:
+      conv.ToFixed(value, numDigits, &builder);
+      break;
+    default:
+      CHECK(mode == DoubleToStringConverter::PRECISION);
+      conv.ToPrecision(value, numDigits, &builder);
+      break;
+  }
+  const size_t length = builder.position();
+  builder.Finalize();
+  result->append(buffer, length);
+}
+
+/**
+ * As above, but for floating point
+ */
+template <class Tgt, class Src>
+typename std::enable_if<
+  std::is_floating_point<Src>::value
+  && IsSomeString<Tgt>::value>::type
+toAppend(Src value, Tgt * result) {
+  toAppend(
+    value, result, double_conversion::DoubleToStringConverter::SHORTEST, 0);
+}
+
+/**
+ * Variadic conversion to string. Appends each element in turn.
+ */
+template <class T, class... Ts>
+typename std::enable_if<sizeof...(Ts) >= 2
+  && IsSomeString<
+  typename std::remove_pointer<
+    typename detail::last_element<Ts...>::type
+  >::type>::value>::type
+toAppend(const T& v, const Ts&... vs) {
+  toAppend(v, detail::getLastElement(vs...));
+  toAppend(vs...);
+}
+
+/**
+ * Variadic base case: do nothing.
+ */
+template <class Tgt>
+typename std::enable_if<IsSomeString<Tgt>::value>::type
+toAppend(Tgt* result) {
+}
+
+/**
+ * Variadic base case: do nothing.
+ */
+template <class Delimiter, class Tgt>
+typename std::enable_if<IsSomeString<Tgt>::value>::type
+toAppendDelim(const Delimiter& delim, Tgt* result) {
+}
+
+/**
+ * 1 element: same as toAppend.
+ */
+template <class Delimiter, class T, class Tgt>
+typename std::enable_if<IsSomeString<Tgt>::value>::type
+toAppendDelim(const Delimiter& delim, const T& v, Tgt* tgt) {
+  toAppend(v, tgt);
+}
+
+/**
+ * Append to string with a delimiter in between elements.
+ */
+template <class Delimiter, class T, class... Ts>
+typename std::enable_if<sizeof...(Ts) >= 2
+  && IsSomeString<
+  typename std::remove_pointer<
+    typename detail::last_element<Ts...>::type
+  >::type>::value>::type
+toAppendDelim(const Delimiter& delim, const T& v, const Ts&... vs) {
+  toAppend(v, delim, detail::getLastElement(vs...));
+  toAppendDelim(delim, vs...);
+}
+
+/**
+ * to<SomeString>(v1, v2, ...) uses toAppend() (see below) as back-end
+ * for all types.
+ */
+template <class Tgt, class... Ts>
+typename std::enable_if<IsSomeString<Tgt>::value, Tgt>::type
+to(const Ts&... vs) {
+  Tgt result;
+  toAppend(vs..., &result);
+  return result;
+}
+
+/**
+ * toDelim<SomeString>(delim, v1, v2, ...) uses toAppendDelim() as
+ * back-end for all types.
+ */
+template <class Tgt, class Delim, class... Ts>
+typename std::enable_if<IsSomeString<Tgt>::value, Tgt>::type
+toDelim(const Delim& delim, const Ts&... vs) {
+  Tgt result;
+  toAppendDelim(delim, vs..., &result);
+  return result;
+}
+
+/*******************************************************************************
+ * Conversions from string types to integral types.
+ ******************************************************************************/
+
+namespace detail {
+
+/**
+ * Finds the first non-digit in a string. The number of digits
+ * searched depends on the precision of the Tgt integral. Assumes the
+ * string starts with NO whitespace and NO sign.
+ *
+ * The semantics of the routine is:
+ *   for (;; ++b) {
+ *     if (b >= e || !isdigit(*b)) return b;
+ *   }
+ *
+ *  Complete unrolling marks bottom-line (i.e. entire conversion)
+ *  improvements of 20%.
+ */
+  template <class Tgt>
+  const char* findFirstNonDigit(const char* b, const char* e) {
+    for (; b < e; ++b) {
+      auto const c = static_cast<unsigned>(*b) - '0';
+      if (c >= 10) break;
+    }
+    return b;
+  }
+
+  // Maximum value of number when represented as a string
+  template <class T> struct MaxString {
+    static const char*const value;
+  };
+
+
+/*
+ * Lookup tables that converts from a decimal character value to an integral
+ * binary value, shifted by a decimal "shift" multiplier.
+ * For all character values in the range '0'..'9', the table at those
+ * index locations returns the actual decimal value shifted by the multiplier.
+ * For all other values, the lookup table returns an invalid OOR value.
+ */
+// Out-of-range flag value, larger than the largest value that can fit in
+// four decimal bytes (9999), but four of these added up together should
+// still not overflow uint16_t.
+constexpr int32_t OOR = 10000;
+
+__attribute__((aligned(16))) constexpr uint16_t shift1[] = {
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 0-9
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  10
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  20
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  30
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, 0,         //  40
+  1, 2, 3, 4, 5, 6, 7, 8, 9, OOR, OOR,
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  60
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  70
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  80
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  90
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 100
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 110
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 120
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 130
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 140
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 150
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 160
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 170
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 180
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 190
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 200
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 210
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 220
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 230
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 240
+  OOR, OOR, OOR, OOR, OOR, OOR                       // 250
+};
+
+__attribute__((aligned(16))) constexpr uint16_t shift10[] = {
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 0-9
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  10
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  20
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  30
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, 0,         //  40
+  10, 20, 30, 40, 50, 60, 70, 80, 90, OOR, OOR,
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  60
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  70
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  80
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  90
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 100
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 110
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 120
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 130
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 140
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 150
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 160
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 170
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 180
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 190
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 200
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 210
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 220
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 230
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 240
+  OOR, OOR, OOR, OOR, OOR, OOR                       // 250
+};
+
+__attribute__((aligned(16))) constexpr uint16_t shift100[] = {
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 0-9
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  10
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  20
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  30
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, 0,         //  40
+  100, 200, 300, 400, 500, 600, 700, 800, 900, OOR, OOR,
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  60
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  70
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  80
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  90
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 100
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 110
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 120
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 130
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 140
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 150
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 160
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 170
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 180
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 190
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 200
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 210
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 220
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 230
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 240
+  OOR, OOR, OOR, OOR, OOR, OOR                       // 250
+};
+
+__attribute__((aligned(16))) constexpr uint16_t shift1000[] = {
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 0-9
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  10
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  20
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  30
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, 0,         //  40
+  1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, OOR, OOR,
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  60
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  70
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  80
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  //  90
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 100
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 110
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 120
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 130
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 140
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 150
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 160
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 170
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 180
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 190
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 200
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 210
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 220
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 230
+  OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR, OOR,  // 240
+  OOR, OOR, OOR, OOR, OOR, OOR                       // 250
+};
+
+/**
+ * String represented as a pair of pointers to char to unsigned
+ * integrals. Assumes NO whitespace before or after, and also that the
+ * string is composed entirely of digits. Tgt must be unsigned, and no
+ * sign is allowed in the string (even it's '+'). String may be empty,
+ * in which case digits_to throws.
+ */
+  template <class Tgt>
+  Tgt digits_to(const char * b, const char * e) {
+
+    static_assert(!std::is_signed<Tgt>::value, "Unsigned type expected");
+    assert(b <= e);
+
+    const size_t size = e - b;
+
+    /* Although the string is entirely made of digits, we still need to
+     * check for overflow.
+     */
+    if (size >= std::numeric_limits<Tgt>::digits10 + 1) {
+      // Leading zeros? If so, recurse to keep things simple
+      if (b < e && *b == '0') {
+        for (++b;; ++b) {
+          if (b == e) return 0; // just zeros, e.g. "0000"
+          if (*b != '0') return digits_to<Tgt>(b, e);
+        }
+      }
+      FOLLY_RANGE_CHECK(size == std::numeric_limits<Tgt>::digits10 + 1 &&
+                        strncmp(b, detail::MaxString<Tgt>::value, size) <= 0,
+                        "Numeric overflow upon conversion");
+    }
+
+    // Here we know that the number won't overflow when
+    // converted. Proceed without checks.
+
+    Tgt result = 0;
+
+    for (; e - b >= 4; b += 4) {
+      result *= 10000;
+      const int32_t r0 = shift1000[static_cast<size_t>(b[0])];
+      const int32_t r1 = shift100[static_cast<size_t>(b[1])];
+      const int32_t r2 = shift10[static_cast<size_t>(b[2])];
+      const int32_t r3 = shift1[static_cast<size_t>(b[3])];
+      const auto sum = r0 + r1 + r2 + r3;
+      assert(sum < OOR && "Assumption: string only has digits");
+      result += sum;
+    }
+
+    switch (e - b) {
+      case 3: {
+        const int32_t r0 = shift100[static_cast<size_t>(b[0])];
+        const int32_t r1 = shift10[static_cast<size_t>(b[1])];
+        const int32_t r2 = shift1[static_cast<size_t>(b[2])];
+        const auto sum = r0 + r1 + r2;
+        assert(sum < OOR && "Assumption: string only has digits");
+        return result * 1000 + sum;
+      }
+      case 2: {
+        const int32_t r0 = shift10[static_cast<size_t>(b[0])];
+        const int32_t r1 = shift1[static_cast<size_t>(b[1])];
+        const auto sum = r0 + r1;
+        assert(sum < OOR && "Assumption: string only has digits");
+        return result * 100 + sum;
+      }
+      case 1: {
+        const int32_t sum = shift1[static_cast<size_t>(b[0])];
+        assert(sum < OOR && "Assumption: string only has digits");
+        return result * 10 + sum;
+      }
+    }
+
+    assert(b == e);
+    FOLLY_RANGE_CHECK(size > 0, "Found no digits to convert in input");
+    return result;
+  }
+
+
+  bool str_to_bool(StringPiece * src);
+
+}                                 // namespace detail
+
+/**
+ * String represented as a pair of pointers to char to unsigned
+ * integrals. Assumes NO whitespace before or after.
+ */
+template <class Tgt>
+typename std::enable_if<
+  std::is_integral<Tgt>::value && !std::is_signed<Tgt>::value
+  && !std::is_same<typename std::remove_cv<Tgt>::type, bool>::value,
+  Tgt>::type
+to(const char * b, const char * e) {
+  return detail::digits_to<Tgt>(b, e);
+}
+
+/**
+ * String represented as a pair of pointers to char to signed
+ * integrals. Assumes NO whitespace before or after. Allows an
+ * optional leading sign.
+ */
+template <class Tgt>
+typename std::enable_if<
+  std::is_integral<Tgt>::value && std::is_signed<Tgt>::value,
+  Tgt>::type
+to(const char * b, const char * e) {
+  FOLLY_RANGE_CHECK(b < e, "Empty input string in conversion to integral");
+  if (!isdigit(*b)) {
+    if (*b == '-') {
+      Tgt result = -to<typename std::make_unsigned<Tgt>::type>(b + 1, e);
+      FOLLY_RANGE_CHECK(result <= 0, "Negative overflow.");
+      return result;
+    }
+    FOLLY_RANGE_CHECK(*b == '+', "Invalid lead character");
+    ++b;
+  }
+  Tgt result = to<typename std::make_unsigned<Tgt>::type>(b, e);
+  FOLLY_RANGE_CHECK(result >= 0, "Overflow.");
+  return result;
+}
+
+/**
+ * Parsing strings to integrals. These routines differ from
+ * to<integral>(string) in that they take a POINTER TO a StringPiece
+ * and alter that StringPiece to reflect progress information.
+ */
+
+/**
+ * StringPiece to integrals, with progress information. Alters the
+ * StringPiece parameter to munch the already-parsed characters.
+ */
+template <class Tgt>
+typename std::enable_if<
+  std::is_integral<Tgt>::value
+  && !std::is_same<typename std::remove_cv<Tgt>::type, bool>::value,
+  Tgt>::type
+to(StringPiece * src) {
+
+  auto b = src->data(), past = src->data() + src->size();
+  for (;; ++b) {
+    FOLLY_RANGE_CHECK(b < past, "No digits found in input string");
+    if (!isspace(*b)) break;
+  }
+
+  auto m = b;
+
+  // First digit is customized because we test for sign
+  bool negative = false;
+  /* static */ if (std::is_signed<Tgt>::value) {
+    if (!isdigit(*m)) {
+      if (*m == '-') {
+        negative = true;
+      } else {
+        FOLLY_RANGE_CHECK(*m == '+', "Invalid leading character in conversion"
+                          " to integral");
+      }
+      ++b;
+      ++m;
+    }
+  }
+  FOLLY_RANGE_CHECK(m < past, "No digits found in input string");
+  FOLLY_RANGE_CHECK(isdigit(*m), "Non-digit character found");
+  m = detail::findFirstNonDigit<Tgt>(m + 1, past);
+
+  Tgt result;
+  /* static */ if (!std::is_signed<Tgt>::value) {
+    result = detail::digits_to<typename std::make_unsigned<Tgt>::type>(b, m);
+  } else {
+    auto t = detail::digits_to<typename std::make_unsigned<Tgt>::type>(b, m);
+    if (negative) {
+      result = -t;
+      FOLLY_RANGE_CHECK(is_non_positive(result), "Negative overflow");
+    } else {
+      result = t;
+      FOLLY_RANGE_CHECK(is_non_negative(result), "Overflow");
+    }
+  }
+  src->advance(m - src->data());
+  return result;
+}
+
+/**
+ * StringPiece to bool, with progress information. Alters the
+ * StringPiece parameter to munch the already-parsed characters.
+ */
+template <class Tgt>
+typename std::enable_if<
+  std::is_same<typename std::remove_cv<Tgt>::type, bool>::value,
+  Tgt>::type
+to(StringPiece * src) {
+  return detail::str_to_bool(src);
+}
+
+namespace detail {
+
+/**
+ * Enforce that the suffix following a number is made up only of whitespace.
+ */
+inline void enforceWhitespace(const char* b, const char* e) {
+  for (; b != e; ++b) {
+    FOLLY_RANGE_CHECK(isspace(*b), to<std::string>("Non-whitespace: ", *b));
+  }
+}
+
+}  // namespace detail
+
+/**
+ * String or StringPiece to integrals. Accepts leading and trailing
+ * whitespace, but no non-space trailing characters.
+ */
+template <class Tgt>
+typename std::enable_if<
+  std::is_integral<Tgt>::value,
+  Tgt>::type
+to(StringPiece src) {
+  Tgt result = to<Tgt>(&src);
+  detail::enforceWhitespace(src.data(), src.data() + src.size());
+  return result;
+}
+
+/*******************************************************************************
+ * Conversions from string types to floating-point types.
+ ******************************************************************************/
+
+/**
+ * StringPiece to double, with progress information. Alters the
+ * StringPiece parameter to munch the already-parsed characters.
+ */
+template <class Tgt>
+inline typename std::enable_if<
+  std::is_floating_point<Tgt>::value,
+  Tgt>::type
+to(StringPiece *const src) {
+  using namespace double_conversion;
+  static StringToDoubleConverter
+    conv(StringToDoubleConverter::ALLOW_TRAILING_JUNK
+         | StringToDoubleConverter::ALLOW_LEADING_SPACES,
+         0.0,
+         // return this for junk input string
+         std::numeric_limits<double>::quiet_NaN(),
+         nullptr, nullptr);
+
+  FOLLY_RANGE_CHECK(!src->empty(), "No digits found in input string");
+
+  int length;
+  auto result = conv.StringToDouble(src->data(), src->size(),
+                                       &length); // processed char count
+
+  if (!std::isnan(result)) {
+    src->advance(length);
+    return result;
+  }
+
+  for (;; src->advance(1)) {
+    if (src->empty()) {
+      throw std::range_error("Unable to convert an empty string"
+                             " to a floating point value.");
+    }
+    if (!isspace(src->front())) {
+      break;
+    }
+  }
+
+  // Was that "inf[inity]"?
+  if (src->size() >= 3 && toupper((*src)[0]) == 'I'
+        && toupper((*src)[1]) == 'N' && toupper((*src)[2]) == 'F') {
+    if (src->size() >= 8 &&
+        toupper((*src)[3]) == 'I' &&
+        toupper((*src)[4]) == 'N' &&
+        toupper((*src)[5]) == 'I' &&
+        toupper((*src)[6]) == 'T' &&
+        toupper((*src)[7]) == 'Y') {
+      src->advance(8);
+    } else {
+      src->advance(3);
+    }
+    return std::numeric_limits<Tgt>::infinity();
+  }
+
+  // Was that "-inf[inity]"?
+  if (src->size() >= 4 && toupper((*src)[0]) == '-'
+      && toupper((*src)[1]) == 'I' && toupper((*src)[2]) == 'N'
+      && toupper((*src)[3]) == 'F') {
+    if (src->size() >= 9 &&
+        toupper((*src)[4]) == 'I' &&
+        toupper((*src)[5]) == 'N' &&
+        toupper((*src)[6]) == 'I' &&
+        toupper((*src)[7]) == 'T' &&
+        toupper((*src)[8]) == 'Y') {
+      src->advance(9);
+    } else {
+      src->advance(4);
+    }
+    return -std::numeric_limits<Tgt>::infinity();
+  }
+
+  // "nan"?
+  if (src->size() >= 3 && toupper((*src)[0]) == 'N'
+        && toupper((*src)[1]) == 'A' && toupper((*src)[2]) == 'N') {
+    src->advance(3);
+    return std::numeric_limits<Tgt>::quiet_NaN();
+  }
+
+  // "-nan"?
+  if (src->size() >= 4 &&
+      toupper((*src)[0]) == '-' &&
+      toupper((*src)[1]) == 'N' &&
+      toupper((*src)[2]) == 'A' &&
+      toupper((*src)[3]) == 'N') {
+    src->advance(4);
+    return -std::numeric_limits<Tgt>::quiet_NaN();
+  }
+
+  // All bets are off
+  throw std::range_error("Unable to convert \"" + src->toString()
+                         + "\" to a floating point value.");
+}
+
+/**
+ * Any string, const char*, or StringPiece to double.
+ */
+template <class Tgt>
+typename std::enable_if<
+  std::is_floating_point<Tgt>::value,
+  Tgt>::type
+to(StringPiece src) {
+  Tgt result = to<double>(&src);
+  detail::enforceWhitespace(src.data(), src.data() + src.size());
+  return result;
+}
+
+/*******************************************************************************
+ * Integral to floating point and back
+ ******************************************************************************/
+
+/**
+ * Checked conversion from integral to flating point and back. The
+ * result must be convertible back to the source type without loss of
+ * precision. This seems Draconian but sometimes is what's needed, and
+ * complements existing routines nicely. For various rounding
+ * routines, see <math>.
+ */
+template <class Tgt, class Src>
+typename std::enable_if<
+  (std::is_integral<Src>::value && std::is_floating_point<Tgt>::value)
+  ||
+  (std::is_floating_point<Src>::value && std::is_integral<Tgt>::value),
+  Tgt>::type
+to(const Src & value) {
+  Tgt result = value;
+  auto witness = static_cast<Src>(result);
+  if (value != witness) {
+    throw std::range_error(
+      to<std::string>("to<>: loss of precision when converting ", value,
+                      " to type ", typeid(Tgt).name()).c_str());
+  }
+  return result;
+}
+
+/*******************************************************************************
+ * Enum to anything and back
+ ******************************************************************************/
+
+#if defined(__clang__) || __GNUC_PREREQ(4, 7)
+// std::underlying_type became available by gcc 4.7.0
+
+template <class Tgt, class Src>
+typename std::enable_if<std::is_enum<Src>::value, Tgt>::type
+to(const Src & value) {
+  return to<Tgt>(static_cast<typename std::underlying_type<Src>::type>(value));
+}
+
+template <class Tgt, class Src>
+typename std::enable_if<std::is_enum<Tgt>::value, Tgt>::type
+to(const Src & value) {
+  return static_cast<Tgt>(to<typename std::underlying_type<Tgt>::type>(value));
+}
+
+#else
+
+template <class Tgt, class Src>
+typename std::enable_if<std::is_enum<Src>::value, Tgt>::type
+to(const Src & value) {
+  /* static */ if (Src(-1) < 0) {
+    /* static */ if (sizeof(Src) <= sizeof(int)) {
+      return to<Tgt>(static_cast<int>(value));
+    } else {
+      return to<Tgt>(static_cast<long>(value));
+    }
+  } else {
+    /* static */ if (sizeof(Src) <= sizeof(int)) {
+      return to<Tgt>(static_cast<unsigned int>(value));
+    } else {
+      return to<Tgt>(static_cast<unsigned long>(value));
+    }
+  }
+}
+
+template <class Tgt, class Src>
+typename std::enable_if<std::is_enum<Tgt>::value, Tgt>::type
+to(const Src & value) {
+  /* static */ if (Tgt(-1) < 0) {
+    /* static */ if (sizeof(Tgt) <= sizeof(int)) {
+      return static_cast<Tgt>(to<int>(value));
+    } else {
+      return static_cast<Tgt>(to<long>(value));
+    }
+  } else {
+    /* static */ if (sizeof(Tgt) <= sizeof(int)) {
+      return static_cast<Tgt>(to<unsigned int>(value));
+    } else {
+      return static_cast<Tgt>(to<unsigned long>(value));
+    }
+  }
+}
+
+#endif // gcc 4.7 onwards
+
+} // namespace folly
+
+// FOLLY_CONV_INTERNAL is defined by Conv.cpp.  Keep the FOLLY_RANGE_CHECK
+// macro for use in Conv.cpp, but #undefine it everywhere else we are included,
+// to avoid defining this global macro name in other files that include Conv.h.
+#ifndef FOLLY_CONV_INTERNAL
+#undef FOLLY_RANGE_CHECK
+#endif
+
+#endif /* FOLLY_BASE_CONV_H_ */
--- /dev/null
+++ b/hphp/submodules/folly/folly/CPortability.h
@@ -0,0 +1,56 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef CPORTABILITY_H
+#define CPORTABILITY_H
+
+/* These definitions are in a separate file so that they
+ * may be included from C- as well as C++-based projects. */
+
+/* Define a convenience macro to test when address sanitizer is being used
+ * across the different compilers (e.g. clang, gcc) */
+#if defined(__clang__)
+# if __has_feature(address_sanitizer)
+#  define FOLLY_SANITIZE_ADDRESS 1
+# endif
+#elif defined (__GNUC__) && \
+      (((__GNUC__ == 4) && (__GNUC_MINOR__ >= 8)) || (__GNUC__ >= 5)) && \
+      __SANITIZE_ADDRESS__
+# define FOLLY_SANITIZE_ADDRESS 1
+#endif
+
+/* Define attribute wrapper for function attribute used to disable
+ * address sanitizer instrumentation. Unfortunately, this attribute
+ * has issues when inlining is used, so disable that as well. */
+#ifdef FOLLY_SANITIZE_ADDRESS
+# if defined(__clang__)
+#  if __has_attribute(__no_address_safety_analysis__)
+#   define FOLLY_DISABLE_ADDRESS_SANITIZER \
+      __attribute__((__no_address_safety_analysis__, __noinline__))
+#  elif __has_attribute(__no_sanitize_address__)
+#   define FOLLY_DISABLE_ADDRESS_SANITIZER \
+      __attribute__((__no_sanitize_address__, __noinline__))
+#  endif
+# elif defined(__GNUC__)
+#  define FOLLY_DISABLE_ADDRESS_SANITIZER \
+     __attribute__((__no_address_safety_analysis__, __noinline__))
+# endif
+#endif
+#ifndef FOLLY_DISABLE_ADDRESS_SANITIZER
+# define FOLLY_DISABLE_ADDRESS_SANITIZER
+#endif
+
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/CpuId.h
@@ -0,0 +1,119 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_CPUID_H_
+#define FOLLY_CPUID_H_
+
+#include <cstdint>
+
+namespace folly {
+
+/**
+ * Identification of an Intel CPU.
+ * Supports CPUID (EAX=1) feature flags.
+ * Values from http://www.intel.com/content/www/us/en/processors/processor-identification-cpuid-instruction-note.html
+ */
+class CpuId {
+ public:
+  CpuId() {
+#if defined(__x86_64__) || defined(__i386__)
+    __asm__("cpuid" : "=c"(c_), "=d"(d_) : "a"(1) : "ebx");
+#else
+    // On non-Intel, none of these features exist; at least not in the same form
+    // as they do on Intel
+    c_ = 0;
+    d_ = 0;
+#endif
+  }
+#define X(name, r, bit) bool name() const { return r & (1U << bit); }
+#define C(name, bit) X(name, c_, bit)
+#define D(name, bit) X(name, d_, bit)
+  C(sse3, 0)
+  C(pclmuldq, 1)
+  C(dtes64, 2)
+  C(monitor, 3)
+  C(dscpl, 4)
+  C(vmx, 5)
+  C(smx, 6)
+  C(eist, 7)
+  C(tm2, 8)
+  C(ssse3, 9)
+  C(cnxtid, 10)
+  // 11 is reserved
+  C(fma, 12)
+  C(cx16, 13)
+  C(xtpr, 14)
+  C(pdcm, 15)
+  // 16 is reserved
+  C(pcid, 17)
+  C(dca, 18)
+  C(sse41, 19)
+  C(sse42, 20)
+  C(x2apic, 21)
+  C(movbe, 22)
+  C(popcnt, 23)
+  C(tscdeadline, 24)
+  C(aes, 25)
+  C(xsave, 26)
+  C(osxsave, 27)
+  C(avx, 28)
+  C(f16c, 29)
+  C(rdrand, 30)
+  // 31 is not used
+  D(fpu, 0)
+  D(vme, 1)
+  D(de, 2)
+  D(pse, 3)
+  D(tsc, 4)
+  D(msr, 5)
+  D(pae, 6)
+  D(mce, 7)
+  D(cx8, 8)
+  D(apic, 9)
+  // 10 is reserved
+  D(sep, 11)
+  D(mtrr, 12)
+  D(pge, 13)
+  D(mca, 14)
+  D(cmov, 15)
+  D(pat, 16)
+  D(pse36, 17)
+  D(psn, 18)
+  D(clfsh, 19)
+  // 20 is reserved
+  D(ds, 21)
+  D(acpi, 22)
+  D(mmx, 23)
+  D(fxsr, 24)
+  D(sse, 25)
+  D(sse2, 26)
+  D(ss, 27)
+  D(htt, 28)
+  D(tm, 29)
+  // 30 is reserved
+  D(pbe, 31)
+#undef D
+#undef C
+#undef X
+ private:
+  uint32_t c_;  // ECX
+  uint32_t d_;  // EDX
+};
+
+}  // namespace folly
+
+#endif /* FOLLY_CPUID_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/Demangle.cpp
@@ -0,0 +1,142 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Demangle.h"
+
+#include <algorithm>
+#include <string.h>
+
+#include "folly/Malloc.h"
+
+#if FOLLY_HAVE_CPLUS_DEMANGLE_V3_CALLBACK
+# include <cxxabi.h>
+
+// From libiberty
+//
+// TODO(tudorb): Detect this with autoconf for the open-source version.
+//
+// __attribute__((weak)) doesn't work, because cplus_demangle_v3_callback
+// is exported by an object file in libiberty.a, and the ELF spec says
+// "The link editor does not extract archive members to resolve undefined weak
+// symbols" (but, interestingly enough, will resolve undefined weak symbols
+// with definitions from archive members that were extracted in order to
+// resolve an undefined global (strong) symbol)
+
+# ifndef DMGL_NO_OPTS
+#  define FOLLY_DEFINED_DMGL 1
+#  define DMGL_NO_OPTS    0          /* For readability... */
+#  define DMGL_PARAMS     (1 << 0)   /* Include function args */
+#  define DMGL_ANSI       (1 << 1)   /* Include const, volatile, etc */
+#  define DMGL_JAVA       (1 << 2)   /* Demangle as Java rather than C++. */
+#  define DMGL_VERBOSE    (1 << 3)   /* Include implementation details.  */
+#  define DMGL_TYPES      (1 << 4)   /* Also try to demangle type encodings.  */
+#  define DMGL_RET_POSTFIX (1 << 5)  /* Print function return types (when
+                                        present) after function signature */
+# endif
+
+extern "C" int cplus_demangle_v3_callback(
+    const char* mangled,
+    int options,  // We use DMGL_PARAMS | DMGL_TYPES, aka 0x11
+    void (*callback)(const char*, size_t, void*),
+    void* arg);
+
+#endif
+
+namespace {
+
+// glibc doesn't have strlcpy
+size_t my_strlcpy(char* dest, const char* src, size_t size) {
+  size_t len = strlen(src);
+  if (size != 0) {
+    size_t n = std::min(len, size - 1);  // always null terminate!
+    memcpy(dest, src, n);
+    dest[n] = '\0';
+  }
+  return len;
+}
+
+}  // namespace
+
+namespace folly {
+
+#if FOLLY_HAVE_CPLUS_DEMANGLE_V3_CALLBACK
+
+fbstring demangle(const char* name) {
+  int status;
+  size_t len = 0;
+  // malloc() memory for the demangled type name
+  char* demangled = abi::__cxa_demangle(name, nullptr, &len, &status);
+  if (status != 0) {
+    return name;
+  }
+  // len is the length of the buffer (including NUL terminator and maybe
+  // other junk)
+  return fbstring(demangled, strlen(demangled), len, AcquireMallocatedString());
+}
+
+namespace {
+
+struct DemangleBuf {
+  char* dest;
+  size_t remaining;
+  size_t total;
+};
+
+void demangleCallback(const char* str, size_t size, void* p) {
+  DemangleBuf* buf = static_cast<DemangleBuf*>(p);
+  size_t n = std::min(buf->remaining, size);
+  memcpy(buf->dest, str, n);
+  buf->dest += n;
+  buf->remaining -= n;
+  buf->total += size;
+}
+
+}  // namespace
+
+size_t demangle(const char* name, char* out, size_t outSize) {
+  DemangleBuf dbuf;
+  dbuf.dest = out;
+  dbuf.remaining = outSize ? outSize - 1 : 0;   // leave room for null term
+  dbuf.total = 0;
+
+  // Unlike most library functions, this returns 1 on success and 0 on failure
+  int status = cplus_demangle_v3_callback(
+      name,
+      DMGL_PARAMS | DMGL_ANSI | DMGL_TYPES,
+      demangleCallback,
+      &dbuf);
+  if (status == 0) {  // failed, return original
+    return my_strlcpy(out, name, outSize);
+  }
+  if (outSize != 0) {
+    *dbuf.dest = '\0';
+  }
+  return dbuf.total;
+}
+
+#else
+
+fbstring demangle(const char* name) {
+  return name;
+}
+
+size_t demangle(const char* name, char* out, size_t outSize) {
+  return my_strlcpy(out, name, outSize);
+}
+
+#endif
+
+} // folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/Demangle.h
@@ -0,0 +1,62 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "folly/FBString.h"
+
+namespace folly {
+
+/**
+ * Return the demangled (prettyfied) version of a C++ type.
+ *
+ * This function tries to produce a human-readable type, but the type name will
+ * be returned unchanged in case of error or if demangling isn't supported on
+ * your system.
+ *
+ * Use for debugging -- do not rely on demangle() returning anything useful.
+ *
+ * This function may allocate memory (and therefore throw std::bad_alloc).
+ */
+fbstring demangle(const char* name);
+inline fbstring demangle(const std::type_info& type) {
+  return demangle(type.name());
+}
+
+/**
+ * Return the demangled (prettyfied) version of a C++ type in a user-provided
+ * buffer.
+ *
+ * The semantics are the same as for snprintf or strlcpy: bufSize is the size
+ * of the buffer, the string is always null-terminated, and the return value is
+ * the number of characters (not including the null terminator) that would have
+ * been written if the buffer was big enough. (So a return value >= bufSize
+ * indicates that the output was truncated)
+ *
+ * This function does not allocate memory and is async-signal-safe.
+ *
+ * Note that the underlying function for the fbstring-returning demangle is
+ * somewhat standard (abi::__cxa_demangle, which uses malloc), the underlying
+ * function for this version is less so (cplus_demangle_v3_callback from
+ * libiberty), so it is possible for the fbstring version to work, while this
+ * version returns the original, mangled name.
+ */
+size_t demangle(const char* name, char* buf, size_t bufSize);
+inline size_t demangle(const std::type_info& type, char* buf, size_t bufSize) {
+  return demangle(type.name(), buf, bufSize);
+}
+
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/detail/AtomicHashUtils.h
@@ -0,0 +1,37 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// Some utilities used by AtomicHashArray and AtomicHashMap
+//
+// Note: no include guard; different -inl.h files include this and
+// undef it more than once in a translation unit.
+
+#if !(defined(__x86__) || defined(__i386__) || defined(__x86_64__))
+#define FOLLY_SPIN_WAIT(condition)                \
+   for (int counter = 0; condition; ++counter) {  \
+     if (counter < 10000) continue;               \
+     pthread_yield();                             \
+   }
+#else
+#define FOLLY_SPIN_WAIT(condition)              \
+  for (int counter = 0; condition; ++counter) { \
+    if (counter < 10000) {                      \
+      asm volatile("pause");                    \
+      continue;                                 \
+    }                                           \
+    pthread_yield();                            \
+  }
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/detail/BitIteratorDetail.h
@@ -0,0 +1,93 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_DETAIL_BITITERATORDETAIL_H_
+#define FOLLY_DETAIL_BITITERATORDETAIL_H_
+
+#include <iterator>
+#include <type_traits>
+#include <boost/iterator/iterator_adaptor.hpp>
+
+namespace folly {
+
+template <class BaseIter> class BitIterator;
+
+namespace bititerator_detail {
+
+// Reference to a bit.
+// Templatize on both parent reference and value types to capture
+// const-ness correctly and to work with the case where Ref is a
+// reference-like type (not T&), just like our BitReference here.
+template <class Ref, class Value>
+class BitReference {
+ public:
+  BitReference(Ref r, size_t bit) : ref_(r), bit_(bit) { }
+
+  operator bool() const {
+    return ref_ & (one_ << bit_);
+  }
+
+  BitReference& operator=(bool b) {
+    if (b) {
+      set();
+    } else {
+      clear();
+    }
+    return *this;
+  }
+
+  void set() {
+    ref_ |= (one_ << bit_);
+  }
+
+  void clear() {
+    ref_ &= ~(one_ << bit_);
+  }
+
+  void flip() {
+    ref_ ^= (one_ << bit_);
+  }
+
+ private:
+  // shortcut to avoid writing static_cast everywhere
+  const static Value one_ = 1;
+
+  Ref ref_;
+  size_t bit_;
+};
+
+template <class BaseIter>
+struct BitIteratorBase {
+  static_assert(std::is_integral<typename BaseIter::value_type>::value,
+                "BitIterator may only be used with integral types");
+  typedef boost::iterator_adaptor<
+    BitIterator<BaseIter>,      // Derived
+    BaseIter,                   // Base
+    bool,                       // Value
+    boost::use_default,         // CategoryOrTraversal
+    bititerator_detail::BitReference<
+      typename std::iterator_traits<BaseIter>::reference,
+      typename std::iterator_traits<BaseIter>::value_type
+    >,  // Reference
+    ssize_t> type;
+};
+
+
+}  // namespace bititerator_detail
+}  // namespace folly
+
+#endif /* FOLLY_DETAIL_BITITERATORDETAIL_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/detail/BitsDetail.h
@@ -0,0 +1,47 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_DETAIL_BITSDETAIL_H_
+#define FOLLY_DETAIL_BITSDETAIL_H_
+
+namespace folly {
+namespace detail {
+
+// If we're targeting an architecture with popcnt support, use
+// __builtin_popcount directly, as it's presumably inlined.
+// If not, use runtime detection using __attribute__((ifunc))
+// (see Bits.cpp)
+#ifdef __POPCNT__
+
+inline int popcount(unsigned int x) {
+  return __builtin_popcount(x);
+}
+inline int popcountll(unsigned long long x) {
+  return __builtin_popcountll(x);
+}
+
+#else   /* !__POPCNT__ */
+
+int popcount(unsigned int x);
+int popcountll(unsigned long long x);
+
+#endif  /* !__POPCNT__ */
+
+}  // namespace detail
+}  // namespace folly
+
+#endif /* FOLLY_DETAIL_BITSDETAIL_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/detail/CacheLocality.cpp
@@ -0,0 +1,276 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/detail/CacheLocality.h"
+
+#define _GNU_SOURCE 1 // for RTLD_NOLOAD
+#include <dlfcn.h>
+#include <fstream>
+
+#include "folly/Conv.h"
+#include "folly/Exception.h"
+#include "folly/FileUtil.h"
+#include "folly/Format.h"
+#include "folly/ScopeGuard.h"
+
+namespace folly { namespace detail {
+
+///////////// CacheLocality
+
+/// Returns the best real CacheLocality information available
+static CacheLocality getSystemLocalityInfo() {
+  try {
+    return CacheLocality::readFromSysfs();
+  } catch (...) {
+    // keep trying
+  }
+
+  long numCpus = sysconf(_SC_NPROCESSORS_CONF);
+  if (numCpus <= 0) {
+    // This shouldn't happen, but if it does we should try to keep
+    // going.  We are probably not going to be able to parse /sys on
+    // this box either (although we will try), which means we are going
+    // to fall back to the SequentialThreadId splitter.  On my 16 core
+    // (x hyperthreading) dev box 16 stripes is enough to get pretty good
+    // contention avoidance with SequentialThreadId, and there is little
+    // improvement from going from 32 to 64.  This default gives us some
+    // wiggle room
+    numCpus = 32;
+  }
+  return CacheLocality::uniform(numCpus);
+}
+
+template <>
+const CacheLocality& CacheLocality::system<std::atomic>() {
+  static CacheLocality cache(getSystemLocalityInfo());
+  return cache;
+}
+
+// Each level of cache has sharing sets, which are the set of cpus
+// that share a common cache at that level.  These are available in a
+// hex bitset form (/sys/devices/system/cpu/cpu0/index0/shared_cpu_map,
+// for example).  They are also available in a human-readable list form,
+// as in /sys/devices/system/cpu/cpu0/index0/shared_cpu_list.  The list
+// is a comma-separated list of numbers and ranges, where the ranges are
+// a pair of decimal numbers separated by a '-'.
+//
+// To sort the cpus for optimum locality we don't really need to parse
+// the sharing sets, we just need a unique representative from the
+// equivalence class.  The smallest value works fine, and happens to be
+// the first decimal number in the file.  We load all of the equivalence
+// class information from all of the cpu*/index* directories, order the
+// cpus first by increasing last-level cache equivalence class, then by
+// the smaller caches.  Finally, we break ties with the cpu number itself.
+
+/// Returns the first decimal number in the string, or throws an exception
+/// if the string does not start with a number terminated by ',', '-',
+/// '\n', or eos.
+static ssize_t parseLeadingNumber(const std::string& line) {
+  auto raw = line.c_str();
+  char *end;
+  unsigned val = strtoul(raw, &end, 10);
+  if (end == raw || (*end != ',' && *end != '-' && *end != '\n')) {
+    throw std::runtime_error(to<std::string>(
+        "error parsing list '", line, "'").c_str());
+  }
+  return val;
+}
+
+CacheLocality CacheLocality::readFromSysfsTree(
+    const std::function<std::string(std::string)>& mapping) {
+  // number of equivalence classes per level
+  std::vector<size_t> numCachesByLevel;
+
+  // the list of cache equivalence classes, where equivalance classes
+  // are named by the smallest cpu in the class
+  std::vector<std::vector<size_t>> equivClassesByCpu;
+
+  std::vector<size_t> cpus;
+
+  while (true) {
+    auto cpu = cpus.size();
+    std::vector<size_t> levels;
+    for (size_t index = 0; ; ++index) {
+      auto dir = format("/sys/devices/system/cpu/cpu{}/cache/index{}/",
+                        cpu, index).str();
+      auto cacheType = mapping(dir + "type");
+      auto equivStr = mapping(dir + "shared_cpu_list");
+      if (cacheType.size() == 0 || equivStr.size() == 0) {
+        // no more caches
+        break;
+      }
+      if (cacheType[0] == 'I') {
+        // cacheType in { "Data", "Instruction", "Unified" }. skip icache
+        continue;
+      }
+      auto equiv = parseLeadingNumber(equivStr);
+      auto level = levels.size();
+      levels.push_back(equiv);
+
+      if (equiv == cpu) {
+        // we only want to count the equiv classes once, so we do it when
+        // we first encounter them
+        while (numCachesByLevel.size() <= level) {
+          numCachesByLevel.push_back(0);
+        }
+        numCachesByLevel[level]++;
+      }
+    }
+
+    if (levels.size() == 0) {
+      // no levels at all for this cpu, we must be done
+      break;
+    }
+    equivClassesByCpu.emplace_back(std::move(levels));
+    cpus.push_back(cpu);
+  }
+
+  if (cpus.size() == 0) {
+    throw std::runtime_error("unable to load cache sharing info");
+  }
+
+  std::sort(cpus.begin(), cpus.end(), [&](size_t lhs, size_t rhs) -> bool {
+    // sort first by equiv class of cache with highest index, direction
+    // doesn't matter.  If different cpus have different numbers of
+    // caches then this code might produce a sub-optimal ordering, but
+    // it won't crash
+    auto& lhsEquiv = equivClassesByCpu[lhs];
+    auto& rhsEquiv = equivClassesByCpu[rhs];
+    for (int i = std::min(lhsEquiv.size(), rhsEquiv.size()) - 1; i >= 0; --i) {
+      if (lhsEquiv[i] != rhsEquiv[i]) {
+        return lhsEquiv[i] < rhsEquiv[i];
+      }
+    }
+
+    // break ties deterministically by cpu
+    return lhs < rhs;
+  });
+
+  // the cpus are now sorted by locality, with neighboring entries closer
+  // to each other than entries that are far away.  For striping we want
+  // the inverse map, since we are starting with the cpu
+  std::vector<size_t> indexes(cpus.size());
+  for (int i = 0; i < cpus.size(); ++i) {
+    indexes[cpus[i]] = i;
+  }
+
+  return CacheLocality{
+      cpus.size(), std::move(numCachesByLevel), std::move(indexes) };
+}
+
+CacheLocality CacheLocality::readFromSysfs() {
+  return readFromSysfsTree([](std::string name) {
+    std::ifstream xi(name.c_str());
+    std::string rv;
+    std::getline(xi, rv);
+    return rv;
+  });
+}
+
+
+CacheLocality CacheLocality::uniform(size_t numCpus) {
+  CacheLocality rv;
+
+  rv.numCpus = numCpus;
+
+  // one cache shared by all cpus
+  rv.numCachesByLevel.push_back(numCpus);
+
+  // no permutations in locality index mapping
+  for (size_t cpu = 0; cpu < numCpus; ++cpu) {
+    rv.localityIndexByCpu.push_back(cpu);
+  }
+
+  return rv;
+}
+
+////////////// Getcpu
+
+/// Resolves the dynamically loaded symbol __vdso_getcpu, returning null
+/// on failure
+static Getcpu::Func loadVdsoGetcpu() {
+  void* h = dlopen("linux-vdso.so.1", RTLD_LAZY | RTLD_LOCAL | RTLD_NOLOAD);
+  if (h == nullptr) {
+    return nullptr;
+  }
+
+  auto func = Getcpu::Func(dlsym(h, "__vdso_getcpu"));
+  if (func == nullptr) {
+    // technically a null result could either be a failure or a successful
+    // lookup of a symbol with the null value, but the second can't actually
+    // happen for this symbol.  No point holding the handle forever if
+    // we don't need the code
+    dlclose(h);
+  }
+
+  return func;
+}
+
+Getcpu::Func Getcpu::vdsoFunc() {
+  static Func func = loadVdsoGetcpu();
+  return func;
+}
+
+/////////////// SequentialThreadId
+
+template<>
+std::atomic<size_t> SequentialThreadId<std::atomic>::prevId(0);
+
+template<>
+__thread size_t SequentialThreadId<std::atomic>::currentId(0);
+
+/////////////// AccessSpreader
+
+template<>
+const AccessSpreader<std::atomic>
+AccessSpreader<std::atomic>::stripeByCore(
+    CacheLocality::system<>().numCachesByLevel.front());
+
+template<>
+const AccessSpreader<std::atomic>
+AccessSpreader<std::atomic>::stripeByChip(
+    CacheLocality::system<>().numCachesByLevel.back());
+
+template<>
+AccessSpreaderArray<std::atomic,128>
+AccessSpreaderArray<std::atomic,128>::sharedInstance = {};
+
+/// Always claims to be on CPU zero, node zero
+static int degenerateGetcpu(unsigned* cpu, unsigned* node, void* unused) {
+  if (cpu != nullptr) {
+    *cpu = 0;
+  }
+  if (node != nullptr) {
+    *node = 0;
+  }
+  return 0;
+}
+
+template<>
+Getcpu::Func AccessSpreader<std::atomic>::pickGetcpuFunc(size_t numStripes) {
+  if (numStripes == 1) {
+    // there's no need to call getcpu if there is only one stripe.
+    // This should not be common, so we don't want to waste a test and
+    // branch in the main code path, but we might as well use a faster
+    // function pointer
+    return &degenerateGetcpu;
+  } else {
+    auto best = Getcpu::vdsoFunc();
+    return best ? best : &SequentialThreadId<std::atomic>::getcpu;
+  }
+}
+
+} } // namespace folly::detail
--- /dev/null
+++ b/hphp/submodules/folly/folly/detail/CacheLocality.h
@@ -0,0 +1,366 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_DETAIL_CACHELOCALITY_H_
+#define FOLLY_DETAIL_CACHELOCALITY_H_
+
+#include <sched.h>
+#include <atomic>
+#include <cassert>
+#include <functional>
+#include <limits>
+#include <string>
+#include <type_traits>
+#include <vector>
+#include "folly/Likely.h"
+
+namespace folly { namespace detail {
+
+// This file contains several classes that might be useful if you are
+// trying to dynamically optimize cache locality: CacheLocality reads
+// cache sharing information from sysfs to determine how CPUs should be
+// grouped to minimize contention, Getcpu provides fast access to the
+// current CPU via __vdso_getcpu, and AccessSpreader uses these two to
+// optimally spread accesses among a predetermined number of stripes.
+//
+// AccessSpreader<>::current(n) microbenchmarks at 22 nanos, which is
+// substantially less than the cost of a cache miss.  This means that we
+// can effectively use it to reduce cache line ping-pong on striped data
+// structures such as IndexedMemPool or statistics counters.
+//
+// Because CacheLocality looks at all of the cache levels, it can be
+// used for different levels of optimization.  AccessSpreader(2) does
+// per-chip spreading on a dual socket system.  AccessSpreader(numCpus)
+// does perfect per-cpu spreading.  AccessSpreader(numCpus / 2) does
+// perfect L1 spreading in a system with hyperthreading enabled.
+
+struct CacheLocality {
+
+  /// 1 more than the maximum value that can be returned from sched_getcpu
+  /// or getcpu.  This is the number of hardware thread contexts provided
+  /// by the processors
+  size_t numCpus;
+
+  /// Holds the number of caches present at each cache level (0 is
+  /// the closest to the cpu).  This is the number of AccessSpreader
+  /// stripes needed to avoid cross-cache communication at the specified
+  /// layer.  numCachesByLevel.front() is the number of L1 caches and
+  /// numCachesByLevel.back() is the number of last-level caches.
+  std::vector<size_t> numCachesByLevel;
+
+  /// A map from cpu (from sched_getcpu or getcpu) to an index in the
+  /// range 0..numCpus-1, where neighboring locality indices are more
+  /// likely to share caches then indices far away.  All of the members
+  /// of a particular cache level be contiguous in their locality index.
+  /// For example, if numCpus is 32 and numCachesByLevel.back() is 2,
+  /// then cpus with a locality index < 16 will share one last-level
+  /// cache and cpus with a locality index >= 16 will share the other.
+  std::vector<size_t> localityIndexByCpu;
+
+
+  /// Returns the best CacheLocality information available for the current
+  /// system, cached for fast access.  This will be loaded from sysfs if
+  /// possible, otherwise it will be correct in the number of CPUs but
+  /// not in their sharing structure.
+  ///
+  /// If you are into yo dawgs, this is a shared cache of the local
+  /// locality of the shared caches.
+  ///
+  /// The template parameter here is used to allow injection of a
+  /// repeatable CacheLocality structure during testing.  Rather than
+  /// inject the type of the CacheLocality provider into every data type
+  /// that transitively uses it, all components select between the default
+  /// sysfs implementation and a deterministic implementation by keying
+  /// off the type of the underlying atomic.  See DeterministicScheduler.
+  template <template<typename> class Atom = std::atomic>
+  static const CacheLocality& system();
+
+
+  /// Reads CacheLocality information from a tree structured like
+  /// the sysfs filesystem.  The provided function will be evaluated
+  /// for each sysfs file that needs to be queried.  The function
+  /// should return a string containing the first line of the file
+  /// (not including the newline), or an empty string if the file does
+  /// not exist.  The function will be called with paths of the form
+  /// /sys/devices/system/cpu/cpu*/cache/index*/{type,shared_cpu_list} .
+  /// Throws an exception if no caches can be parsed at all.
+  static CacheLocality readFromSysfsTree(
+      const std::function<std::string(std::string)>& mapping);
+
+  /// Reads CacheLocality information from the real sysfs filesystem.
+  /// Throws an exception if no cache information can be loaded.
+  static CacheLocality readFromSysfs();
+
+  /// Returns a usable (but probably not reflective of reality)
+  /// CacheLocality structure with the specified number of cpus and a
+  /// single cache level that associates one cpu per cache.
+  static CacheLocality uniform(size_t numCpus);
+
+  enum {
+    /// Memory locations on the same cache line are subject to false
+    /// sharing, which is very bad for performance.  Microbenchmarks
+    /// indicate that pairs of cache lines also see interference under
+    /// heavy use of atomic operations (observed for atomic increment on
+    /// Sandy Bridge).  See FOLLY_ALIGN_TO_AVOID_FALSE_SHARING
+    kFalseSharingRange = 128
+  };
+
+  static_assert(kFalseSharingRange == 128,
+      "FOLLY_ALIGN_TO_AVOID_FALSE_SHARING should track kFalseSharingRange");
+};
+
+// TODO replace __attribute__ with alignas and 128 with kFalseSharingRange
+
+/// An attribute that will cause a variable or field to be aligned so that
+/// it doesn't have false sharing with anything at a smaller memory address.
+#define FOLLY_ALIGN_TO_AVOID_FALSE_SHARING __attribute__((aligned(128)))
+
+/// Holds a function pointer to the VDSO implementation of getcpu(2),
+/// if available
+struct Getcpu {
+  /// Function pointer to a function with the same signature as getcpu(2).
+  typedef int (*Func)(unsigned* cpu, unsigned* node, void* unused);
+
+  /// Returns a pointer to the VDSO implementation of getcpu(2), if
+  /// available, or nullptr otherwise
+  static Func vdsoFunc();
+};
+
+/// A class that lazily binds a unique (for each implementation of Atom)
+/// identifier to a thread.  This is a fallback mechanism for the access
+/// spreader if we are in testing (using DeterministicAtomic) or if
+/// __vdso_getcpu can't be dynamically loaded
+template <template<typename> class Atom>
+struct SequentialThreadId {
+
+  /// Returns the thread id assigned to the current thread
+  static size_t get() {
+    auto rv = currentId;
+    if (UNLIKELY(rv == 0)) {
+      rv = currentId = ++prevId;
+    }
+    return rv;
+  }
+
+  /// Fills the thread id into the cpu and node out params (if they
+  /// are non-null).  This method is intended to act like getcpu when a
+  /// fast-enough form of getcpu isn't available or isn't desired
+  static int getcpu(unsigned* cpu, unsigned* node, void* unused) {
+    auto id = get();
+    if (cpu) {
+      *cpu = id;
+    }
+    if (node) {
+      *node = id;
+    }
+    return 0;
+  }
+
+ private:
+  static Atom<size_t> prevId;
+
+  // TODO: switch to thread_local
+  static __thread size_t currentId;
+};
+
+template <template<typename> class Atom, size_t kMaxCpus>
+struct AccessSpreaderArray;
+
+/// AccessSpreader arranges access to a striped data structure in such a
+/// way that concurrently executing threads are likely to be accessing
+/// different stripes.  It does NOT guarantee uncontended access.
+/// Your underlying algorithm must be thread-safe without spreading, this
+/// is merely an optimization.  AccessSpreader::current(n) is typically
+/// much faster than a cache miss (22 nanos on my dev box, tested fast
+/// in both 2.6 and 3.2 kernels).
+///
+/// You are free to create your own AccessSpreader-s or to cache the
+/// results of AccessSpreader<>::shared(n), but you will probably want
+/// to use one of the system-wide shared ones.  Calling .current() on
+/// a particular AccessSpreader instance only saves about 1 nanosecond
+/// over calling AccessSpreader<>::shared(n).
+///
+/// If available (and not using the deterministic testing implementation)
+/// AccessSpreader uses the getcpu system call via VDSO and the
+/// precise locality information retrieved from sysfs by CacheLocality.
+/// This provides optimal anti-sharing at a fraction of the cost of a
+/// cache miss.
+///
+/// When there are not as many stripes as processors, we try to optimally
+/// place the cache sharing boundaries.  This means that if you have 2
+/// stripes and run on a dual-socket system, your 2 stripes will each get
+/// all of the cores from a single socket.  If you have 16 stripes on a
+/// 16 core system plus hyperthreading (32 cpus), each core will get its
+/// own stripe and there will be no cache sharing at all.
+///
+/// AccessSpreader has a fallback mechanism for when __vdso_getcpu can't be
+/// loaded, or for use during deterministic testing.  Using sched_getcpu or
+/// the getcpu syscall would negate the performance advantages of access
+/// spreading, so we use a thread-local value and a shared atomic counter
+/// to spread access out.
+///
+/// AccessSpreader is templated on the template type that is used
+/// to implement atomics, as a way to instantiate the underlying
+/// heuristics differently for production use and deterministic unit
+/// testing.  See DeterministicScheduler for more.  If you aren't using
+/// DeterministicScheduler, you can just use the default template parameter
+/// all of the time.
+template <template<typename> class Atom = std::atomic>
+struct AccessSpreader {
+
+  /// Returns a never-destructed shared AccessSpreader instance.
+  /// numStripes should be > 0.
+  static const AccessSpreader& shared(size_t numStripes) {
+    // sharedInstances[0] actually has numStripes == 1
+    assert(numStripes > 0);
+
+    // the last shared element handles all large sizes
+    return AccessSpreaderArray<Atom,kMaxCpus>::sharedInstance[
+        std::min(size_t(kMaxCpus), numStripes)];
+  }
+
+  /// Returns the stripe associated with the current CPU, assuming
+  /// that there are numStripes (non-zero) stripes.  Equivalent to
+  /// AccessSpreader::shared(numStripes)->current.
+  static size_t current(size_t numStripes) {
+    return shared(numStripes).current();
+  }
+
+  /// stripeByCore uses 1 stripe per L1 cache, according to
+  /// CacheLocality::system<>().  Use stripeByCore.numStripes() to see
+  /// its width, or stripeByCore.current() to get the current stripe
+  static const AccessSpreader stripeByCore;
+
+  /// stripeByChip uses 1 stripe per last-level cache, which is the fewest
+  /// number of stripes for which off-chip communication can be avoided
+  /// (assuming all caches are on-chip).  Use stripeByChip.numStripes()
+  /// to see its width, or stripeByChip.current() to get the current stripe
+  static const AccessSpreader stripeByChip;
+
+
+  /// Constructs an AccessSpreader that will return values from
+  /// 0 to numStripes-1 (inclusive), precomputing the mapping
+  /// from CPU to stripe.  There is no use in having more than
+  /// CacheLocality::system<Atom>().localityIndexByCpu.size() stripes or
+  /// kMaxCpus stripes
+  explicit AccessSpreader(size_t spreaderNumStripes,
+                          const CacheLocality& cacheLocality =
+                              CacheLocality::system<Atom>(),
+                          Getcpu::Func getcpuFunc = nullptr)
+    : getcpuFunc_(getcpuFunc ? getcpuFunc : pickGetcpuFunc(spreaderNumStripes))
+    , numStripes_(spreaderNumStripes)
+  {
+    auto n = cacheLocality.numCpus;
+    for (size_t cpu = 0; cpu < kMaxCpus && cpu < n; ++cpu) {
+      auto index = cacheLocality.localityIndexByCpu[cpu];
+      assert(index < n);
+      // as index goes from 0..n, post-transform value goes from
+      // 0..numStripes
+      stripeByCpu[cpu] = (index * numStripes_) / n;
+      assert(stripeByCpu[cpu] < numStripes_);
+    }
+    for (size_t cpu = n; cpu < kMaxCpus; ++cpu) {
+      stripeByCpu[cpu] = stripeByCpu[cpu - n];
+    }
+  }
+
+  /// Returns 1 more than the maximum value that can be returned from
+  /// current()
+  size_t numStripes() const {
+    return numStripes_;
+  }
+
+  /// Returns the stripe associated with the current CPU
+  size_t current() const {
+    unsigned cpu;
+    getcpuFunc_(&cpu, nullptr, nullptr);
+    return stripeByCpu[cpu % kMaxCpus];
+  }
+
+ private:
+
+  /// If there are more cpus than this nothing will crash, but there
+  /// might be unnecessary sharing
+  enum { kMaxCpus = 128 };
+
+  typedef uint8_t CompactStripe;
+
+  static_assert((kMaxCpus & (kMaxCpus - 1)) == 0,
+      "kMaxCpus should be a power of two so modulo is fast");
+  static_assert(kMaxCpus - 1 <= std::numeric_limits<CompactStripe>::max(),
+      "stripeByCpu element type isn't wide enough");
+
+
+  /// Points to the getcpu-like function we are using to obtain the
+  /// current cpu.  It should not be assumed that the returned cpu value
+  /// is in range.  We use a member for this instead of a static so that
+  /// this fetch preloads a prefix the stripeByCpu array
+  Getcpu::Func getcpuFunc_;
+
+  /// A precomputed map from cpu to stripe.  Rather than add a layer of
+  /// indirection requiring a dynamic bounds check and another cache miss,
+  /// we always precompute the whole array
+  CompactStripe stripeByCpu[kMaxCpus];
+
+  size_t numStripes_;
+
+  /// Returns the best getcpu implementation for this type and width
+  /// of AccessSpreader
+  static Getcpu::Func pickGetcpuFunc(size_t numStripes);
+};
+
+/// An array of kMaxCpus+1 AccessSpreader<Atom> instances constructed
+/// with default params, with the zero-th element having 1 stripe
+template <template<typename> class Atom, size_t kMaxStripe>
+struct AccessSpreaderArray {
+
+  AccessSpreaderArray() {
+    for (size_t i = 0; i <= kMaxStripe; ++i) {
+      new (raw + i) AccessSpreader<Atom>(std::max(size_t(1), i));
+    }
+  }
+
+  ~AccessSpreaderArray() {
+    for (size_t i = 0; i <= kMaxStripe; ++i) {
+      auto p = static_cast<AccessSpreader<Atom>*>(static_cast<void*>(raw + i));
+      p->~AccessSpreader();
+    }
+  }
+
+  AccessSpreader<Atom> const& operator[] (size_t index) const {
+    return *static_cast<AccessSpreader<Atom> const*>(
+        static_cast<void const*>(raw + index));
+  }
+
+ private:
+
+  // AccessSpreader uses sharedInstance
+  friend AccessSpreader<Atom>;
+
+  static AccessSpreaderArray<Atom,kMaxStripe> sharedInstance;
+
+
+  /// aligned_storage is uninitialized, we use placement new since there
+  /// is no AccessSpreader default constructor
+  typename std::aligned_storage<sizeof(AccessSpreader<Atom>),
+                                CacheLocality::kFalseSharingRange>::type
+      raw[kMaxStripe + 1];
+};
+
+} }
+
+#endif /* FOLLY_DETAIL_CacheLocality_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/detail/ChecksumDetail.h
@@ -0,0 +1,58 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_DETAIL_CHECKSUMDETAIL_H_
+#define FOLLY_DETAIL_CHECKSUMDETAIL_H_
+
+namespace folly { namespace detail {
+
+/**
+ * Compute a CRC-32C checksum of a buffer using a hardware-accelerated
+ * implementation.
+ *
+ * @note This function is exposed to support special cases where the
+ *       calling code is absolutely certain it ought to invoke a hardware-
+ *       accelerated CRC-32C implementation - unit tests, for example.  For
+ *       all other scenarios, please call crc32c() and let it pick an
+ *       implementation based on the capabilities of the underlying CPU.
+ */
+uint32_t crc32c_hw(const uint8_t* data, size_t nbytes,
+    uint32_t startingChecksum = ~0U);
+
+/**
+ * Check whether a hardware-accelerated CRC-32C implementation is
+ * supported on the current CPU.
+ */
+bool crc32c_hw_supported();
+
+/**
+ * Compute a CRC-32C checksum of a buffer using a portable,
+ * software-only implementation.
+ *
+ * @note This function is exposed to support special cases where the
+ *       calling code is absolutely certain it wants to use the software
+ *       implementation instead of the hardware-accelerated code - unit
+ *       tests, for example.  For all other scenarios, please call crc32c()
+ *       and let it pick an implementation based on the capabilities of
+ *       the underlying CPU.
+ */
+uint32_t crc32c_sw(const uint8_t* data, size_t nbytes,
+    uint32_t startingChecksum = ~0U);
+
+
+}} // folly::detail
+
+#endif /* FOLLY_DETAIL_CHECKSUMDETAIL_H_ */
--- /dev/null
+++ b/hphp/submodules/folly/folly/detail/Clock.cpp
@@ -0,0 +1,53 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/detail/Clock.h"
+
+#if __MACH__
+#include <errno.h>
+#include <mach/mach_time.h>
+
+static mach_timebase_info_data_t tb_info;
+static bool tb_init = mach_timebase_info(&tb_info) == KERN_SUCCESS;
+
+int clock_gettime(clockid_t clk_id, struct timespec* ts) {
+  if (!tb_init) {
+    errno = EINVAL;
+    return -1;
+  }
+
+  uint64_t now_ticks = mach_absolute_time();
+  uint64_t now_ns = (now_ticks * tb_info.numer) / tb_info.denom;
+  ts->tv_sec = now_ns / 1000000000;
+  ts->tv_nsec = now_ns % 1000000000;
+
+  return 0;
+}
+
+int clock_getres(clockid_t clk_id, struct timespec* ts) {
+  if (!tb_init) {
+    errno = EINVAL;
+    return -1;
+  }
+
+  ts->tv_sec = 0;
+  ts->tv_nsec = tb_info.numer / tb_info.denom;
+
+  return 0;
+}
+#else
+#error No clock_gettime(2) compatibility wrapper available for this platform.
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/detail/Clock.h
@@ -0,0 +1,38 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_DETAIL_CLOCK_H_
+#define FOLLY_DETAIL_CLOCK_H_
+
+#include <ctime>
+#include <cstdint>
+
+#ifndef FOLLY_NO_CONFIG
+#include "folly/folly-config.h"
+#endif
+
+#if FOLLY_HAVE_CLOCK_GETTIME
+#error This should only be used as a workaround for platforms \
+          that do not support clock_gettime(2).
+#endif
+
+typedef uint8_t clockid_t;
+#define CLOCK_REALTIME 0
+
+int clock_gettime(clockid_t clk_id, struct timespec* ts);
+int clock_getres(clockid_t clk_id, struct timespec* ts);
+
+#endif /* FOLLY_DETAIL_CLOCK_H_ */
--- /dev/null
+++ b/hphp/submodules/folly/folly/detail/DiscriminatedPtrDetail.h
@@ -0,0 +1,165 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_DETAIL_DISCRIMINATEDPTRDETAIL_H_
+#define FOLLY_DETAIL_DISCRIMINATEDPTRDETAIL_H_
+
+#include <type_traits>
+
+namespace folly {
+namespace dptr_detail {
+
+/**
+ * Given a target type and a list of types, return the 1-based index of the
+ * type in the list of types.  Fail to compile if the target type doesn't
+ * appear in the list.
+ *
+ * GetIndex<int, void, char, int>::value == 3
+ * GetIndex<int, void, char>::value -> fails to compile
+ */
+template <typename... Types> struct GetTypeIndex;
+
+// When recursing, we never reach the 0- or 1- template argument base case
+// unless the target type is not in the list.  If the target type is in the
+// list, we stop recursing when it is at the head of the remaining type
+// list via the GetTypeIndex<T, T, Types...> partial specialization.
+template <typename T, typename... Types>
+struct GetTypeIndex<T, T, Types...> {
+  static const size_t value = 1;
+};
+
+template <typename T, typename U, typename... Types>
+struct GetTypeIndex<T, U, Types...> {
+  static const size_t value = 1 + GetTypeIndex<T, Types...>::value;
+};
+
+// Generalize std::is_same for variable number of type arguments
+template <typename... Types>
+struct IsSameType;
+
+template <>
+struct IsSameType<> {
+  static const bool value = true;
+};
+
+template <typename T>
+struct IsSameType<T> {
+  static const bool value = true;
+};
+
+template <typename T, typename U, typename... Types>
+struct IsSameType<T, U, Types...> {
+  static const bool value =
+    std::is_same<T,U>::value && IsSameType<U, Types...>::value;
+};
+
+// Define type as the type of all T in (non-empty) Types..., asserting that
+// all types in Types... are the same.
+template <typename... Types>
+struct SameType;
+
+template <typename T, typename... Types>
+struct SameType<T, Types...> {
+  typedef T type;
+  static_assert(IsSameType<T, Types...>::value,
+                "Not all types in pack are the same");
+};
+
+// Determine the result type of applying a visitor of type V on a pointer
+// to type T.
+template <typename V, typename T>
+struct VisitorResult1 {
+  typedef typename std::result_of<V (T*)>::type type;
+};
+
+// Determine the result type of applying a visitor of type V on a const pointer
+// to type T.
+template <typename V, typename T>
+struct ConstVisitorResult1 {
+  typedef typename std::result_of<V (const T*)>::type type;
+};
+
+// Determine the result type of applying a visitor of type V on pointers of
+// all types in Types..., asserting that the type is the same for all types
+// in Types...
+template <typename V, typename... Types>
+struct VisitorResult {
+  typedef typename SameType<
+    typename VisitorResult1<V,Types>::type...>::type type;
+};
+
+// Determine the result type of applying a visitor of type V on const pointers
+// of all types in Types..., asserting that the type is the same for all types
+// in Types...
+template <typename V, typename... Types>
+struct ConstVisitorResult {
+  typedef typename SameType<
+    typename ConstVisitorResult1<V,Types>::type...>::type type;
+};
+
+template <typename V, typename R, typename... Types> struct ApplyVisitor1;
+
+template <typename V, typename R>
+struct ApplyVisitor1<V, R> {
+  R operator()(size_t index, V&& visitor, void* ptr) const {
+    CHECK(false);  // NOTREACHED
+  }
+};
+
+template <typename V, typename R, typename T, typename... Types>
+struct ApplyVisitor1<V, R, T, Types...> {
+  R operator()(size_t index, V&& visitor, void* ptr) const {
+    return (index == 1 ? visitor(static_cast<T*>(ptr)) :
+            ApplyVisitor1<V, R, Types...>()(
+              index - 1, std::forward<V>(visitor), ptr));
+  }
+};
+
+template <typename V, typename R, typename... Types> struct ApplyConstVisitor1;
+
+template <typename V, typename R>
+struct ApplyConstVisitor1<V, R> {
+  R operator()(size_t index, V&& visitor, void* ptr) const {
+    CHECK(false);  // NOTREACHED
+  }
+};
+
+template <typename V, typename R, typename T, typename... Types>
+struct ApplyConstVisitor1<V, R, T, Types...> {
+  R operator()(size_t index, V&& visitor, void* ptr) const {
+    return (index == 1 ? visitor(static_cast<const T*>(ptr)) :
+            ApplyConstVisitor1<V, R, Types...>()(
+              index - 1, std::forward<V>(visitor), ptr));
+  }
+};
+
+template <typename V, typename... Types>
+struct ApplyVisitor
+  : ApplyVisitor1<
+      V, typename VisitorResult<V, Types...>::type, Types...> {
+};
+
+template <typename V, typename... Types>
+struct ApplyConstVisitor
+  : ApplyConstVisitor1<
+      V, typename ConstVisitorResult<V, Types...>::type, Types...> {
+};
+
+}  // namespace dptr_detail
+}  // namespace folly
+
+#endif /* FOLLY_DETAIL_DISCRIMINATEDPTRDETAIL_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/detail/FileUtilDetail.h
@@ -0,0 +1,112 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_DETAIL_FILEUTILDETAIL_H_
+#define FOLLY_DETAIL_FILEUTILDETAIL_H_
+
+#include <cerrno>
+#include <unistd.h>
+
+#include <sys/uio.h>
+
+/**
+ * Helper functions and templates for FileUtil.cpp.  Declared here so
+ * they can be unittested.
+ */
+namespace folly { namespace fileutil_detail {
+
+// Wrap call to f(args) in loop to retry on EINTR
+template<class F, class... Args>
+ssize_t wrapNoInt(F f, Args... args) {
+  ssize_t r;
+  do {
+    r = f(args...);
+  } while (r == -1 && errno == EINTR);
+  return r;
+}
+
+inline void incr(ssize_t n) { }
+inline void incr(ssize_t n, off_t& offset) { offset += n; }
+
+// Wrap call to read/pread/write/pwrite(fd, buf, count, offset?) to retry on
+// incomplete reads / writes.  The variadic argument magic is there to support
+// an additional argument (offset) for pread / pwrite; see the incr() functions
+// above which do nothing if the offset is not present and increment it if it
+// is.
+template <class F, class... Offset>
+ssize_t wrapFull(F f, int fd, void* buf, size_t count, Offset... offset) {
+  char* b = static_cast<char*>(buf);
+  ssize_t totalBytes = 0;
+  ssize_t r;
+  do {
+    r = f(fd, b, count, offset...);
+    if (r == -1) {
+      if (errno == EINTR) {
+        continue;
+      }
+      return r;
+    }
+
+    totalBytes += r;
+    b += r;
+    count -= r;
+    incr(r, offset...);
+  } while (r != 0 && count);  // 0 means EOF
+
+  return totalBytes;
+}
+
+// Wrap call to readv/preadv/writev/pwritev(fd, iov, count, offset?) to
+// retry on incomplete reads / writes.
+template <class F, class... Offset>
+ssize_t wrapvFull(F f, int fd, iovec* iov, int count, Offset... offset) {
+  ssize_t totalBytes = 0;
+  ssize_t r;
+  do {
+    r = f(fd, iov, count, offset...);
+    if (r == -1) {
+      if (errno == EINTR) {
+        continue;
+      }
+      return r;
+    }
+
+    if (r == 0) {
+      break;  // EOF
+    }
+
+    totalBytes += r;
+    incr(r, offset...);
+    while (r != 0 && count != 0) {
+      if (r >= iov->iov_len) {
+        r -= iov->iov_len;
+        ++iov;
+        --count;
+      } else {
+        iov->iov_base = static_cast<char*>(iov->iov_base) + r;
+        iov->iov_len -= r;
+        r = 0;
+      }
+    }
+  } while (count);
+
+  return totalBytes;
+}
+
+}}  // namespaces
+
+#endif /* FOLLY_DETAIL_FILEUTILDETAIL_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/detail/FingerprintPolynomial.h
@@ -0,0 +1,146 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_BUILD_FINGERPRINTPOLYNOMIAL_H_
+#define FOLLY_BUILD_FINGERPRINTPOLYNOMIAL_H_
+
+#include <cstdint>
+
+namespace folly {
+namespace detail {
+
+/**
+ * Representation of a polynomial of degree DEG over GF(2) (that is,
+ * with binary coefficients).
+ *
+ * Probably of no use outside of Fingerprint code; used by
+ * GenerateFingerprintTables and the unittest.
+ */
+template <int DEG>
+class FingerprintPolynomial {
+ public:
+  FingerprintPolynomial() {
+    for (int i = 0; i < size(); i++) {
+      val_[i] = 0;
+    }
+  }
+
+  explicit FingerprintPolynomial(const uint64_t* vals) {
+    for (int i = 0; i < size(); i++) {
+      val_[i] = vals[i];
+    }
+  }
+
+  void write(uint64_t* out) const {
+    for (int i = 0; i < size(); i++) {
+      out[i] = val_[i];
+    }
+  }
+
+  void add(const FingerprintPolynomial<DEG>& other) {
+    for (int i = 0; i < size(); i++) {
+      val_[i] ^= other.val_[i];
+    }
+  }
+
+  // Multiply by X.  The actual degree must be < DEG.
+  void mulX() {
+    CHECK_EQ(0, val_[0] & (1UL<<63));
+    uint64_t b = 0;
+    for (int i = size()-1; i >= 0; i--) {
+      uint64_t nb = val_[i] >> 63;
+      val_[i] = (val_[i] << 1) | b;
+      b = nb;
+    }
+  }
+
+  // Compute (this * X) mod P(X), where P(X) is a monic polynomial of degree
+  // DEG+1 (represented as a FingerprintPolynomial<DEG> object, with the
+  // implicit coefficient of X^(DEG+1)==1)
+  //
+  // This is a bit tricky. If k=DEG+1:
+  // Let P(X) = X^k + p_(k-1) * X^(k-1) + ... + p_1 * X + p_0
+  // Let this = A(X) = a_(k-1) * X^(k-1) + ... + a_1 * X + a_0
+  // Then:
+  //   A(X) * X
+  // = a_(k-1) * X^k + (a_(k-2) * X^(k-1) + ... + a_1 * X^2 + a_0 * X)
+  // = a_(k-1) * X^k + (the binary representation of A, left shift by 1)
+  //
+  // if a_(k-1) = 0, we can ignore the first term.
+  // if a_(k-1) = 1, then:
+  //   X^k mod P(X)
+  // = X^k - P(X)
+  // = P(X) - X^k
+  // = p_(k-1) * X^(k-1) + ... + p_1 * X + p_0
+  // = exactly the binary representation passed in as an argument to this
+  //   function!
+  //
+  // So A(X) * X mod P(X) is:
+  //   the binary representation of A, left shift by 1,
+  //   XOR p if a_(k-1) == 1
+  void mulXmod(const FingerprintPolynomial<DEG>& p) {
+    bool needXOR = (val_[0] & (1UL<<63));
+    val_[0] &= ~(1UL<<63);
+    mulX();
+    if (needXOR) {
+      add(p);
+    }
+  }
+
+  // Compute (this * X^k) mod P(X) by repeatedly multiplying by X (see above)
+  void mulXkmod(int k, const FingerprintPolynomial<DEG>& p) {
+    for (int i = 0; i < k; i++) {
+      mulXmod(p);
+    }
+  }
+
+  // add X^k, where k <= DEG
+  void addXk(int k) {
+    DCHECK_GE(k, 0);
+    DCHECK_LE(k, DEG);
+    int word_offset = (DEG - k) / 64;
+    int bit_offset = 63 - (DEG - k) % 64;
+    val_[word_offset] ^= (1UL << bit_offset);
+  }
+
+  // Set the highest 8 bits to val.
+  // If val is interpreted as polynomial of degree 7, then this sets *this
+  // to val * X^(DEG-7)
+  void setHigh8Bits(uint8_t val) {
+    val_[0] = ((uint64_t)val) << (64-8);
+    for (int i = 1; i < size(); i++) {
+      val_[i] = 0;
+    }
+  }
+
+  static int size() {
+    return 1 + DEG/64;
+  }
+ private:
+  // Internal representation: big endian
+  // val_[0] contains the highest order coefficients, with bit 63 as the
+  // highest order coefficient
+  //
+  // If DEG+1 is not a multiple of 64,  val_[size()-1] only uses the highest
+  // order (DEG+1)%64 bits (the others are always 0)
+  uint64_t val_[1 + DEG/64];
+};
+
+}  // namespace detail
+}  // namespace folly
+
+#endif /* FOLLY_BUILD_FINGERPRINTPOLYNOMIAL_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/detail/FunctionalExcept.cpp
@@ -0,0 +1,35 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/detail/FunctionalExcept.h"
+
+#include <stdexcept>
+
+FOLLY_NAMESPACE_STD_BEGIN
+
+void __throw_length_error(const char* msg) {
+  throw std::length_error(msg);
+}
+
+void __throw_logic_error(const char* msg) {
+  throw std::logic_error(msg);
+}
+
+void __throw_out_of_range(const char* msg) {
+  throw std::out_of_range(msg);
+}
+
+FOLLY_NAMESPACE_STD_END
--- /dev/null
+++ b/hphp/submodules/folly/folly/detail/FunctionalExcept.h
@@ -0,0 +1,30 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_DETAIL_FUNCTIONAL_EXCEPT_H
+#define FOLLY_DETAIL_FUNCTIONAL_EXCEPT_H
+
+#include "folly/Portability.h"
+
+FOLLY_NAMESPACE_STD_BEGIN
+
+void __throw_length_error(const char* msg) FOLLY_NORETURN;
+void __throw_logic_error(const char* msg) FOLLY_NORETURN;
+void __throw_out_of_range(const char* msg) FOLLY_NORETURN;
+
+FOLLY_NAMESPACE_STD_END
+
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/detail/Futex.cpp
@@ -0,0 +1,40 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <folly/detail/Futex.h>
+
+namespace folly { namespace detail {
+
+/* see Futex.h */
+FutexResult futexErrnoToFutexResult(int returnVal, int futexErrno) {
+  if (returnVal == 0) {
+    return FutexResult::AWOKEN;
+  }
+  switch(futexErrno) {
+    case ETIMEDOUT:
+      return FutexResult::TIMEDOUT;
+    case EINTR:
+      return FutexResult::INTERRUPTED;
+    case EWOULDBLOCK:
+      return FutexResult::VALUE_CHANGED;
+    default:
+      assert(false);
+      /* Shouldn't reach here. Just return one of the FutexResults */
+      return FutexResult::VALUE_CHANGED;
+  }
+}
+
+}}
--- /dev/null
+++ b/hphp/submodules/folly/folly/detail/Futex.h
@@ -0,0 +1,186 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <atomic>
+#include <chrono>
+#include <limits>
+#include <assert.h>
+#include <errno.h>
+#include <linux/futex.h>
+#include <sys/syscall.h>
+#include <unistd.h>
+#include <boost/noncopyable.hpp>
+
+using std::chrono::steady_clock;
+using std::chrono::system_clock;
+using std::chrono::time_point;
+
+namespace folly { namespace detail {
+
+enum class FutexResult {
+  VALUE_CHANGED, /* Futex value didn't match expected */
+  AWOKEN,        /* futex wait matched with a futex wake */
+  INTERRUPTED,   /* Spurious wake-up or signal caused futex wait failure */
+  TIMEDOUT
+};
+
+/* Converts return value and errno from a futex syscall to a FutexResult */
+FutexResult futexErrnoToFutexResult(int returnVal, int futexErrno);
+
+/**
+ * Futex is an atomic 32 bit unsigned integer that provides access to the
+ * futex() syscall on that value.  It is templated in such a way that it
+ * can interact properly with DeterministicSchedule testing.
+ *
+ * If you don't know how to use futex(), you probably shouldn't be using
+ * this class.  Even if you do know how, you should have a good reason
+ * (and benchmarks to back you up).
+ */
+template <template <typename> class Atom = std::atomic>
+struct Futex : Atom<uint32_t>, boost::noncopyable {
+
+  explicit Futex(uint32_t init = 0) : Atom<uint32_t>(init) {}
+
+  /** Puts the thread to sleep if this->load() == expected.  Returns true when
+   *  it is returning because it has consumed a wake() event, false for any
+   *  other return (signal, this->load() != expected, or spurious wakeup). */
+  bool futexWait(uint32_t expected, uint32_t waitMask = -1);
+
+  /** Similar to futexWait but also accepts a timeout that gives the time until
+   *  when the call can block (time is the absolute time i.e time since epoch).
+   *  Allowed clock types: std::chrono::system_clock, std::chrono::steady_clock.
+   *  Returns one of FutexResult values.
+   *
+   *  NOTE: On some systems steady_clock is just an alias for system_clock,
+   *  and is not actually steady.*/
+  template <class Clock, class Duration = typename Clock::duration>
+  FutexResult futexWaitUntil(uint32_t expected,
+                             const time_point<Clock, Duration>& absTime,
+                             uint32_t waitMask = -1);
+
+  /** Wakens up to count waiters where (waitMask & wakeMask) != 0,
+   *  returning the number of awoken threads. */
+  int futexWake(int count = std::numeric_limits<int>::max(),
+                uint32_t wakeMask = -1);
+
+  private:
+
+  /** Futex wait implemented via syscall SYS_futex. absTimeout gives
+   *  time till when the wait can block. If it is nullptr the call will
+   *  block until a matching futex wake is received. extraOpFlags can be
+   *  used to specify addtional flags to add to the futex operation (by
+   *  default only FUTEX_WAIT_BITSET and FUTEX_PRIVATE_FLAG are included).
+   *  Returns 0 on success or -1 on error, with errno set to one of the
+   *  values listed in futex(2). */
+  int futexWaitImpl(uint32_t expected,
+                    const struct timespec* absTimeout,
+                    int extraOpFlags,
+                    uint32_t waitMask);
+};
+
+template <>
+inline int
+Futex<std::atomic>::futexWaitImpl(uint32_t expected,
+                                  const struct timespec* absTimeout,
+                                  int extraOpFlags,
+                                  uint32_t waitMask) {
+  assert(sizeof(*this) == sizeof(int));
+
+  /* Unlike FUTEX_WAIT, FUTEX_WAIT_BITSET requires an absolute timeout
+   * value - http://locklessinc.com/articles/futex_cheat_sheet/ */
+  int rv = syscall(
+      SYS_futex,
+      this, /* addr1 */
+      FUTEX_WAIT_BITSET | FUTEX_PRIVATE_FLAG | extraOpFlags, /* op */
+      expected, /* val */
+      absTimeout, /* timeout */
+      nullptr, /* addr2 */
+      waitMask); /* val3 */
+
+  assert(rv == 0 ||
+         errno == EWOULDBLOCK ||
+         errno == EINTR ||
+         (absTimeout != nullptr && errno == ETIMEDOUT));
+
+  return rv;
+}
+
+template <>
+inline bool Futex<std::atomic>::futexWait(uint32_t expected,
+                                          uint32_t waitMask) {
+  return futexWaitImpl(expected, nullptr, 0 /* extraOpFlags */, waitMask) == 0;
+}
+
+template <>
+inline int Futex<std::atomic>::futexWake(int count, uint32_t wakeMask) {
+  assert(sizeof(*this) == sizeof(int));
+  int rv = syscall(SYS_futex,
+                   this, /* addr1 */
+                   FUTEX_WAKE_BITSET | FUTEX_PRIVATE_FLAG, /* op */
+                   count, /* val */
+                   nullptr, /* timeout */
+                   nullptr, /* addr2 */
+                   wakeMask); /* val3 */
+  assert(rv >= 0);
+  return rv;
+}
+
+/* Convert std::chrono::time_point to struct timespec */
+template <class Clock, class Duration = typename Clock::Duration>
+struct timespec timePointToTimeSpec(const time_point<Clock, Duration>& tp) {
+  using std::chrono::nanoseconds;
+  using std::chrono::seconds;
+  using std::chrono::duration_cast;
+
+  struct timespec ts;
+  auto duration = tp.time_since_epoch();
+  auto secs = duration_cast<seconds>(duration);
+  auto nanos = duration_cast<nanoseconds>(duration - secs);
+  ts.tv_sec = secs.count();
+  ts.tv_nsec = nanos.count();
+  return ts;
+}
+
+template <template<typename> class Atom> template<class Clock, class Duration>
+inline FutexResult
+Futex<Atom>::futexWaitUntil(
+               uint32_t expected,
+               const time_point<Clock, Duration>& absTime,
+               uint32_t waitMask) {
+
+  static_assert(std::is_same<Clock,system_clock>::value ||
+                std::is_same<Clock,steady_clock>::value,
+                "Only std::system_clock or std::steady_clock supported");
+
+  struct timespec absTimeSpec = timePointToTimeSpec(absTime);
+  int extraOpFlags = 0;
+
+  /* We must use FUTEX_CLOCK_REALTIME flag if we are getting the time_point
+   * from the system clock (CLOCK_REALTIME). This check also works correctly for
+   * broken glibc in which steady_clock is a typedef to system_clock.*/
+  if (std::is_same<Clock,system_clock>::value) {
+    extraOpFlags = FUTEX_CLOCK_REALTIME;
+  } else {
+    assert(Clock::is_steady);
+  }
+
+  const int rv = futexWaitImpl(expected, &absTimeSpec, extraOpFlags, waitMask);
+  return futexErrnoToFutexResult(rv, errno);
+}
+
+}}
--- /dev/null
+++ b/hphp/submodules/folly/folly/detail/GroupVarintDetail.h
@@ -0,0 +1,104 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_DETAIL_GROUPVARINTDETAIL_H_
+#define FOLLY_DETAIL_GROUPVARINTDETAIL_H_
+
+#include <stddef.h>
+
+namespace folly {
+
+template <typename T>
+class GroupVarint;
+
+namespace detail {
+
+template <typename T>
+struct GroupVarintTraits;
+
+template <>
+struct GroupVarintTraits<uint32_t> {
+  enum {
+    kGroupSize = 4,
+    kHeaderSize = 1,
+  };
+};
+
+template <>
+struct GroupVarintTraits<uint64_t> {
+  enum {
+    kGroupSize = 5,
+    kHeaderSize = 2,
+  };
+};
+
+template <typename T>
+class GroupVarintBase {
+ protected:
+  typedef GroupVarintTraits<T> Traits;
+  enum { kHeaderSize = Traits::kHeaderSize };
+
+ public:
+  typedef T type;
+
+  /**
+   * Number of integers encoded / decoded in one pass.
+   */
+  enum { kGroupSize = Traits::kGroupSize };
+
+  /**
+   * Maximum encoded size.
+   */
+  enum { kMaxSize = kHeaderSize + sizeof(type) * kGroupSize };
+
+  /**
+   * Maximum size for n values.
+   */
+  static size_t maxSize(size_t n) {
+    // Full groups
+    size_t total = (n / kGroupSize) * kFullGroupSize;
+    // Incomplete last group, if any
+    n %= kGroupSize;
+    if (n) {
+      total += kHeaderSize + n * sizeof(type);
+    }
+    return total;
+  }
+
+  /**
+   * Size of n values starting at p.
+   */
+  static size_t totalSize(const T* p, size_t n) {
+    size_t size = 0;
+    for (; n >= kGroupSize; n -= kGroupSize, p += kGroupSize) {
+      size += Derived::size(p);
+    }
+    if (n) {
+      size += Derived::partialSize(p, n);
+    }
+    return size;
+  }
+
+ private:
+  typedef GroupVarint<T> Derived;
+  enum { kFullGroupSize = kHeaderSize + kGroupSize * sizeof(type) };
+};
+
+}  // namespace detail
+}  // namespace folly
+
+#endif /* FOLLY_DETAIL_GROUPVARINTDETAIL_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/detail/Malloc.cpp
@@ -0,0 +1,26 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/detail/Malloc.h"
+
+extern "C" {
+
+#if !FOLLY_HAVE_WEAK_SYMBOLS
+int (*rallocm)(void**, size_t*, size_t, size_t, int) = nullptr;
+int (*allocm)(void**, size_t*, size_t, int) = nullptr;
+#endif
+
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/detail/Malloc.h
@@ -0,0 +1,38 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_DETAIL_MALLOC_H
+#define FOLLY_DETAIL_MALLOC_H
+
+#include <stdlib.h>
+
+#ifndef FOLLY_NO_CONFIG
+#include "folly/folly-config.h"
+#endif
+
+extern "C" {
+
+#if FOLLY_HAVE_WEAK_SYMBOLS
+int rallocm(void**, size_t*, size_t, size_t, int) __attribute__((weak));
+int allocm(void**, size_t*, size_t, int) __attribute__((weak));
+#else
+extern int (*rallocm)(void**, size_t*, size_t, size_t, int);
+extern int (*allocm)(void**, size_t*, size_t, int);
+#endif
+
+}
+
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/detail/MemoryIdler.cpp
@@ -0,0 +1,177 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "MemoryIdler.h"
+#include <folly/Logging.h>
+#include <folly/Malloc.h>
+#include <folly/ScopeGuard.h>
+#include <folly/detail/CacheLocality.h>
+#include <limits.h>
+#include <pthread.h>
+#include <stdio.h>
+#include <string.h>
+#include <unistd.h>
+#include <sys/mman.h>
+#include <utility>
+
+
+// weak linking means the symbol will be null if not available, instead
+// of a link failure
+extern "C" int mallctl(const char *name, void *oldp, size_t *oldlenp,
+                       void *newp, size_t newlen)
+    __attribute__((weak));
+
+
+namespace folly { namespace detail {
+
+AtomicStruct<std::chrono::steady_clock::duration>
+MemoryIdler::defaultIdleTimeout(std::chrono::seconds(5));
+
+
+/// Calls mallctl, optionally reading and/or writing an unsigned value
+/// if in and/or out is non-null.  Logs on error
+static unsigned mallctlWrapper(const char* cmd, const unsigned* in,
+                               unsigned* out) {
+  size_t outLen = sizeof(unsigned);
+  int err = mallctl(cmd,
+                    out, out ? &outLen : nullptr,
+                    const_cast<unsigned*>(in), in ? sizeof(unsigned) : 0);
+  if (err != 0) {
+    FB_LOG_EVERY_MS(WARNING, 10000)
+      << "mallctl " << cmd << ": " << strerror(err) << " (" << err << ")";
+  }
+  return err;
+}
+
+void MemoryIdler::flushLocalMallocCaches() {
+  if (usingJEMalloc()) {
+    if (!mallctl) {
+      FB_LOG_EVERY_MS(ERROR, 10000) << "mallctl weak link failed";
+      return;
+    }
+
+    // "tcache.flush" was renamed to "thread.tcache.flush" in jemalloc 3
+    (void)mallctlWrapper("thread.tcache.flush", nullptr, nullptr);
+
+    // By default jemalloc has 4 arenas per cpu, and then assigns each
+    // thread to one of those arenas.  This means that in any service
+    // that doesn't perform a lot of context switching, the chances that
+    // another thread will be using the current thread's arena (and hence
+    // doing the appropriate dirty-page purging) are low.  Some good
+    // tuned configurations (such as that used by hhvm) use fewer arenas
+    // and then pin threads to avoid contended access.  In that case,
+    // purging the arenas is counter-productive.  We use the heuristic
+    // that if narenas <= 2 * num_cpus then we shouldn't do anything here,
+    // which detects when the narenas has been reduced from the default
+    unsigned narenas;
+    unsigned arenaForCurrent;
+    if (mallctlWrapper("arenas.narenas", nullptr, &narenas) == 0 &&
+        narenas > 2 * CacheLocality::system().numCpus &&
+        mallctlWrapper("thread.arena", nullptr, &arenaForCurrent) == 0) {
+      (void)mallctlWrapper("arenas.purge", &arenaForCurrent, nullptr);
+    }
+  }
+}
+
+
+#ifdef __x86_64__
+
+static const size_t s_pageSize = sysconf(_SC_PAGESIZE);
+static __thread uintptr_t tls_stackLimit;
+static __thread size_t tls_stackSize;
+
+static void fetchStackLimits() {
+  pthread_attr_t attr;
+#if defined(_GNU_SOURCE) && defined(__linux__) // Linux+GNU extension
+  pthread_getattr_np(pthread_self(), &attr);
+#else
+  pthread_attr_init(&attr);
+#endif
+  SCOPE_EXIT { pthread_attr_destroy(&attr); };
+
+  void* addr;
+  size_t rawSize;
+  int err;
+  if ((err = pthread_attr_getstack(&attr, &addr, &rawSize))) {
+    // unexpected, but it is better to continue in prod than do nothing
+    FB_LOG_EVERY_MS(ERROR, 10000) << "pthread_attr_getstack error " << err;
+    assert(false);
+    tls_stackSize = 1;
+    return;
+  }
+  assert(addr != nullptr);
+  assert(rawSize >= PTHREAD_STACK_MIN);
+
+  // glibc subtracts guard page from stack size, even though pthread docs
+  // seem to imply the opposite
+  size_t guardSize;
+  if (pthread_attr_getguardsize(&attr, &guardSize) != 0) {
+    guardSize = 0;
+  }
+  assert(rawSize > guardSize);
+
+  // stack goes down, so guard page adds to the base addr
+  tls_stackLimit = uintptr_t(addr) + guardSize;
+  tls_stackSize = rawSize - guardSize;
+
+  assert((tls_stackLimit & (s_pageSize - 1)) == 0);
+}
+
+static __attribute__((noinline)) uintptr_t getStackPtr() {
+  char marker;
+  auto rv = uintptr_t(&marker);
+  return rv;
+}
+
+void MemoryIdler::unmapUnusedStack(size_t retain) {
+  if (tls_stackSize == 0) {
+    fetchStackLimits();
+  }
+  if (tls_stackSize <= std::max(size_t(1), retain)) {
+    // covers both missing stack info, and impossibly large retain
+    return;
+  }
+
+  auto sp = getStackPtr();
+  assert(sp >= tls_stackLimit);
+  assert(sp - tls_stackLimit < tls_stackSize);
+
+  auto end = (sp - retain) & ~(s_pageSize - 1);
+  if (end <= tls_stackLimit) {
+    // no pages are eligible for unmapping
+    return;
+  }
+
+  size_t len = end - tls_stackLimit;
+  assert((len & (s_pageSize - 1)) == 0);
+  if (madvise((void*)tls_stackLimit, len, MADV_DONTNEED) != 0) {
+    // It is likely that the stack vma hasn't been fully grown.  In this
+    // case madvise will apply dontneed to the present vmas, then return
+    // errno of ENOMEM.  We can also get an EAGAIN, theoretically.
+    // EINVAL means either an invalid alignment or length, or that some
+    // of the pages are locked or shared.  Neither should occur.
+    assert(errno == EAGAIN || errno == ENOMEM);
+  }
+}
+
+#else
+
+void MemoryIdler::unmapUnusedStack(size_t retain) {
+}
+
+#endif
+
+}}
--- /dev/null
+++ b/hphp/submodules/folly/folly/detail/MemoryIdler.h
@@ -0,0 +1,143 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_DETAIL_MEMORYIDLER_H
+#define FOLLY_DETAIL_MEMORYIDLER_H
+
+#include <atomic>
+#include <chrono>
+#include <folly/AtomicStruct.h>
+#include <folly/Hash.h>
+#include <folly/Traits.h>
+#include "Futex.h"
+
+namespace folly {
+
+// gcc 4.7 doesn't do std::is_trivial correctly, override so we can use
+// AtomicStruct<duration>
+template<>
+struct IsTriviallyCopyable<std::chrono::steady_clock::duration>
+  : std::true_type {};
+
+}
+
+namespace folly { namespace detail {
+
+/// MemoryIdler provides helper routines that allow routines to return
+/// some assigned memory resources back to the system.  The intended
+/// use is that when a thread is waiting for a long time (perhaps it
+/// is in a LIFO thread pool and hasn't been needed for a long time)
+/// it should release its thread-local malloc caches (both jemalloc and
+/// tcmalloc use these for better performance) and unmap the stack pages
+/// that contain no useful data.
+struct MemoryIdler {
+
+  /// Returns memory from thread-local allocation pools to the global
+  /// pool, if we know how to for the current malloc implementation.
+  /// jemalloc is supported.
+  static void flushLocalMallocCaches();
+
+
+  enum {
+    /// This value is a tradeoff between reclaiming memory and triggering
+    /// a page fault immediately on wakeup.  Note that the actual unit
+    /// of idling for the stack is pages, so the actual stack that
+    /// will be available on wakeup without a page fault is between
+    /// kDefaultStackToRetain and kDefaultStackToRetain + PageSize -
+    /// 1 bytes.
+    kDefaultStackToRetain = 1024,
+  };
+
+  /// Uses madvise to discard the portion of the thread's stack that
+  /// currently doesn't hold any data, trying to ensure that no page
+  /// faults will occur during the next retain bytes of stack allocation
+  static void unmapUnusedStack(size_t retain = kDefaultStackToRetain);
+
+
+  /// The system-wide default for the amount of time a blocking
+  /// thread should wait before reclaiming idle memory.  Set this to
+  /// Duration::max() to never wait.  The default value is 5 seconds.
+  /// Endpoints using this idle timeout might randomly wait longer to
+  /// avoid synchronizing their flushes.
+  static AtomicStruct<std::chrono::steady_clock::duration> defaultIdleTimeout;
+
+
+  /// Equivalent to fut.futexWait(expected, waitMask), but calls
+  /// flushLocalMallocCaches() and unmapUnusedStack(stackToRetain)
+  /// after idleTimeout has passed (if it has passed).  Internally uses
+  /// fut.futexWait and fut.futexWaitUntil.  Like futexWait, returns
+  /// false if interrupted with a signal.  The actual timeout will be
+  /// pseudo-randomly chosen to be between idleTimeout and idleTimeout *
+  /// (1 + timeoutVariationFraction), to smooth out the behavior in a
+  /// system with bursty requests.  The default is to wait up to 50%
+  /// extra, so on average 25% extra
+  template <template <typename> class Atom,
+            typename Clock = std::chrono::steady_clock>
+  static bool futexWait(
+      Futex<Atom>& fut,
+      uint32_t expected,
+      uint32_t waitMask = -1,
+      typename Clock::duration idleTimeout
+          = defaultIdleTimeout.load(std::memory_order_acquire),
+      size_t stackToRetain = kDefaultStackToRetain,
+      float timeoutVariationFrac = 0.5) {
+
+    if (idleTimeout == Clock::duration::max()) {
+      // no need to use futexWaitUntil if no timeout is possible
+      return fut.futexWait(expected, waitMask);
+    }
+
+    if (idleTimeout.count() > 0) {
+      auto begin = Clock::now();
+
+      if (timeoutVariationFrac > 0) {
+        // hash the pthread_t and the time to get the adjustment.
+        // Standard hash func isn't very good, so bit mix the result
+        auto pr = std::make_pair(pthread_self(),
+                                 begin.time_since_epoch().count());
+        std::hash<decltype(pr)> hash_fn;
+        uint64_t h = folly::hash::twang_mix64(hash_fn(pr));
+
+        // multiplying the duration by a floating point doesn't work, grr..
+        auto extraFrac =
+            timeoutVariationFrac / std::numeric_limits<uint64_t>::max() * h;
+        uint64_t tics = idleTimeout.count() * (1 + extraFrac);
+        idleTimeout = typename Clock::duration(tics);
+      }
+
+      while (true) {
+        auto rv = fut.futexWaitUntil(expected, begin + idleTimeout, waitMask);
+        if (rv == FutexResult::TIMEDOUT) {
+          // timeout is over
+          break;
+        }
+        // finished before timeout hit, no flush
+        assert(rv == FutexResult::VALUE_CHANGED || rv == FutexResult::AWOKEN ||
+               rv == FutexResult::INTERRUPTED);
+        return rv == FutexResult::AWOKEN;
+      }
+    }
+
+    // flush, then wait with no timeout
+    flushLocalMallocCaches();
+    unmapUnusedStack(stackToRetain);
+    return fut.futexWait(expected, waitMask);
+  }
+};
+
+}} // namespace folly::detail
+
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/detail/MPMCPipelineDetail.h
@@ -0,0 +1,126 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "folly/MPMCQueue.h"
+
+namespace folly {
+
+template <class T, class... Stages> class MPMCPipeline;
+
+template <class T, size_t Amp> class MPMCPipelineStage {
+ public:
+  typedef T value_type;
+  static constexpr size_t kAmplification = Amp;
+};
+
+namespace detail {
+
+/**
+ * Helper template to determine value type and amplification whether or not
+ * we use MPMCPipelineStage<>
+ */
+template <class T> struct PipelineStageInfo {
+  static constexpr size_t kAmplification = 1;
+  typedef T value_type;
+};
+
+template <class T, size_t Amp>
+struct PipelineStageInfo<MPMCPipelineStage<T, Amp>> {
+  static constexpr size_t kAmplification = Amp;
+  typedef T value_type;
+};
+
+/**
+ * Wrapper around MPMCQueue (friend) that keeps track of tickets.
+ */
+template <class T>
+class MPMCPipelineStageImpl {
+ public:
+  typedef T value_type;
+  template <class U, class... Stages> friend class MPMCPipeline;
+
+  // Implicit so that MPMCPipeline construction works
+  /* implicit */ MPMCPipelineStageImpl(size_t capacity) : queue_(capacity) { }
+  MPMCPipelineStageImpl() { }
+
+  // only use on first stage, uses queue_.pushTicket_ instead of existing
+  // ticket
+  template <class... Args>
+  void blockingWrite(Args&&... args) noexcept {
+    queue_.blockingWrite(std::forward<Args>(args)...);
+  }
+
+  template <class... Args>
+  bool write(Args&&... args) noexcept {
+    return queue_.write(std::forward<Args>(args)...);
+  }
+
+  template <class... Args>
+  void blockingWriteWithTicket(uint64_t ticket, Args&&... args) noexcept {
+    queue_.enqueueWithTicket(ticket, std::forward<Args>(args)...);
+  }
+
+  uint64_t blockingRead(T& elem) noexcept {
+    uint64_t ticket = queue_.popTicket_++;
+    queue_.dequeueWithTicket(ticket, elem);
+    return ticket;
+  }
+
+  bool read(T& elem) noexcept {  // only use on last stage, won't track ticket
+    return queue_.read(elem);
+  }
+
+  template <class... Args>
+  bool readAndGetTicket(uint64_t& ticket, T& elem) noexcept {
+    if (queue_.tryObtainReadyPopTicket(ticket)) {
+      queue_.dequeueWithTicket(ticket, elem);
+      return true;
+    } else {
+      return false;
+    }
+  }
+
+  // See MPMCQueue<T>::writeCount; only works for the first stage
+  uint64_t writeCount() const noexcept {
+    return queue_.writeCount();
+  }
+
+  uint64_t readCount() const noexcept {
+    return queue_.readCount();
+  }
+
+ private:
+  MPMCQueue<T> queue_;
+};
+
+// Product of amplifications of a tuple of PipelineStageInfo<X>
+template <class Tuple> struct AmplificationProduct;
+
+template <> struct AmplificationProduct<std::tuple<>> {
+  static constexpr size_t value = 1;
+};
+
+template <class T, class... Ts>
+struct AmplificationProduct<std::tuple<T, Ts...>> {
+  static constexpr size_t value =
+    T::kAmplification *
+    AmplificationProduct<std::tuple<Ts...>>::value;
+};
+
+}}  // namespaces
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/detail/SlowFingerprint.h
@@ -0,0 +1,93 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_DETAIL_SLOWFINGERPRINT_H_
+#define FOLLY_DETAIL_SLOWFINGERPRINT_H_
+
+#include "folly/Fingerprint.h"
+#include "folly/detail/FingerprintPolynomial.h"
+#include "folly/Range.h"
+
+namespace folly {
+namespace detail {
+
+/**
+ * Slow, one-bit-at-a-time implementation of the Rabin fingerprint.
+ *
+ * This is useful as a reference implementation to test the Broder optimization
+ * for correctness in the unittest; it's probably too slow for any real use.
+ */
+template <int BITS>
+class SlowFingerprint {
+ public:
+  SlowFingerprint()
+    : poly_(FingerprintTable<BITS>::poly) {
+    // Use the same starting value as Fingerprint, (1 << (BITS-1))
+    fp_.addXk(BITS-1);
+  }
+
+  SlowFingerprint& update8(uint8_t v) {
+    updateLSB(v, 8);
+    return *this;
+  }
+
+  SlowFingerprint& update32(uint32_t v) {
+    updateLSB(v, 32);
+    return *this;
+  }
+
+  SlowFingerprint& update64(uint64_t v) {
+    updateLSB(v, 64);
+    return *this;
+  }
+
+  SlowFingerprint& update(const folly::StringPiece& str) {
+    const char* p = str.start();
+    for (int i = str.size(); i != 0; p++, i--) {
+      update8(static_cast<uint8_t>(*p));
+    }
+    return *this;
+  }
+
+  void write(uint64_t* out) const {
+    fp_.write(out);
+  }
+
+ private:
+  void updateBit(bool bit) {
+    fp_.mulXmod(poly_);
+    if (bit) {
+      fp_.addXk(0);
+    }
+  }
+
+  void updateLSB(uint64_t val, int bits) {
+    val <<= (64-bits);
+    for (; bits != 0; --bits) {
+      updateBit(val & (1UL << 63));
+      val <<= 1;
+    }
+  }
+
+  const FingerprintPolynomial<BITS-1> poly_;
+  FingerprintPolynomial<BITS-1> fp_;
+};
+
+}  // namespace detail
+}  // namespace folly
+
+#endif /* FOLLY_DETAIL_SLOWFINGERPRINT_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/detail/Stats.h
@@ -0,0 +1,127 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_DETAIL_STATS_H_
+#define FOLLY_DETAIL_STATS_H_
+
+#include <chrono>
+#include <cstdint>
+#include <type_traits>
+
+namespace folly { namespace detail {
+
+/*
+ * Helper function to compute the average, given a specified input type and
+ * return type.
+ */
+
+// If the input is long double, divide using long double to avoid losing
+// precision.
+template <typename ReturnType>
+ReturnType avgHelper(long double sum, uint64_t count) {
+  if (count == 0) { return ReturnType(0); }
+  const long double countf = count;
+  return static_cast<ReturnType>(sum / countf);
+}
+
+// In all other cases divide using double precision.
+// This should be relatively fast, and accurate enough for most use cases.
+template <typename ReturnType, typename ValueType>
+typename std::enable_if<!std::is_same<typename std::remove_cv<ValueType>::type,
+                                      long double>::value,
+                        ReturnType>::type
+avgHelper(ValueType sum, uint64_t count) {
+  if (count == 0) { return ReturnType(0); }
+  const double sumf = sum;
+  const double countf = count;
+  return static_cast<ReturnType>(sumf / countf);
+}
+
+/*
+ * Helper function to compute the rate per Interval,
+ * given the specified count recorded over the elapsed time period.
+ */
+template <typename ReturnType=double,
+          typename TimeType=std::chrono::seconds,
+          typename Interval=TimeType>
+ReturnType rateHelper(ReturnType count, TimeType elapsed) {
+  if (elapsed == TimeType(0)) {
+    return 0;
+  }
+
+  // Use std::chrono::duration_cast to convert between the native
+  // duration and the desired interval.  However, convert the rates,
+  // rather than just converting the elapsed duration.  Converting the
+  // elapsed time first may collapse it down to 0 if the elapsed interval
+  // is less than the desired interval, which will incorrectly result in
+  // an infinite rate.
+  typedef std::chrono::duration<
+      ReturnType, std::ratio<TimeType::period::den,
+                             TimeType::period::num>> NativeRate;
+  typedef std::chrono::duration<
+      ReturnType, std::ratio<Interval::period::den,
+                             Interval::period::num>> DesiredRate;
+
+  NativeRate native(count / elapsed.count());
+  DesiredRate desired = std::chrono::duration_cast<DesiredRate>(native);
+  return desired.count();
+}
+
+
+template<typename T>
+struct Bucket {
+ public:
+  typedef T ValueType;
+
+  Bucket()
+    : sum(ValueType()),
+      count(0) {}
+
+  void clear() {
+    sum = ValueType();
+    count = 0;
+  }
+
+  void add(const ValueType& s, uint64_t c) {
+    // TODO: It would be nice to handle overflow here.
+    sum += s;
+    count += c;
+  }
+
+  Bucket& operator+=(const Bucket& o) {
+    add(o.sum, o.count);
+    return *this;
+  }
+
+  Bucket& operator-=(const Bucket& o) {
+    // TODO: It would be nice to handle overflow here.
+    sum -= o.sum;
+    count -= o.count;
+    return *this;
+  }
+
+  template <typename ReturnType>
+  ReturnType avg() const {
+    return avgHelper<ReturnType>(sum, count);
+  }
+
+  ValueType sum;
+  uint64_t count;
+};
+
+}} // folly::detail
+
+#endif // FOLLY_DETAIL_STATS_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/detail/ThreadLocalDetail.h
@@ -0,0 +1,422 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_DETAIL_THREADLOCALDETAIL_H_
+#define FOLLY_DETAIL_THREADLOCALDETAIL_H_
+
+#include <limits.h>
+#include <pthread.h>
+
+#include <mutex>
+#include <string>
+#include <vector>
+
+#include <glog/logging.h>
+
+#include "folly/Foreach.h"
+#include "folly/Exception.h"
+#include "folly/Malloc.h"
+
+namespace folly {
+namespace threadlocal_detail {
+
+/**
+ * Base class for deleters.
+ */
+class DeleterBase {
+ public:
+  virtual ~DeleterBase() { }
+  virtual void dispose(void* ptr, TLPDestructionMode mode) const = 0;
+};
+
+/**
+ * Simple deleter class that calls delete on the passed-in pointer.
+ */
+template <class Ptr>
+class SimpleDeleter : public DeleterBase {
+ public:
+  virtual void dispose(void* ptr, TLPDestructionMode mode) const {
+    delete static_cast<Ptr>(ptr);
+  }
+};
+
+/**
+ * Custom deleter that calls a given callable.
+ */
+template <class Ptr, class Deleter>
+class CustomDeleter : public DeleterBase {
+ public:
+  explicit CustomDeleter(Deleter d) : deleter_(d) { }
+  virtual void dispose(void* ptr, TLPDestructionMode mode) const {
+    deleter_(static_cast<Ptr>(ptr), mode);
+  }
+ private:
+  Deleter deleter_;
+};
+
+
+/**
+ * POD wrapper around an element (a void*) and an associated deleter.
+ * This must be POD, as we memset() it to 0 and memcpy() it around.
+ */
+struct ElementWrapper {
+  void dispose(TLPDestructionMode mode) {
+    if (ptr != NULL) {
+      DCHECK(deleter != NULL);
+      deleter->dispose(ptr, mode);
+      if (ownsDeleter) {
+        delete deleter;
+      }
+      ptr = NULL;
+      deleter = NULL;
+      ownsDeleter = false;
+    }
+  }
+
+  template <class Ptr>
+  void set(Ptr p) {
+    DCHECK(ptr == NULL);
+    DCHECK(deleter == NULL);
+
+    if (p) {
+      // We leak a single object here but that is ok.  If we used an
+      // object directly, there is a chance that the destructor will be
+      // called on that static object before any of the ElementWrappers
+      // are disposed and that isn't so nice.
+      static auto d = new SimpleDeleter<Ptr>();
+      ptr = p;
+      deleter = d;
+      ownsDeleter = false;
+    }
+  }
+
+  template <class Ptr, class Deleter>
+  void set(Ptr p, Deleter d) {
+    DCHECK(ptr == NULL);
+    DCHECK(deleter == NULL);
+    if (p) {
+      ptr = p;
+      deleter = new CustomDeleter<Ptr,Deleter>(d);
+      ownsDeleter = true;
+    }
+  }
+
+  void* ptr;
+  DeleterBase* deleter;
+  bool ownsDeleter;
+};
+
+/**
+ * Per-thread entry.  Each thread using a StaticMeta object has one.
+ * This is written from the owning thread only (under the lock), read
+ * from the owning thread (no lock necessary), and read from other threads
+ * (under the lock).
+ */
+struct ThreadEntry {
+  ElementWrapper* elements;
+  size_t elementsCapacity;
+  ThreadEntry* next;
+  ThreadEntry* prev;
+};
+
+// Held in a singleton to track our global instances.
+// We have one of these per "Tag", by default one for the whole system
+// (Tag=void).
+//
+// Creating and destroying ThreadLocalPtr objects, as well as thread exit
+// for threads that use ThreadLocalPtr objects collide on a lock inside
+// StaticMeta; you can specify multiple Tag types to break that lock.
+template <class Tag>
+struct StaticMeta {
+  static StaticMeta<Tag>& instance() {
+    // Leak it on exit, there's only one per process and we don't have to
+    // worry about synchronization with exiting threads.
+    static bool constructed = (inst_ = new StaticMeta<Tag>());
+    (void)constructed; // suppress unused warning
+    return *inst_;
+  }
+
+  int nextId_;
+  std::vector<int> freeIds_;
+  std::mutex lock_;
+  pthread_key_t pthreadKey_;
+  ThreadEntry head_;
+
+  void push_back(ThreadEntry* t) {
+    t->next = &head_;
+    t->prev = head_.prev;
+    head_.prev->next = t;
+    head_.prev = t;
+  }
+
+  void erase(ThreadEntry* t) {
+    t->next->prev = t->prev;
+    t->prev->next = t->next;
+    t->next = t->prev = t;
+  }
+
+#if !__APPLE__
+  static __thread ThreadEntry threadEntry_;
+#endif
+  static StaticMeta<Tag>* inst_;
+
+  StaticMeta() : nextId_(1) {
+    head_.next = head_.prev = &head_;
+    int ret = pthread_key_create(&pthreadKey_, &onThreadExit);
+    checkPosixError(ret, "pthread_key_create failed");
+
+    ret = pthread_atfork(/*prepare*/ &StaticMeta::preFork,
+                         /*parent*/ &StaticMeta::onForkParent,
+                         /*child*/ &StaticMeta::onForkChild);
+    checkPosixError(ret, "pthread_atfork failed");
+  }
+  ~StaticMeta() {
+    LOG(FATAL) << "StaticMeta lives forever!";
+  }
+
+  static ThreadEntry* getThreadEntry() {
+#if !__APPLE__
+    return &threadEntry_;
+#else
+    ThreadEntry* threadEntry =
+        static_cast<ThreadEntry*>(pthread_getspecific(inst_->pthreadKey_));
+    if (!threadEntry) {
+        threadEntry = new ThreadEntry();
+        int ret = pthread_setspecific(inst_->pthreadKey_, threadEntry);
+        checkPosixError(ret, "pthread_setspecific failed");
+    }
+    return threadEntry;
+#endif
+  }
+
+  static void preFork(void) {
+    instance().lock_.lock();  // Make sure it's created
+  }
+
+  static void onForkParent(void) {
+    inst_->lock_.unlock();
+  }
+
+  static void onForkChild(void) {
+    // only the current thread survives
+    inst_->head_.next = inst_->head_.prev = &inst_->head_;
+    ThreadEntry* threadEntry = getThreadEntry();
+    // If this thread was in the list before the fork, add it back.
+    if (threadEntry->elementsCapacity != 0) {
+      inst_->push_back(threadEntry);
+    }
+    inst_->lock_.unlock();
+  }
+
+  static void onThreadExit(void* ptr) {
+    auto & meta = instance();
+#if !__APPLE__
+    ThreadEntry* threadEntry = getThreadEntry();
+
+    DCHECK_EQ(ptr, &meta);
+    DCHECK_GT(threadEntry->elementsCapacity, 0);
+#else
+    ThreadEntry* threadEntry = static_cast<ThreadEntry*>(ptr);
+#endif
+    {
+      std::lock_guard<std::mutex> g(meta.lock_);
+      meta.erase(threadEntry);
+      // No need to hold the lock any longer; the ThreadEntry is private to this
+      // thread now that it's been removed from meta.
+    }
+    FOR_EACH_RANGE(i, 0, threadEntry->elementsCapacity) {
+      threadEntry->elements[i].dispose(TLPDestructionMode::THIS_THREAD);
+    }
+    free(threadEntry->elements);
+    threadEntry->elements = NULL;
+    pthread_setspecific(meta.pthreadKey_, NULL);
+
+#if __APPLE__
+    // Allocated in getThreadEntry(); free it
+    delete threadEntry;
+#endif
+  }
+
+  static int create() {
+    int id;
+    auto & meta = instance();
+    std::lock_guard<std::mutex> g(meta.lock_);
+    if (!meta.freeIds_.empty()) {
+      id = meta.freeIds_.back();
+      meta.freeIds_.pop_back();
+    } else {
+      id = meta.nextId_++;
+    }
+    return id;
+  }
+
+  static void destroy(size_t id) {
+    try {
+      auto & meta = instance();
+      // Elements in other threads that use this id.
+      std::vector<ElementWrapper> elements;
+      {
+        std::lock_guard<std::mutex> g(meta.lock_);
+        for (ThreadEntry* e = meta.head_.next; e != &meta.head_; e = e->next) {
+          if (id < e->elementsCapacity && e->elements[id].ptr) {
+            elements.push_back(e->elements[id]);
+
+            /*
+             * Writing another thread's ThreadEntry from here is fine;
+             * the only other potential reader is the owning thread --
+             * from onThreadExit (which grabs the lock, so is properly
+             * synchronized with us) or from get(), which also grabs
+             * the lock if it needs to resize the elements vector.
+             *
+             * We can't conflict with reads for a get(id), because
+             * it's illegal to call get on a thread local that's
+             * destructing.
+             */
+            e->elements[id].ptr = nullptr;
+            e->elements[id].deleter = nullptr;
+            e->elements[id].ownsDeleter = false;
+          }
+        }
+        meta.freeIds_.push_back(id);
+      }
+      // Delete elements outside the lock
+      FOR_EACH(it, elements) {
+        it->dispose(TLPDestructionMode::ALL_THREADS);
+      }
+    } catch (...) { // Just in case we get a lock error or something anyway...
+      LOG(WARNING) << "Destructor discarding an exception that was thrown.";
+    }
+  }
+
+  /**
+   * Reserve enough space in the ThreadEntry::elements for the item
+   * @id to fit in.
+   */
+  static void reserve(int id) {
+    auto& meta = instance();
+    ThreadEntry* threadEntry = getThreadEntry();
+    size_t prevCapacity = threadEntry->elementsCapacity;
+    // Growth factor < 2, see folly/docs/FBVector.md; + 5 to prevent
+    // very slow start.
+    size_t newCapacity = static_cast<size_t>((id + 5) * 1.7);
+    assert(newCapacity > prevCapacity);
+    ElementWrapper* reallocated = nullptr;
+
+    // Need to grow. Note that we can't call realloc, as elements is
+    // still linked in meta, so another thread might access invalid memory
+    // after realloc succeeds. We'll copy by hand and update our ThreadEntry
+    // under the lock.
+    if (usingJEMalloc()) {
+      bool success = false;
+      size_t newByteSize = newCapacity * sizeof(ElementWrapper);
+      size_t realByteSize = 0;
+
+      // Try to grow in place.
+      //
+      // Note that rallocm(ALLOCM_ZERO) will only zero newly allocated memory,
+      // even if a previous allocation allocated more than we requested.
+      // This is fine; we always use ALLOCM_ZERO with jemalloc and we
+      // always expand our allocation to the real size.
+      if (prevCapacity * sizeof(ElementWrapper) >=
+          jemallocMinInPlaceExpandable) {
+        success = (rallocm(reinterpret_cast<void**>(&threadEntry->elements),
+                           &realByteSize,
+                           newByteSize,
+                           0,
+                           ALLOCM_NO_MOVE | ALLOCM_ZERO) == ALLOCM_SUCCESS);
+
+      }
+
+      // In-place growth failed.
+      if (!success) {
+        // Note that, unlike calloc,allocm(... ALLOCM_ZERO) zeros all
+        // allocated bytes (*realByteSize) and not just the requested
+        // bytes (newByteSize)
+        success = (allocm(reinterpret_cast<void**>(&reallocated),
+                          &realByteSize,
+                          newByteSize,
+                          ALLOCM_ZERO) == ALLOCM_SUCCESS);
+      }
+
+      if (success) {
+        // Expand to real size
+        assert(realByteSize / sizeof(ElementWrapper) >= newCapacity);
+        newCapacity = realByteSize / sizeof(ElementWrapper);
+      } else {
+        throw std::bad_alloc();
+      }
+    } else {  // no jemalloc
+      // calloc() is simpler than malloc() followed by memset(), and
+      // potentially faster when dealing with a lot of memory, as it can get
+      // already-zeroed pages from the kernel.
+      reallocated = static_cast<ElementWrapper*>(
+          calloc(newCapacity, sizeof(ElementWrapper)));
+      if (!reallocated) {
+        throw std::bad_alloc();
+      }
+    }
+
+    // Success, update the entry
+    {
+      std::lock_guard<std::mutex> g(meta.lock_);
+
+      if (prevCapacity == 0) {
+        meta.push_back(threadEntry);
+      }
+
+      if (reallocated) {
+       /*
+        * Note: we need to hold the meta lock when copying data out of
+        * the old vector, because some other thread might be
+        * destructing a ThreadLocal and writing to the elements vector
+        * of this thread.
+        */
+        memcpy(reallocated, threadEntry->elements,
+               sizeof(ElementWrapper) * prevCapacity);
+        using std::swap;
+        swap(reallocated, threadEntry->elements);
+      }
+      threadEntry->elementsCapacity = newCapacity;
+    }
+
+    free(reallocated);
+
+#if !__APPLE__
+    if (prevCapacity == 0) {
+      pthread_setspecific(meta.pthreadKey_, &meta);
+    }
+#endif
+  }
+
+  static ElementWrapper& get(size_t id) {
+    ThreadEntry* threadEntry = getThreadEntry();
+    if (UNLIKELY(threadEntry->elementsCapacity <= id)) {
+      reserve(id);
+      assert(threadEntry->elementsCapacity > id);
+    }
+    return threadEntry->elements[id];
+  }
+};
+
+#if !__APPLE__
+template <class Tag> __thread ThreadEntry StaticMeta<Tag>::threadEntry_ = {0};
+#endif
+template <class Tag> StaticMeta<Tag>* StaticMeta<Tag>::inst_ = nullptr;
+
+}  // namespace threadlocal_detail
+}  // namespace folly
+
+#endif /* FOLLY_DETAIL_THREADLOCALDETAIL_H_ */
--- /dev/null
+++ b/hphp/submodules/folly/folly/detail/UncaughtExceptionCounter.h
@@ -0,0 +1,92 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_DETAIL_UNCAUGHTEXCEPTIONCOUNTER_H_
+#define FOLLY_DETAIL_UNCAUGHTEXCEPTIONCOUNTER_H_
+
+#include <exception>
+
+#if defined(__GNUG__) || defined(__CLANG__)
+#define FOLLY_EXCEPTION_COUNT_USE_CXA_GET_GLOBALS
+namespace __cxxabiv1 {
+// forward declaration (originally defined in unwind-cxx.h from from libstdc++)
+struct __cxa_eh_globals;
+// declared in cxxabi.h from libstdc++-v3
+extern "C" __cxa_eh_globals* __cxa_get_globals() noexcept;
+}
+#elif defined(_MSC_VER) && (_MSC_VER >= 1400) // MSVC++ 8.0 or greater
+#define FOLLY_EXCEPTION_COUNT_USE_GETPTD
+// forward declaration (originally defined in mtdll.h from MSVCRT)
+struct _tiddata;
+extern "C" _tiddata* _getptd(); // declared in mtdll.h from MSVCRT
+#else
+// Raise an error when trying to use this on unsupported platforms.
+#error "Unsupported platform, don't include this header."
+#endif
+
+
+namespace folly { namespace detail {
+
+/**
+ * Used to check if a new uncaught exception was thrown by monitoring the
+ * number of uncaught exceptions.
+ *
+ * Usage:
+ *  - create a new UncaughtExceptionCounter object
+ *  - call isNewUncaughtException() on the new object to check if a new
+ *    uncaught exception was thrown since the object was created
+ */
+class UncaughtExceptionCounter {
+ public:
+  UncaughtExceptionCounter()
+    : exceptionCount_(getUncaughtExceptionCount()) {}
+
+  UncaughtExceptionCounter(const UncaughtExceptionCounter& other)
+    : exceptionCount_(other.exceptionCount_) {}
+
+  bool isNewUncaughtException() noexcept {
+    return getUncaughtExceptionCount() > exceptionCount_;
+  }
+
+ private:
+  int getUncaughtExceptionCount() noexcept;
+
+  int exceptionCount_;
+};
+
+/**
+ * Returns the number of uncaught exceptions.
+ *
+ * This function is based on Evgeny Panasyuk's implementation from here:
+ * http://fburl.com/15190026
+ */
+inline int UncaughtExceptionCounter::getUncaughtExceptionCount() noexcept {
+#if defined(FOLLY_EXCEPTION_COUNT_USE_CXA_GET_GLOBALS)
+  // __cxa_get_globals returns a __cxa_eh_globals* (defined in unwind-cxx.h).
+  // The offset below returns __cxa_eh_globals::uncaughtExceptions.
+  return *(reinterpret_cast<unsigned int*>(static_cast<char*>(
+      static_cast<void*>(__cxxabiv1::__cxa_get_globals())) + sizeof(void*)));
+#elif defined(FOLLY_EXCEPTION_COUNT_USE_GETPTD)
+  // _getptd() returns a _tiddata* (defined in mtdll.h).
+  // The offset below returns _tiddata::_ProcessingThrow.
+  return *(reinterpret_cast<int*>(static_cast<char*>(
+      static_cast<void*>(_getptd())) + sizeof(void*) * 28 + 0x4 * 8));
+#endif
+}
+
+}} // namespaces
+
+#endif /* FOLLY_DETAIL_UNCAUGHTEXCEPTIONCOUNTER_H_ */
--- /dev/null
+++ b/hphp/submodules/folly/folly/DiscriminatedPtr.h
@@ -0,0 +1,221 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Discriminated pointer: Type-safe pointer to one of several types.
+ *
+ * Similar to boost::variant, but has no space overhead over a raw pointer, as
+ * it relies on the fact that (on x86_64) there are 16 unused bits in a
+ * pointer.
+ *
+ * @author Tudor Bosman (tudorb@fb.com)
+ */
+
+#ifndef FOLLY_DISCRIMINATEDPTR_H_
+#define FOLLY_DISCRIMINATEDPTR_H_
+
+#include <limits>
+#include <stdexcept>
+#include <glog/logging.h>
+#include "folly/Likely.h"
+#include "folly/detail/DiscriminatedPtrDetail.h"
+
+#ifndef __x86_64__
+# error "DiscriminatedPtr is x64-specific code."
+#endif
+
+namespace folly {
+
+/**
+ * Discriminated pointer.
+ *
+ * Given a list of types, a DiscriminatedPtr<Types...> may point to an object
+ * of one of the given types, or may be empty.  DiscriminatedPtr is type-safe:
+ * you may only get a pointer to the type that you put in, otherwise get
+ * throws an exception (and get_nothrow returns nullptr)
+ *
+ * This pointer does not do any kind of lifetime management -- it's not a
+ * "smart" pointer.  You are responsible for deallocating any memory used
+ * to hold pointees, if necessary.
+ */
+template <typename... Types>
+class DiscriminatedPtr {
+  // <, not <=, as our indexes are 1-based (0 means "empty")
+  static_assert(sizeof...(Types) < std::numeric_limits<uint16_t>::max(),
+                "too many types");
+
+ public:
+  /**
+   * Create an empty DiscriminatedPtr.
+   */
+  DiscriminatedPtr() : data_(0) {
+  }
+
+  /**
+   * Create a DiscriminatedPtr that points to an object of type T.
+   * Fails at compile time if T is not a valid type (listed in Types)
+   */
+  template <typename T>
+  explicit DiscriminatedPtr(T* ptr) {
+    set(ptr, typeIndex<T>());
+  }
+
+  /**
+   * Set this DiscriminatedPtr to point to an object of type T.
+   * Fails at compile time if T is not a valid type (listed in Types)
+   */
+  template <typename T>
+  void set(T* ptr) {
+    set(ptr, typeIndex<T>());
+  }
+
+  /**
+   * Get a pointer to the object that this DiscriminatedPtr points to, if it is
+   * of type T.  Fails at compile time if T is not a valid type (listed in
+   * Types), and returns nullptr if this DiscriminatedPtr is empty or points to
+   * an object of a different type.
+   */
+  template <typename T>
+  T* get_nothrow() noexcept {
+    void* p = LIKELY(hasType<T>()) ? ptr() : nullptr;
+    return static_cast<T*>(p);
+  }
+
+  template <typename T>
+  const T* get_nothrow() const noexcept {
+    const void* p = LIKELY(hasType<T>()) ? ptr() : nullptr;
+    return static_cast<const T*>(p);
+  }
+
+  /**
+   * Get a pointer to the object that this DiscriminatedPtr points to, if it is
+   * of type T.  Fails at compile time if T is not a valid type (listed in
+   * Types), and throws std::invalid_argument if this DiscriminatedPtr is empty
+   * or points to an object of a different type.
+   */
+  template <typename T>
+  T* get() {
+    if (UNLIKELY(!hasType<T>())) {
+      throw std::invalid_argument("Invalid type");
+    }
+    return static_cast<T*>(ptr());
+  }
+
+  template <typename T>
+  const T* get() const {
+    if (UNLIKELY(!hasType<T>())) {
+      throw std::invalid_argument("Invalid type");
+    }
+    return static_cast<const T*>(ptr());
+  }
+
+  /**
+   * Return true iff this DiscriminatedPtr is empty.
+   */
+  bool empty() const {
+    return index() == 0;
+  }
+
+  /**
+   * Return true iff the object pointed by this DiscriminatedPtr has type T,
+   * false otherwise.  Fails at compile time if T is not a valid type (listed
+   * in Types...)
+   */
+  template <typename T>
+  bool hasType() const {
+    return index() == typeIndex<T>();
+  }
+
+  /**
+   * Clear this DiscriminatedPtr, making it empty.
+   */
+  void clear() {
+    data_ = 0;
+  }
+
+  /**
+   * Assignment operator from a pointer of type T.
+   */
+  template <typename T>
+  DiscriminatedPtr& operator=(T* ptr) {
+    set(ptr);
+    return *this;
+  }
+
+  /**
+   * Apply a visitor to this object, calling the appropriate overload for
+   * the type currently stored in DiscriminatedPtr.  Throws invalid_argument
+   * if the DiscriminatedPtr is empty.
+   *
+   * The visitor must meet the following requirements:
+   *
+   * - The visitor must allow invocation as a function by overloading
+   *   operator(), unambiguously accepting all values of type T* (or const T*)
+   *   for all T in Types...
+   * - All operations of the function object on T* (or const T*) must
+   *   return the same type (or a static_assert will fire).
+   */
+  template <typename V>
+  typename dptr_detail::VisitorResult<V, Types...>::type apply(V&& visitor) {
+    size_t n = index();
+    if (n == 0) throw std::invalid_argument("Empty DiscriminatedPtr");
+    return dptr_detail::ApplyVisitor<V, Types...>()(
+      n, std::forward<V>(visitor), ptr());
+  }
+
+  template <typename V>
+  typename dptr_detail::ConstVisitorResult<V, Types...>::type apply(V&& visitor)
+  const {
+    size_t n = index();
+    if (n == 0) throw std::invalid_argument("Empty DiscriminatedPtr");
+    return dptr_detail::ApplyConstVisitor<V, Types...>()(
+      n, std::forward<V>(visitor), ptr());
+  }
+
+ private:
+  /**
+   * Get the 1-based type index of T in Types.
+   */
+  template <typename T>
+  size_t typeIndex() const {
+    return dptr_detail::GetTypeIndex<T, Types...>::value;
+  }
+
+  uint16_t index() const { return data_ >> 48; }
+  void* ptr() const {
+    return reinterpret_cast<void*>(data_ & ((1ULL << 48) - 1));
+  }
+
+  void set(void* p, uint16_t v) {
+    uintptr_t ip = reinterpret_cast<uintptr_t>(p);
+    CHECK(!(ip >> 48));
+    ip |= static_cast<uintptr_t>(v) << 48;
+    data_ = ip;
+  }
+
+  /**
+   * We store a pointer in the least significant 48 bits of data_, and a type
+   * index (0 = empty, or 1-based index in Types) in the most significant 16
+   * bits.  We rely on the fact that pointers have their most significant 16
+   * bits clear on x86_64.
+   */
+  uintptr_t data_;
+};
+
+}  // namespace folly
+
+#endif /* FOLLY_DISCRIMINATEDPTR_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/docs/AtomicHashMap.md
@@ -0,0 +1,134 @@
+`folly/AtomicHashmap.h`
+----------------------
+
+`folly/AtomicHashmap.h` introduces a synchronized UnorderedAssociativeContainer
+implementation designed for extreme performance in heavily multithreaded
+environments (about 2-5x faster than tbb::concurrent_hash_map) and good memory
+usage properties.  Find and iteration are wait-free, insert has key-level lock
+granularity, there is minimal memory overhead, and permanent 32-bit ids can be
+used to reference each element.
+
+
+### Limitations
+***
+
+Although it can provide extreme performance, AtomicHashmap has some unique
+limitations as well.
+
+* The space for erased elements cannot be reclaimed (they are tombstoned
+forever) so it's generally not a good idea to use this if you're erasing things
+a lot.
+
+* Only supports 32 or 64 bit keys - this is because they must be atomically
+compare-and-swap'ed.
+
+* Growth beyond initialization reduces performance - if you don't know
+the approximate number of elements you'll be inserting into the map, you
+probably shouldn't use this class.
+
+* Must manage synchronization externally in order to modify values in the map
+after insertion.  Lock pools are a common way to do this, or you may
+consider using `folly::PackedSyncPtr<T>` as your `ValueT`.
+
+* Must define special reserved key values for empty, erased, and locked
+elements.
+
+For a complete list of limitations and departures from the
+UnorderedAssociativeContainer concept, see `folly/AtomicHashMap.h`
+
+
+### Unique Features
+***
+
+* `value_type` references remain valid as long as the map itself.  Note this is
+not true for most other probing hash maps which will move elements when
+rehashing, which is necessary for them to grow.  AtomicHashMap grows by chaining
+additional slabs, so elements never need to be moved.
+
+* Unique 32-bit ids can be used to reference elements in the map via
+`iterator::getIndex()`.  This can be helpful to save memory in the rest of the
+application by replacing 64-bit pointers or keys.
+
+* Iterators are never invalidated.  This means you can iterate through the map
+while simultaneously inserting and erasing.  This is particularly useful for
+non-blocking map serialization.
+
+
+### Usage
+***
+
+Usage is similar to most maps, although note the conspicuous lack of operator[]
+which encourages non thread-safe access patterns.
+
+Below is a synchronized key counter implementation that allows the counter
+values to be incremented in parallel with serializing all the values to a
+string.
+
+```Cpp
+   class Counters {
+    private:
+     AtomicHashMap<int64_t,int64_t> ahm;
+
+    public:
+     explicit Counters(size_t numCounters) : ahm(numCounters) {}
+
+     void increment(int64_t obj_id) {
+       auto ret = ahm.insert(make_pair(obj_id, 1));
+       if (!ret.first) {
+         // obj_id already exists, increment
+         NoBarrier_AtomicIncrement(&ret.first->second, 1);
+       }
+     }
+
+     int64_t getValue(int64_t obj_id) {
+       auto ret = ahm.find(obj_id);
+       return ret != ahm.end() ? ret->second : 0;
+     }
+
+     // Serialize the counters without blocking increments
+     string toString() {
+       string ret = "{\n";
+       ret.reserve(ahm.size() * 32);
+       for (const auto& e : ahm) {
+         ret += folly::to<string>(
+           "  [", e.first, ":", NoBarrier_Load(&e.second), "]\n");
+       }
+       ret += "}\n";
+       return ret;
+     }
+   };
+```
+
+### Implementation
+***
+
+AtomicHashMap is a composition of AtomicHashArray submaps, which implement the
+meat of the functionality.  Only one AHA is created on initialization, and
+additional submaps are appended if the first one gets full.  If the AHM grows,
+there will be multiple submaps that must be probed in series to find a given
+key.  The more growth, the more submaps will be chained, and the slower it will
+get.  If the initial size estimate is good, only one submap will ever be created
+and performance will be optimal.
+
+AtomicHashArray is a fixed-size probing hash map (also referred to as an open
+addressed hash map) where hash collisions are resolved by checking subsequent
+elements.  This means that they can be allocated in slabs as arrays of
+value_type elements, have excellent cache performance, and have no memory
+overhead from storing pointers.
+
+The algorithm is simple - when inserting, the key is hash-mod'ed to an offset,
+and that element-key is atomically compare-and-swap'ed with the locked key
+value.  If successful, the value is written and the element-key is unlocked by
+setting it to the input key value.  If the compare fails, the next element is
+tried until success or the map is full.
+
+Finds are even simpler.  The key is hash-mod'ed to an offset, and the
+element-key is examined.  If it is the same as the input key, the reference is
+returned, if it's the empty key, failure is returned, otherwise the next key is
+tried.  This can be done wait-free without any atomic instructions because the
+elements are always in a valid state.
+
+Erase is done by finding the key, then compare-and-swap'ing the element-key with
+the reserved erased key value.  If the swap succeeds, return success, otherwise
+return failure (the element was erased by a competing thread).  If the key does
+not exist, return failure.
--- /dev/null
+++ b/hphp/submodules/folly/folly/docs/Benchmark.md
@@ -0,0 +1,295 @@
+`folly/Benchmark.h`
+-----------------
+
+`folly/Benchmark.h` provides a simple framework for writing and
+executing benchmarks. Currently the framework targets only
+single-threaded testing (though you can internally use fork-join
+parallelism and measure total run time).
+
+To use this library, you need to be using gcc 4.6 or later. Include
+`folly/Benchmark.h` and make sure `folly/benchmark.cpp` is part of the
+build (either directly or packaged with a library).
+
+### Overview
+***
+
+Using `folly/Benchmark.h` is very simple. Here's an example:
+
+``` Cpp
+    #include "folly/Benchmark.h"
+    #include "folly/Foreach.h"
+    #include <vector>
+    using namespace std;
+    using namespace folly;
+    BENCHMARK(insertFrontVector) {
+      // Let's insert 100 elements at the front of a vector
+      vector<int> v;
+      FOR_EACH_RANGE (i, 0, 100) {
+        v.insert(v.begin(), i);
+      }
+    }
+    BENCHMARK(insertBackVector) {
+      // Let's insert 100 elements at the back of a vector
+      vector<int> v;
+      FOR_EACH_RANGE (i, 0, 100) {
+        v.insert(v.end(), i);
+      }
+    }
+    int main() {
+      runBenchmarks();
+    }
+```
+
+Compiling and running this code produces to the standard output:
+
+```
+    ===============================================================================
+    test.cpp                                              relative ns/iter  iters/s
+    ===============================================================================
+    insertFrontVector                                                3.84K  260.38K
+    insertBackVector                                                 1.61K  622.75K
+    ===============================================================================
+```
+
+Let's worry about the empty column "relative" later. The table
+contains, for each benchmark, the time spent per call and the converse
+number of calls per second. Numbers are represented in metric notation
+(K for thousands, M for millions etc). As expected, in this example
+the second function is much faster (fewer ns/iter and more iters/s).
+
+The macro `BENCHMARK` introduces a function and also adds it to an
+internal array containing all benchmarks in the system. The defined
+function takes no arguments and returns `void`.
+
+The framework calls the function many times to collect statistics
+about it. Sometimes the function itself would want to do that
+iteration---for example how about inserting `n` elements instead of
+100 elements? To do the iteration internally, use `BENCHMARK` with two
+parameters. The second parameter is the number of iterations and is
+passed by the framework down to the function. The type of the count is
+implicitly `unsigned`. Consider a slightly reworked example:
+
+``` Cpp
+    #include "folly/Benchmark.h"
+    #include "folly/Foreach.h"
+    #include <vector>
+    using namespace std;
+    using namespace folly;
+    BENCHMARK(insertFrontVector, n) {
+      vector<int> v;
+      FOR_EACH_RANGE (i, 0, n) {
+        v.insert(v.begin(), i);
+      }
+    }
+    BENCHMARK(insertBackVector, n) {
+      vector<int> v;
+      FOR_EACH_RANGE (i, 0, n) {
+        v.insert(v.end(), i);
+      }
+    }
+    int main() {
+      runBenchmarks();
+    }
+```
+
+The produced numbers are substantially different:
+
+```
+    ===============================================================================
+    Benchmark                                             relative ns/iter  iters/s
+    ===============================================================================
+    insertFrontVector                                               39.92    25.05M
+    insertBackVector                                                 3.46   288.89M
+    ===============================================================================
+```
+
+Now the numbers indicate the speed of one single insertion because the
+framework assumed the user-defined function used internal iteration
+(which it does). So inserting at the back of a vector is more than 10
+times faster than inserting at the front! Speaking of comparisons...
+
+### Baselines
+***
+
+Choosing one or more good baselines is a crucial activity in any
+measurement. Without a baseline there is little information to derive
+from the sheer numbers. If, for example, you do experimentation with
+algorithms, a good baseline is often an established approach (e.g. the
+built-in `std::sort` for sorting). Essentially all experimental
+numbers should be compared against some baseline.
+
+To support baseline-driven measurements, `folly/Benchmark.h` defines
+`BENCHMARK_RELATIVE`, which works much like `BENCHMARK`, except it
+considers the most recent lexically-ocurring `BENCHMARK` a baseline,
+and fills the "relative" column. Say, for example, we want to use
+front insertion for a vector as a baseline and see how back insertion
+compares with it:
+
+``` Cpp
+    #include "folly/Benchmark.h"
+    #include "folly/Foreach.h"
+    #include <vector>
+    using namespace std;
+    using namespace folly;
+    BENCHMARK(insertFrontVector, n) {
+      vector<int> v;
+      FOR_EACH_RANGE (i, 0, n) {
+        v.insert(v.begin(), i);
+      }
+    }
+    BENCHMARK_RELATIVE(insertBackVector, n) {
+      vector<int> v;
+      FOR_EACH_RANGE (i, 0, n) {
+        v.insert(v.end(), i);
+      }
+    }
+    int main() {
+      runBenchmarks();
+    }
+```
+
+This program prints something like:
+
+```
+    ===============================================================================
+    Benchmark                                             relative ns/iter  iters/s
+    ===============================================================================
+    insertFrontVector                                               42.65    23.45M
+    insertBackVector                                     1208.24%    3.53   283.30M
+    ===============================================================================
+```
+
+showing the 1208.24% relative speed advantage of inserting at the back
+compared to front. The scale is chosen in such a way that 100% means
+identical speed, numbers smaller than 100% indicate the benchmark is
+slower than the baseline, and numbers greater than 100% indicate the
+benchmark is faster. For example, if you see 42% that means the speed
+of the benchmark is 0.42 of the baseline speed. If you see 123%, it
+means the benchmark is 23% or 1.23 times faster.
+
+To close the current benchmark group and start another, simply use
+`BENCHMARK` again.
+
+### Ars Gratia Artis
+***
+
+If you want to draw a horizontal line of dashes (e.g. at the end of a
+group or for whatever reason), use `BENCHMARK_DRAW_LINE()`. The line
+fulfills a purely aesthetic role; it doesn't interact with
+measurements in any way.
+
+``` Cpp
+    BENCHMARK(foo) {
+      Foo foo;
+      foo.doSomething();
+    }
+
+    BENCHMARK_DRAW_LINE();
+
+    BENCHMARK(bar) {
+      Bar bar;
+      bar.doSomething();
+    }
+```
+
+### Suspending a benchmark
+***
+
+Sometimes benchmarking code must to some preparation work that is
+physically inside the benchmark function, but should not take part to
+its time budget. To temporarily suspend the benchmark, use the
+pseudo-statement `BENCHMARK_SUSPEND` as follows:
+
+``` Cpp
+    BENCHMARK(insertBackVector, n) {
+      vector<int> v;
+      BENCHMARK_SUSPEND {
+        v.reserve(n);
+      }
+      FOR_EACH_RANGE (i, 0, n) {
+        v.insert(v.end(), i);
+      }
+    }
+```
+
+The preallocation effected with `v.reserve(n)` will not count toward
+the total run time of the benchmark.
+
+Only the main thread should call `BENCHMARK_SUSPEND` (and of course it
+should not call it while other threads are doing actual work). This is
+because the timer is application-global.
+
+If the scope introduced by `BENCHMARK_SUSPEND` is not desired, you may
+want to "manually" use the `BenchmarkSuspender` type. Constructing
+such an object suspends time measurement, and destroying it resumes
+the measurement. If you want to resume time measurement before the
+destructor, call `dismiss` against the `BenchmarkSuspender`
+object. The previous example could have been written like this:
+
+``` Cpp
+    BENCHMARK(insertBackVector, n) {
+      BenchmarkSuspender braces;
+      vector<int> v;
+      v.reserve(n);
+      braces.dismiss();
+      FOR_EACH_RANGE (i, 0, n) {
+        v.insert(v.end(), i);
+      }
+    }
+```
+
+### `doNotOptimizeAway`
+***
+
+Finally, the small utility function `doNotOptimizeAway` prevents
+compiler optimizations that may interfere with benchmarking . Call
+doNotOptimizeAway(var) against variables that you use for
+benchmarking but otherwise are useless. The compiler tends to do a
+good job at eliminating unused variables, and this function fools it
+into thinking a variable is in fact needed. Example:
+
+``` Cpp
+    BENCHMARK(fpOps, n) {
+      double d = 1;
+      FOR_EACH_RANGE (i, 1, n) {
+        d += i;
+        d -= i;
+        d *= i;
+        d /= i;
+      }
+      doNotOptimizeAway(d);
+    }
+```
+
+### A look under the hood
+***
+
+`folly/Benchmark.h` has a simple, systematic approach to collecting
+timings.
+
+First, it organizes measurements in several large epochs, and takes
+the minimum over all epochs. Taking the minimum gives the closest
+result to the real runtime. Benchmark timings are not a regular random
+variable that fluctuates around an average. Instead, the real time
+we're looking for is one to which there's a variety of additive noise
+(i.e. there is no noise that could actually shorten the benchmark time
+below its real value). In theory, taking an infinite amount of samples
+and keeping the minimum is the actual time that needs
+measuring. That's why the accuracy of benchmarking increases with the
+number of epochs.
+
+Clearly, in real functioning there will also be noise and a variety of
+effects caused by the running context. But the noise during the
+benchmark (straight setup, simple looping) is a poor model for the
+noise in the real application. So taking the minimum across several
+epochs is the most informative result.
+
+Inside each epoch, the function measured is iterated an increasing
+number of times until the total runtime is large enough to make noise
+negligible. At that point the time is collected, and the time per
+iteration is computed. As mentioned, the minimum time per iteration
+over all epochs is the final result.
+
+The timer function used is `clock_gettime` with the `CLOCK_REALTIME`
+clock id. Note that you must use a recent Linux kernel (2.6.38 or
+newer), otherwise the resolution of `CLOCK_REALTIME` is inadequate.
--- /dev/null
+++ b/hphp/submodules/folly/folly/docs/Conv.md
@@ -0,0 +1,217 @@
+`folly/Conv.h`
+-------------
+
+`folly/Conv.h` is a one-stop-shop for converting values across
+types. Its main features are simplicity of the API (only the
+names `to` and `toAppend` must be memorized), speed
+(folly is significantly faster, sometimes by an order of magnitude,
+than comparable APIs), and correctness.
+
+### Synopsis
+***
+
+All examples below are assume to have included `folly/Conv.h`
+and issued `using namespace folly;` You will need:
+
+``` Cpp
+    // To format as text and append to a string, use toAppend.
+    fbstring str;
+    toAppend(2.5, &str);
+    CHECK_EQ(str, "2.5");
+
+    // Multiple arguments are okay, too. Just put the pointer to string at the end.
+    toAppend(" is ", 2, " point ", 5, &str);
+    CHECK_EQ(str, "2.5 is 2 point 5");
+
+    // You don't need to use fbstring (although it's much faster for conversions and in general).
+    std::string stdStr;
+    toAppend("Pi is about ", 22.0 / 7, &stdStr);
+    // In general, just use to<TargetType>(sourceValue). It returns its result by value.
+    stdStr = to<std::string>("Variadic ", "arguments also accepted.");
+
+    // to<fbstring> is 2.5x faster than to<std::string> for typical workloads.
+    str = to<fbstring>("Variadic ", "arguments also accepted.");
+```
+
+### Integral-to-integral conversion
+***
+
+Using `to<Target>(value)` to convert one integral type to another
+will behave as follows:
+
+* If the target type can accommodate all possible values of the
+  source value, the value is implicitly converted. No further
+  action is taken. Example:
+
+``` Cpp
+        short x;
+        unsigned short y;
+        ...
+        auto a = to<int>(x); // zero overhead conversion
+        auto b = to<int>(y); // zero overhead conversion
+```
+
+* Otherwise, `to` inserts bounds checks and throws
+  `std::range_error` if the target type cannot accommodate the
+  source value. Example:
+
+``` Cpp
+    short x;
+    unsigned short y;
+    long z;
+    ...
+    x = 123;
+    auto a = to<unsigned short>(x); // fine
+    x = -1;
+    a = to<unsigned short>(x); // THROWS
+    z = 2000000000;
+    auto b = to<int>(z); // fine
+    z += 1000000000;
+    b = to<int>(z); // THROWS
+    auto b = to<unsigned int>(z); // fine
+```
+
+### Anything-to-string conversion
+***
+
+As mentioned, there are two primitives for converting anything to
+string: `to` and `toAppend`. They support the same set of source
+types, literally by definition (`to` is implemented in terms of
+`toAppend` for all types). The call `toAppend(value, &str)`
+formats and appends `value` to `str` whereas
+`to<StringType>(value)` formats `value` as a `StringType` and
+returns the result by value. Currently, the supported
+`StringType`s are `std::string` and `fbstring`
+
+Both `toAppend` and `to` with a string type as a target support
+variadic arguments. Each argument is converted in turn. For
+`toAppend` the last argument in a variadic list must be the
+address of a supported string type (no need to specify the string
+type as a template argument).
+
+#### Integral-to-string conversion
+
+Nothing special here - integrals are converted to strings in
+decimal format, with a '-' prefix for negative values. Example:
+
+``` Cpp
+    auto a = to<fbstring>(123);
+    assert(a == "123");
+    a = to<fbstring>(-456);
+    assert(a == "-456");
+```
+
+The conversion implementation is aggressively optimized. It
+converts two digits at a time assisted by fixed-size tables.
+Converting a `long` to an `fbstring` is 3.6x faster than using
+`boost::lexical_cast` and 2.5x faster than using `sprintf` even
+though the latter is used in conjunction with a stack-allocated
+constant-size buffer.
+
+Note that converting integral types to `fbstring` has a
+particular advantage compared to converting to `std::string`
+No integral type (<= 64 bits) has more than 20 decimal digits
+including sign. Since `fbstring` employs the small string
+optimization for up to 23 characters, converting an integral
+to `fbstring` is guaranteed to not allocate memory, resulting
+in significant speed and memory locality gains. Benchmarks
+reveal a 2x gain on a typical workload.
+
+#### `char` to string conversion
+
+Although `char` is technically an integral type, most of the time
+you want the string representation of `'a'` to be `"a"`, not `96`
+That's why `folly/Conv.h` handles `char` as a special case that
+does the expected thing. Note that `signed char` and `unsigned
+char` are still considered integral types.
+
+
+#### Floating point to string conversion
+
+`folly/Conv.h` uses [V8's double conversion](http://code.google.com/p/double-conversion/)
+routines. They are accurate and fast; on typical workloads,
+`to<fbstring>(doubleValue)` is 1.9x faster than `sprintf` and
+5.5x faster than `boost::lexical_cast` (It is also 1.3x faster
+than `to<std::string>(doubleValue)`
+
+#### `const char*` to string conversion
+
+For completeness, `folly/Conv.h` supports `const char*` including
+i.e. string literals. The "conversion" consists, of course, of
+the string itself. Example:
+
+``` Cpp
+    auto s = to<fbstring>("Hello, world");
+    assert(s == "Hello, world");
+```
+
+#### Anything from string conversion (i.e. parsing)
+***
+
+`folly/Conv.h` includes three kinds of parsing routines:
+
+* `to<Type>(const char* begin, const char* end)` rigidly
+  converts the range [begin, end) to `Type` These routines have
+  drastic restrictions (e.g. allow no leading or trailing
+  whitespace) and are intended as an efficient back-end for more
+  tolerant routines.
+* `to<Type>(stringy)` converts `stringy` to `Type` Value
+  `stringy` may be of type `const char*`, `StringPiece`,
+  `std::string`, or `fbstring` (Technically, the requirement is
+  that `stringy` implicitly converts to a `StringPiece`
+* `to<Type>(&stringPiece)` parses with progress information:
+  given `stringPiece` of type `StringPiece` it parses as much
+  as possible from it as type `Type` and alters `stringPiece`
+  to remove the munched characters. This is easiest clarified
+  by an example:
+
+``` Cpp
+    fbstring s = " 1234 angels on a pin";
+    StringPiece pc(s);
+    auto x = to<int>(&pc);
+    assert(x == 1234);
+    assert(pc == " angels on a pin";
+```
+
+Note how the routine ate the leading space but not the trailing one.
+
+#### Parsing integral types
+
+Parsing integral types is unremarkable - decimal format is
+expected, optional `'+'` or `'-'` sign for signed types, but no
+optional `'+'` is allowed for unsigned types. The one remarkable
+element is speed - parsing typical `long` values is 6x faster than
+`sscanf`. `folly/Conv.h` uses aggressive loop unrolling and
+table-assisted SIMD-style code arrangement that avoids integral
+division (slow) and data dependencies across operations
+(ILP-unfriendly). Example:
+
+``` Cpp
+    fbstring str = "  12345  ";
+    assert(to<int>(str) == 12345);
+    str = "  12345six seven eight";
+    StringPiece pc(str);
+    assert(to<int>(&pc) == 12345);
+    assert(str == "six seven eight");
+```
+
+#### Parsing floating-point types
+
+`folly/Conv.h` uses, again, [V8's double-conversion](http://code.google.com/p/double-conversion/)
+routines as back-end. The speed is 3x faster than `sscanf` and
+1.7x faster than in-home routines such as `parse<double>` But
+the more important detail is accuracy - even if you do code a
+routine that works faster than `to<double>` chances are it is
+incorrect and will fail in a variety of corner cases. Using
+`to<double>` is strongly recommended.
+
+Note that if an unparsable string is passed to `to<double>` `NaN`
+is returned, which can be tested for as follows:
+
+``` Cpp
+    fbstring str = "not a double";
+    double d = to<double>(str);
+    if (std::isnan(d)) {
+      // string could not be parsed
+    }
+```
--- /dev/null
+++ b/hphp/submodules/folly/folly/docs/DynamicConverter.md
@@ -0,0 +1,72 @@
+`folly/DynamicConverter.h`
+--------------------------
+
+When dynamic objects contain data of a known type, it is sometimes
+useful to have its well-typed representation. A broad set of
+type-conversions are contained in `DynamicConverter.h`, and
+facilitate the transformation of dynamic objects into their well-typed
+format.
+
+### Usage
+***
+
+Simply pass a dynamic into a templated convertTo:
+
+```
+    dynamic d = { { 1, 2, 3 }, { 4, 5 } }; // a vector of vector of int
+    auto vvi = convertTo<fbvector<fbvector<int>>>(d);
+```
+
+### Supported Types
+***
+
+convertTo naturally supports conversions to
+
+1. arithmetic types (such as int64_t, unsigned short, bool, and double)
+2. fbstring, std::string
+3. containers and map-containers
+
+NOTE:
+
+convertTo<Type> will assume that Type is a container if
+* it has a Type::value_type, and
+* it has a Type::iterator, and
+* it has a constructor that accepts two InputIterators
+
+Additionally, convertTo<Type> will assume that Type is a map if
+* it has a Type::key_type, and
+* it has a Type::mapped_type, and
+* value_type is a pair of const key_type and mapped_type
+
+If Type meets the container criteria, then it will be constructed
+by calling its InputIterator constructor.
+
+### Customization
+***
+
+If you want to use convertTo to convert dynamics into your own custom
+class, then all you have to do is provide a template specialization
+of DynamicConverter with the static method convert. Make sure you put it
+in namespace folly.
+
+Example:
+
+``` Cpp
+    struct Token {
+      int kind_;
+      fbstring lexeme_;
+      
+      explicit Token(int kind, const fbstring& lexeme)
+        : kind_(kind), lexeme_(lexeme) {}
+    };
+    namespace folly {
+    template <> struct DynamicConverter<Token> {
+      static Token convert(const dynamic& d) {
+        int k = convertTo<int>(d["KIND"]);
+        fbstring lex = convertTo<fbstring>(d["LEXEME"]);
+        return Token(k, lex);
+      }
+    };
+    }
+```
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/docs/Dynamic.md
@@ -0,0 +1,202 @@
+`folly/dynamic.h`
+-----------------
+
+`folly/dynamic.h` provides a runtime dynamically typed value for
+C++, similar to the way languages with runtime type systems work
+(e.g. Python). It can hold types from a predetermined set of types
+(ints, bools, arrays of other dynamics, etc), similar to something like
+`boost::variant`, but the syntax is intended to be a little more like
+using the native type directly.
+
+To use `dynamic`, you need to be using gcc 4.6 or later. You'll want to
+include `folly/dynamic.h` (or perhaps also `folly/json.h`).
+
+### Overview
+***
+
+Here are some code samples to get started (assumes a `using
+folly::dynamic;` was used):
+
+``` Cpp
+    dynamic twelve = 12; // creates a dynamic that holds an integer
+    dynamic str = "string"; // yep, this one is an fbstring
+
+    // A few other types.
+    dynamic nul = nullptr;
+    dynamic boolean = false;
+
+    // Arrays can be initialized with brackets.
+    dynamic array = { "array ", "of ", 4, " elements" };
+    assert(array.size() == 4);
+    dynamic emptyArray = {};
+    assert(array.empty());
+
+    // Maps from dynamics to dynamics are called objects.  The
+    // dynamic::object constant is how you make an empty map from dynamics
+    // to dynamics.
+    dynamic map = dynamic::object;
+    map["something"] = 12;
+    map["another_something"] = map["something"] * 2;
+
+    // Dynamic objects may be intialized this way
+    dynamic map2 = dynamic::object("something", 12)("another_something", 24);
+```
+
+### Runtime Type Checking and Conversions
+***
+
+Any operation on a dynamic requires checking at runtime that the
+type is compatible with the operation. If it isn't, you'll get a
+`folly::TypeError`. Other exceptions can also be thrown if
+you try to do something impossible (e.g. if you put a very large
+64-bit integer in and try to read it out as a double).
+
+More examples should hopefully clarify this:
+
+``` Cpp
+    dynamic dint = 42;
+
+    dynamic str = "foo";
+    dynamic anotherStr = str + "something"; // fine
+    dynamic thisThrows = str + dint; // TypeError is raised
+```
+
+Explicit type conversions can be requested for some of the basic types:
+
+``` Cpp
+    dynamic dint = 12345678;
+    dynamic doub = dint.asDouble(); // doub will hold 12345678.0
+    dynamic str = dint.asString(); // str == "12345678"
+
+    dynamic hugeInt = std::numeric_limits<int64_t>::max();
+    dynamic hugeDoub = hugeInt.asDouble();  // throws a folly/Conv.h error,
+                                            // since it can't fit in a double
+```
+
+For more complicated conversions, see [DynamicConverter](DynamicConverter.md).
+
+### Iteration and Lookup
+***
+
+You can iterate over dynamic arrays as you would over any C++ sequence container.
+
+``` Cpp
+    dynamic array = {2, 3, "foo"};
+
+    for (auto& val : array) {
+      doSomethingWith(val);
+    }
+```
+
+You can iterate over dynamic maps by calling `items()`, `keys()`,
+`values()`, which behave similarly to the homonymous methods of Python
+dictionaries.
+
+``` Cpp
+    dynamic obj = dynamic::object(2, 3)("hello", "world")("x", 4);
+
+    for (auto& pair : obj.items()) {
+      // Key is pair.first, value is pair.second
+      processKey(pair.first);
+      processValue(pair.second);
+    }
+
+    for (auto& key : obj.keys()) {
+      processKey(key);
+    }
+
+    for (auto& value : obj.values()) {
+      processValue(value);
+    }
+```
+
+You can find an element by key in a dynamic map using the `find()` method,
+which returns an iterator compatible with `items()`:
+
+``` Cpp
+    dynamic obj = dynamic::object(2, 3)("hello", "world")("x", 4);
+
+    auto pos = obj.find("hello");
+    // pos->first is "hello"
+    // pos->second is "world"
+
+    auto pos = obj.find("no_such_key);
+    // pos == obj.items().end()
+```
+
+
+### Use for JSON
+***
+
+The original motivation for implementing this type was to try to
+make dealing with json documents in C++ almost as easy as it is
+in languages with dynamic type systems (php or javascript, etc).
+The reader can judge whether we're anywhere near that goal, but
+here's what it looks like:
+
+``` Cpp
+    // Parsing JSON strings and using them.
+    std::string jsonDocument = R"({"key":12,"key2":[false, null, true, "yay"]})";
+    dynamic parsed = folly::parseJson(jsonDocument);
+    assert(parsed["key"] == 12);
+    assert(parsed["key2"][0] == false);
+    assert(parsed["key2"][1] == nullptr);
+
+    // Building the same document programatically.
+    dynamic sonOfAJ = dynamic::object
+      ("key", 12)
+      ("key2", { false, nullptr, true, "yay" });
+
+    // Printing.  (See also folly::toPrettyJson)
+    auto str = folly::toJson(sonOfAJ);
+    assert(jsonDocument.compare(str) == 0);
+```
+
+### Performance
+***
+
+Dynamic typing is more expensive than static typing, even when
+you do it in C++. ;)
+
+However, some effort has been made to keep `folly::dynamic` and
+the json (de)serialization at least reasonably performant for
+common cases. The heap is only used for arrays and objects, and
+move construction is fully supported. String formatting
+internally also uses the highly performant `folly::to<>` (see
+`folly/Conv.h`).
+
+A trade off to keep in mind though, is that
+`sizeof(folly::dynamic)` is 64 bytes. You probably don't want to
+use it if you need to allocate large numbers of them (prefer
+static types, etc).
+
+### Some Design Rationale
+***
+
+**Q. Why is there no default constructor?**
+
+This is a bit of a limitation of `std::initializer_list<>` for
+this use case. The expression `dynamic d = {}` is required by the
+standard to call the default constructor if one exists (the
+reasoning for this makes sense, since `{}` is part of the concept
+of "uniform initialization", and is intended for use with things
+like `std::vector`). It would be surprising if this expression
+didn't leave `d.isArray()` true, but on the other hand it would
+also be surprising if `dynamic d` left `d.isArray()` as true. The
+solution was just to disallow uninitialized dynamics: every
+dynamic must start out being assigned to some value (or nullptr).
+
+**Q. Why doesn't a dynamic string support begin(), end(), and operator[]?**
+
+The value_type of a dynamic iterator is `dynamic`, and `operator[]`
+(or the `at()` function) has to return a reference to a dynamic.  If
+we wanted this to work for strings, this would mean we'd have to
+support dynamics with a character type, and moreover that the internal
+representation of strings would be such that we can hand out
+references to dynamic as accessors on individual characters.  There
+are a lot of potential efficiency drawbacks with this, and it seems
+like a feature that is not needed too often in practice.
+
+**Q. Isn't this just a poor imitation of the C# language feature?**
+
+Pretty much.
--- /dev/null
+++ b/hphp/submodules/folly/folly/docs/FBString.md
@@ -0,0 +1,46 @@
+`folly/FBString.h`
+------------------
+
+`fbstring` is a drop-in replacement for `std::string`. The main
+benefit of `fbstring` is significantly increased performance on
+virtually all important primitives. This is achieved by using a
+three-tiered storage strategy and by cooperating with the memory
+allocator. In particular, `fbstring` is designed to detect use of
+jemalloc and cooperate with it to achieve significant improvements in
+speed and memory usage.
+
+`fbstring` supports x32 and x64 architectures. Porting it to big endian
+architectures would require some changes.
+
+### Storage strategies
+***
+
+* Small strings (<= 23 chars) are stored in-situ without memory
+  allocation.
+
+* Medium strings (24 - 255 chars) are stored in malloc-allocated
+  memory and copied eagerly.
+
+* Large strings (> 255 chars) are stored in malloc-allocated memory and
+  copied lazily.
+
+### Implementation highlights
+***
+
+* 100% compatible with `std::string`.
+
+* Thread-safe reference counted copy-on-write for strings "large"
+  strings (> 255 chars).
+
+* Uses `malloc` instead of allocators.
+
+* Jemalloc-friendly. `fbstring` automatically detects if application
+  uses jemalloc and if so, significantly improves allocation
+  strategy by using non-standard jemalloc extensions.
+
+* `find()` is implemented using simplified Boyer-Moore
+  algorithm. Casual tests indicate a 30x speed improvement over
+  `string::find()` for successful searches and a 1.5x speed
+  improvement for failed searches.
+
+* Offers conversions to and from `std::string`.
\ No newline at end of file
--- /dev/null
+++ b/hphp/submodules/folly/folly/docs/FBVector.md
@@ -0,0 +1,242 @@
+`folly/FBvector.h`
+------------------
+
+Simply replacing `std::vector` with `folly::fbvector` (after
+having included the `folly/FBVector.h` header file) will
+improve the performance of your C++ code using vectors with
+common coding patterns. The improvements are always non-negative,
+almost always measurable, frequently significant, sometimes
+dramatic, and occasionally spectacular.
+
+### Sample
+***
+
+    folly::fbvector<int> numbers({0, 1, 2, 3});
+    numbers.reserve(10);
+    for (int i = 4; i < 10; i++) {
+      numbers.push_back(i * 2);
+    }
+    assert(numbers[6] == 12);
+
+### Motivation
+***
+
+`std::vector` is the stalwart abstraction many use for
+dynamically-allocated arrays in C++. It is also the best known
+and most used of all containers. It may therefore seem a
+surprise that `std::vector` leaves important - and sometimes
+vital - efficiency opportunities on the table. This document
+explains how our own drop-in abstraction `fbvector` improves key
+performance aspects of `std::vector`. Refer to
+folly/test/FBVectorTest.cpp for a few benchmarks.
+
+### Memory Handling
+***
+
+It is well known that `std::vector` grows exponentially (at a
+constant factor) in order to avoid quadratic growth performance.
+The trick is choosing a good factor (any factor greater than 1
+ensures O(1) amortized append complexity towards infinity). A
+factor that's too small causes frequent vector reallocation; one
+that's too large forces the vector to consume much more memory
+than needed. The initial HP implementation by Stepanov used a
+growth factor of 2, i.e. whenever you'd `push_back` into a vector
+without there being room, it would double the current capacity.
+
+With time, other compilers reduced the growth factor to 1.5, but
+gcc has staunchly used a growth factor of 2. In fact it can be
+mathematically proven that a growth factor of 2 is rigorously the
+<i>worst</i> possible because it never allows the vector to reuse
+any of its previously-allocated memory. That makes the vector cache-
+unfriendly and memory manager unfriendly.
+
+To see why that's the case, consider a large vector of capacity C
+residing somewhere at the beginning of an initially unoccupied
+chunk. When the request for growth comes about, the vector
+(assuming no in-place resizing, see the appropriate section in
+this document) will allocate a chunk next to its current chunk,
+copy its existing data, and then deallocate the old chunk. So now
+we have a chunk of size C followed by a chunk of size k * C.
+Continuing this process we'll then have a chunk of size k * k * C
+to the right and so on. That leads to a series of the form (using
+^^ for power):
+
+    C, C*k,  C*k^^2, C*k^^3, ...
+
+If we choose k = 2 we know that every element in the series will
+be strictly larger than the sum of all previous ones because of
+the remarkable equality:
+
+    1 + 2^^1 + 2^^2 + 2^^3... + 2^^n = 2^^(n+1) - 1
+
+What that really means is that the new request for a chunk will
+be never satisfiable by coalescing all previously-used chunks.
+This is not quite what you'd want.
+
+We would of course want the vector to not crawl forward in
+memory, but instead to move back to its previously-allocated
+chunks. Any number smaller than 2 guarantees that you'll be able
+at some point to reuse the previous chunks. Going through the
+math reveals the equation:
+
+    k^^n <= 1 + k + k^^2 + ... + k^^(n-2)
+
+If some number n satisfies that equation, it means you can reuse
+memory after n reallocations. The graphical solver below reveals
+that choosing k = 1.5 (blue line) allows memory reuse after 4
+reallocations, choosing k = 1.45 (red line) allows memory reuse
+after 3 reallocations, and choosing k = 1.3 (black line) allows
+reuse after only 2 reallocations.
+
+![graphical solutions](./Fbvector--graphical_solutions.png)
+
+Of course, the above makes a number of simplifying assumptions
+about how the memory allocator works, but definitely you don't
+want to choose the theoretically absolute worst growth factor.
+`fbvector` uses a growth factor of 1.5. That does not impede good
+performance at small sizes because of the way `fbvector`
+cooperates with jemalloc (below).
+
+### The jemalloc Connection
+***
+
+Virtually all modern allocators allocate memory in fixed-size
+quanta that are chosen to minimize management overhead while at
+the same time offering good coverage at low slack. For example, an
+allocator may choose blocks of doubling size (32, 64, 128,
+<t_co>, ...) up to 4096, and then blocks of size multiples of a
+page up until 1MB, and then 512KB increments and so on.
+
+As discussed above, `std::vector` also needs to (re)allocate in
+quanta. The next quantum is usually defined in terms of the
+current size times the infamous growth constant. Because of this
+setup, `std::vector` has some slack memory at the end much like
+an allocated block has some slack memory at the end.
+
+It doesn't take a rocket surgeon to figure out that an allocator-
+aware `std::vector` would be a marriage made in heaven: the
+vector could directly request blocks of "perfect" size from the
+allocator so there would be virtually no slack in the allocator.
+Also, the entire growth strategy could be adjusted to work
+perfectly with allocator's own block growth strategy. That's
+exactly what `fbvector` does - it automatically detects the use
+of jemalloc and adjusts its reallocation strategy accordingly.
+
+But wait, there's more. Many memory allocators do not support in-
+place reallocation, although most of them could. This comes from
+the now notorious design of `realloc()` to opaquely perform
+either in-place reallocation or an allocate-memcpy-deallocate
+cycle. Such lack of control subsequently forced all clib-based
+allocator designs to avoid in-place reallocation, and that
+includes C++'s `new` and `std:allocator`. This is a major loss of
+efficiency because an in-place reallocation, being very cheap,
+may mean a much less aggressive growth strategy. In turn that
+means less slack memory and faster reallocations.
+
+### Object Relocation
+***
+
+One particularly sensitive topic about handling C++ values is
+that they are all conservatively considered <i>non-
+relocatable</i>. In contrast, a relocatable value would preserve
+its invariant even if its bits were moved arbitrarily in memory.
+For example, an `int32` is relocatable because moving its 4 bytes
+would preserve its actual value, so the address of that value
+does not "matter" to its integrity.
+
+C++'s assumption of non-relocatable values hurts everybody for
+the benefit of a few questionable designs. The issue is that
+moving a C++ object "by the book" entails (a) creating a new copy
+from the existing value; (b) destroying the old value. This is
+quite vexing and violates common sense; consider this
+hypothetical conversation between Captain Picard and an
+incredulous alien:
+
+Incredulous Alien: "So, this teleporter, how does it work?"<br>
+Picard: "It beams people and arbitrary matter from one place to
+another."<br> Incredulous Alien: "Hmmm... is it safe?"<br>
+Picard: "Yes, but earlier models were a hassle. They'd clone the
+person to another location. Then the teleporting chief would have
+to shoot the original. Ask O'Brien, he was an intern during those
+times. A bloody mess, that's what it was."
+
+Only a tiny minority of objects are genuinely non-relocatable:
+
+* Objects that use internal pointers, e.g.:
+
+    class Ew {
+      char buffer[1024];
+      char * pointerInsideBuffer;
+    public:
+      Ew() : pointerInsideBuffer(buffer) {}
+      ...
+    }
+
+* Objects that need to update "observers" that store pointers to them.
+
+The first class of designs can always be redone at small or no
+cost in efficiency. The second class of objects should not be
+values in the first place - they should be allocated with `new`
+and manipulated using (smart) pointers. It is highly unusual for
+a value to have observers that alias pointers to it.
+
+Relocatable objects are of high interest to `std::vector` because
+such knowledge makes insertion into the vector and vector
+reallocation considerably faster: instead of going to Picard's
+copy-destroy cycle, relocatable objects can be moved around
+simply by using `memcpy` or `memmove`. This optimization can
+yield arbitrarily high wins in efficiency; for example, it
+transforms `vector< vector<double> >` or `vector< hash_map<int,
+string> >` from risky liabilities into highly workable
+compositions.
+
+In order to allow fast relocation without risk, `fbvector` uses a
+trait `folly::IsRelocatable` defined in `"folly/Traits.h"`. By default,
+`folly::IsRelocatable::value` conservatively yields false. If
+you know that your type `Widget` is in fact relocatable, go right
+after `Widget`'s definition and write this:
+
+    // at global namespace level
+    namespace folly {
+      struct IsRelocatable<Widget> : boost::true_type {};
+    }
+
+If you don't do this, `fbvector<Widget>` will fail to compile
+with a `BOOST_STATIC_ASSERT`.
+
+#### Additional Constraints
+
+Similar improvements are possible in presence of a "simple" type
+- more specifically, one that has a trivial assignment (i.e.
+assignment is the same as bitblitting the bits over) or a nothrow
+default constructor. These traits are used gainfully by
+`fbvector` in a variety of places. Fortunately, these traits are
+already present in the C++ standard (well, currently in Boost).
+To summarize, in order to work with `fbvector`, a type `Widget`
+must pass:
+
+    BOOST_STATIC_ASSERT(
+      IsRelocatable<Widget>::value &&
+      (boost::has_trivial_assign<T>::value || boost::has_nothrow_constructor<T>::value));
+
+These traits go hand in hand; for example, it would be very
+difficult to design a class that satisfies one branch of the
+conjunction above but not the other. `fbvector` uses these simple
+constraints to minimize the number of copies made on many common
+operations such as `push_back`, `insert`, or `resize`.
+
+To make it easy for you to state assumptions about a given type
+or family of parameterized types, check Traits.h and in
+particular handy family of macros FOLLY_ASSUME_FBVECTOR_COMPATIBLE*.
+
+### Miscellaneous
+***
+
+`fbvector` uses a careful implementation all around to make
+sure it doesn't lose efficiency through the cracks. Some future
+directions may be in improving raw memory copying (`memcpy` is
+not an intrinsic in gcc and does not work terribly well for
+large chunks) and in furthering the collaboration with
+jemalloc. Have fun!
+
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/docs/Format.md
@@ -0,0 +1,181 @@
+`folly/Format.h`
+----------------
+
+`folly/Format.h` provides a fast, powerful, type-safe, flexible facility
+for formatting text, using a specification language similar to Python's
+[str.format](http://docs.python.org/library/string.html#formatstrings).
+By default, it can format strings, numbers (integral and floating point),
+and dynamically-typed `folly::dynamic` objects, and can extract values from
+random-access containers and string-keyed maps.  In many cases, `format` is
+faster than `sprintf` as well as being fully type-safe.
+
+To use `format`, you need to be using gcc 4.6 or later.  You'll want to include
+`folly/Format.h`.
+
+### Overview
+***
+
+Here are some code samples to get started:
+
+``` Cpp
+using folly::format;
+using folly::vformat;
+
+// Objects produced by format() can be streamed without creating
+// an intermediary string; {} yields the next argument using default
+// formatting.
+std::cout << format("The answers are {} and {}", 23, 42);
+// => "The answers are 23 and 42"
+
+// To insert a literal '{' or '}', just double it.
+std::cout << format("{} {{}} {{{}}}", 23, 42);
+// => "23 {} {42}"
+
+// Arguments can be referenced out of order, even multiple times
+std::cout << format("The answers are {1}, {0}, and {1} again", 23, 42);
+// => "The answers are 42, 23, and 42 again"
+
+// It's perfectly fine to not reference all arguments
+std::cout << format("The only answer is {1}", 23, 42);
+// => "The only answer is 42"
+
+// Values can be extracted from indexable containers
+// (random-access sequences and integral-keyed maps), and also from
+// string-keyed maps
+std::vector<int> v {23, 42};
+std::map<std::string, std::string> m { {"what", "answer"} };
+std::cout << format("The only {1[what]} is {0[1]}", v, m);
+// => "The only answer is 42"
+
+// If you only have one container argument, vformat makes the syntax simpler
+std::map<std::string, std::string> m { {"what", "answer"}, {"value", "42"} };
+std::cout << vformat("The only {what} is {value}", m);
+// => "The only answer is 42"
+// same as
+std::cout << format("The only {0[what]} is {0[value]}", m);
+// => "The only answer is 42"
+
+// {} works for vformat too
+std::vector<int> v {42, 23};
+std::cout << vformat("{} {}", v);
+// => "42 23"
+
+// format and vformat work with pairs and tuples
+std::tuple<int, std::string, int> t {42, "hello", 23};
+std::cout << vformat("{0} {2} {1}", t);
+// => "42 23 hello"
+
+// Format supports width, alignment, arbitrary fill, and various
+// format specifiers, with meanings similar to printf
+// "X<10": fill with 'X', left-align ('<'), width 10
+std::cout << format("{:X<10} {}", "hello", "world");
+// => "helloXXXXX world"
+
+// Format supports printf-style format specifiers
+std::cout << format("{0:05d} decimal = {0:04x} hex", 42);
+// => "00042 decimal = 002a hex"
+
+// Formatter objects may be written to a string using folly::to or
+// folly::toAppend (see folly/Conv.h), or by calling their appendTo(),
+// str(), and fbstr() methods
+std::string s = format("The only answer is {}", 42).str();
+std::cout << s;
+// => "The only answer is 42"
+```
+
+
+### Format string syntax
+***
+
+Format string (`format`):
+`"{" [arg_index] ["[" key "]"] [":" format_spec] "}"`
+
+- `arg_index`: index of argument to format; default = next argument.  Note
+  that a format string may have either default argument indexes or
+  non-default argument indexes, but not both (to avoid confusion).
+- `key`: if the argument is a container (C-style array or pointer,
+  `std::array`, vector, deque, map), you may use this
+  to select the element to format; works with random-access sequences and
+  integer- and string-keyed maps.  Multiple level keys work as well, with
+  components separated with "."; for example, given
+  `map<string, map<string, string>> m`, `{[foo.bar]}` selects
+  `m["foo"]["bar"]`.
+- `format_spec`: format specification, see below
+
+Format string (`vformat`):
+`"{" [ key ] [":" format_spec] "}"`
+
+- `key`: select the argument to format from the container argument;
+  works with random-access sequences and integer- and string-keyed maps.
+  Multiple level keys work as well, with components separated with "."; for
+  example, given `map<string, map<string, string>> m`, `{foo.bar}` selects
+  `m["foo"]["bar"]`.
+- `format_spec`: format specification, see below
+
+Format specification:
+`[[fill] align] [sign] ["#"] ["0"] [width] [","] ["." precision] [type]`
+
+- `fill` (may only be specified if `align` is also specified): pad with this
+  character ('` `' (space) or '`0`' (zero) might be useful; space is default)
+- `align`: one of '`<`', '`>`', '`=`', '`^`':
+    - '`<`': left-align (default for most objects)
+    - '`>`': right-align (default for numbers)
+    - '`=`': pad after sign, but before significant digits; used to print
+            `-0000120`; only valid for numbers
+    - '`^`': center
+- `sign`: one of '`+`', '`-`', ' ' (space) (only valid for numbers)
+    - '`+`': output '`+`' if positive or zero, '`-`' if negative
+    - '`-`': output '`-`' if negative, nothing otherwise (default)
+    - '` `' (space): output '` `' (space) if positive or zero, '`-`' if negative
+- '`#`': output base prefix (`0` for octal, `0b` or `0B` for binary, `0x` or
+  `0X` for hexadecimal; only valid for integers)
+- '`0`': 0-pad after sign, same as specifying "`0=`" as the `fill` and
+  `align` parameters (only valid for numbers)
+- `width`: minimum field width
+- '`,`' (comma): output comma as thousands' separator (only valid for integers,
+  and only for decimal output)
+- `precision` (not allowed for integers):
+    - for floating point values, number of digits after decimal point ('`f`' or
+      '`F`' presentation) or number of significant digits ('`g`' or '`G`')
+    - for others, maximum field size (truncate subsequent characters)
+- `type`: presentation format, see below
+
+Presentation formats:
+
+- Strings (`folly::StringPiece`, `std::string`, `folly::fbstring`,
+  `const char*`):
+    - '`s`' (default)
+- Integers:
+    - '`b`': output in binary (base 2) ("`0b`" prefix if '`#`' specified)
+    - '`B`': output in binary (base 2) ("`0B`" prefix if '`#`' specified)
+    - '`c`': output as a character (cast to `char`)
+    - '`d`': output in decimal (base 10) (default)
+    - '`o`': output in octal (base 8)
+    - '`O`': output in octal (base 8) (same as '`o`')
+    - '`x`': output in hexadecimal (base 16) (lower-case digits above 9)
+    - '`X`': output in hexadecimal (base 16) (upper-case digits above 9)
+    - '`n`': locale-aware output (currently same as '`d`')
+- `bool`:
+    - default: output "`true`" or "`false`" as strings
+    - integer presentations allowed as well
+- `char`:
+    - same as other integers, but default is '`c`' instead of '`d`'
+- Floating point (`float`, `double`; `long double` is not implemented):
+    - '`e`': scientific notation using '`e`' as exponent character
+    - '`E`': scientific notation using '`E`' as exponent character
+    - '`f`': fixed point
+    - '`F`': fixed point (same as '`f`')
+    - '`g`': general; use either '`f`' or '`e`' depending on magnitude (default)
+    - '`G`': general; use either '`f`' or '`E`' depending on magnitude
+    - '`n`': locale-aware version of '`g`' (currently same as '`g`')
+    - '`%`': percentage: multiply by 100 then display as '`f`'
+
+
+### Extension
+***
+
+You can extend `format` for your own class by providing a specialization for
+`folly::FormatValue`.  See `folly/Format.h` and `folly/FormatArg.h` for
+details, and the existing specialization for `folly::dynamic` in
+`folly/dynamic-inl.h` for an implementation example.
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/docs/.gitignore
@@ -0,0 +1 @@
+*.html
--- /dev/null
+++ b/hphp/submodules/folly/folly/docs/GroupVarint.md
@@ -0,0 +1,46 @@
+`folly/GroupVarint.h`
+---------------------
+
+`folly/GroupVarint.h` is an implementation of variable-length encoding for 32-
+and 64-bit integers using the Group Varint encoding scheme as described in
+Jeff Dean's [WSDM 2009 talk][wsdm] and in [Information Retrieval: Implementing
+and Evaluating Search Engines][irbook].
+
+[wsdm]: http://research.google.com/people/jeff/WSDM09-keynote.pdf
+[irbook]: http://www.ir.uwaterloo.ca/book/addenda-06-index-compression.html
+
+Briefly, a group of four 32-bit integers is encoded as a sequence of variable
+length, between 5 and 17 bytes; the first byte encodes the length (in bytes)
+of each integer in the group.  A group of five 64-bit integers is encoded as a
+sequence of variable length, between 7 and 42 bytes; the first two bytes
+encode the length (in bytes) of each integer in the group.
+
+`GroupVarint.h` defines a few classes:
+
+* `GroupVarint<T>`, where `T` is `uint32_t` or `uint64_t`:
+
+    Basic encoding / decoding interface, mainly aimed at encoding / decoding
+    one group at a time.
+
+* `GroupVarintEncoder<T, Output>`, where `T` is `uint32_t` or `uint64_t`,
+  and `Output` is a functor that accepts `StringPiece` objects as arguments:
+
+    Streaming encoder: add values one at a time, and they will be
+    flushed to the output one group at a time.  Handles the case where
+    the last group is incomplete (the number of integers to encode isn't
+    a multiple of the group size)
+
+* `GroupVarintDecoder<T>`, where `T` is `uint32_t` or `uint64_t`:
+
+    Streaming decoder: extract values one at a time.  Handles the case where
+    the last group is incomplete.
+
+The 32-bit implementation is significantly faster than the 64-bit
+implementation; on platforms supporting the SSSE3 instruction set, we
+use the PSHUFB instruction to speed up lookup, as described in [SIMD-Based
+Decoding of Posting Lists][cikmpaper] (CIKM 2011).
+
+[cikmpaper]: http://www.stepanovpapers.com/CIKM_2011.pdf
+
+For more details, see the header file `folly/GroupVarint.h` and the
+associated test file `folly/test/GroupVarintTest.cpp`.
--- /dev/null
+++ b/hphp/submodules/folly/folly/docs/Histogram.md
@@ -0,0 +1,104 @@
+`folly/Histogram.h`
+-------------------
+
+### Classes
+***
+
+#### `Histogram`
+
+`Histogram.h` defines a simple histogram class, templated on the type of data
+you want to store.  This class is useful for tracking a large stream of data
+points, where you want to remember the overall distribution of the data, but do
+not need to remember each data point individually.
+
+Each histogram bucket stores the number of data points that fell in the bucket,
+as well as the overall sum of the data points in the bucket.  Note that no
+overflow checking is performed, so if you have a bucket with a large number of
+very large values, it may overflow and cause inaccurate data for this bucket.
+As such, the histogram class is not well suited to storing data points with
+very large values.  However, it works very well for smaller data points such as
+request latencies, request or response sizes, etc.
+
+In addition to providing access to the raw bucket data, the `Histogram` class
+also provides methods for estimating percentile values.  This allows you to
+estimate the median value (the 50th percentile) and other values such as the
+95th or 99th percentiles.
+
+All of the buckets have the same width.  The number of buckets and bucket width
+is fixed for the lifetime of the histogram.  As such, you do need to know your
+expected data range ahead of time in order to have accurate statistics.  The
+histogram does keep one bucket to store all data points that fall below the
+histogram minimum, and one bucket for the data points above the maximum.
+However, because these buckets don't have a good lower/upper bound, percentile
+estimates in these buckets may be inaccurate.
+
+#### `HistogramBuckets`
+
+The `Histogram` class is built on top of `HistogramBuckets`.
+`HistogramBuckets` provides an API very similar to `Histogram`, but allows a
+user-defined bucket class.  This allows users to implement more complex
+histogram types that store more than just the count and sum in each bucket.
+
+When computing percentile estimates `HistogramBuckets` allows user-defined
+functions for computing the average value and data count in each bucket.  This
+allows you to define more complex buckets which may have multiple different
+ways of computing the average value and the count.
+
+For example, one use case could be tracking timeseries data in each bucket.
+Each set of timeseries data can have independent data in the bucket, which can
+show how the data distribution is changing over time.
+
+### Example Usage
+***
+
+Say we have code that sends many requests to remote services, and want to
+generate a histogram showing how long the requests take.  The following code
+will initialize histogram with 50 buckets, tracking values between 0 and 5000.
+(There are 50 buckets since the bucket width is specified as 100.  If the
+bucket width is not an even multiple of the histogram range, the last bucket
+will simply be shorter than the others.)
+
+``` Cpp
+    folly::Histogram<int64_t> latencies(100, 0, 5000);
+```
+
+The addValue() method is used to add values to the histogram.  Each time a
+request finishes we can add its latency to the histogram:
+
+``` Cpp
+    latencies.addValue(now - startTime);
+```
+
+You can access each of the histogram buckets to display the overall
+distribution.  Note that bucket 0 tracks all data points that were below the
+specified histogram minimum, and the last bucket tracks the data points that
+were above the maximum.
+
+``` Cpp
+    unsigned int numBuckets = latencies.getNumBuckets();
+    cout << "Below min: " << latencies.getBucketByIndex(0).count << "\n";
+    for (unsigned int n = 1; n < numBuckets - 1; ++n) {
+      cout << latencies.getBucketMin(n) << "-" << latencies.getBucketMax(n)
+           << ": " << latencies.getBucketByIndex(n).count << "\n";
+    }
+    cout << "Above max: "
+         << latencies.getBucketByIndex(numBuckets - 1).count << "\n";
+```
+
+You can also use the `getPercentileEstimate()` method to estimate the value at
+the Nth percentile in the distribution.  For example, to estimate the median,
+as well as the 95th and 99th percentile values:
+
+``` Cpp
+    int64_t median = latencies.getPercentileEstimate(0.5);
+    int64_t p95 = latencies.getPercentileEstimate(0.95);
+    int64_t p99 = latencies.getPercentileEstimate(0.99);
+```
+
+### Thread Safety
+***
+
+Note that `Histogram` and `HistogramBuckets` objects are not thread-safe.  If
+you wish to access a single `Histogram` from multiple threads, you must perform
+your own locking to ensure that multiple threads do not access it at the same
+time.
--- /dev/null
+++ b/hphp/submodules/folly/folly/docs/Makefile
@@ -0,0 +1,33 @@
+SOURCES=$(wildcard *.md)
+PDF=$(SOURCES:%.md=%.pdf)
+HTML=$(SOURCES:%.md=%.html)
+INSTALL=install -c -m 644
+PYTHON=python
+PANDOCARGS=-s
+PANDOC=/usr/bin/pandoc
+
+export LANGUAGE=C
+export LC_ALL=C
+
+all: html index.html
+
+pdf: $(PDF)
+
+html: $(HTML)
+
+# This needs pandoc 1.9 or later to work
+%.pdf: %.md
+	$(PANDOC) -f markdown -o $*.pdf $*.md
+
+%.html: %.md style.css
+	$(PANDOC) $(PANDOCARGS) -H style.css -f markdown -t html --toc -o $*.html $*.md
+
+docs.md: $(SOURCES) style.css
+	$(PANDOC) $(PANDOCARGS) -H style.css -f markdown -t markdown --toc -o $@ *.md
+
+index.html: $(SOURCES) style.css
+	$(PANDOC) $(PANDOCARGS) -H style.css -f markdown -t html --toc -o $@ *.md
+
+
+clean:
+	$(RM) $(PDF) $(HTML) index.html
--- /dev/null
+++ b/hphp/submodules/folly/folly/docs/Overview.md
@@ -0,0 +1,260 @@
+`folly/`
+------
+
+### Introduction
+
+Folly (acronymed loosely after Facebook Open Source Library) is a
+library of C++11 components designed with practicality and efficiency
+in mind. It complements (as opposed to competing against) offerings
+such as Boost and of course `std`. In fact, we embark on defining our
+own component only when something we need is either not available, or
+does not meet the needed performance profile.
+
+Performance concerns permeate much of Folly, sometimes leading to
+designs that are more idiosyncratic than they would otherwise be (see
+e.g. `PackedSyncPtr.h`, `SmallLocks.h`). Good performance at large
+scale is a unifying theme in all of Folly.
+
+### Logical Design
+
+Folly is a collection of relatively independent components, some as
+simple as a few symbols. There is no restriction on internal
+dependencies, meaning that a given folly module may use any other
+folly components.
+
+All symbols are defined in the top-level namespace `folly`, except of
+course macros. Macro names are ALL_UPPERCASE. Namespace `folly`
+defines other internal namespaces such as `internal` or `detail`. User
+code should not depend on symbols in those namespaces.
+
+### Physical Design
+
+At the top level Folly uses the classic "stuttering" scheme
+`folly/folly` used by Boost and others. The first directory serves as
+an installation root of the library (with possible versioning a la
+`folly-1.0/`), and the second is to distinguish the library when
+including files, e.g. `#include "folly/FBString.h"`.
+
+The directory structure is flat (mimicking the namespace structure),
+i.e. we don't have an elaborate directory hierarchy (it is possible
+this will change in future versions). The subdirectory `experimental`
+contains files that are used inside folly and possibly at Facebook but
+not considered stable enough for client use. Your code should not use
+files in `folly/experimental` lest it may break when you update Folly.
+
+The `folly/folly/test` subdirectory includes the unittests for all
+components, usually named `ComponentXyzTest.cpp` for each
+`ComponentXyz.*`. The `folly/folly/docs` directory contains
+documentation.
+
+### Compatibility
+
+Currently, `folly` has been tested on gcc 4.6 on 64-bit installations
+of Fedora 17, Ubuntu 12.04, and Debian wheezy. It might work unmodified
+on other 64-bit Linux platforms.
+
+### Components
+
+Below is a list of Folly components in alphabetical order, along with
+a brief description of each.
+
+#### `Arena.h`, `ThreadCachedArena.h`
+
+Simple arena for memory allocation: multiple allocations get freed all
+at once. With threaded version.
+
+#### [`AtomicHashMap.h`, `AtomicHashArray.h`](AtomicHashMap.md)
+
+High-performance atomic hash map with almost lock-free operation.
+
+#### [`Benchmark.h`](Benchmark.md)
+
+A small framework for benchmarking code. Client code registers
+benchmarks, optionally with an argument that dictates the scale of the
+benchmark (iterations, working set size etc). The framework runs
+benchmarks (subject to a command-line flag) and produces formatted
+output with timing information.
+
+#### `Bits.h`
+
+Various bit manipulation utilities optimized for speed; includes functions
+that wrap the
+[ffsl(l)](http://linux.die.net/man/3/ffsll) primitives in a uniform
+interface.
+
+#### `ConcurrentSkipList.h`
+
+An implementation of the structure described in [A Provably Correct
+Scalable Concurrent Skip
+List](http://www.cs.tau.ac.il/~shanir/nir-pubs-web/Papers/OPODIS2006-BA.pdf)
+by Herlihy et al.
+
+#### [`Conv.h`](Conv.md)
+
+A variety of data conversion routines (notably to and from string),
+optimized for speed and safety.
+
+#### `DiscriminatedPtr.h`
+
+Similar to `boost::variant`, but restricted to pointers only. Uses the
+highest-order unused 16 bits in a pointer as discriminator. So
+`sizeof(DiscriminatedPtr<int, string, Widget>) == sizeof(void*)`.
+
+#### [`dynamic.h`](Dynamic.md)
+
+Dynamically-typed object, created with JSON objects in mind.
+
+#### `Endian.h`
+
+Endian conversion primitives.
+
+####`Escape.h`
+
+Escapes a string in C style.
+
+####`eventfd.h`
+
+Wrapper around the
+[`eventfd`](http://www.kernel.org/doc/man-pages/online/pages/man2/eventfd.2.html)
+system call.
+
+####[`FBString.h`](FBString.md)
+
+A drop-in implementation of `std::string` with a variety of optimizations.
+
+####[`FBVector.h`](FBVector.md)
+
+A mostly drop-in implementation of `std::vector` with a variety of
+optimizations.
+
+####`Foreach.h`
+
+Pseudo-statements (implemented as macros) for iteration.
+
+####[`Format.h`](Format.md)
+
+Python-style formatting utilities.
+
+####[`GroupVarint.h`](GroupVarint.md)
+
+[Group Varint
+encoding](http://www.ir.uwaterloo.ca/book/addenda-06-index-compression.html)
+for 32-bit values.
+
+####`Hash.h`
+
+Various popular hash function implementations.
+
+####[`Histogram.h`](Histogram.md)
+
+A simple class for collecting histogram data.
+
+####`IntrusiveList.h`
+
+Convenience type definitions for using `boost::intrusive_list`.
+
+####`json.h`
+
+JSON serializer and deserializer. Uses `dynamic.h`.
+
+####`Likely.h`
+
+Wrappers around [`__builtin_expect`](http://gcc.gnu.org/onlinedocs/gcc/Other-Builtins.html).
+
+####`Malloc.h`
+
+Memory allocation helpers, particularly when using jemalloc.
+
+####`MapUtil.h`
+
+Helpers for finding items in associative containers (such as
+`std::map` and `std::unordered_map`).
+
+####[`PackedSyncPtr.h`](PackedSyncPtr.md)
+
+A highly specialized data structure consisting of a pointer, a 1-bit
+spin lock, and a 15-bit integral, all inside one 64-bit word.
+
+####`Preprocessor.h`
+
+Necessarily evil stuff.
+
+####`PrettyPrint.h`
+
+Pretty-printer for numbers that appends suffixes of unit used: bytes
+(kb, MB, ...), metric suffixes (k, M, G, ...), and time (s, ms, us,
+ns, ...).
+
+####[`ProducerConsumerQueue.h`](ProducerConsumerQueue.md)
+
+Lock free single-reader, single-writer queue.
+
+####`Random.h`
+
+Defines only one function---`randomNumberSeed()`.
+
+####`Range.h`
+
+Boost-style range facility and the `StringPiece` specialization.
+
+####`RWSpinLock.h`
+
+Fast and compact reader-writer spin lock.
+
+####`ScopeGuard.h`
+
+C++11 incarnation of the old [ScopeGuard](http://drdobbs.com/184403758) idiom.
+
+####[`SmallLocks.h`](SmallLocks.md)
+
+Very small spin locks (1 byte and 1 bit).
+
+####`small_vector.h`
+
+Vector with the small buffer optimization and an optional embedded
+`PicoSpinLock`.
+
+####`sorted_vector_types.h`
+
+Collections similar to `std::map` but implemented as sorted vectors.
+
+####`StlAllocator.h`
+
+STL allocator wrapping a simple allocate/deallocate interface.
+
+####`String.h`
+
+String utilities that connect `folly::fbstring` with `std::string`.
+
+####`Subprocess.h`
+
+Subprocess library, modeled after Python's subprocess module.
+
+####[`Synchronized.h`](Synchronized.md)
+
+High-level synchronization library.
+
+####`System.h`
+
+Demangling and errno utilities.
+
+####[`ThreadCachedInt.h`](ThreadCachedInt.md)
+
+High-performance atomic increment using thread caching.
+
+####[`ThreadLocal.h`](ThreadLocal.md)
+
+Improved thread local storage for non-trivial types.
+
+####`TimeoutQueue.h`
+
+Queue with per-item timeout.
+
+####`Traits.h`
+
+Type traits that complement those defined in the standard C++11 header
+`<traits>`.
+
+####`Unicode.h`
+
+Defines the `codePointToUtf8` function.
--- /dev/null
+++ b/hphp/submodules/folly/folly/docs/PackedSyncPtr.md
@@ -0,0 +1,77 @@
+`folly/PackedSyncPtr.h`
+----------------------
+
+A highly specialized data structure consisting of a pointer, a 1-bit
+spin lock, and a 15-bit integral packed into `sizeof(void*)`.
+
+Typical application is for microsharding of many elements within containers.
+Because there is no memory overhead, an arbitrarily large number of locks can be
+used to minimize lock contention with no memory penalty.  Additionally,
+excellent cache performance is obtained by storing the lock inline with the
+pointer (no additional cache miss or false sharing).  Finally, because it uses a
+simple spinlock mechanism, the cost of aqcuiring an uncontended lock is minimal.
+
+### Usage
+***
+
+This is not a "smart" pointer: nothing automagical is going on
+here.  Locking is up to the user.  Resource deallocation is up to
+the user.  Locks are never acquired or released outside explicit
+calls to lock() and unlock().
+
+Change the value of the raw pointer with set(), but you must hold
+the lock when calling this function if multiple threads could be
+using it.
+
+Here is an example of using a PackedSyncPtr to build a synchronized vector with
+no memory overhead - the spinlock and size are stored in the 16 unused bits of
+pointer, the rest of which points to the actual data.  See
+`folly/small_vector.h` for a complete implementation of this concept.
+
+``` Cpp
+    template<typename T>
+    class SyncVec {
+      PackedSyncPtr<T> base;
+
+     public:
+      SyncVec() { base.init(); }
+
+      void push_back(const T& t) {
+        base.set(
+          static_cast<T*>(realloc(base.get(), (base.extra() + 1) * sizeof(T))));
+        base[base.extra()] = t;
+        base.setExtra(base.extra() + 1);
+      }
+
+      size_t size() const {
+        return base.extra();
+      }
+
+      void lock() {
+        base.lock();
+      }
+
+      void unlock() {
+        base.unlock();
+      }
+
+      T* begin() const {
+        return base.get();
+      }
+
+      T* end() const {
+        return base.get() + base.extra();
+      }
+    };
+```
+
+### Implementation
+***
+
+This is using an x64-specific detail about the effective virtual
+address space.  Long story short: the upper two bytes of all our
+pointers will be zero in reality---and if you have a couple billion
+such pointers in core, it makes pretty good sense to try to make
+use of that memory.  The exact details can be perused here:
+
+[http://en.wikipedia.org/wiki/X86-64#Canonical_form_addresses](http://en.wikipedia.org/wiki/X86-64#Canonical_form_addresses)
--- /dev/null
+++ b/hphp/submodules/folly/folly/docs/ProducerConsumerQueue.md
@@ -0,0 +1,82 @@
+`folly/ProducerConsumerQueue.h`
+-------------------------------
+
+The `folly::ProducerConsumerQueue` class is a one-producer
+one-consumer queue with very low synchronization overhead.
+
+The queue must be created with a fixed maximum size (and allocates
+that many cells of sizeof(T)), and it provides just a few simple
+operations:
+
+ * `read`: Attempt to read the value at the front to the queue into a variable,
+           returns `false` iff queue was empty.
+ * `write`: Emplace a value at the end of the queue, returns `false` iff the
+            queue was full.
+ * `frontPtr`: Retrieve a pointer to the item at the front of the queue, or
+               `nullptr` if it is empty.
+ * `popFront`: Remove the item from the front of the queue (queue must not be
+               empty).
+ * `isEmpty`: Check if the queue is empty.
+ * `isFull`: Check if the queue is full.
+ * `sizeGuess`: Returns the number of entries in the queue. Because of the
+                way we coordinate threads, this guess could be slightly wrong
+                when called by the producer/consumer thread, and it could be
+                wildly inaccurate if called from any other threads. Hence,
+                only call from producer/consumer threads!
+
+All of these operations are wait-free.  The read operations (including
+`frontPtr` and `popFront`) and write operations must only be called by the
+reader and writer thread, respectively. `isFull`, `isEmpty`, and `sizeGuess`
+may be called by either thread, but the return values from `read`, `write`, or
+`frontPtr` are sufficient for most cases.
+
+`write` may fail if the queue is full, and `read` may fail if the queue is
+empty, so in many situations it is important to choose the queue size such that
+the queue filling  or staying empty for long is unlikely.
+
+### Example
+***
+
+A toy example that doesn't really do anything useful:
+
+``` Cpp
+    folly::ProducerConsumerQueue<folly::fbstring> queue;
+
+    std::thread reader([&queue] {
+      for (;;) {
+        folly::fbstring str;
+        while (!queue.read(str)) {
+          //spin until we get a value
+          continue;
+        }
+
+        sink(str);
+      }
+    });
+
+    // producer thread:
+    for (;;) {
+      folly::fbstring str = source();
+      while (!queue.write(str)) {
+        //spin until the queue has room
+        continue;
+      }
+    }
+```
+
+Alternatively, the consumer may be written as follows to use the 'front' value
+in place, thus avoiding moves or copies:
+
+``` Cpp
+    std::thread reader([&queue] {
+      for (;;) {
+        folly::fbstring* pval;
+        do {
+          pval = queue.frontPtr();
+        } while (!pval); // spin until we get a value;
+
+        sink(*pval);
+        queue.popFront();
+      }
+    });
+```
--- /dev/null
+++ b/hphp/submodules/folly/folly/docs/SmallLocks.md
@@ -0,0 +1,24 @@
+`folly/SmallLocks.h`
+--------------------
+
+This module is currently x64 only.
+
+This header defines two very small mutex types.  These are useful in
+highly memory-constrained environments where contention is unlikely.
+The purpose of these is to allow fine-grained locking in massive data
+structures where memory is at a premium.  Often, each record may have
+a spare bit or byte lying around, so sometimes these can be tacked on
+with no additional memory cost.
+
+There are two types exported from this header.  `MicroSpinLock` is a
+single byte lock, and `PicoSpinLock` can be wrapped around an
+integer to use a single bit as a lock.  Why do we have both?
+Because you can't use x64 `bts` on a single byte, so
+`sizeof(MicroSpinLock)` is smaller than `sizeof(PicoSpinLock)` can
+be, giving it some use cases.
+
+Both the locks in this header model the C++11 Lockable concept.  So
+you can use `std::lock_guard` or `std::unique_lock` to lock them in an
+RAII way if you want.
+
+Additional information is in the header.
--- /dev/null
+++ b/hphp/submodules/folly/folly/docs/small_vector.md
@@ -0,0 +1,69 @@
+`folly/small_vector.h`
+----------------------
+
+`folly::small_vector<T,Int=1,...>` is a sequence container that
+implements small buffer optimization. It behaves similarly to
+std::vector, except until a certain number of elements are reserved it
+does not use the heap.
+
+Like standard vector, it is guaranteed to use contiguous memory.  (So,
+after it spills to the heap all the elements live in the heap buffer.)
+
+Simple usage example:
+
+``` Cpp
+    small_vector<int,2> vec;
+    vec.push_back(0); // Stored in-place on stack
+    vec.push_back(1); // Still on the stack
+    vec.push_back(2); // Switches to heap buffer.
+```
+
+### Details
+***
+
+This class is useful in either of following cases:
+
+* Short-lived stack vectors with few elements (or maybe with a
+  usually-known number of elements), if you want to avoid malloc.
+
+* If the vector(s) are usually under a known size and lookups are very
+  common, you'll save an extra cache miss in the common case when the
+  data is kept in-place.
+
+* You have billions of these vectors and don't want to waste space on
+  `std::vector`'s capacity tracking.  This vector lets `malloc` track our
+  allocation capacity.  (Note that this slows down the
+  insertion/reallocation code paths significantly; if you need those
+  to be fast you should use `fbvector`.)
+
+The last two cases were the main motivation for implementing it.
+
+There are also a couple of flags you can pass into this class
+template to customize its behavior.  You can provide them in any
+order after the in-place count.  They are all in the `namespace
+small_vector_policy`.
+
+* `NoHeap` - Avoid the heap entirely.  (Throws `std::length_error` if
+  you would've spilled out of the in-place allocation.)
+
+* `OneBitMutex` - On x64 platforms, this spends one bit of the
+  `size_type` to provide a spin lock that you can use for whatever you
+  want.
+
+* `<Any integral type>` - customizes the amount of space we spend on
+  tracking the size of the vector.
+
+A couple more examples:
+
+``` Cpp
+    // With space for 32 in situ unique pointers, and only using a
+    // 4-byte size_type.
+    small_vector<std::unique_ptr<int>, 32, uint32_t> v;
+
+    // A inline vector of up to 256 ints which will not use the
+    // heap and comes with a spin lock.
+    small_vector<int, 256, NoHeap, OneBitMutex> v;
+
+    // Same as the above, but making the size_type smaller too.
+    small_vector<int, 256, NoHeap, uint16_t, OneBitMutex> v;
+```
--- /dev/null
+++ b/hphp/submodules/folly/folly/docs/style.css
@@ -0,0 +1,7 @@
+<style type="text/css">
+pre.literal-block, pre.doctest-block, pre.sourceCode {
+    margin-left: 2em;
+    margin-right: 2em;
+    background-color: #eeeeee
+}
+</style>
--- /dev/null
+++ b/hphp/submodules/folly/folly/docs/Synchronized.md
@@ -0,0 +1,609 @@
+`folly/Synchronized.h`
+----------------------
+
+`folly/Synchronized.h` introduces a simple abstraction for mutex-
+based concurrency. It replaces convoluted, unwieldy, and just
+plain wrong code with simple constructs that are easy to get
+right and difficult to get wrong.
+
+### Motivation
+
+Many of our multithreaded Thrift services (not to mention general
+concurrent C++ code) use shared data structures associated with
+locks. This follows the time-honored adage of mutex-based
+concurrency control "associate mutexes with data, not code".
+Examples are abundant and easy to find. For example:
+
+``` Cpp
+
+    class AdPublisherHandler : public AdPopulatorIf,
+                               public fb303::FacebookBase,
+                               public ZkBaseApplication {
+      ...
+      OnDemandUpdateIdMap adsToBeUpdated_;
+      ReadWriteMutex adsToBeUpdatedLock_;
+
+      OnDemandUpdateIdMap limitsToBeUpdated_;
+      ReadWriteMutex limitsToBeUpdatedLock_;
+
+      OnDemandUpdateIdMap campaignsToBeUpdated_;
+      ReadWriteMutex campaignsToBeUpdatedLock_;
+      ...
+    };
+```
+
+Whenever the code needs to read or write some of the protected
+data, it acquires the mutex for reading or for reading and
+writing. For example:
+
+``` Cpp
+    void AdPublisherHandler::requestUpdateAdId(const int64_t adId,
+                                               const int32_t dbId) {
+      checkDbHandlingStatus(dbId);
+      RWGuard g(adsToBeUpdatedLock_, RW_WRITE);
+      adsToBeUpdated_[dbId][adId] = 1;
+      adPublisherMonitor_->addStatValue("request_adId_update", 1, dbId);
+      LOG(INFO) << "received request to update ad id " << adId;
+    }
+```
+
+The pattern is an absolute classic and present everywhere.
+However, it is inefficient, makes incorrect code easy to
+write, is prone to deadlocking, and is bulkier than it could
+otherwise be. To expand:
+
+* In the code above, for example, the critical section is only
+  the line right after `RWGuard`'s definition; it is frivolous
+  that everything else (including a splurging `LOG(INFO)`) keeps
+  the lock acquired for no good reason. This is because the
+  locked regions are not visible; the guard's construction
+  introduces a critical section as long as the remainder of the
+  current scope.
+* The correctness of the technique is entirely predicated on
+  convention. There is no ostensible error for code that:
+
+    * manipulates a piece of data without acquiring its lock first
+    * acquires a different lock instead of the intended one
+    * acquires a lock in read mode but modifies the guarded data structure
+    * acquires a lock in read-write mode although it only has `const`
+      access to the guarded data
+    * acquires one lock when another lock is already held, which may
+      lead to deadlocks if another thread acquires locks in the
+      inverse order
+
+### Introduction to `folly/Synchronized.h`
+
+The same code sample could be rewritten with `Synchronized`
+as follows:
+
+``` Cpp
+    class AdPublisherHandler : public AdPopulatorIf,
+                               public fb303::FacebookBase,
+                               public ZkBaseApplication {
+      ...
+      Synchronized<OnDemandUpdateIdMap>
+        adsToBeUpdated_,
+        limitsToBeUpdated_,
+        campaignsToBeUpdated_;
+      ...
+    };
+
+    void AdPublisherHandler::requestUpdateAdId(const int64_t adId,
+                                               const int32_t dbId) {
+      checkDbHandlingStatus(dbId);
+      SYNCHRONIZED (adsToBeUpdated_) {
+        adsToBeUpdated_[dbId][adId] = 1;
+      }
+      adPublisherMonitor_->addStatValue("request_adId_update", 1, dbId);
+      LOG(INFO) << "received request to update ad id " << adId;
+    }
+```
+
+The rewrite does at maximum efficiency what needs to be done:
+acquires the lock associated with the `OnDemandUpdateIdMap`
+object, writes to the map, and releases the lock immediately
+thereafter.
+
+On the face of it, that's not much to write home about, and not
+an obvious improvement over the previous state of affairs. But
+the features at work invisible in the code above are as important
+as those that are visible:
+
+* Unlike before, the data and the mutex protecting it are
+  inextricably encapsulated together.
+* Critical sections are readily visible and emphasize code that
+  needs to do minimal work and be subject to extra scrutiny.
+* Dangerous nested `SYNCHRONIZED` statements are more visible
+  than sequenced declarations of guards at the same level. (This
+  is not foolproof because a method call issued inside a
+  `SYNCHRONIZED` scope may open its own `SYNCHRONIZED` block.) A
+  construct `SYNCHRONIZED_DUAL`, discussed later in this
+  document, allows locking two objects quasi-simultaneously in
+  the same order in all threads, thus avoiding deadlocks.
+* If you tried to use `adsToBeUpdated_` outside the
+  `SYNCHRONIZED` scope, you wouldn't be able to; it is virtually
+  impossible to tease the map object without acquiring the
+  correct lock. However, inside the `SYNCHRONIZED` scope, the
+  *same* name serves as the actual underlying object of type
+  `OnDemandUpdateIdMap` (which is a map of maps).
+* Outside `SYNCHRONIZED`, if you just want to call one
+  method, you can do so by using `adsToBeUpdated_` as a
+  pointer like this:
+
+    `adsToBeUpdated_->clear();`
+
+This acquires the mutex, calls `clear()` against the underlying
+map object, and releases the mutex immediately thereafter.
+
+`Synchronized` offers several other methods, which are described
+in detail below.
+
+### Template class `Synchronized<T>`
+
+##### Constructors
+
+The default constructor default-initializes the data and its
+associated mutex.
+
+
+The copy constructor locks the source for reading and copies its
+data into the target. (The target is not locked as an object
+under construction is only accessed by one thread.)
+
+Finally, `Synchronized<T>` defines an explicit constructor that
+takes an object of type `T` and copies it. For example:
+
+``` Cpp
+    // Default constructed
+    Synchronized< map<string, int> > syncMap1;
+
+    // Copy constructed
+    Synchronized< map<string, int> > syncMap2(syncMap1);
+
+    // Initializing from an existing map
+    map<string, int> init;
+    init["world"] = 42;
+    Synchronized< map<string, int> > syncMap3(init);
+    EXPECT_EQ(syncMap3->size(), 1);
+```
+
+#### Assignment, swap, and copying
+
+The canonical assignment operator locks both objects involved and
+then copies the underlying data objects. The mutexes are not
+copied. The locks are acquired in increasing address order, so
+deadlock is avoided. For example, there is no problem if one
+thread assigns `a = b` and the other assigns `b = a` (other than
+that design probably deserving a Razzie award). Similarly, the
+`swap` method takes a reference to another `Synchronized<T>`
+object and swaps the data. Again, locks are acquired in a well-
+defined order. The mutexes are not swapped.
+
+An additional assignment operator accepts a `const T&` on the
+right-hand side. The operator copies the datum inside a
+critical section.
+
+In addition to assignment operators, `Synchronized<T>` has move
+assignment operators.
+
+An additional `swap` method accepts a `T&` and swaps the data
+inside a critical section. This is by far the preferred method of
+changing the guarded datum wholesale because it keeps the lock
+only for a short time, thus lowering the pressure on the mutex.
+
+To get a copy of the guarded data, there are two methods
+available: `void copy(T*)` and `T copy()`. The first copies data
+to a provided target and the second returns a copy by value. Both
+operations are done under a read lock. Example:
+
+``` Cpp
+    Synchronized< fbvector<fbstring> > syncVec1, syncVec2;
+    fbvector<fbstring> vec;
+
+    // Assign
+    syncVec1 = syncVec2;
+    // Assign straight from vector
+    syncVec1 = vec;
+
+    // Swap
+    syncVec1.swap(syncVec2);
+    // Swap with vector
+    syncVec1.swap(vec);
+
+    // Copy to given target
+    syncVec1.copy(&vec);
+    // Get a copy by value
+    auto copy = syncVec1.copy();
+```
+
+#### `LockedPtr operator->()` and `ConstLockedPtr operator->() const`
+
+We've already seen `operator->` at work. Essentially calling a
+method `obj->foo(x, y, z)` calls the method `foo(x, y, z)` inside
+a critical section as long-lived as the call itself. For example:
+
+``` Cpp
+    void fun(Synchronized< fbvector<fbstring> > & vec) {
+      vec->push_back("hello");
+      vec->push_back("world");
+    }
+```
+
+The code above appends two elements to `vec`, but the elements
+won't appear necessarily one after another. This is because in
+between the two calls the mutex is released, and another thread
+may modify the vector. At the cost of anticipating a little, if
+you want to make sure you insert "world" right after "hello", you
+should do this:
+
+``` Cpp
+    void fun(Synchronized< fbvector<fbstring> > & vec) {
+      SYNCHRONIZED (vec) {
+        vec.push_back("hello");
+        vec.push_back("world");
+      }
+    }
+```
+
+This brings us to a cautionary discussion. The way `operator->`
+works is rather ingenious with creating an unnamed temporary that
+enforces locking and all, but it's not a panacea. Between two
+uses of `operator->`, other threads may change the synchronized
+object in arbitrary ways, so you shouldn't assume any sort of
+sequential consistency. For example, the innocent-looking code
+below may be patently wrong.
+
+If another thread clears the vector in between the call to
+`empty` and the call to `pop_back`, this code ends up attempting
+to extract an element from an empty vector. Needless to say,
+iteration a la:
+
+``` Cpp
+    // No. NO. NO!
+    FOR_EACH_RANGE (i, vec->begin(), vec->end()) {
+      ...
+    }
+```
+
+is a crime punishable by long debugging nights.
+
+If the `Synchronized<T>` object involved is `const`-qualified,
+then you'll only be able to call `const` methods through `operator->`. 
+So, for example, `vec->push_back("xyz")` won't work if `vec`
+were `const`-qualified. The locking mechanism capitalizes on the
+assumption that `const` methods don't modify their underlying
+data and only acquires a read lock (as opposed to a read and
+write lock), which is cheaper but works only if the immutability
+assumption holds. Note that this is strictly not the case because
+`const`-ness can always be undone via `mutable` members, casts,
+and surreptitious access to shared data. Our code is seldom
+guilty of such, and we also assume the STL uses no shenanigans.
+But be warned.
+
+#### `asConst()`
+
+Consider:
+
+``` Cpp
+    void fun(Synchronized<fbvector<fbstring>> & vec) {
+      if (vec->size() > 1000000) {
+        LOG(WARNING) << "The blinkenlights are overloaded.";
+      }
+      vec->push_back("another blinkenlight");
+    }
+```
+
+This code is correct (at least according to a trivial intent),
+but less efficient than it could otherwise be. This is because
+the call `vec->size()` acquires a full read-write lock, but only
+needs a read lock. We need to help the type system here by
+telling it "even though `vec` is a mutable object, consider it a
+constant for this call". This should be easy enough because
+conversion to const is trivial - just issue `const_cast<const
+Synchronized<fbvector<fbstring>>&>(vec)`. Ouch. To make that
+operation simpler - a lot simpler - `Synchronized<T>` defines the
+method `asConst()`, which is a glorious one-liner. With `asConst`
+in tow, it's very easy to achieve what we wanted:
+
+``` Cpp
+    void fun(Synchronized<fbvector<fbstring>> & vec) {
+      if (vec.asConst()->size() > 1000000) {
+        LOG(WARNING) << "The blinkenlights are overloaded.";
+      }
+      vec->push_back("another blinkenlight");
+    }
+```
+
+QED (Quite Easy Done). This concludes the documentation for
+`Synchronized<T>`.
+
+### `SYNCHRONIZED`
+
+The `SYNCHRONIZED` macro introduces a pseudo-statement that adds
+a whole new level of usability to `Synchronized<T>`. As
+discussed, `operator->` can only lock over the duration of a
+call, so it is insufficient for complex operations. With
+`SYNCHRONIZED` you get to lock the object in a scoped manner (not
+unlike Java's `synchronized` statement) and to directly access
+the object inside that scope.
+
+`SYNCHRONIZED` has two forms. We've seen the first one a couple
+of times already:
+
+``` Cpp
+    void fun(Synchronized<fbvector<int>> & vec) {
+      SYNCHRONIZED (vec) {
+        vec.push_back(42);
+        CHECK(vec.back() == 42);
+        ...
+      }
+    }
+```
+
+The scope introduced by `SYNCHRONIZED` is a critical section
+guarded by `vec`'s mutex. In addition to doing that,
+`SYNCHRONIZED` also does an interesting sleight of hand: it binds
+the name `vec` inside the scope to the underlying `fbvector<int>`
+object - as opposed to `vec`'s normal type, which is
+`Synchronized<fbvector<int>>`. This fits very nice the "form
+follow function" - inside the critical section you have earned
+access to the actual data, and the name bindings reflect that as
+well. `SYNCHRONIZED(xyz)` essentially cracks `xyz` temporarily
+and gives you access to its innards.
+
+Now, what if `fun` wants to take a pointer to
+`Synchronized<fbvector<int>>` - let's call it `pvec`? Generally,
+what if we want to synchronize on an expression as opposed to a
+symbolic variable? In that case `SYNCHRONIZED(*pvec)` would not
+work because "`*pvec`" is not a name. That's where the second
+form of `SYNCHRONIZED` kicks in:
+
+``` Cpp
+    void fun(Synchronized<fbvector<int>> * pvec) {
+      SYNCHRONIZED (vec, *pvec) {
+        vec.push_back(42);
+        CHECK(vec.back() == 42);
+        ...
+      }
+    }
+```
+
+Ha, so now we pass two arguments to `SYNCHRONIZED`. The first
+argument is the name bound to the data, and the second argument
+is the expression referring to the `Synchronized<T>` object. So
+all cases are covered.
+
+### `SYNCHRONIZED_CONST`
+
+Recall from the discussion about `asConst()` that we
+sometimes want to voluntarily restrict access to an otherwise
+mutable object. The `SYNCHRONIZED_CONST` pseudo-statement
+makes that intent easily realizable and visible to
+maintainers. For example:
+
+``` Cpp
+    void fun(Synchronized<fbvector<int>> & vec) {
+      fbvector<int> local;
+      SYNCHRONIZED_CONST (vec) {
+        CHECK(vec.size() > 42);
+        local = vec;
+      }
+      local.resize(42000);
+      SYNCHRONIZED (vec) {
+        local.swap(vec);
+      }
+    }
+```
+
+Inside a `SYNCHRONIZED_CONST(xyz)` scope, `xyz` is bound to a `const`-
+qualified datum. The corresponding lock is a read lock.
+
+`SYNCHRONIZED_CONST` also has a two-arguments version, just like
+`SYNCHRONIZED`. In fact, `SYNCHRONIZED_CONST(a)` simply expands
+to `SYNCHRONIZED(a, a.asConst())` and `SYNCHRONIZED_CONST(a, b)`
+expands to `SYNCHRONIZED(a, (b).asConst())`. The type system and
+`SYNCHRONIZED` take care of the rest.
+
+### `TIMED_SYNCHRONIZED` and `TIMED_SYNCHRONIZED_CONST`
+
+These pseudo-statements allow you to acquire the mutex with a
+timeout. Example:
+
+``` Cpp
+    void fun(Synchronized<fbvector<int>> & vec) {
+      TIMED_SYNCHRONIZED (10, vec) {
+        if (vec) {
+          vec->push_back(42);
+          CHECK(vec->back() == 42);
+        } else {
+            LOG(INFO) << "Dognabbit, I've been waiting over here for 10 milliseconds and couldn't get through!";
+        }
+      }
+    }
+```
+
+If the mutex acquisition was successful within a number of
+milliseconds dictated by its first argument, `TIMED_SYNCHRONIZED`
+binds its second argument to a pointer to the protected object.
+Otherwise, the pointer will be `NULL`. (Contrast that with
+`SYNCHRONIZED`), which always succeeds so it binds the protected
+object to a reference.) Inside the `TIMED_SYNCHRONIZED` statement
+you must, of course, make sure the pointer is not null to make
+sure the operation didn't time out.
+
+`TIMED_SYNCHRONIZED` takes two or three parameters. The first is
+always the timeout, and the remaining one or two are just like
+the parameters of `SYNCHRONIZED`.
+
+Issuing `TIMED_SYNCHRONIZED` with a zero timeout is an
+opportunistic attempt to acquire the mutex.
+
+### `UNSYNCHRONIZED`
+
+`SYNCHRONIZED` is a good mechanism for enforcing scoped
+synchronization, but it has the inherent limitation that it
+requires the critical section to be, well, scoped. Sometimes the
+code structure requires a fleeting "escape" from the iron fist of
+synchronization. Clearly, simple cases are handled with sequenced
+`SYNCHRONIZED` scopes:
+
+``` Cpp
+    Synchronized<map<int, string>> dic;
+    ...
+    SYNCHRONIZED (dic) {
+      if (dic.find(0) != dic.end()) {
+        return;
+      }
+    }
+    LOG(INFO) << "Key 0 not found, inserting it."
+    SYNCHRONIZED (dic) {
+      dic[0] = "zero";
+    }
+```
+
+For more complex, nested flow control, you may want to use the
+`UNSYNCHRONIZED` macro. It (only) works inside a `SYNCHRONIZED`
+pseudo-statement and temporarily unlocks the mutex:
+
+``` Cpp
+
+    Synchronized<map<int, string>> dic;
+    ...
+    SYNCHRONIZED (dic) {
+      auto i = dic.find(0);
+      if (i != dic.end()) {
+        UNSYNCHRONIZED (dic) {
+          LOG(INFO) << "Key 0 not found, inserting it."
+        }
+        dic[0] = "zero";
+      } else {
+        *i = "zero";
+      }
+    }
+    LOG(INFO) << "Key 0 not found, inserting it."
+    SYNCHRONIZED (dic) {
+      dic[0] = "zero";
+    }
+```
+
+Clearly `UNSYNCHRONIZED` comes with specific caveats and
+liabilities. You must assume that during the `UNSYNCHRONIZED`
+section, other threads might have changed the protected structure
+in arbitrary ways. In the example above, you cannot use the
+iterator `i` and you cannot assume that the key `0` is not in the
+map; another thread might have inserted it while you were
+bragging on `LOG(INFO)`.
+
+### `SYNCHRONIZED_DUAL`
+
+Sometimes locking just one object won't be able to cut the mustard. Consider a
+function that needs to lock two `Synchronized` objects at the
+same time - for example, to copy some data from one to the other.
+At first sight, it looks like nested `SYNCHRONIZED` statements
+will work just fine:
+
+``` Cpp
+    void fun(Synchronized<fbvector<int>> & a, Synchronized<fbvector<int>> & b) {
+      SYNCHRONIZED (a) {
+        SYNCHRONIZED (b) {
+          ... use a and b ...
+        }
+      }
+    }
+```
+
+This code compiles and may even run most of the time, but embeds
+a deadly peril: if one threads call `fun(x, y)` and another
+thread calls `fun(y, x)`, then the two threads are liable to
+deadlocking as each thread will be waiting for a lock the other
+is holding. This issue is a classic that applies regardless of
+the fact the objects involved have the same type.
+
+This classic problem has a classic solution: all threads must
+acquire locks in the same order. The actual order is not
+important, just the fact that the order is the same in all
+threads. Many libraries simply acquire mutexes in increasing
+order of their address, which is what we'll do, too. The pseudo-
+statement `SYNCHRONIZED_DUAL` takes care of all details of proper
+locking of two objects and offering their innards:
+
+``` Cpp
+    void fun(Synchronized<fbvector<int>> & a, Synchronized<fbvector<int>> & b) {
+      SYNCHRONIZED_DUAL (myA, a, myB, b) {
+        ... use myA and myB ...
+      }
+    }
+```
+
+To avoid potential confusions, `SYNCHRONIZED_DUAL` only defines a
+four-arguments version. The code above locks `a` and `b` in
+increasing order of their address and offers their data under the
+names `myA` and `myB`, respectively.
+
+### Synchronizing several data items with one mutex
+
+The library is geared at protecting one object of a given type
+with a mutex. However, sometimes we'd like to protect two or more
+members with the same mutex. Consider for example a bidirectional
+map, i.e. a map that holds an `int` to `string` mapping and also
+the converse `string` to `int` mapping. The two maps would need
+to be manipulated simultaneously. There are at least two designs
+that come to mind.
+
+#### Using a nested `struct`
+
+You can easily pack the needed data items in a little struct.
+For example:
+
+``` Cpp
+    class Server {
+      struct BiMap {
+        map<int, string> direct;
+        map<string, int> inverse;
+      };
+      Synchronized<BiMap> bimap_;
+      ...
+    };
+    ...
+    SYNCHRONIZED (bymap_) {
+      bymap_.direct[0] = "zero";
+      bymap_.inverse["zero"] = 0;
+    }
+```
+
+With this code in tow you get to use `bimap_` just like any other
+`Synchronized` object, without much effort.
+
+#### Using `std::tuple`
+
+If you won't stop short of using a spaceship-era approach,
+`std::tuple` is there for you. The example above could be
+rewritten for the same functionality like this:
+
+``` Cpp
+    class Server {
+      Synchronized<tuple<map<int, string>, map<string, int>>> bimap_;
+      ...
+    };
+    ...
+    SYNCHRONIZED (bymap_) {
+      get<0>(bymap_)[0] = "zero";
+      get<1>(bymap_)["zero"] = 0;
+    }
+```
+
+The code uses `std::get` with compile-time integers to access the
+fields in the tuple. The relative advantages and disadvantages of
+using a local struct vs. `std::tuple` are quite obvious - in the
+first case you need to invest in the definition, in the second
+case you need to put up with slightly more verbose and less clear
+access syntax.
+
+### Summary
+
+`Synchronized` and its supporting tools offer you a simple,
+robust paradigm for mutual exclusion-based concurrency. Instead
+of manually pairing data with the mutexes that protect it and
+relying on convention to use them appropriately, you can benefit
+of encapsulation and typechecking to offload a large part of that
+task and to provide good guarantees.
--- /dev/null
+++ b/hphp/submodules/folly/folly/docs/ThreadCachedInt.md
@@ -0,0 +1,98 @@
+`folly/ThreadCachedInt.h`
+----------------------
+
+High-performance atomic increment using thread caching.
+
+`folly/ThreadCachedInt.h` introduces a integer class designed for high
+performance increments from multiple threads simultaneously without
+loss of precision.  It has two read modes, `readFast` gives a potentially stale
+value with one load, and `readFull` gives the exact value, but is much slower,
+as discussed below.
+
+
+### Performance
+***
+
+Increment performance is up to 10x greater than `std::atomic_fetch_add` in high
+contention environments.  See `folly/test/ThreadCachedIntTest.h` for more
+comprehensive benchmarks.
+
+`readFast` is as fast as a single load.
+
+`readFull`, on the other hand, requires acquiring a mutex and iterating through
+a list to accumulate the values of all the thread local counters, so is
+significantly slower than `readFast`.
+
+
+### Usage
+***
+
+Create an instance and increment it with `increment` or the operator overloads.
+Read the value with `readFast` for quick, potentially stale data, or `readFull`
+for a more expensive but precise result. There are additional convenience
+functions as well, such as `set`.
+
+``` Cpp
+    ThreadCachedInt<int64_t> val;
+    EXPECT_EQ(0, val.readFast());
+    ++val;                        // increment in thread local counter only
+    EXPECT_EQ(0, val.readFast()); // increment has not been flushed
+    EXPECT_EQ(1, val.readFull()); // accumulates all thread local counters
+    val.set(2);
+    EXPECT_EQ(2, val.readFast());
+    EXPECT_EQ(2, val.readFull());
+```
+
+### Implementation
+***
+
+`folly::ThreadCachedInt` uses `folly::ThreadLocal` to store thread specific
+objects that each have a local counter.  When incrementing, the thread local
+instance is incremented.  If the local counter passes the cache size, the value
+is flushed to the global counter with an atomic increment.  It is this global
+counter that is read with `readFast` via a simple load, but will not count any
+of the updates that haven't been flushed.
+
+In order to read the exact value, `ThreadCachedInt` uses the extended
+`readAllThreads()` API of `folly::ThreadLocal` to iterate through all the
+references to all the associated thread local object instances.  This currently
+requires acquiring a global mutex and iterating through the references,
+accumulating the counters along with the global counter.  This also means that
+the first use of the object from a new thread will acquire the mutex in order to
+insert the thread local reference into the list.  By default, there is one
+global mutex per integer type used in `ThreadCachedInt`.  If you plan on using a
+lot of `ThreadCachedInt`s in your application, considering breaking up the
+global mutex by introducing additional `Tag` template parameters.
+
+`set` simply sets the global counter value, and marks all the thread local
+instances as needing to be reset.  When iterating with `readFull`, thread local
+counters that have been marked as reset are skipped.  When incrementing, thread
+local counters marked for reset are set to zero and unmarked for reset.
+
+Upon destruction, thread local counters are flushed to the parent so that counts
+are not lost after increments in temporary threads.  This requires grabbing the
+global mutex to make sure the parent itself wasn't destroyed in another thread
+already.
+
+### Alternate Implementations
+***
+
+There are of course many ways to skin a cat, and you may notice there is a
+partial alternate implementation in `folly/test/ThreadCachedIntTest.cpp` that
+provides similar performance.  `ShardedAtomicInt` simply uses an array of
+`std::atomic<int64_t>`'s and hashes threads across them to do low-contention
+atomic increments, and `readFull` just sums up all the ints.
+
+This sounds great, but in order to get the contention low enough to get similar
+performance as ThreadCachedInt with 24 threads, `ShardedAtomicInt` needs about
+2000 ints to hash across.  This uses about 20x more memory, and the lock-free
+`readFull` has to sum up all 2048 ints, which ends up being a about 50x slower
+than `ThreadCachedInt` in low contention situations, which is hopefully the
+common case since it's designed for high-write, low read access patterns.
+Performance of `readFull` is about the same speed as `ThreadCachedInt` in high
+contention environments.
+
+Depending on the operating conditions, it may make more sense to use one
+implementation over the other.  For example, a lower contention environment will
+probably be able to use a `ShardedAtomicInt` with a much smaller array without
+hurting performance, while improving memory consumption and perf of `readFull`.
--- /dev/null
+++ b/hphp/submodules/folly/folly/docs/ThreadLocal.md
@@ -0,0 +1,106 @@
+`folly/ThreadLocal.h`
+----------------------
+
+Improved thread local storage for non-trivial types.
+
+ * ~4x faster than `boost::thread_specific_ptr`.
+ * Similar speed as using `pthread_getspecific` directly, but only consumes a
+   single `pthread_key_t` per `Tag` template param.
+ * Expands on the `thread_specific_ptr` API with `accessAllThreads` and extended
+   custom deleter support.
+
+
+### Usage
+***
+
+The API of `ThreadLocalPtr` is very close to `boost::thread_specific_ptr` with
+the notable addition of the `accessAllThreads` method.  There is also a
+`ThreadLocal` class which is a thin wrapper around `ThreadLocalPtr` that manages
+allocation automatically (creates a new object the first time it is dereferenced
+from each thread).
+
+`ThreadLocalPtr` simply gives you a place to put and access a pointer local to
+each thread such that it will be destroyed appropriately.
+
+```Cpp
+{
+  folly::ThreadLocalPtr<Widget> w;
+  w.reset(new Widget(0), Widget::customDeleterA);
+  std::thread([&w]() {
+    w.reset(new Widget(1), Widget::customDeleterB);
+    w.get()->mangleWidget();
+  } // Widget(1) is destroyed with customDeleterB
+} // Widget(0) is destroyed with customDeleterA
+```
+
+Note that `customDeleterB` will get called with
+`TLPDestructionMode::THIS_THREAD` and `customerDeleterA` will get called with
+`TLPDestructionMode::ALL_THREADS`.  This is to distinguish between thread exit
+vs. the entire `ThreadLocalPtr` getting destroyed, in which case there is
+cleanup work that may be avoided.
+
+The `accessAllThreads` interface is provided to walk all the thread local child
+objects of a parent.  `accessAllThreads` initializes an accessor
+which holds a global lock that blocks all creation and destruction of
+`ThreadLocal` objects with the same `Tag` and can be used as an iterable
+container. Typical use is for frequent write, infrequent read data access
+patterns such as counters.  Note that you must specify a unique Tag type so you
+don't block other ThreadLocal object usage, and you should try to minimize the
+lifetime of the accessor so the lock is held for as short as possible).
+
+The following example is a simplification of `folly/ThreadCachedInt.h`.  It
+keeps track of a counter value and allows multiple threads to add to the count
+without synchronization.  In order to get the total count, `read()` iterates
+through all the thread local values via `accessAllThreads()` and sums them up.
+`class NewTag` is used to break the global mutex so that this class won't block
+other `ThreadLocal` usage when `read()` is called.
+
+Note that `read()` holds the global mutex which blocks construction,
+destruction, and `read()` for other `SimpleThreadCachedInt`'s, but does not
+block `add()`.  Also, since it uses the unique `NewTag`, `SimpleThreadCachedInt`
+does not affect other `ThreadLocal` usage.
+
+```Cpp
+class SimpleThreadCachedInt {
+
+  class NewTag;  // Segments the global mutex
+  ThreadLocal<int,NewTag> val_;
+
+ public:
+  void add(int val) {
+    *val_ += val;  // operator*() gives a reference to the thread local instance
+  }
+
+  int read() {
+    int ret = 0;
+    // accessAllThreads acquires the global lock
+    for (const auto& i : val_.accessAllThreads()) {
+      ret += i;
+    }  // Global lock is released on scope exit
+    return ret;
+  }
+};
+```
+
+
+### Implementation
+***
+
+We keep a `__thread` array of pointers to objects (`ThreadEntry::elements`)
+where each array has an index for each unique instance of the `ThreadLocalPtr`
+object.  Each `ThreadLocalPtr` object has a unique id that is an index into
+these arrays so we can fetch the correct object from thread local storage
+very efficiently.
+
+In order to prevent unbounded growth of the id space and thus huge
+`ThreadEntry::elements` arrays, for example due to continuous creation and
+destruction of `ThreadLocalPtr` objects, we keep track of all active instances
+by linking them together into a list.  When an instance is destroyed we remove
+it from the chain and insert the id into `freeIds_` for reuse.  These operations
+require a global mutex, but only happen at construction and destruction time.
+`accessAllThreads` also acquires this global mutex.
+
+We use a single global `pthread_key_t` per `Tag` to manage object destruction
+and memory cleanup upon thread exit because there is a finite number of
+`pthread_key_t`'s available per machine.
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/docs/Traits.md
@@ -0,0 +1,175 @@
+'folly/Traits.h'
+-----------------
+
+Implements traits complementary to those provided in <boost/type_traits.h>
+
+  * Implements `IsRelocatable` trait.
+  * Implements `IsOneOf` trait
+  * Macros to state the assumptions easily
+
+### Motivation
+***
+
+`<boost/type_traits.hpp>` is the Boost type-traits library defining a 
+variety of traits such as `is_integral` or `is_floating_point`. This helps 
+to gain more information about a given type.
+Many traits introduced by Boost have been standardized in C++11.
+
+`folly/Traits.h` implements traits complementing those present in boost. 
+
+
+### IsRelocatable
+***
+
+In C++, the default way to move an object is by 
+calling the copy constructor and destroying the old copy 
+instead of directly copying the memory contents by using memcpy(). 
+The conservative approach of moving an object assumes that the copied 
+object is not relocatable. 
+The two following code sequences should be semantically equivalent for a
+relocatable type:
+
+```Cpp
+{
+  void conservativeMove(T * from, T * to) {
+    new(to) T(from);
+    (*from).~T();
+  }
+}
+
+{
+  void optimizedMove(T * from, T * to) {
+    memcpy(to, from, sizeof(T));
+  }
+}
+```
+
+Very few C++ types are non-relocatable.
+The type defined below maintains a pointer inside an embedded buffer and 
+hence would be non-relocatable. Moving the object by simply copying its 
+memory contents would leave the internal pointer pointing to the old buffer.
+
+```Cpp
+class NonRelocatableType {
+private:
+  char buffer[1024];
+  char * pointerToBuffer;
+  ...
+public:
+  NonRelocatableType() : pointerToBuffer(buffer) {}
+  ...
+};
+```
+
+We can optimize the task of moving a relocatable type T using memcpy. 
+IsRelocatable<T>::value describes the ability of moving around memory 
+a value of type T by using memcpy.
+
+### Usage
+***
+
+  * Declaring types
+
+    ```Cpp
+    template <class T1, class T2>
+    class MyParameterizedType;
+
+    class MySimpleType;
+    ```
+
+  * Declaring a type as relocatable
+
+    Appending the lines below after definition of My*Type 
+    (`MyParameterizedType` or `MySimpleType`) will declare it as relocatable
+
+    ```Cpp
+    /* Definition of My*Type goes here */
+    // global namespace (not inside any namespace)
+    namespace folly {
+      // defining specialization of IsRelocatable for MySimpleType
+      template <>
+      struct IsRelocatable<MySimpleType> : boost::true_type {};
+      // defining specialization of IsRelocatable for MyParameterizedType
+      template <class T1, class T2>
+      struct IsRelocatable<MyParameterizedType<T1, T2>>
+          : ::boost::true_type {};
+    }
+    ```
+
+  * To make it easy to state assumptions for a regular type or a family of 
+    parameterized type, various macros can be used as shown below.
+
+  * Stating that a type is Relocatable using a macro
+
+    ```Cpp
+    // global namespace
+    namespace folly {
+      // For a Regular Type
+      FOLLY_ASSUME_RELOCATABLE(MySimpleType);
+      // For a Parameterized Type
+      FOLLY_ASSUME_RELOCATABLE(MyParameterizedType<T1, T2>);
+    }
+    ```
+
+  * Stating that a type has no throw constructor using a macro
+
+    ```Cpp
+    namespace boost {
+      // For a Regular Type
+      FOLLY_ASSUME_HAS_NOTHROW_CONSTRUCTOR(MySimpleType);
+      // For a Parameterized Type
+      FOLLY_ASSUME_HAS_NOTHROW_CONSTRUCTOR(MyParameterizedType<T1, T2>);
+    }
+    ```
+
+`fbvector` only works with relocatable objects. If assumptions are not stated 
+explicitly, `fbvector<MySimpleType>` or `fbvector<MyParameterizedType>` 
+will fail to compile due to assertion below:
+
+```Cpp
+BOOST_STATIC_ASSERT(
+  IsRelocatable<My*Type>::value
+);
+```
+
+FOLLY_ASSUME_FBVECTOR_COMPATIBLE*(type) macros can be used to state that type 
+is relocatable and has nothrow constructor.
+
+  * Stating that a type is `fbvector-compatible` using macros
+    i.e. relocatable and has nothrow default constructor
+
+    ```Cpp
+    // at global level, i.e no namespace
+    // macro for regular type
+    FOLLY_ASSUME_FBVECTOR_COMPATIBLE(MySimpleType);
+    // macro for types having 2 template parameters (MyParameterizedType)
+    FOLLY_ASSUME_FBVECTOR_COMPATIBLE_2(MyParameterizedType);
+    ```
+
+Similarly, 
+
+  * FOLLY_ASSUME_FBVECTOR_COMPATIBLE_1(MyTypeHavingOneParameter) macro is 
+    for family of parameterized types having 1 parameter
+
+  * FOLLY_ASSUME_FBVECTOR_COMPATIBLE_3(MyTypeHavingThreeParameters) macro is 
+    for family of parameterized types having 3 parameters
+
+  * FOLLY_ASSUME_FBVECTOR_COMPATIBLE_4(MyTypeHavingFourParameters) macro is 
+    for family of parameterized types having 4 parameters
+
+Few common types, namely `std::basic_string`, `std::vector`, `std::list`,
+`std::map`, `std::deque`, `std::set`, `std::unique_ptr`, `std::shared_ptr`,
+`std::function`, `boost::shared_ptr` which are compatible with `fbvector` are
+already instantiated and declared compatible with `fbvector`. `fbvector` can be
+directly used with any of these C++ types.
+
+`std::pair` can be safely assumed to be compatible with `fbvector` if both of
+its components are.
+
+### IsOneOf
+***
+
+`boost::is_same<T1, T2>::value` can be used to test if types of T1 and T2 are 
+same. `folly::IsOneOf<T, T1, Ts...>::value` can be used to test if type of T1 
+matches the type of one of the other template parameter, T1, T2, ...Tn.
+Recursion is used to implement this type trait.
--- /dev/null
+++ b/hphp/submodules/folly/folly/DynamicConverter.h
@@ -0,0 +1,340 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// @author Nicholas Ormrod <njormrod@fb.com>
+
+#ifndef DYNAMIC_CONVERTER_H
+#define DYNAMIC_CONVERTER_H
+
+#include "folly/dynamic.h"
+namespace folly {
+  template <typename T> T convertTo(const dynamic&);
+  template <typename T> dynamic toDynamic(const T&);
+}
+
+/**
+ * convertTo returns a well-typed representation of the input dynamic.
+ *
+ * Example:
+ *
+ *   dynamic d = { { 1, 2, 3 }, { 4, 5 } }; // a vector of vector of int
+ *   auto vvi = convertTo<fbvector<fbvector<int>>>(d);
+ *
+ * See docs/DynamicConverter.md for supported types and customization
+ */
+
+
+#include <type_traits>
+#include <iterator>
+#include <boost/iterator/iterator_adaptor.hpp>
+#include <boost/mpl/has_xxx.hpp>
+#include "folly/Likely.h"
+
+namespace folly {
+
+///////////////////////////////////////////////////////////////////////////////
+// traits
+
+namespace dynamicconverter_detail {
+
+BOOST_MPL_HAS_XXX_TRAIT_DEF(value_type);
+BOOST_MPL_HAS_XXX_TRAIT_DEF(iterator);
+BOOST_MPL_HAS_XXX_TRAIT_DEF(mapped_type);
+
+template <typename T> struct class_is_container {
+  typedef std::reverse_iterator<T*> some_iterator;
+  enum { value = has_value_type<T>::value &&
+                 has_iterator<T>::value &&
+              std::is_constructible<T, some_iterator, some_iterator>::value };
+};
+
+template <typename T> struct class_is_range {
+  enum { value = has_value_type<T>::value &&
+                 has_iterator<T>::value };
+};
+
+
+template <typename T> struct is_container
+  : std::conditional<
+      std::is_class<T>::value,
+      class_is_container<T>,
+      std::false_type
+    >::type {};
+
+template <typename T> struct is_range
+  : std::conditional<
+      std::is_class<T>::value,
+      class_is_range<T>,
+      std::false_type
+    >::type {};
+
+template <typename T> struct is_map
+  : std::integral_constant<
+      bool,
+      is_range<T>::value && has_mapped_type<T>::value
+    > {};
+
+} // namespace dynamicconverter_detail
+
+///////////////////////////////////////////////////////////////////////////////
+// custom iterators
+
+/**
+ * We have iterators that dereference to dynamics, but need iterators
+ * that dereference to typename T.
+ *
+ * Implementation details:
+ *   1. We cache the value of the dereference operator. This is necessary
+ *      because boost::iterator_adaptor requires *it to return a
+ *      reference.
+ *   2. For const reasons, we cannot call operator= to refresh the
+ *      cache: we must call the destructor then placement new.
+ */
+
+namespace dynamicconverter_detail {
+
+template<typename T>
+struct Dereferencer {
+  static inline void
+  derefToCache(T* mem, const dynamic::const_item_iterator& it) {
+    throw TypeError("array", dynamic::Type::OBJECT);
+  }
+
+  static inline void derefToCache(T* mem, const dynamic::const_iterator& it) {
+    new (mem) T(convertTo<T>(*it));
+  }
+};
+
+template<typename F, typename S>
+struct Dereferencer<std::pair<F, S>> {
+  static inline void
+  derefToCache(std::pair<F, S>* mem, const dynamic::const_item_iterator& it) {
+    new (mem) std::pair<F, S>(
+        convertTo<F>(it->first), convertTo<S>(it->second)
+    );
+  }
+
+  // Intentional duplication of the code in Dereferencer
+  template <typename T>
+  static inline void derefToCache(T* mem, const dynamic::const_iterator& it) {
+    new (mem) T(convertTo<T>(*it));
+  }
+};
+
+template <typename T, typename It>
+class Transformer : public boost::iterator_adaptor<
+                             Transformer<T, It>,
+                             It,
+                             typename T::value_type
+                           > {
+  friend class boost::iterator_core_access;
+
+  typedef typename T::value_type ttype;
+
+  mutable ttype cache_;
+  mutable bool valid_;
+
+  void increment() {
+    ++this->base_reference();
+    valid_ = false;
+  }
+
+  ttype& dereference() const {
+    if (LIKELY(!valid_)) {
+      cache_.~ttype();
+      Dereferencer<ttype>::derefToCache(&cache_, this->base_reference());
+      valid_ = true;
+    }
+    return cache_;
+  }
+
+public:
+  explicit Transformer(const It& it)
+    : Transformer::iterator_adaptor_(it), valid_(false) {}
+};
+
+// conversion factory
+template <typename T, typename It>
+static inline std::move_iterator<Transformer<T, It>>
+conversionIterator(const It& it) {
+  return std::make_move_iterator(Transformer<T, It>(it));
+}
+
+} // namespace dynamicconverter_detail
+
+///////////////////////////////////////////////////////////////////////////////
+// DynamicConverter specializations
+
+/**
+ * Each specialization of DynamicConverter has the function
+ *     'static T convert(const dynamic&);'
+ */
+
+// default - intentionally unimplemented
+template <typename T, typename Enable = void> struct DynamicConverter;
+
+// boolean
+template <>
+struct DynamicConverter<bool> {
+  static bool convert(const dynamic& d) {
+    return d.asBool();
+  }
+};
+
+// integrals
+template <typename T>
+struct DynamicConverter<T,
+    typename std::enable_if<std::is_integral<T>::value &&
+                            !std::is_same<T, bool>::value>::type> {
+  static T convert(const dynamic& d) {
+    return folly::to<T>(d.asInt());
+  }
+};
+
+// floating point
+template <typename T>
+struct DynamicConverter<T,
+    typename std::enable_if<std::is_floating_point<T>::value>::type> {
+  static T convert(const dynamic& d) {
+    return folly::to<T>(d.asDouble());
+  }
+};
+
+// fbstring
+template <>
+struct DynamicConverter<folly::fbstring> {
+  static folly::fbstring convert(const dynamic& d) {
+    return d.asString();
+  }
+};
+
+// std::string
+template <>
+struct DynamicConverter<std::string> {
+  static std::string convert(const dynamic& d) {
+    return d.asString().toStdString();
+  }
+};
+
+// std::pair
+template <typename F, typename S>
+struct DynamicConverter<std::pair<F,S>> {
+  static std::pair<F, S> convert(const dynamic& d) {
+    if (d.isArray() && d.size() == 2) {
+      return std::make_pair(convertTo<F>(d[0]), convertTo<S>(d[1]));
+    } else if (d.isObject() && d.size() == 1) {
+      auto it = d.items().begin();
+      return std::make_pair(convertTo<F>(it->first), convertTo<S>(it->second));
+    } else {
+      throw TypeError("array (size 2) or object (size 1)", d.type());
+    }
+  }
+};
+
+// containers
+template <typename C>
+struct DynamicConverter<C,
+    typename std::enable_if<
+      dynamicconverter_detail::is_container<C>::value>::type> {
+  static C convert(const dynamic& d) {
+    if (d.isArray()) {
+      return C(dynamicconverter_detail::conversionIterator<C>(d.begin()),
+               dynamicconverter_detail::conversionIterator<C>(d.end()));
+    } else if (d.isObject()) {
+      return C(dynamicconverter_detail::conversionIterator<C>
+                 (d.items().begin()),
+               dynamicconverter_detail::conversionIterator<C>
+                 (d.items().end()));
+    } else {
+      throw TypeError("object or array", d.type());
+    }
+  }
+};
+
+///////////////////////////////////////////////////////////////////////////////
+// DynamicConstructor specializations
+
+/**
+ * Each specialization of DynamicConstructor has the function
+ *     'static dynamic construct(const C&);'
+ */
+
+// default
+template <typename C, typename Enable = void>
+struct DynamicConstructor {
+  static dynamic construct(const C& x) {
+    return dynamic(x);
+  }
+};
+
+// maps
+template<typename C>
+struct DynamicConstructor<C,
+    typename std::enable_if<
+      dynamicconverter_detail::is_map<C>::value>::type> {
+  static dynamic construct(const C& x) {
+    dynamic d = dynamic::object;
+    for (auto& pair : x) {
+      d.insert(toDynamic(pair.first), toDynamic(pair.second));
+    }
+    return d;
+  }
+};
+
+// other ranges
+template<typename C>
+struct DynamicConstructor<C,
+    typename std::enable_if<
+      !dynamicconverter_detail::is_map<C>::value &&
+      !std::is_constructible<StringPiece, const C&>::value &&
+      dynamicconverter_detail::is_range<C>::value>::type> {
+  static dynamic construct(const C& x) {
+    dynamic d = {};
+    for (auto& item : x) {
+      d.push_back(toDynamic(item));
+    }
+    return d;
+  }
+};
+
+// pair
+template<typename A, typename B>
+struct DynamicConstructor<std::pair<A, B>, void> {
+  static dynamic construct(const std::pair<A, B>& x) {
+    dynamic d = {};
+    d.push_back(toDynamic(x.first));
+    d.push_back(toDynamic(x.second));
+    return d;
+  }
+};
+
+///////////////////////////////////////////////////////////////////////////////
+// implementation
+
+template <typename T>
+T convertTo(const dynamic& d) {
+  return DynamicConverter<typename std::remove_cv<T>::type>::convert(d);
+}
+
+template<typename T>
+dynamic toDynamic(const T& x) {
+  return DynamicConstructor<typename std::remove_cv<T>::type>::construct(x);
+}
+
+} // namespace folly
+
+#endif // DYNAMIC_CONVERTER_H
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/dynamic.cpp
@@ -0,0 +1,44 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/dynamic.h"
+
+namespace folly {
+
+//////////////////////////////////////////////////////////////////////
+
+#define DEF_TYPE(T, str, typen)                                 \
+  template<> char const dynamic::TypeInfo<T>::name[] = str;       \
+  template<> dynamic::Type const dynamic::TypeInfo<T>::type = typen
+
+DEF_TYPE(void*,               "null",    dynamic::NULLT);
+DEF_TYPE(bool,                "boolean", dynamic::BOOL);
+DEF_TYPE(fbstring,            "string",  dynamic::STRING);
+DEF_TYPE(dynamic::Array,      "array",   dynamic::ARRAY);
+DEF_TYPE(double,              "double",  dynamic::DOUBLE);
+DEF_TYPE(int64_t,             "int64",   dynamic::INT64);
+DEF_TYPE(dynamic::ObjectImpl, "object",  dynamic::OBJECT);
+
+#undef DEF_TYPE
+
+const char* dynamic::typeName() const {
+  return typeName(type_);
+}
+
+//////////////////////////////////////////////////////////////////////
+
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/dynamic.h
@@ -0,0 +1,525 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * This is a runtime dynamically typed value.  It holds types from a
+ * specific predetermined set of types (ints, bools, arrays, etc).  In
+ * particular, it can be used as a convenient in-memory representation
+ * for complete json objects.
+ *
+ * In general you can try to use these objects as if they were the
+ * type they represent (although in some cases with a slightly less
+ * complete interface than the raw type), and it'll just throw a
+ * TypeError if it is used in an illegal way.
+ *
+ * Some examples:
+ *
+ *   dynamic twelve = 12;
+ *   dynamic str = "string";
+ *   dynamic map = dynamic::object;
+ *   map[str] = twelve;
+ *   map[str + "another_str"] = { "array", "of", 4, "elements" };
+ *   map.insert("null_element", nullptr);
+ *   ++map[str];
+ *   assert(map[str] == 13);
+ *
+ *   // Building a complex object with a sub array inline:
+ *   dynamic d = dynamic::object
+ *     ("key", "value")
+ *     ("key2", { "a", "array" })
+ *     ;
+ *
+ * Also see folly/json.h for the serialization and deserialization
+ * functions for JSON.
+ *
+ * Note: dynamic is not DefaultConstructible.  Rationale:
+ *
+ *   - The intuitive thing to initialize a defaulted dynamic to would
+ *     be nullptr.
+ *
+ *   - However, the expression dynamic d = {} is required to call the
+ *     default constructor by the standard, which is confusing
+ *     behavior for dynamic unless the default constructor creates an
+ *     empty array.
+ *
+ * Additional documentation is in folly/docs/Dynamic.md.
+ *
+ * @author Jordan DeLong <delong.j@fb.com>
+ */
+
+#ifndef FOLLY_DYNAMIC_H_
+#define FOLLY_DYNAMIC_H_
+
+#include <unordered_map>
+#include <memory>
+#include <string>
+#include <utility>
+#include <ostream>
+#include <type_traits>
+#include <initializer_list>
+#include <vector>
+#include <cstdint>
+#include <boost/operators.hpp>
+
+#include "folly/Traits.h"
+#include "folly/FBString.h"
+
+namespace folly {
+
+//////////////////////////////////////////////////////////////////////
+
+struct dynamic;
+struct TypeError;
+
+//////////////////////////////////////////////////////////////////////
+
+struct dynamic : private boost::operators<dynamic> {
+  enum Type {
+    NULLT,
+    ARRAY,
+    BOOL,
+    DOUBLE,
+    INT64,
+    OBJECT,
+    STRING,
+  };
+
+  /*
+   * We support direct iteration of arrays, and indirect iteration of objects.
+   * See begin(), end(), keys(), values(), and items() for more.
+   *
+   * Array iterators dereference as the elements in the array.
+   * Object key iterators dereference as the keys in the object.
+   * Object value iterators dereference as the values in the object.
+   * Object item iterators dereference as pairs of (key, value).
+   */
+private:
+  typedef std::vector<dynamic> Array;
+public:
+  typedef Array::const_iterator const_iterator;
+  typedef dynamic value_type;
+  struct const_key_iterator;
+  struct const_value_iterator;
+  struct const_item_iterator;
+
+  /*
+   * Creation routines for making dynamic objects.  Objects are maps
+   * from key to value (so named due to json-related origins here).
+   *
+   * Example:
+   *
+   *   // Make a fairly complex dynamic:
+   *   dynamic d = dynamic::object("key", "value1")
+   *                              ("key2", { "value", "with", 4, "words" });
+   *
+   *   // Build an object in a few steps:
+   *   dynamic d = dynamic::object;
+   *   d["key"] = 12;
+   *   d["something_else"] = { 1, 2, 3, nullptr };
+   */
+private:
+  struct ObjectMaker;
+
+public:
+  static ObjectMaker object();
+  static ObjectMaker object(dynamic&&, dynamic&&);
+  static ObjectMaker object(dynamic const&, dynamic&&);
+  static ObjectMaker object(dynamic&&, dynamic const&);
+  static ObjectMaker object(dynamic const&, dynamic const&);
+
+  /*
+   * String compatibility constructors.
+   */
+  /* implicit */ dynamic(char const* val);
+  /* implicit */ dynamic(std::string const& val);
+
+  /*
+   * This is part of the plumbing for object(), above.  Used to create
+   * a new object dynamic.
+   */
+  /* implicit */ dynamic(ObjectMaker (*)());
+  /* implicit */ dynamic(ObjectMaker const&) = delete;
+  /* implicit */ dynamic(ObjectMaker&&);
+
+  /*
+   * Create a new array from an initializer list.
+   *
+   * For example:
+   *
+   *   dynamic v = { 1, 2, 3, "foo" };
+   */
+  /* implicit */ dynamic(std::initializer_list<dynamic> il);
+
+  /*
+   * Conversion constructors from most of the other types.
+   */
+  template<class T> /* implicit */ dynamic(T t);
+
+  /*
+   * Create a dynamic that is an array of the values from the supplied
+   * iterator range.
+   */
+  template<class Iterator> dynamic(Iterator first, Iterator last);
+
+  dynamic(dynamic const&);
+  dynamic(dynamic&&);
+  ~dynamic();
+
+  /*
+   * "Deep" equality comparison.  This will compare all the way down
+   * an object or array, and is potentially expensive.
+   */
+  bool operator==(dynamic const& o) const;
+
+  /*
+   * For all types except object this returns the natural ordering on
+   * those types.  For objects, we throw TypeError.
+   */
+  bool operator<(dynamic const& o) const;
+
+  /*
+   * General operators.
+   *
+   * These throw TypeError when used with types or type combinations
+   * that don't support them.
+   *
+   * These functions may also throw if you use 64-bit integers with
+   * doubles when the integers are too big to fit in a double.
+   */
+  dynamic& operator+=(dynamic const&);
+  dynamic& operator-=(dynamic const&);
+  dynamic& operator*=(dynamic const&);
+  dynamic& operator/=(dynamic const&);
+  dynamic& operator%=(dynamic const&);
+  dynamic& operator|=(dynamic const&);
+  dynamic& operator&=(dynamic const&);
+  dynamic& operator^=(dynamic const&);
+  dynamic& operator++();
+  dynamic& operator--();
+
+  /*
+   * Assignment from other dynamics.  Because of the implicit conversion
+   * to dynamic from its potential types, you can use this to change the
+   * type pretty intuitively.
+   *
+   * Basic guarantee only.
+   */
+  dynamic& operator=(dynamic const&);
+  dynamic& operator=(dynamic&&);
+
+  /*
+   * For simple dynamics (not arrays or objects), this prints the
+   * value to an std::ostream in the expected way.  Respects the
+   * formatting manipulators that have been sent to the stream
+   * already.
+   *
+   * If the dynamic holds an object or array, this prints them in a
+   * format very similar to JSON.  (It will in fact actually be JSON
+   * as long as the dynamic validly represents a JSON object---i.e. it
+   * can't have non-string keys.)
+   */
+  friend std::ostream& operator<<(std::ostream&, dynamic const&);
+
+  /*
+   * Returns true if this dynamic is of the specified type.
+   */
+  bool isString() const;
+  bool isObject() const;
+  bool isBool() const;
+  bool isNull() const;
+  bool isArray() const;
+  bool isDouble() const;
+  bool isInt() const;
+
+  /*
+   * Returns: isInt() || isDouble().
+   */
+  bool isNumber() const;
+
+  /*
+   * Returns the type of this dynamic.
+   */
+  Type type() const;
+
+  /*
+   * Returns the type of this dynamic as a printable string.
+   */
+  const char* typeName() const;
+
+  /*
+   * Extract a value while trying to convert to the specified type.
+   * Throws exceptions if we cannot convert from the real type to the
+   * requested type.
+   *
+   * Note you can only use this to access integral types or strings,
+   * since arrays and objects are generally best dealt with as a
+   * dynamic.
+   */
+  fbstring asString() const;
+  double   asDouble() const;
+  int64_t  asInt() const;
+  bool     asBool() const;
+
+  /*
+   * It is occasionally useful to access a string's internal pointer
+   * directly, without the type conversion of `asString()`.
+   *
+   * These will throw a TypeError if the dynamic is not a string.
+   */
+  const char* data()  const;
+  const char* c_str() const;
+
+  /*
+   * Returns: true if this dynamic is null, an empty array, an empty
+   * object, or an empty string.
+   */
+  bool empty() const;
+
+  /*
+   * If this is an array or an object, returns the number of elements
+   * contained.  If it is a string, returns the length.  Otherwise
+   * throws TypeError.
+   */
+  std::size_t size() const;
+
+  /*
+   * You can iterate over the values of the array.  Calling these on
+   * non-arrays will throw a TypeError.
+   */
+  const_iterator begin()  const;
+  const_iterator end()    const;
+
+private:
+  /*
+   * Helper object returned by keys(), values(), and items().
+   */
+  template <class T> struct IterableProxy;
+
+public:
+  /*
+   * You can iterate over the keys, values, or items (std::pair of key and
+   * value) in an object.  Calling these on non-objects will throw a TypeError.
+   */
+  IterableProxy<const_key_iterator> keys() const;
+  IterableProxy<const_value_iterator> values() const;
+  IterableProxy<const_item_iterator> items() const;
+
+  /*
+   * AssociativeContainer-style find interface for objects.  Throws if
+   * this is not an object.
+   *
+   * Returns: items().end() if the key is not present, or a
+   * const_item_iterator pointing to the item.
+   */
+  const_item_iterator find(dynamic const&) const;
+
+
+  /*
+   * If this is an object, returns whether it contains a field with
+   * the given name.  Otherwise throws TypeError.
+   */
+  std::size_t count(dynamic const&) const;
+
+  /*
+   * For objects or arrays, provides access to sub-fields by index or
+   * field name.
+   *
+   * Using these with dynamic objects that are not arrays or objects
+   * will throw a TypeError.  Using an index that is out of range or
+   * object-element that's not present throws std::out_of_range.
+   */
+  dynamic const& at(dynamic const&) const;
+  dynamic&       at(dynamic const&);
+
+  /*
+   * Like 'at', above, except it returns either a pointer to the contained
+   * object or nullptr if it wasn't found. This allows a key to be tested for
+   * containment and retrieved in one operation. Example:
+   *
+   *   if (auto* found = d.get_ptr(key))
+   *     // use *found;
+   *
+   * Using these with dynamic objects that are not arrays or objects
+   * will throw a TypeError.
+   */
+  const dynamic* get_ptr(dynamic const&) const;
+  dynamic* get_ptr(dynamic const&);
+
+  /*
+   * This works for access to both objects and arrays.
+   *
+   * In the case of an array, the index must be an integer, and this will throw
+   * std::out_of_range if it is less than zero or greater than size().
+   *
+   * In the case of an object, the non-const overload inserts a null
+   * value if the key isn't present.  The const overload will throw
+   * std::out_of_range if the key is not present.
+   *
+   * These functions do not invalidate iterators.
+   */
+  dynamic&       operator[](dynamic const&);
+  dynamic const& operator[](dynamic const&) const;
+
+  /*
+   * Only defined for objects, throws TypeError otherwise.
+   *
+   * getDefault will return the value associated with the supplied key, the
+   * supplied default otherwise. setDefault will set the key to the supplied
+   * default if it is not yet set, otherwise leaving it. setDefault returns
+   * a reference to the existing value if present, the new value otherwise.
+   */
+  dynamic
+  getDefault(const dynamic& k, const dynamic& v = dynamic::object) const;
+  dynamic&& getDefault(const dynamic& k, dynamic&& v) const;
+  template<class K, class V = dynamic>
+  dynamic& setDefault(K&& k, V&& v = dynamic::object);
+
+  /*
+   * Resizes an array so it has at n elements, using the supplied
+   * default to fill new elements.  Throws TypeError if this dynamic
+   * is not an array.
+   *
+   * May invalidate iterators.
+   *
+   * Post: size() == n
+   */
+  void resize(std::size_t n, dynamic const& = nullptr);
+
+  /*
+   * Inserts the supplied key-value pair to an object, or throws if
+   * it's not an object.
+   *
+   * Invalidates iterators.
+   */
+  template<class K, class V> void insert(K&&, V&& val);
+
+  /*
+   * Erase an element from a dynamic object, by key.
+   *
+   * Invalidates iterators to the element being erased.
+   *
+   * Returns the number of elements erased (i.e. 1 or 0).
+   */
+  std::size_t erase(dynamic const& key);
+
+  /*
+   * Erase an element from a dynamic object or array, using an
+   * iterator or an iterator range.
+   *
+   * In arrays, invalidates iterators to elements after the element
+   * being erased.  In objects, invalidates iterators to the elements
+   * being erased.
+   *
+   * Returns a new iterator to the first element beyond any elements
+   * removed, or end() if there are none.  (The iteration order does
+   * not change.)
+   */
+  const_iterator erase(const_iterator it);
+  const_iterator erase(const_iterator first, const_iterator last);
+
+  const_key_iterator erase(const_key_iterator it);
+  const_key_iterator erase(const_key_iterator first, const_key_iterator last);
+
+  const_value_iterator erase(const_value_iterator it);
+  const_value_iterator erase(const_value_iterator first,
+                             const_value_iterator last);
+
+  const_item_iterator erase(const_item_iterator it);
+  const_item_iterator erase(const_item_iterator first,
+                            const_item_iterator last);
+  /*
+   * Append elements to an array.  If this is not an array, throws
+   * TypeError.
+   *
+   * Invalidates iterators.
+   */
+  void push_back(dynamic const&);
+  void push_back(dynamic&&);
+
+  /*
+   * Remove an element from the back of an array.  If this is not an array,
+   * throws TypeError.
+   *
+   * Does not invalidate iterators.
+   */
+  void pop_back();
+
+  /*
+   * Get a hash code.  This function is called by a std::hash<>
+   * specialization, also.
+   *
+   * Throws TypeError if this is an object, array, or null.
+   */
+  std::size_t hash() const;
+
+private:
+  friend struct TypeError;
+  struct ObjectImpl;
+  template<class T> struct TypeInfo;
+  template<class T> struct CompareOp;
+  template<class T> struct GetAddrImpl;
+  template<class T> struct PrintImpl;
+
+  template<class T> T const& get() const;
+  template<class T> T&       get();
+  template<class T> T*       get_nothrow();
+  template<class T> T const* get_nothrow() const;
+  template<class T> T*       getAddress();
+  template<class T> T const* getAddress() const;
+
+  template<class T> T asImpl() const;
+
+  static char const* typeName(Type);
+  void destroy();
+  void print(std::ostream&) const;
+  void print_as_pseudo_json(std::ostream&) const; // see json.cpp
+
+private:
+  Type type_;
+  union Data {
+    explicit Data() : nul(nullptr) {}
+    ~Data() {}
+
+    // XXX: gcc does an ICE if we use std::nullptr_t instead of void*
+    // here.  See http://gcc.gnu.org/bugzilla/show_bug.cgi?id=50361
+    void* nul;
+    Array array;
+    bool boolean;
+    double doubl;
+    int64_t integer;
+    fbstring string;
+
+    /*
+     * Objects are placement new'd here.  We have to use a char buffer
+     * because we don't know the type here (std::unordered_map<> with
+     * dynamic would be parameterizing a std:: template with an
+     * incomplete type right now).  (Note that in contrast we know it
+     * is ok to do this with fbvector because we own it.)
+     */
+    typename std::aligned_storage<
+      sizeof(std::unordered_map<int,int>),
+      alignof(std::unordered_map<int,int>)
+    >::type objectBuffer;
+  } u_;
+};
+
+//////////////////////////////////////////////////////////////////////
+
+}
+
+#include "folly/dynamic-inl.h"
+
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/dynamic-inl.h
@@ -0,0 +1,914 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_DYNAMIC_INL_H_
+#define FOLLY_DYNAMIC_INL_H_
+
+#include <functional>
+#include <boost/iterator/iterator_adaptor.hpp>
+#include <boost/iterator/iterator_facade.hpp>
+#include "folly/Likely.h"
+#include "folly/Conv.h"
+#include "folly/Format.h"
+
+//////////////////////////////////////////////////////////////////////
+
+namespace std {
+
+template<>
+struct hash< ::folly::dynamic> {
+  size_t operator()(::folly::dynamic const& d) const {
+    return d.hash();
+  }
+};
+
+}
+
+//////////////////////////////////////////////////////////////////////
+
+// This is a higher-order preprocessor macro to aid going from runtime
+// types to the compile time type system.
+#define FB_DYNAMIC_APPLY(type, apply) do {         \
+  switch ((type)) {                             \
+  case NULLT:   apply(void*);          break;   \
+  case ARRAY:   apply(Array);          break;   \
+  case BOOL:    apply(bool);           break;   \
+  case DOUBLE:  apply(double);         break;   \
+  case INT64:   apply(int64_t);        break;   \
+  case OBJECT:  apply(ObjectImpl);     break;   \
+  case STRING:  apply(fbstring);       break;   \
+  default:      CHECK(0); abort();              \
+  }                                             \
+} while (0)
+
+//////////////////////////////////////////////////////////////////////
+
+namespace folly {
+
+struct TypeError : std::runtime_error {
+  explicit TypeError(const std::string& expected, dynamic::Type actual)
+    : std::runtime_error(to<std::string>("TypeError: expected dynamic "
+        "type `", expected, '\'', ", but had type `",
+        dynamic::typeName(actual), '\''))
+  {}
+  explicit TypeError(const std::string& expected,
+      dynamic::Type actual1, dynamic::Type actual2)
+    : std::runtime_error(to<std::string>("TypeError: expected dynamic "
+        "types `", expected, '\'', ", but had types `",
+        dynamic::typeName(actual1), "' and `", dynamic::typeName(actual2),
+        '\''))
+  {}
+};
+
+
+//////////////////////////////////////////////////////////////////////
+
+namespace detail {
+
+  // This helper is used in destroy() to be able to run destructors on
+  // types like "int64_t" without a compiler error.
+  struct Destroy {
+    template<class T> static void destroy(T* t) { t->~T(); }
+  };
+
+  /*
+   * The enable_if junk here is necessary to avoid ambiguous
+   * conversions relating to bool and double when you implicitly
+   * convert an int or long to a dynamic.
+   */
+  template<class T, class Enable = void> struct ConversionHelper;
+  template<class T>
+  struct ConversionHelper<
+    T,
+    typename std::enable_if<
+      std::is_integral<T>::value && !std::is_same<T,bool>::value
+    >::type
+  > {
+    typedef int64_t type;
+  };
+  template<class T>
+  struct ConversionHelper<
+    T,
+    typename std::enable_if<
+      (!std::is_integral<T>::value || std::is_same<T,bool>::value) &&
+      !std::is_same<T,std::nullptr_t>::value
+    >::type
+  > {
+    typedef T type;
+  };
+  template<class T>
+  struct ConversionHelper<
+    T,
+    typename std::enable_if<
+      std::is_same<T,std::nullptr_t>::value
+    >::type
+  > {
+    typedef void* type;
+  };
+
+  /*
+   * Helper for implementing numeric conversions in operators on
+   * numbers.  Just promotes to double when one of the arguments is
+   * double, or throws if either is not a numeric type.
+   */
+  template<template<class> class Op>
+  dynamic numericOp(dynamic const& a, dynamic const& b) {
+    if (!a.isNumber() || !b.isNumber()) {
+      throw TypeError("numeric", a.type(), b.type());
+    }
+    if (a.type() != b.type()) {
+      auto& integ  = a.isInt() ? a : b;
+      auto& nonint = a.isInt() ? b : a;
+      return Op<double>()(to<double>(integ.asInt()), nonint.asDouble());
+    }
+    if (a.isDouble()) {
+      return Op<double>()(a.asDouble(), b.asDouble());
+    }
+    return Op<int64_t>()(a.asInt(), b.asInt());
+  }
+
+}
+
+//////////////////////////////////////////////////////////////////////
+
+/*
+ * We're doing this instead of a simple member typedef to avoid the
+ * undefined behavior of parameterizing std::unordered_map<> with an
+ * incomplete type.
+ *
+ * Note: Later we may add separate order tracking here (a multi-index
+ * type of thing.)
+ */
+struct dynamic::ObjectImpl : std::unordered_map<dynamic, dynamic> {};
+
+//////////////////////////////////////////////////////////////////////
+
+// Helper object for creating objects conveniently.  See object and
+// the dynamic::dynamic(ObjectMaker&&) ctor.
+struct dynamic::ObjectMaker {
+  friend struct dynamic;
+
+  explicit ObjectMaker() : val_(dynamic::object) {}
+  explicit ObjectMaker(dynamic const& key, dynamic val)
+    : val_(dynamic::object)
+  {
+    val_.insert(key, std::move(val));
+  }
+  explicit ObjectMaker(dynamic&& key, dynamic val)
+    : val_(dynamic::object)
+  {
+    val_.insert(std::move(key), std::move(val));
+  }
+
+  // Make sure no one tries to save one of these into an lvalue with
+  // auto or anything like that.
+  ObjectMaker(ObjectMaker&&) = default;
+  ObjectMaker(ObjectMaker const&) = delete;
+  ObjectMaker& operator=(ObjectMaker const&) = delete;
+  ObjectMaker& operator=(ObjectMaker&&) = delete;
+
+  // These return rvalue-references instead of lvalue-references to allow
+  // constructs like this to moved instead of copied:
+  //  dynamic a = dynamic::object("a", "b")("c", "d")
+  ObjectMaker&& operator()(dynamic const& key, dynamic val) {
+    val_.insert(key, std::move(val));
+    return std::move(*this);
+  }
+
+  ObjectMaker&& operator()(dynamic&& key, dynamic val) {
+    val_.insert(std::move(key), std::move(val));
+    return std::move(*this);
+  }
+
+private:
+  dynamic val_;
+};
+
+// This looks like a case for perfect forwarding, but our use of
+// std::initializer_list for constructing dynamic arrays makes it less
+// functional than doing this manually.
+inline dynamic::ObjectMaker dynamic::object() { return ObjectMaker(); }
+inline dynamic::ObjectMaker dynamic::object(dynamic&& a, dynamic&& b) {
+  return ObjectMaker(std::move(a), std::move(b));
+}
+inline dynamic::ObjectMaker dynamic::object(dynamic const& a, dynamic&& b) {
+  return ObjectMaker(a, std::move(b));
+}
+inline dynamic::ObjectMaker dynamic::object(dynamic&& a, dynamic const& b) {
+  return ObjectMaker(std::move(a), b);
+}
+inline dynamic::ObjectMaker
+dynamic::object(dynamic const& a, dynamic const& b) {
+  return ObjectMaker(a, b);
+}
+
+//////////////////////////////////////////////////////////////////////
+
+struct dynamic::const_item_iterator
+  : boost::iterator_adaptor<dynamic::const_item_iterator,
+                            dynamic::ObjectImpl::const_iterator> {
+  /* implicit */ const_item_iterator(base_type b) : iterator_adaptor_(b) { }
+
+ private:
+  friend class boost::iterator_core_access;
+};
+
+struct dynamic::const_key_iterator
+  : boost::iterator_adaptor<dynamic::const_key_iterator,
+                            dynamic::ObjectImpl::const_iterator,
+                            dynamic const> {
+  /* implicit */ const_key_iterator(base_type b) : iterator_adaptor_(b) { }
+
+ private:
+  dynamic const& dereference() const {
+    return base_reference()->first;
+  }
+  friend class boost::iterator_core_access;
+};
+
+struct dynamic::const_value_iterator
+  : boost::iterator_adaptor<dynamic::const_value_iterator,
+                            dynamic::ObjectImpl::const_iterator,
+                            dynamic const> {
+  /* implicit */ const_value_iterator(base_type b) : iterator_adaptor_(b) { }
+
+ private:
+  dynamic const& dereference() const {
+    return base_reference()->second;
+  }
+  friend class boost::iterator_core_access;
+};
+
+//////////////////////////////////////////////////////////////////////
+
+inline dynamic::dynamic(ObjectMaker (*)())
+  : type_(OBJECT)
+{
+  new (getAddress<ObjectImpl>()) ObjectImpl();
+}
+
+inline dynamic::dynamic(char const* s)
+  : type_(STRING)
+{
+  new (&u_.string) fbstring(s);
+}
+
+inline dynamic::dynamic(std::string const& s)
+  : type_(STRING)
+{
+  new (&u_.string) fbstring(s);
+}
+
+inline dynamic::dynamic(std::initializer_list<dynamic> il)
+  : type_(ARRAY)
+{
+  new (&u_.array) Array(il.begin(), il.end());
+}
+
+inline dynamic::dynamic(ObjectMaker&& maker)
+  : type_(OBJECT)
+{
+  new (getAddress<ObjectImpl>())
+    ObjectImpl(std::move(*maker.val_.getAddress<ObjectImpl>()));
+}
+
+inline dynamic::dynamic(dynamic const& o)
+  : type_(NULLT)
+{
+  *this = o;
+}
+
+inline dynamic::dynamic(dynamic&& o)
+  : type_(NULLT)
+{
+  *this = std::move(o);
+}
+
+inline dynamic::~dynamic() { destroy(); }
+
+template<class T>
+dynamic::dynamic(T t) {
+  typedef typename detail::ConversionHelper<T>::type U;
+  type_ = TypeInfo<U>::type;
+  new (getAddress<U>()) U(std::move(t));
+}
+
+template<class Iterator>
+dynamic::dynamic(Iterator first, Iterator last)
+  : type_(ARRAY)
+{
+  new (&u_.array) Array(first, last);
+}
+
+//////////////////////////////////////////////////////////////////////
+
+inline dynamic::const_iterator dynamic::begin() const {
+  return get<Array>().begin();
+}
+inline dynamic::const_iterator dynamic::end() const {
+  return get<Array>().end();
+}
+
+template <class It>
+struct dynamic::IterableProxy {
+  typedef It const_iterator;
+  typedef typename It::value_type value_type;
+
+  /* implicit */ IterableProxy(const dynamic::ObjectImpl* o) : o_(o) { }
+
+  It begin() const {
+    return o_->begin();
+  }
+
+  It end() const {
+    return o_->end();
+  }
+
+ private:
+  const dynamic::ObjectImpl* o_;
+};
+
+inline dynamic::IterableProxy<dynamic::const_key_iterator> dynamic::keys()
+  const {
+  return &(get<ObjectImpl>());
+}
+
+inline dynamic::IterableProxy<dynamic::const_value_iterator> dynamic::values()
+  const {
+  return &(get<ObjectImpl>());
+}
+
+inline dynamic::IterableProxy<dynamic::const_item_iterator> dynamic::items()
+  const {
+  return &(get<ObjectImpl>());
+}
+
+inline bool dynamic::isString() const { return get_nothrow<fbstring>(); }
+inline bool dynamic::isObject() const { return get_nothrow<ObjectImpl>(); }
+inline bool dynamic::isBool()   const { return get_nothrow<bool>(); }
+inline bool dynamic::isArray()  const { return get_nothrow<Array>(); }
+inline bool dynamic::isDouble() const { return get_nothrow<double>(); }
+inline bool dynamic::isInt()    const { return get_nothrow<int64_t>(); }
+inline bool dynamic::isNull()   const { return get_nothrow<void*>(); }
+inline bool dynamic::isNumber() const { return isInt() || isDouble(); }
+
+inline dynamic::Type dynamic::type() const {
+  return type_;
+}
+
+inline fbstring dynamic::asString() const { return asImpl<fbstring>(); }
+inline double   dynamic::asDouble() const { return asImpl<double>(); }
+inline int64_t  dynamic::asInt()    const { return asImpl<int64_t>(); }
+inline bool     dynamic::asBool()   const { return asImpl<bool>(); }
+
+inline const char* dynamic::data()  const { return get<fbstring>().data();  }
+inline const char* dynamic::c_str() const { return get<fbstring>().c_str(); }
+
+template<class T>
+struct dynamic::CompareOp {
+  static bool comp(T const& a, T const& b) { return a < b; }
+};
+template<>
+struct dynamic::CompareOp<dynamic::ObjectImpl> {
+  static bool comp(ObjectImpl const& a, ObjectImpl const& b) {
+    // This code never executes; it is just here for the compiler.
+    return false;
+  }
+};
+
+inline bool dynamic::operator<(dynamic const& o) const {
+  if (UNLIKELY(type_ == OBJECT || o.type_ == OBJECT)) {
+    throw TypeError("object", type_);
+  }
+  if (type_ != o.type_) {
+    return type_ < o.type_;
+  }
+
+#define FB_X(T) return CompareOp<T>::comp(*getAddress<T>(),   \
+                                          *o.getAddress<T>())
+  FB_DYNAMIC_APPLY(type_, FB_X);
+#undef FB_X
+}
+
+inline bool dynamic::operator==(dynamic const& o) const {
+  if (type() != o.type()) {
+    if (isNumber() && o.isNumber()) {
+      auto& integ = isInt() ? *this : o;
+      auto& doubl = isInt() ? o     : *this;
+      return integ.asInt() == doubl.asDouble();
+    }
+    return false;
+  }
+
+#define FB_X(T) return *getAddress<T>() == *o.getAddress<T>();
+  FB_DYNAMIC_APPLY(type_, FB_X);
+#undef FB_X
+}
+
+inline dynamic& dynamic::operator+=(dynamic const& o) {
+  if (type() == STRING && o.type() == STRING) {
+    *getAddress<fbstring>() += *o.getAddress<fbstring>();
+    return *this;
+  }
+  *this = detail::numericOp<std::plus>(*this, o);
+  return *this;
+}
+
+inline dynamic& dynamic::operator-=(dynamic const& o) {
+  *this = detail::numericOp<std::minus>(*this, o);
+  return *this;
+}
+
+inline dynamic& dynamic::operator*=(dynamic const& o) {
+  *this = detail::numericOp<std::multiplies>(*this, o);
+  return *this;
+}
+
+inline dynamic& dynamic::operator/=(dynamic const& o) {
+  *this = detail::numericOp<std::divides>(*this, o);
+  return *this;
+}
+
+#define FB_DYNAMIC_INTEGER_OP(op)                           \
+  inline dynamic& dynamic::operator op(dynamic const& o) {  \
+    if (!isInt() || !o.isInt()) {                           \
+      throw TypeError("int64", type(), o.type());           \
+    }                                                       \
+    *getAddress<int64_t>() op o.asInt();                    \
+    return *this;                                           \
+  }
+
+FB_DYNAMIC_INTEGER_OP(%=)
+FB_DYNAMIC_INTEGER_OP(|=)
+FB_DYNAMIC_INTEGER_OP(&=)
+FB_DYNAMIC_INTEGER_OP(^=)
+
+#undef FB_DYNAMIC_INTEGER_OP
+
+inline dynamic& dynamic::operator++() {
+  ++get<int64_t>();
+  return *this;
+}
+
+inline dynamic& dynamic::operator--() {
+  --get<int64_t>();
+  return *this;
+}
+
+inline dynamic& dynamic::operator=(dynamic const& o) {
+  if (&o != this) {
+    destroy();
+#define FB_X(T) new (getAddress<T>()) T(*o.getAddress<T>())
+    FB_DYNAMIC_APPLY(o.type_, FB_X);
+#undef FB_X
+    type_ = o.type_;
+  }
+  return *this;
+}
+
+inline dynamic& dynamic::operator=(dynamic&& o) {
+  if (&o != this) {
+    destroy();
+#define FB_X(T) new (getAddress<T>()) T(std::move(*o.getAddress<T>()))
+    FB_DYNAMIC_APPLY(o.type_, FB_X);
+#undef FB_X
+    type_ = o.type_;
+  }
+  return *this;
+}
+
+inline dynamic& dynamic::operator[](dynamic const& k) {
+  if (!isObject() && !isArray()) {
+    throw TypeError("object/array", type());
+  }
+  if (isArray()) {
+    return at(k);
+  }
+  auto& obj = get<ObjectImpl>();
+  auto ret = obj.insert({k, nullptr});
+  return ret.first->second;
+}
+
+inline dynamic const& dynamic::operator[](dynamic const& idx) const {
+  return at(idx);
+}
+
+inline dynamic dynamic::getDefault(const dynamic& k, const dynamic& v) const {
+  auto& obj = get<ObjectImpl>();
+  auto it = obj.find(k);
+  return it == obj.end() ? v : it->second;
+}
+
+inline dynamic&& dynamic::getDefault(const dynamic& k, dynamic&& v) const {
+  auto& obj = get<ObjectImpl>();
+  auto it = obj.find(k);
+  if (it != obj.end()) {
+    v = it->second;
+  }
+
+  return std::move(v);
+}
+
+template<class K, class V> inline dynamic& dynamic::setDefault(K&& k, V&& v) {
+  auto& obj = get<ObjectImpl>();
+  return obj.insert(std::make_pair(std::forward<K>(k),
+                                   std::forward<V>(v))).first->second;
+}
+
+inline dynamic* dynamic::get_ptr(dynamic const& idx) {
+  return const_cast<dynamic*>(const_cast<dynamic const*>(this)->get_ptr(idx));
+}
+
+inline const dynamic* dynamic::get_ptr(dynamic const& idx) const {
+  if (auto* parray = get_nothrow<Array>()) {
+    if (!idx.isInt()) {
+      throw TypeError("int64", idx.type());
+    }
+    if (idx >= parray->size()) {
+      return nullptr;
+    }
+    return &(*parray)[idx.asInt()];
+  } else if (auto* pobject = get_nothrow<ObjectImpl>()) {
+    auto it = pobject->find(idx);
+    if (it == pobject->end()) {
+      return nullptr;
+    }
+    return &it->second;
+  } else {
+    throw TypeError("object/array", type());
+  }
+}
+
+inline dynamic& dynamic::at(dynamic const& idx) {
+  return const_cast<dynamic&>(const_cast<dynamic const*>(this)->at(idx));
+}
+
+inline dynamic const& dynamic::at(dynamic const& idx) const {
+  if (auto* parray = get_nothrow<Array>()) {
+    if (!idx.isInt()) {
+      throw TypeError("int64", idx.type());
+    }
+    if (idx >= parray->size()) {
+      throw std::out_of_range("out of range in dynamic array");
+    }
+    return (*parray)[idx.asInt()];
+  } else if (auto* pobject = get_nothrow<ObjectImpl>()) {
+    auto it = pobject->find(idx);
+    if (it == pobject->end()) {
+      throw std::out_of_range(to<std::string>(
+          "couldn't find key ", idx.asString(), " in dynamic object"));
+    }
+    return it->second;
+  } else {
+    throw TypeError("object/array", type());
+  }
+}
+
+inline bool dynamic::empty() const {
+  if (isNull()) {
+    return true;
+  }
+  return !size();
+}
+
+inline std::size_t dynamic::size() const {
+  if (auto* ar = get_nothrow<Array>()) {
+    return ar->size();
+  }
+  if (auto* obj = get_nothrow<ObjectImpl>()) {
+    return obj->size();
+  }
+  if (auto* str = get_nothrow<fbstring>()) {
+    return str->size();
+  }
+  throw TypeError("array/object", type());
+}
+
+inline std::size_t dynamic::count(dynamic const& key) const {
+  return find(key) != items().end();
+}
+
+inline dynamic::const_item_iterator dynamic::find(dynamic const& key) const {
+  return get<ObjectImpl>().find(key);
+}
+
+template<class K, class V> inline void dynamic::insert(K&& key, V&& val) {
+  auto& obj = get<ObjectImpl>();
+  auto rv = obj.insert(std::make_pair(std::forward<K>(key),
+                                      std::forward<V>(val)));
+  if (!rv.second) {
+    // note, the second use of std:forward<V>(val) is only correct
+    // if the first one did not result in a move. obj[key] = val
+    // would be preferrable but doesn't compile because dynamic
+    // is (intentionally) not default constructable
+    rv.first->second = std::forward<V>(val);
+  }
+}
+
+inline std::size_t dynamic::erase(dynamic const& key) {
+  auto& obj = get<ObjectImpl>();
+  return obj.erase(key);
+}
+
+inline dynamic::const_iterator dynamic::erase(const_iterator it) {
+  auto& arr = get<Array>();
+  // std::vector doesn't have an erase method that works on const iterators,
+  // even though the standard says it should, so this hack converts to a
+  // non-const iterator before calling erase.
+  return get<Array>().erase(arr.begin() + (it - arr.begin()));
+}
+
+inline dynamic::const_iterator
+dynamic::erase(const_iterator first, const_iterator last) {
+  auto& arr = get<Array>();
+  return get<Array>().erase(
+    arr.begin() + (first - arr.begin()),
+    arr.begin() + (last - arr.begin()));
+}
+
+inline dynamic::const_key_iterator dynamic::erase(const_key_iterator it) {
+  return const_key_iterator(get<ObjectImpl>().erase(it.base()));
+}
+
+inline dynamic::const_key_iterator dynamic::erase(const_key_iterator first,
+                                                  const_key_iterator last) {
+  return const_key_iterator(get<ObjectImpl>().erase(first.base(),
+                                                    last.base()));
+}
+
+inline dynamic::const_value_iterator dynamic::erase(const_value_iterator it) {
+  return const_value_iterator(get<ObjectImpl>().erase(it.base()));
+}
+
+inline dynamic::const_value_iterator dynamic::erase(const_value_iterator first,
+                                                    const_value_iterator last) {
+  return const_value_iterator(get<ObjectImpl>().erase(first.base(),
+                                                      last.base()));
+}
+
+inline dynamic::const_item_iterator dynamic::erase(const_item_iterator it) {
+  return const_item_iterator(get<ObjectImpl>().erase(it.base()));
+}
+
+inline dynamic::const_item_iterator dynamic::erase(const_item_iterator first,
+                                                   const_item_iterator last) {
+  return const_item_iterator(get<ObjectImpl>().erase(first.base(),
+                                                     last.base()));
+}
+
+inline void dynamic::resize(std::size_t sz, dynamic const& c) {
+  auto& array = get<Array>();
+  array.resize(sz, c);
+}
+
+inline void dynamic::push_back(dynamic const& v) {
+  auto& array = get<Array>();
+  array.push_back(v);
+}
+
+inline void dynamic::push_back(dynamic&& v) {
+  auto& array = get<Array>();
+  array.push_back(std::move(v));
+}
+
+inline void dynamic::pop_back() {
+  auto& array = get<Array>();
+  array.pop_back();
+}
+
+inline std::size_t dynamic::hash() const {
+  switch (type()) {
+  case OBJECT:
+  case ARRAY:
+  case NULLT:
+    throw TypeError("not null/object/array", type());
+  case INT64:
+    return std::hash<int64_t>()(asInt());
+  case DOUBLE:
+    return std::hash<double>()(asDouble());
+  case BOOL:
+    return std::hash<bool>()(asBool());
+  case STRING:
+    return std::hash<fbstring>()(asString());
+  default:
+    CHECK(0); abort();
+  }
+}
+
+//////////////////////////////////////////////////////////////////////
+
+template<class T> struct dynamic::TypeInfo {
+  static char const name[];
+  static Type const type;
+};
+
+#define FB_DEC_TYPE(T)                                      \
+  template<> char const dynamic::TypeInfo<T>::name[];       \
+  template<> dynamic::Type const dynamic::TypeInfo<T>::type
+
+FB_DEC_TYPE(void*);
+FB_DEC_TYPE(bool);
+FB_DEC_TYPE(fbstring);
+FB_DEC_TYPE(dynamic::Array);
+FB_DEC_TYPE(double);
+FB_DEC_TYPE(int64_t);
+FB_DEC_TYPE(dynamic::ObjectImpl);
+
+#undef FB_DEC_TYPE
+
+template<class T>
+T dynamic::asImpl() const {
+  switch (type()) {
+  case INT64:    return to<T>(*get_nothrow<int64_t>());
+  case DOUBLE:   return to<T>(*get_nothrow<double>());
+  case BOOL:     return to<T>(*get_nothrow<bool>());
+  case STRING:   return to<T>(*get_nothrow<fbstring>());
+  default:
+    throw TypeError("int/double/bool/string", type());
+  }
+}
+
+// Return a T* to our type, or null if we're not that type.
+template<class T>
+T* dynamic::get_nothrow() {
+  if (type_ != TypeInfo<T>::type) {
+    return nullptr;
+  }
+  return getAddress<T>();
+}
+
+template<class T>
+T const* dynamic::get_nothrow() const {
+  return const_cast<dynamic*>(this)->get_nothrow<T>();
+}
+
+// Return T* for where we can put a T, without type checking.  (Memory
+// might be uninitialized, even.)
+template<class T>
+T* dynamic::getAddress() {
+  return GetAddrImpl<T>::get(u_);
+}
+
+template<class T>
+T const* dynamic::getAddress() const {
+  return const_cast<dynamic*>(this)->getAddress<T>();
+}
+
+template<class T> struct dynamic::GetAddrImpl {};
+template<> struct dynamic::GetAddrImpl<void*> {
+  static void** get(Data& d) { return &d.nul; }
+};
+template<> struct dynamic::GetAddrImpl<dynamic::Array> {
+  static Array* get(Data& d) { return &d.array; }
+};
+template<> struct dynamic::GetAddrImpl<bool> {
+  static bool* get(Data& d) { return &d.boolean; }
+};
+template<> struct dynamic::GetAddrImpl<int64_t> {
+  static int64_t* get(Data& d) { return &d.integer; }
+};
+template<> struct dynamic::GetAddrImpl<double> {
+  static double* get(Data& d) { return &d.doubl; }
+};
+template<> struct dynamic::GetAddrImpl<fbstring> {
+  static fbstring* get(Data& d) { return &d.string; }
+};
+template<> struct dynamic::GetAddrImpl<dynamic::ObjectImpl> {
+  static_assert(sizeof(ObjectImpl) <= sizeof(Data::objectBuffer),
+    "In your implementation, std::unordered_map<> apparently takes different"
+    " amount of space depending on its template parameters.  This is "
+    "weird.  Make objectBuffer bigger if you want to compile dynamic.");
+
+  static ObjectImpl* get(Data& d) {
+    void* data = &d.objectBuffer;
+    return static_cast<ObjectImpl*>(data);
+  }
+};
+
+template<class T>
+T& dynamic::get() {
+  if (auto* p = get_nothrow<T>()) {
+    return *p;
+  }
+  throw TypeError(TypeInfo<T>::name, type());
+}
+
+template<class T>
+T const& dynamic::get() const {
+  return const_cast<dynamic*>(this)->get<T>();
+}
+
+inline char const* dynamic::typeName(Type t) {
+#define FB_X(T) return TypeInfo<T>::name
+  FB_DYNAMIC_APPLY(t, FB_X);
+#undef FB_X
+}
+
+inline void dynamic::destroy() {
+  // This short-circuit speeds up some microbenchmarks.
+  if (type_ == NULLT) return;
+
+#define FB_X(T) detail::Destroy::destroy(getAddress<T>())
+  FB_DYNAMIC_APPLY(type_, FB_X);
+#undef FB_X
+  type_ = NULLT;
+  u_.nul = nullptr;
+}
+
+//////////////////////////////////////////////////////////////////////
+
+/*
+ * Helper for implementing operator<<.  Throws if the type shouldn't
+ * support it.
+ */
+template<class T>
+struct dynamic::PrintImpl {
+  static void print(dynamic const&, std::ostream& out, T const& t) {
+    out << t;
+  }
+};
+template<>
+struct dynamic::PrintImpl<dynamic::ObjectImpl> {
+  static void print(dynamic const& d,
+                    std::ostream& out,
+                    dynamic::ObjectImpl const&) {
+    d.print_as_pseudo_json(out);
+  }
+};
+template<>
+struct dynamic::PrintImpl<dynamic::Array> {
+  static void print(dynamic const& d,
+                    std::ostream& out,
+                    dynamic::Array const&) {
+    d.print_as_pseudo_json(out);
+  }
+};
+
+inline void dynamic::print(std::ostream& out) const {
+#define FB_X(T) PrintImpl<T>::print(*this, out, *getAddress<T>())
+  FB_DYNAMIC_APPLY(type_, FB_X);
+#undef FB_X
+}
+
+inline std::ostream& operator<<(std::ostream& out, dynamic const& d) {
+  d.print(out);
+  return out;
+}
+
+//////////////////////////////////////////////////////////////////////
+
+// Secialization of FormatValue so dynamic objects can be formatted
+template <>
+class FormatValue<dynamic> {
+ public:
+  explicit FormatValue(const dynamic& val) : val_(val) { }
+
+  template <class FormatCallback>
+  void format(FormatArg& arg, FormatCallback& cb) const {
+    switch (val_.type()) {
+    case dynamic::NULLT:
+      FormatValue<std::nullptr_t>(nullptr).format(arg, cb);
+      break;
+    case dynamic::BOOL:
+      FormatValue<bool>(val_.asBool()).format(arg, cb);
+      break;
+    case dynamic::INT64:
+      FormatValue<int64_t>(val_.asInt()).format(arg, cb);
+      break;
+    case dynamic::STRING:
+      FormatValue<fbstring>(val_.asString()).format(arg, cb);
+      break;
+    case dynamic::DOUBLE:
+      FormatValue<double>(val_.asDouble()).format(arg, cb);
+      break;
+    case dynamic::ARRAY:
+      FormatValue(val_.at(arg.splitIntKey())).format(arg, cb);
+      break;
+    case dynamic::OBJECT:
+      FormatValue(val_.at(arg.splitKey().toFbstring())).format(arg, cb);
+      break;
+    }
+  }
+
+ private:
+  const dynamic& val_;
+};
+
+}
+
+#undef FB_DYNAMIC_APPLY
+
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/Exception.h
@@ -0,0 +1,115 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_EXCEPTION_H_
+#define FOLLY_EXCEPTION_H_
+
+#include <errno.h>
+
+#include <cstdio>
+#include <stdexcept>
+#include <system_error>
+
+#include "folly/Conv.h"
+#include "folly/FBString.h"
+#include "folly/Likely.h"
+#include "folly/Portability.h"
+
+namespace folly {
+
+// Various helpers to throw appropriate std::system_error exceptions from C
+// library errors (returned in errno, as positive return values (many POSIX
+// functions), or as negative return values (Linux syscalls))
+//
+// The *Explicit functions take an explicit value for errno.
+
+// Helper to throw std::system_error
+void throwSystemErrorExplicit(int err, const char*) FOLLY_NORETURN;
+inline void throwSystemErrorExplicit(int err, const char* msg) {
+  throw std::system_error(err, std::system_category(), msg);
+}
+
+template <class... Args>
+void throwSystemErrorExplicit(int, Args&&... args) FOLLY_NORETURN;
+template <class... Args>
+void throwSystemErrorExplicit(int err, Args&&... args) {
+  throwSystemErrorExplicit(
+      err, to<fbstring>(std::forward<Args>(args)...).c_str());
+}
+
+// Helper to throw std::system_error from errno and components of a string
+template <class... Args>
+void throwSystemError(Args&&... args) FOLLY_NORETURN;
+template <class... Args>
+void throwSystemError(Args&&... args) {
+  throwSystemErrorExplicit(errno, std::forward<Args>(args)...);
+}
+
+// Check a Posix return code (0 on success, error number on error), throw
+// on error.
+template <class... Args>
+void checkPosixError(int err, Args&&... args) {
+  if (UNLIKELY(err != 0)) {
+    throwSystemErrorExplicit(err, std::forward<Args>(args)...);
+  }
+}
+
+// Check a Linux kernel-style return code (>= 0 on success, negative error
+// number on error), throw on error.
+template <class... Args>
+void checkKernelError(ssize_t ret, Args&&... args) {
+  if (UNLIKELY(ret < 0)) {
+    throwSystemErrorExplicit(-ret, std::forward<Args>(args)...);
+  }
+}
+
+// Check a traditional Unix return code (-1 and sets errno on error), throw
+// on error.
+template <class... Args>
+void checkUnixError(ssize_t ret, Args&&... args) {
+  if (UNLIKELY(ret == -1)) {
+    throwSystemError(std::forward<Args>(args)...);
+  }
+}
+
+template <class... Args>
+void checkUnixErrorExplicit(ssize_t ret, int savedErrno, Args&&... args) {
+  if (UNLIKELY(ret == -1)) {
+    throwSystemErrorExplicit(savedErrno, std::forward<Args>(args)...);
+  }
+}
+
+// Check the return code from a fopen-style function (returns a non-nullptr
+// FILE* on success, nullptr on error, sets errno).  Works with fopen, fdopen,
+// freopen, tmpfile, etc.
+template <class... Args>
+void checkFopenError(FILE* fp, Args&&... args) {
+  if (UNLIKELY(!fp)) {
+    throwSystemError(std::forward<Args>(args)...);
+  }
+}
+
+template <class... Args>
+void checkFopenErrorExplicit(FILE* fp, int savedErrno, Args&&... args) {
+  if (UNLIKELY(!fp)) {
+    throwSystemErrorExplicit(savedErrno, std::forward<Args>(args)...);
+  }
+}
+
+}  // namespace folly
+
+#endif /* FOLLY_EXCEPTION_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/Bits.h
@@ -0,0 +1,291 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_EXPERIMENTAL_BITS_H_
+#define FOLLY_EXPERIMENTAL_BITS_H_
+
+#include <cstddef>
+#include <type_traits>
+#include <limits>
+
+#include "folly/Bits.h"
+#include "folly/Portability.h"
+#include "folly/Range.h"
+
+namespace folly {
+
+template <class T>
+struct UnalignedNoASan : public Unaligned<T> { };
+
+// As a general rule, bit operations work on unsigned values only;
+// right-shift is arithmetic for signed values, and that can lead to
+// unpleasant bugs.
+
+namespace detail {
+
+/**
+ * Helper class to make Bits<T> (below) work with both aligned values
+ * (T, where T is an unsigned integral type) or unaligned values
+ * (Unaligned<T>, where T is an unsigned integral type)
+ */
+template <class T, class Enable=void> struct BitsTraits;
+
+// Partial specialization for Unaligned<T>, where T is unsigned integral
+// loadRMW is the same as load, but it indicates that it loads for a
+// read-modify-write operation (we write back the bits we won't change);
+// silence the GCC warning in that case.
+template <class T>
+struct BitsTraits<Unaligned<T>, typename std::enable_if<
+    (std::is_integral<T>::value && std::is_unsigned<T>::value)>::type> {
+  typedef T UnderlyingType;
+  static T load(const Unaligned<T>& x) { return x.value; }
+  static void store(Unaligned<T>& x, T v) { x.value = v; }
+  static T loadRMW(const Unaligned<T>& x) {
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wuninitialized"
+// make sure we compile without warning on gcc 4.6 with -Wpragmas
+#if __GNUC_PREREQ(4, 7)
+#pragma GCC diagnostic ignored "-Wmaybe-uninitialized"
+#endif
+    return x.value;
+#pragma GCC diagnostic pop
+  }
+};
+
+// Special version that allows to disable address sanitizer on demand.
+template <class T>
+struct BitsTraits<UnalignedNoASan<T>, typename std::enable_if<
+    (std::is_integral<T>::value && std::is_unsigned<T>::value)>::type> {
+  typedef T UnderlyingType;
+  static T FOLLY_DISABLE_ADDRESS_SANITIZER
+  load(const UnalignedNoASan<T>& x) { return x.value; }
+  static void FOLLY_DISABLE_ADDRESS_SANITIZER
+  store(UnalignedNoASan<T>& x, T v) { x.value = v; }
+  static T FOLLY_DISABLE_ADDRESS_SANITIZER
+  loadRMW(const UnalignedNoASan<T>& x) {
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wuninitialized"
+// make sure we compile without warning on gcc 4.6 with -Wpragmas
+#if __GNUC_PREREQ(4, 7)
+#pragma GCC diagnostic ignored "-Wmaybe-uninitialized"
+#endif
+    return x.value;
+#pragma GCC diagnostic pop
+  }
+};
+
+// Partial specialization for T, where T is unsigned integral
+template <class T>
+struct BitsTraits<T, typename std::enable_if<
+    (std::is_integral<T>::value && std::is_unsigned<T>::value)>::type> {
+  typedef T UnderlyingType;
+  static T load(const T& x) { return x; }
+  static void store(T& x, T v) { x = v; }
+  static T loadRMW(const T& x) {
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wuninitialized"
+#if __GNUC_PREREQ(4, 7)
+#pragma GCC diagnostic ignored "-Wmaybe-uninitialized"
+#endif
+    return x;
+#pragma GCC diagnostic pop
+  }
+};
+
+}  // namespace detail
+
+/**
+ * Wrapper class with static methods for various bit-level operations,
+ * treating an array of T as an array of bits (in little-endian order).
+ * (T is either an unsigned integral type or Unaligned<X>, where X is
+ * an unsigned integral type)
+ */
+template <class T, class Traits=detail::BitsTraits<T>>
+struct Bits {
+  typedef typename Traits::UnderlyingType UnderlyingType;
+  typedef T type;
+  static_assert(sizeof(T) == sizeof(UnderlyingType), "Size mismatch");
+
+  /**
+   * Number of bits in a block.
+   */
+  static constexpr size_t bitsPerBlock =
+    std::numeric_limits<UnderlyingType>::digits;
+
+  /**
+   * Byte index of the given bit.
+   */
+  static constexpr size_t blockIndex(size_t bit) {
+    return bit / bitsPerBlock;
+  }
+
+  /**
+   * Offset in block of the given bit.
+   */
+  static constexpr size_t bitOffset(size_t bit) {
+    return bit % bitsPerBlock;
+  }
+
+  /**
+   * Number of blocks used by the given number of bits.
+   */
+  static constexpr size_t blockCount(size_t nbits) {
+    return nbits / bitsPerBlock + (nbits % bitsPerBlock != 0);
+  }
+
+  /**
+   * Set the given bit.
+   */
+  static void set(T* p, size_t bit);
+
+  /**
+   * Clear the given bit.
+   */
+  static void clear(T* p, size_t bit);
+
+  /**
+   * Test the given bit.
+   */
+  static bool test(const T* p, size_t bit);
+
+  /**
+   * Set count contiguous bits starting at bitStart to the values
+   * from the least significant count bits of value; little endian.
+   * (value & 1 becomes the bit at bitStart, etc)
+   * Precondition: count <= sizeof(T) * 8
+   */
+  static void set(T* p, size_t bitStart, size_t count, UnderlyingType value);
+
+  /**
+   * Get count contiguous bits starting at bitStart.
+   * Precondition: count <= sizeof(T) * 8
+   */
+  static UnderlyingType get(const T* p, size_t bitStart, size_t count);
+
+  /**
+   * Count the number of bits set in a range of blocks.
+   */
+  static size_t count(const T* begin, const T* end);
+
+ private:
+  // Same as set, assumes all bits are in the same block.
+  // (bitStart < sizeof(T) * 8, bitStart + count <= sizeof(T) * 8)
+  static void innerSet(T* p, size_t bitStart, size_t count,
+                       UnderlyingType value);
+
+  // Same as get, assumes all bits are in the same block.
+  // (bitStart < sizeof(T) * 8, bitStart + count <= sizeof(T) * 8)
+  static UnderlyingType innerGet(const T* p, size_t bitStart, size_t count);
+
+  static constexpr UnderlyingType zero = UnderlyingType(0);
+  static constexpr UnderlyingType one = UnderlyingType(1);
+
+  static constexpr UnderlyingType ones(size_t count) {
+    return count < bitsPerBlock ? (one << count) - 1 : ~zero;
+  }
+};
+
+// gcc 4.8 needs more -Wmaybe-uninitialized tickling, as it propagates the
+// taint upstream from loadRMW
+
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wuninitialized"
+#if __GNUC_PREREQ(4, 7)
+#pragma GCC diagnostic ignored "-Wmaybe-uninitialized"
+#endif
+
+template <class T, class Traits>
+inline void Bits<T, Traits>::set(T* p, size_t bit) {
+  T& block = p[blockIndex(bit)];
+  Traits::store(block, Traits::loadRMW(block) | (one << bitOffset(bit)));
+}
+
+template <class T, class Traits>
+inline void Bits<T, Traits>::clear(T* p, size_t bit) {
+  T& block = p[blockIndex(bit)];
+  Traits::store(block, Traits::loadRMW(block) & ~(one << bitOffset(bit)));
+}
+
+template <class T, class Traits>
+inline void Bits<T, Traits>::set(T* p, size_t bitStart, size_t count,
+                                 UnderlyingType value) {
+  assert(count <= sizeof(UnderlyingType) * 8);
+  size_t idx = blockIndex(bitStart);
+  size_t offset = bitOffset(bitStart);
+  if (offset + count <= bitsPerBlock) {
+    innerSet(p + idx, offset, count, value);
+  } else {
+    size_t countInThisBlock = bitsPerBlock - offset;
+    size_t countInNextBlock = count - countInThisBlock;
+    innerSet(p + idx, offset, countInThisBlock,
+             value & ((one << countInThisBlock) - 1));
+    innerSet(p + idx + 1, 0, countInNextBlock, value >> countInThisBlock);
+  }
+}
+
+template <class T, class Traits>
+inline void Bits<T, Traits>::innerSet(T* p, size_t offset, size_t count,
+                                      UnderlyingType value) {
+  // Mask out bits and set new value
+  UnderlyingType v = Traits::loadRMW(*p);
+  v &= ~(ones(count) << offset);
+  v |= (value << offset);
+  Traits::store(*p, v);
+}
+
+#pragma GCC diagnostic pop
+
+template <class T, class Traits>
+inline bool Bits<T, Traits>::test(const T* p, size_t bit) {
+  return Traits::load(p[blockIndex(bit)]) & (one << bitOffset(bit));
+}
+
+template <class T, class Traits>
+inline auto Bits<T, Traits>::get(const T* p, size_t bitStart, size_t count)
+  -> UnderlyingType {
+  assert(count <= sizeof(UnderlyingType) * 8);
+  size_t idx = blockIndex(bitStart);
+  size_t offset = bitOffset(bitStart);
+  if (offset + count <= bitsPerBlock) {
+    return innerGet(p + idx, offset, count);
+  } else {
+    size_t countInThisBlock = bitsPerBlock - offset;
+    size_t countInNextBlock = count - countInThisBlock;
+    UnderlyingType thisBlockValue = innerGet(p + idx, offset, countInThisBlock);
+    UnderlyingType nextBlockValue = innerGet(p + idx + 1, 0, countInNextBlock);
+    return (nextBlockValue << countInThisBlock) | thisBlockValue;
+  }
+}
+
+template <class T, class Traits>
+inline auto Bits<T, Traits>::innerGet(const T* p, size_t offset, size_t count)
+  -> UnderlyingType {
+  return (Traits::load(*p) >> offset) & ones(count);
+}
+
+template <class T, class Traits>
+inline size_t Bits<T, Traits>::count(const T* begin, const T* end) {
+  size_t n = 0;
+  for (; begin != end; ++begin) {
+    n += popcount(Traits::load(*begin));
+  }
+  return n;
+}
+
+}  // namespace folly
+
+#endif /* FOLLY_EXPERIMENTAL_BITS_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/EliasFanoCoding.h
@@ -0,0 +1,567 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * @author Philip Pronin (philipp@fb.com)
+ *
+ * Based on the paper by Sebastiano Vigna,
+ * "Quasi-succinct indices" (arxiv:1206.4300).
+ */
+
+#ifndef FOLLY_EXPERIMENTAL_ELIAS_FANO_CODING_H
+#define FOLLY_EXPERIMENTAL_ELIAS_FANO_CODING_H
+
+#ifndef __GNUC__
+#error EliasFanoCoding.h requires GCC
+#endif
+
+#if !defined(__x86_64__)
+#error EliasFanoCoding.h requires x86_64
+#endif
+
+#include <algorithm>
+#include <cstdlib>
+#include <limits>
+#include <type_traits>
+#include <boost/noncopyable.hpp>
+#include <glog/logging.h>
+
+#include "folly/Bits.h"
+#include "folly/CpuId.h"
+#include "folly/Likely.h"
+#include "folly/Range.h"
+
+#if __BYTE_ORDER__ != __ORDER_LITTLE_ENDIAN__
+#error EliasFanoCoding.h requires little endianness
+#endif
+
+namespace folly { namespace compression {
+
+struct EliasFanoCompressedList {
+  EliasFanoCompressedList()
+    : size(0), numLowerBits(0) { }
+
+  void free() {
+    ::free(const_cast<unsigned char*>(lower.data()));
+    ::free(const_cast<unsigned char*>(upper.data()));
+    ::free(const_cast<unsigned char*>(skipPointers.data()));
+    ::free(const_cast<unsigned char*>(forwardPointers.data()));
+  }
+
+  size_t size;
+  uint8_t numLowerBits;
+
+  // WARNING: EliasFanoCompressedList has no ownership of
+  // lower, upper, skipPointers and forwardPointers.
+  // The 7 bytes following the last byte of lower and upper
+  // sequences should be readable.
+  folly::ByteRange lower;
+  folly::ByteRange upper;
+
+  folly::ByteRange skipPointers;
+  folly::ByteRange forwardPointers;
+};
+
+// Version history:
+// In version 1 skip / forward pointers encoding has been changed,
+// so SkipValue = uint32_t is able to address up to ~4B elements,
+// instead of only ~2B.
+template <class Value,
+          class SkipValue = size_t,
+          size_t kSkipQuantum = 0,     // 0 = disabled
+          size_t kForwardQuantum = 0,  // 0 = disabled
+          size_t kVersion = 0>
+struct EliasFanoEncoder {
+  static_assert(std::is_integral<Value>::value &&
+                std::is_unsigned<Value>::value,
+                "Value should be unsigned integral");
+
+  typedef EliasFanoCompressedList CompressedList;
+
+  typedef Value ValueType;
+  typedef SkipValue SkipValueType;
+
+  static constexpr size_t skipQuantum = kSkipQuantum;
+  static constexpr size_t forwardQuantum = kForwardQuantum;
+  static constexpr size_t version = kVersion;
+
+  static uint8_t defaultNumLowerBits(size_t upperBound, size_t size) {
+    if (size == 0 || upperBound < size) {
+      return 0;
+    }
+    // floor(log(upperBound / size));
+    return folly::findLastSet(upperBound / size) - 1;
+  }
+
+  // WARNING: encode() mallocates lower, upper, skipPointers
+  // and forwardPointers. As EliasFanoCompressedList has
+  // no ownership of them, you need to call free() explicitly.
+  static void encode(const ValueType* list, size_t size,
+                     EliasFanoCompressedList& result) {
+    encode(list, list + size, result);
+  }
+
+  // Range (begin, end) should be sorted.
+  template <class RandomAccessIterator>
+  static void encode(RandomAccessIterator begin,
+                     RandomAccessIterator end,
+                     EliasFanoCompressedList& result) {
+    CHECK(std::is_sorted(begin, end));
+
+    auto list = begin;
+    const size_t size = end - begin;
+
+    if (size == 0) {
+      result = EliasFanoCompressedList();
+      return;
+    }
+
+    const ValueType upperBound = list[size - 1];
+    uint8_t numLowerBits = defaultNumLowerBits(upperBound, size);
+
+    // This is detail::writeBits56 limitation.
+    numLowerBits = std::min<uint8_t>(numLowerBits, 56);
+    CHECK_LT(numLowerBits, 8 * sizeof(Value));  // As we shift by numLowerBits.
+
+    // WARNING: Current read/write logic assumes that the 7 bytes
+    // following the last byte of lower and upper sequences are
+    // readable (stored value doesn't matter and won't be changed),
+    // so we allocate additional 7B, but do not include them in size
+    // of returned value.
+
+    // *** Lower bits.
+    const size_t lowerSize = (numLowerBits * size + 7) / 8;
+    unsigned char* lower = nullptr;
+    if (lowerSize > 0) {  // numLowerBits != 0
+      lower = static_cast<unsigned char*>(calloc(lowerSize + 7, 1));
+      const ValueType lowerMask = (ValueType(1) << numLowerBits) - 1;
+      for (size_t i = 0; i < size; ++i) {
+        const ValueType lowerBits = list[i] & lowerMask;
+        writeBits56(lower, i * numLowerBits, numLowerBits, lowerBits);
+      }
+    }
+
+    // *** Upper bits.
+    // Upper bits are stored using unary delta encoding.
+    // For example, (3 5 5 9) will be encoded as 1000011001000_2.
+    const size_t upperSizeBits =
+      (upperBound >> numLowerBits) +  // Number of 0-bits to be stored.
+      size;                           // 1-bits.
+    const size_t upperSize = (upperSizeBits + 7) / 8;
+    unsigned char* const upper =
+      static_cast<unsigned char*>(calloc(upperSize + 7, 1));
+    for (size_t i = 0; i < size; ++i) {
+      const ValueType upperBits = list[i] >> numLowerBits;
+      const size_t pos = upperBits + i;  // upperBits 0-bits and (i + 1) 1-bits.
+      upper[pos / 8] |= 1U << (pos % 8);
+    }
+
+    // *** Skip pointers.
+    // Store (1-indexed) position of every skipQuantum-th
+    // 0-bit in upper bits sequence.
+    SkipValueType* skipPointers = nullptr;
+    size_t numSkipPointers = 0;
+    /* static */ if (skipQuantum != 0) {
+      // Workaround to avoid 'division by zero' compile-time error.
+      constexpr size_t q = skipQuantum ?: 1;
+      /* static */ if (kVersion > 0) {
+        CHECK_LT(size, std::numeric_limits<SkipValueType>::max());
+      } else {
+        CHECK_LT(upperSizeBits, std::numeric_limits<SkipValueType>::max());
+      }
+      // 8 * upperSize is used here instead of upperSizeBits, as that is
+      // more serialization-friendly way (upperSizeBits isn't known outside of
+      // this function, unlike upperSize; thus numSkipPointers could easily be
+      // deduced from upperSize).
+      numSkipPointers = (8 * upperSize - size) / q;
+      skipPointers = static_cast<SkipValueType*>(
+          numSkipPointers == 0
+            ? nullptr
+            : calloc(numSkipPointers, sizeof(SkipValueType)));
+
+      for (size_t i = 0, pos = 0; i < size; ++i) {
+        const ValueType upperBits = list[i] >> numLowerBits;
+        for (; (pos + 1) * q <= upperBits; ++pos) {
+          /* static */ if (kVersion > 0) {
+            // Since version 1, just the number of preceding 1-bits is stored.
+            skipPointers[pos] = i;
+          } else {
+            skipPointers[pos] = i + (pos + 1) * q;
+          }
+        }
+      }
+    }
+
+    // *** Forward pointers.
+    // Store (1-indexed) position of every forwardQuantum-th
+    // 1-bit in upper bits sequence.
+    SkipValueType* forwardPointers = nullptr;
+    size_t numForwardPointers = 0;
+    /* static */ if (forwardQuantum != 0) {
+      // Workaround to avoid 'division by zero' compile-time error.
+      constexpr size_t q = forwardQuantum ?: 1;
+      /* static */ if (kVersion > 0) {
+        CHECK_LT(upperBound >> numLowerBits,
+                 std::numeric_limits<SkipValueType>::max());
+      } else {
+        CHECK_LT(upperSizeBits, std::numeric_limits<SkipValueType>::max());
+      }
+
+      numForwardPointers = size / q;
+      forwardPointers = static_cast<SkipValueType*>(
+        numForwardPointers == 0
+          ? nullptr
+          : malloc(numForwardPointers * sizeof(SkipValueType)));
+
+      for (size_t i = q - 1, pos = 0; i < size; i += q, ++pos) {
+        const ValueType upperBits = list[i] >> numLowerBits;
+        /* static */ if (kVersion > 0) {
+          // Since version 1, just the number of preceding 0-bits is stored.
+          forwardPointers[pos] = upperBits;
+        } else {
+          forwardPointers[pos] = upperBits + i + 1;
+        }
+      }
+    }
+
+    // *** Result.
+    result.size = size;
+    result.numLowerBits = numLowerBits;
+    result.lower.reset(lower, lowerSize);
+    result.upper.reset(upper, upperSize);
+    result.skipPointers.reset(
+        reinterpret_cast<unsigned char*>(skipPointers),
+        numSkipPointers * sizeof(SkipValueType));
+    result.forwardPointers.reset(
+        reinterpret_cast<unsigned char*>(forwardPointers),
+        numForwardPointers * sizeof(SkipValueType));
+  }
+
+ private:
+  // Writes value (with len up to 56 bits) to data starting at pos-th bit.
+  static void writeBits56(unsigned char* data, size_t pos,
+                          uint8_t len, uint64_t value) {
+    DCHECK_LE(uint32_t(len), 56);
+    DCHECK_EQ(0, value & ~((uint64_t(1) << len) - 1));
+    unsigned char* const ptr = data + (pos / 8);
+    uint64_t ptrv = folly::loadUnaligned<uint64_t>(ptr);
+    ptrv |= value << (pos % 8);
+    folly::storeUnaligned<uint64_t>(ptr, ptrv);
+  }
+};
+
+// NOTE: It's recommended to compile EF coding with -msse4.2, starting
+// with Nehalem, Intel CPUs support POPCNT instruction and gcc will emit
+// it for __builtin_popcountll intrinsic.
+// But we provide an alternative way for the client code: it can switch to
+// the appropriate version of EliasFanoReader<> in realtime (client should
+// implement this switching logic itself) by specifying instruction set to
+// use explicitly.
+namespace instructions {
+
+struct Default {
+  static bool supported() {
+    return true;
+  }
+  static inline uint64_t popcount(uint64_t value) {
+    return __builtin_popcountll(value);
+  }
+  static inline int ctz(uint64_t value) {
+    DCHECK_GT(value, 0);
+    return __builtin_ctzll(value);
+  }
+};
+
+struct Fast : public Default {
+  static bool supported() {
+    folly::CpuId cpuId;
+    return cpuId.popcnt();
+  }
+  static inline uint64_t popcount(uint64_t value) {
+    uint64_t result;
+    asm ("popcntq %1, %0" : "=r" (result) : "r" (value));
+    return result;
+  }
+};
+
+}  // namespace instructions
+
+namespace detail {
+
+template <class Encoder, class Instructions>
+class UpperBitsReader {
+  typedef typename Encoder::SkipValueType SkipValueType;
+ public:
+  typedef typename Encoder::ValueType ValueType;
+
+  explicit UpperBitsReader(const EliasFanoCompressedList& list)
+    : forwardPointers_(list.forwardPointers.data()),
+      skipPointers_(list.skipPointers.data()),
+      start_(list.upper.data()),
+      block_(start_ != nullptr ? folly::loadUnaligned<block_t>(start_) : 0),
+      outer_(0),  // outer offset: number of consumed bytes in upper.
+      inner_(-1),  // inner offset: (bit) position in current block.
+      position_(-1),  // index of current value (= #reads - 1).
+      value_(0) { }
+
+  size_t position() const { return position_; }
+  ValueType value() const { return value_; }
+
+  ValueType next() {
+    // Skip to the first non-zero block.
+    while (block_ == 0) {
+      outer_ += sizeof(block_t);
+      block_ = folly::loadUnaligned<block_t>(start_ + outer_);
+    }
+
+    ++position_;
+    inner_ = Instructions::ctz(block_);
+    block_ &= block_ - 1;
+
+    return setValue();
+  }
+
+  ValueType skip(size_t n) {
+    DCHECK_GT(n, 0);
+
+    position_ += n;  // n 1-bits will be read.
+
+    // Use forward pointer.
+    if (Encoder::forwardQuantum > 0 && n > Encoder::forwardQuantum) {
+      // Workaround to avoid 'division by zero' compile-time error.
+      constexpr size_t q = Encoder::forwardQuantum ?: 1;
+
+      const size_t steps = position_ / q;
+      const size_t dest =
+        folly::loadUnaligned<SkipValueType>(
+            forwardPointers_ + (steps - 1) * sizeof(SkipValueType));
+
+      /* static */ if (Encoder::version > 0) {
+        reposition(dest + steps * q);
+      } else {
+        reposition(dest);
+      }
+      n = position_ + 1 - steps * q;  // n is > 0.
+      // correct inner_ will be set at the end.
+    }
+
+    size_t cnt;
+    // Find necessary block.
+    while ((cnt = Instructions::popcount(block_)) < n) {
+      n -= cnt;
+      outer_ += sizeof(block_t);
+      block_ = folly::loadUnaligned<block_t>(start_ + outer_);
+    }
+
+    // NOTE: Trying to skip half-block here didn't show any
+    // performance improvements.
+
+    DCHECK_GT(n, 0);
+
+    // Kill n - 1 least significant 1-bits.
+    for (size_t i = 0; i < n - 1; ++i) {
+      block_ &= block_ - 1;
+    }
+
+    inner_ = Instructions::ctz(block_);
+    block_ &= block_ - 1;
+
+    return setValue();
+  }
+
+  // Skip to the first element that is >= v and located *after* the current
+  // one (so even if current value equals v, position will be increased by 1).
+  ValueType skipToNext(ValueType v) {
+    DCHECK_GE(v, value_);
+
+    // Use skip pointer.
+    if (Encoder::skipQuantum > 0 && v >= value_ + Encoder::skipQuantum) {
+      // Workaround to avoid 'division by zero' compile-time error.
+      constexpr size_t q = Encoder::skipQuantum ?: 1;
+
+      const size_t steps = v / q;
+      const size_t dest =
+        folly::loadUnaligned<SkipValueType>(
+            skipPointers_ + (steps - 1) * sizeof(SkipValueType));
+
+      /* static */ if (Encoder::version > 0) {
+        reposition(dest + q * steps);
+        position_ = dest - 1;
+      } else {
+        reposition(dest);
+        position_ = dest - q * steps - 1;
+      }
+      // Correct inner_ and value_ will be set during the next()
+      // call at the end.
+
+      // NOTE: Corresponding block of lower bits sequence may be
+      // prefetched here (via __builtin_prefetch), but experiments
+      // didn't show any significant improvements.
+    }
+
+    // Skip by blocks.
+    size_t cnt;
+    size_t skip = v - (8 * outer_ - position_ - 1);
+
+    constexpr size_t kBitsPerBlock = 8 * sizeof(block_t);
+    while ((cnt = Instructions::popcount(~block_)) < skip) {
+      skip -= cnt;
+      position_ += kBitsPerBlock - cnt;
+      outer_ += sizeof(block_t);
+      block_ = folly::loadUnaligned<block_t>(start_ + outer_);
+    }
+
+    // Try to skip half-block.
+    constexpr size_t kBitsPerHalfBlock = 4 * sizeof(block_t);
+    constexpr block_t halfBlockMask = (block_t(1) << kBitsPerHalfBlock) - 1;
+    if ((cnt = Instructions::popcount(~block_ & halfBlockMask)) < skip) {
+      position_ += kBitsPerHalfBlock - cnt;
+      block_ &= ~halfBlockMask;
+    }
+
+    // Just skip until we see expected value.
+    while (next() < v) { }
+    return value_;
+  }
+
+ private:
+  ValueType setValue() {
+    value_ = static_cast<ValueType>(8 * outer_ + inner_ - position_);
+    return value_;
+  }
+
+  void reposition(size_t dest) {
+    outer_ = dest / 8;
+    block_ = folly::loadUnaligned<block_t>(start_ + outer_);
+    block_ &= ~((block_t(1) << (dest % 8)) - 1);
+  }
+
+  typedef unsigned long long block_t;
+  const unsigned char* const forwardPointers_;
+  const unsigned char* const skipPointers_;
+  const unsigned char* const start_;
+  block_t block_;
+  size_t outer_;
+  size_t inner_;
+  size_t position_;
+  ValueType value_;
+};
+
+}  // namespace detail
+
+template <class Encoder,
+          class Instructions = instructions::Default>
+class EliasFanoReader : private boost::noncopyable {
+ public:
+  typedef typename Encoder::ValueType ValueType;
+
+  explicit EliasFanoReader(const EliasFanoCompressedList& list)
+    : list_(list),
+      lowerMask_((ValueType(1) << list_.numLowerBits) - 1),
+      upper_(list),
+      progress_(0),
+      value_(0) {
+    DCHECK(Instructions::supported());
+    // To avoid extra branching during skipTo() while reading
+    // upper sequence we need to know the last element.
+    if (UNLIKELY(list_.size == 0)) {
+      lastValue_ = 0;
+      return;
+    }
+    ValueType lastUpperValue = 8 * list_.upper.size() - list_.size;
+    auto it = list_.upper.end() - 1;
+    DCHECK_NE(*it, 0);
+    lastUpperValue -= 8 - folly::findLastSet(*it);
+    lastValue_ = readLowerPart(list_.size - 1) |
+                 (lastUpperValue << list_.numLowerBits);
+  }
+
+  size_t size() const { return list_.size; }
+
+  size_t position() const { return progress_ - 1; }
+  ValueType value() const { return value_; }
+
+  bool next() {
+    if (UNLIKELY(progress_ == list_.size)) {
+      value_ = std::numeric_limits<ValueType>::max();
+      return false;
+    }
+    value_ = readLowerPart(progress_) |
+             (upper_.next() << list_.numLowerBits);
+    ++progress_;
+    return true;
+  }
+
+  bool skip(size_t n) {
+    CHECK_GT(n, 0);
+
+    progress_ += n - 1;
+    if (LIKELY(progress_ < list_.size)) {
+      value_ = readLowerPart(progress_) |
+               (upper_.skip(n) << list_.numLowerBits);
+      ++progress_;
+      return true;
+    }
+
+    progress_ = list_.size;
+    value_ = std::numeric_limits<ValueType>::max();
+    return false;
+  }
+
+  bool skipTo(ValueType value) {
+    DCHECK_GE(value, value_);
+    if (value <= value_) {
+      return true;
+    }
+    if (value > lastValue_) {
+      progress_ = list_.size;
+      value_ = std::numeric_limits<ValueType>::max();
+      return false;
+    }
+
+    upper_.skipToNext(value >> list_.numLowerBits);
+    progress_ = upper_.position();
+    value_ = readLowerPart(progress_) |
+             (upper_.value() << list_.numLowerBits);
+    ++progress_;
+    while (value_ < value) {
+      value_ = readLowerPart(progress_) |
+               (upper_.next() << list_.numLowerBits);
+      ++progress_;
+    }
+
+    return true;
+  }
+
+ private:
+  ValueType readLowerPart(size_t i) const {
+    const size_t pos = i * list_.numLowerBits;
+    const unsigned char* ptr = list_.lower.data() + (pos / 8);
+    const uint64_t ptrv = folly::loadUnaligned<uint64_t>(ptr);
+    return lowerMask_ & (ptrv >> (pos % 8));
+  }
+
+  const EliasFanoCompressedList list_;
+  const ValueType lowerMask_;
+  detail::UpperBitsReader<Encoder, Instructions> upper_;
+  size_t progress_;
+  ValueType value_;
+  ValueType lastValue_;
+};
+
+}}  // namespaces
+
+#endif  // FOLLY_EXPERIMENTAL_ELIAS_FANO_CODING_H
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/EventCount.h
@@ -0,0 +1,215 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_EXPERIMENTAL_EVENTCOUNT_H_
+#define FOLLY_EXPERIMENTAL_EVENTCOUNT_H_
+
+#include <unistd.h>
+#include <syscall.h>
+#include <linux/futex.h>
+#include <sys/time.h>
+#include <cassert>
+#include <climits>
+#include <atomic>
+#include <thread>
+
+#include "folly/Bits.h"
+#include "folly/Likely.h"
+
+
+namespace folly {
+
+namespace detail {
+
+inline int futex(int* uaddr, int op, int val, const timespec* timeout,
+                 int* uaddr2, int val3) noexcept {
+  return syscall(SYS_futex, uaddr, op, val, timeout, uaddr2, val3);
+}
+
+}  // namespace detail
+
+/**
+ * Event count: a condition variable for lock free algorithms.
+ *
+ * See http://www.1024cores.net/home/lock-free-algorithms/eventcounts for
+ * details.
+ *
+ * Event counts allow you to convert a non-blocking lock-free / wait-free
+ * algorithm into a blocking one, by isolating the blocking logic.  You call
+ * prepareWait() before checking your condition and then either cancelWait()
+ * or wait() depending on whether the condition was true.  When another
+ * thread makes the condition true, it must call notify() / notifyAll() just
+ * like a regular condition variable.
+ *
+ * If "<" denotes the happens-before relationship, consider 2 threads (T1 and
+ * T2) and 3 events:
+ * - E1: T1 returns from prepareWait
+ * - E2: T1 calls wait
+ *   (obviously E1 < E2, intra-thread)
+ * - E3: T2 calls notifyAll
+ *
+ * If E1 < E3, then E2's wait will complete (and T1 will either wake up,
+ * or not block at all)
+ *
+ * This means that you can use an EventCount in the following manner:
+ *
+ * Waiter:
+ *   if (!condition()) {  // handle fast path first
+ *     for (;;) {
+ *       auto key = eventCount.prepareWait();
+ *       if (condition()) {
+ *         eventCount.cancelWait();
+ *         break;
+ *       } else {
+ *         eventCount.wait(key);
+ *       }
+ *     }
+ *  }
+ *
+ *  (This pattern is encapsulated in await())
+ *
+ * Poster:
+ *   make_condition_true();
+ *   eventCount.notifyAll();
+ *
+ * Note that, just like with regular condition variables, the waiter needs to
+ * be tolerant of spurious wakeups and needs to recheck the condition after
+ * being woken up.  Also, as there is no mutual exclusion implied, "checking"
+ * the condition likely means attempting an operation on an underlying
+ * data structure (push into a lock-free queue, etc) and returning true on
+ * success and false on failure.
+ */
+class EventCount {
+ public:
+  EventCount() noexcept : val_(0) { }
+
+  class Key {
+    friend class EventCount;
+    explicit Key(uint32_t e) noexcept : epoch_(e) { }
+    uint32_t epoch_;
+  };
+
+  void notify() noexcept;
+  void notifyAll() noexcept;
+  Key prepareWait() noexcept;
+  void cancelWait() noexcept;
+  void wait(Key key) noexcept;
+
+  /**
+   * Wait for condition() to become true.  Will clean up appropriately if
+   * condition() throws, and then rethrow.
+   */
+  template <class Condition>
+  void await(Condition condition);
+
+ private:
+  void doNotify(int n) noexcept;
+  EventCount(const EventCount&) = delete;
+  EventCount(EventCount&&) = delete;
+  EventCount& operator=(const EventCount&) = delete;
+  EventCount& operator=(EventCount&&) = delete;
+
+  // This requires 64-bit
+  static_assert(sizeof(int) == 4, "bad platform");
+  static_assert(sizeof(uint32_t) == 4, "bad platform");
+  static_assert(sizeof(uint64_t) == 8, "bad platform");
+
+#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__
+  static constexpr size_t kEpochOffset = 1;
+#elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__
+  static constexpr size_t kEpochOffset = 0;  // in units of sizeof(int)
+#else
+# error Your machine uses a weird endianness!
+#endif
+
+  // val_ stores the epoch in the most significant 32 bits and the
+  // waiter count in the least significant 32 bits.
+  std::atomic<uint64_t> val_;
+
+  static constexpr uint64_t kAddWaiter = uint64_t(1);
+  static constexpr uint64_t kSubWaiter = uint64_t(-1);
+  static constexpr size_t  kEpochShift = 32;
+  static constexpr uint64_t kAddEpoch = uint64_t(1) << kEpochShift;
+  static constexpr uint64_t kWaiterMask = kAddEpoch - 1;
+};
+
+inline void EventCount::notify() noexcept {
+  doNotify(1);
+}
+
+inline void EventCount::notifyAll() noexcept {
+  doNotify(INT_MAX);
+}
+
+inline void EventCount::doNotify(int n) noexcept {
+  uint64_t prev = val_.fetch_add(kAddEpoch, std::memory_order_acq_rel);
+  if (UNLIKELY(prev & kWaiterMask)) {
+    detail::futex(reinterpret_cast<int*>(&val_) + kEpochOffset,
+                  FUTEX_WAKE, n, nullptr, nullptr, 0);
+  }
+}
+
+inline EventCount::Key EventCount::prepareWait() noexcept {
+  uint64_t prev = val_.fetch_add(kAddWaiter, std::memory_order_acq_rel);
+  return Key(prev >> kEpochShift);
+}
+
+inline void EventCount::cancelWait() noexcept {
+  // memory_order_relaxed would suffice for correctness, but the faster
+  // #waiters gets to 0, the less likely it is that we'll do spurious wakeups
+  // (and thus system calls).
+  uint64_t prev = val_.fetch_add(kSubWaiter, std::memory_order_seq_cst);
+  assert((prev & kWaiterMask) != 0);
+}
+
+inline void EventCount::wait(Key key) noexcept {
+  while ((val_.load(std::memory_order_acquire) >> kEpochShift) == key.epoch_) {
+    detail::futex(reinterpret_cast<int*>(&val_) + kEpochOffset,
+                  FUTEX_WAIT, key.epoch_, nullptr, nullptr, 0);
+  }
+  // memory_order_relaxed would suffice for correctness, but the faster
+  // #waiters gets to 0, the less likely it is that we'll do spurious wakeups
+  // (and thus system calls)
+  uint64_t prev = val_.fetch_add(kSubWaiter, std::memory_order_seq_cst);
+  assert((prev & kWaiterMask) != 0);
+}
+
+template <class Condition>
+void EventCount::await(Condition condition) {
+  if (condition()) return;  // fast path
+
+  // condition() is the only thing that may throw, everything else is
+  // noexcept, so we can hoist the try/catch block outside of the loop
+  try {
+    for (;;) {
+      auto key = prepareWait();
+      if (condition()) {
+        cancelWait();
+        break;
+      } else {
+        wait(key);
+      }
+    }
+  } catch (...) {
+    cancelWait();
+    throw;
+  }
+}
+
+}  // namespace folly
+
+#endif /* FOLLY_EXPERIMENTAL_EVENTCOUNT_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/exception_tracer/ExceptionAbi.h
@@ -0,0 +1,61 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+#ifndef FOLLY_EXPERIMENTAL_EXCEPTION_TRACER_EXCEPTIONABI_H_
+#define FOLLY_EXPERIMENTAL_EXCEPTION_TRACER_EXCEPTIONABI_H_
+
+// A clone of the relevant parts of unwind-cxx.h from libstdc++
+// The layout of these structures is defined by the ABI.
+
+#include <exception>
+#include <typeinfo>
+
+#include <unwind.h>
+
+namespace __cxxabiv1 {
+
+struct __cxa_exception {
+  std::type_info* exceptionType;
+  void (*exceptionDestructor) (void*);
+  std::unexpected_handler unexpectedHandler;
+  std::terminate_handler terminateHandler;
+  __cxa_exception* nextException;
+
+  int handlerCount;
+  int handlerSwitchValue;
+  const char* actionRecord;
+  const char* languageSpecificData;
+  void* catchTemp;
+  void* adjustedPtr;
+
+  _Unwind_Exception unwindHeader;
+};
+
+struct __cxa_eh_globals {
+  __cxa_exception* caughtExceptions;
+  unsigned int uncaughtExceptions;
+};
+
+extern "C" {
+__cxa_eh_globals* __cxa_get_globals(void) noexcept;
+__cxa_eh_globals* __cxa_get_globals_fast(void) noexcept;
+}
+
+}  // namespace __cxxabiv1
+
+#endif /* FOLLY_EXPERIMENTAL_EXCEPTION_TRACER_EXCEPTIONABI_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/exception_tracer/ExceptionTracerBenchmark.cpp
@@ -0,0 +1,63 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <stdexcept>
+#include <thread>
+#include <vector>
+
+#include <gflags/gflags.h>
+#include <glog/logging.h>
+
+#include "folly/Benchmark.h"
+#include "folly/experimental/exception_tracer/ExceptionTracer.h"
+
+void recurse(int level) {
+  if (level == 0) {
+    throw std::runtime_error("");
+  }
+  recurse(level - 1);
+  folly::doNotOptimizeAway(0);  // prevent tail recursion
+}
+
+void loop(int iters) {
+  for (int i = 0; i < iters * 100; ++i) {
+    try {
+      recurse(100);
+    } catch (const std::exception& e) {
+      folly::exception_tracer::getCurrentExceptions();
+    }
+  }
+}
+
+BENCHMARK(ExceptionTracer, iters) {
+  std::vector<std::thread> threads;
+  constexpr size_t kNumThreads = 10;
+  threads.resize(10);
+  for (auto& t : threads) {
+    t = std::thread([iters] () { loop(iters); });
+  }
+  for (auto& t : threads) {
+    t.join();
+  }
+}
+
+int main(int argc, char* argv[]) {
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  google::InitGoogleLogging(argv[0]);
+  folly::runBenchmarks();
+  return 0;
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/exception_tracer/ExceptionTracer.cpp
@@ -0,0 +1,207 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/experimental/exception_tracer/ExceptionTracer.h"
+
+#include <dlfcn.h>
+#include <exception>
+#include <iostream>
+#include <glog/logging.h>
+
+#include "folly/experimental/exception_tracer/ExceptionAbi.h"
+#include "folly/experimental/exception_tracer/StackTrace.h"
+#include "folly/experimental/symbolizer/Symbolizer.h"
+#include "folly/String.h"
+
+namespace {
+
+using namespace ::folly::exception_tracer;
+using namespace ::folly::symbolizer;
+using namespace __cxxabiv1;
+
+extern "C" {
+StackTraceStack* getExceptionStackTraceStack(void) __attribute__((weak));
+typedef StackTraceStack* (*GetExceptionStackTraceStackType)(void);
+GetExceptionStackTraceStackType getExceptionStackTraceStackFn;
+}
+
+}  // namespace
+
+namespace folly {
+namespace exception_tracer {
+
+std::ostream& operator<<(std::ostream& out, const ExceptionInfo& info) {
+  out << "Exception type: ";
+  if (info.type) {
+    out << folly::demangle(*info.type);
+  } else {
+    out << "(unknown type)";
+  }
+  out << " (" << info.frames.size()
+      << (info.frames.size() == 1 ? " frame" : " frames")
+      << ")\n";
+  try {
+    ssize_t frameCount = info.frames.size();
+    // Skip our own internal frames
+    static constexpr size_t skip = 3;
+
+    if (frameCount > skip) {
+      auto addresses = info.frames.data() + skip;
+      frameCount -= skip;
+
+      std::vector<SymbolizedFrame> frames;
+      frames.resize(frameCount);
+
+      Symbolizer symbolizer;
+      symbolizer.symbolize(addresses, frames.data(), frameCount);
+
+      OStreamSymbolizePrinter osp(out, SymbolizePrinter::COLOR_IF_TTY);
+      osp.println(addresses, frames.data(), frameCount);
+    }
+  } catch (const std::exception& e) {
+    out << "\n !! caught " << folly::exceptionStr(e) << "\n";
+  } catch (...) {
+    out << "\n !!! caught unexpected exception\n";
+  }
+  return out;
+}
+
+namespace {
+
+/**
+ * Is this a standard C++ ABI exception?
+ *
+ * Dependent exceptions (thrown via std::rethrow_exception) aren't --
+ * exc doesn't actually point to a __cxa_exception structure, but
+ * the offset of unwindHeader is correct, so exc->unwindHeader actually
+ * returns a _Unwind_Exception object.  Yeah, it's ugly like that.
+ */
+bool isAbiCppException(const __cxa_exception* exc) {
+  // The least significant four bytes must be "C++\0"
+  static const uint64_t cppClass =
+    ((uint64_t)'C' << 24) |
+    ((uint64_t)'+' << 16) |
+    ((uint64_t)'+' << 8);
+  return (exc->unwindHeader.exception_class & 0xffffffff) == cppClass;
+}
+
+}  // namespace
+
+std::vector<ExceptionInfo> getCurrentExceptions() {
+  struct Once {
+    Once() {
+      // See if linked in with us (getExceptionStackTraceStack is weak)
+      getExceptionStackTraceStackFn = getExceptionStackTraceStack;
+
+      if (!getExceptionStackTraceStackFn) {
+        // Nope, see if it's in a shared library
+        getExceptionStackTraceStackFn =
+          (GetExceptionStackTraceStackType)dlsym(
+              RTLD_NEXT, "getExceptionStackTraceStack");
+      }
+    }
+  };
+  static Once once;
+
+  std::vector<ExceptionInfo> exceptions;
+  auto currentException = __cxa_get_globals()->caughtExceptions;
+  if (!currentException) {
+    return exceptions;
+  }
+
+  StackTraceStack* traceStack = nullptr;
+  if (!getExceptionStackTraceStackFn) {
+    static bool logged = false;
+    if (!logged) {
+      LOG(WARNING)
+        << "Exception tracer library not linked, stack traces not available";
+      logged = true;
+    }
+  } else if ((traceStack = getExceptionStackTraceStackFn()) == nullptr) {
+    static bool logged = false;
+    if (!logged) {
+      LOG(WARNING)
+        << "Exception stack trace invalid, stack traces not available";
+      logged = true;
+    }
+  }
+
+  StackTrace* trace = traceStack ? traceStack->top() : nullptr;
+  while (currentException) {
+    ExceptionInfo info;
+    // Dependent exceptions (thrown via std::rethrow_exception) aren't
+    // standard ABI __cxa_exception objects, and are correctly labeled as
+    // such in the exception_class field.  We could try to extract the
+    // primary exception type in horribly hacky ways, but, for now, NULL.
+    info.type =
+      isAbiCppException(currentException) ?
+      currentException->exceptionType :
+      nullptr;
+    if (traceStack) {
+      CHECK(trace) << "Invalid trace stack!";
+      info.frames.assign(trace->addresses,
+                         trace->addresses + trace->frameCount);
+      trace = traceStack->next(trace);
+    }
+    currentException = currentException->nextException;
+    exceptions.push_back(std::move(info));
+  }
+  CHECK(!trace) << "Invalid trace stack!";
+
+  return exceptions;
+}
+
+namespace {
+
+std::terminate_handler origTerminate = abort;
+std::unexpected_handler origUnexpected = abort;
+
+void dumpExceptionStack(const char* prefix) {
+  auto exceptions = getCurrentExceptions();
+  if (exceptions.empty()) {
+    return;
+  }
+  LOG(ERROR) << prefix << ", exception stack follows";
+  for (auto& exc : exceptions) {
+    LOG(ERROR) << exc << "\n";
+  }
+  LOG(ERROR) << "exception stack complete";
+}
+
+void terminateHandler() {
+  dumpExceptionStack("terminate() called");
+  origTerminate();
+}
+
+void unexpectedHandler() {
+  dumpExceptionStack("Unexpected exception");
+  origUnexpected();
+}
+
+}  // namespace
+
+void installHandlers() {
+  struct Once {
+    Once() {
+      origTerminate = std::set_terminate(terminateHandler);
+      origUnexpected = std::set_unexpected(unexpectedHandler);
+    }
+  };
+  static Once once;
+}
+
+}  // namespace exception_tracer
+}  // namespace folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/exception_tracer/ExceptionTracer.h
@@ -0,0 +1,56 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+//
+// Exception tracer library.
+
+#ifndef FOLLY_EXPERIMENTAL_EXCEPTION_TRACER_EXCEPTIONTRACER_H_
+#define FOLLY_EXPERIMENTAL_EXCEPTION_TRACER_EXCEPTIONTRACER_H_
+
+#include <cstdint>
+#include <iosfwd>
+#include <typeinfo>
+#include <vector>
+
+namespace folly {
+namespace exception_tracer {
+
+struct ExceptionInfo {
+  const std::type_info* type;
+  // The values in frames are IP (instruction pointer) addresses.
+  // They are only filled if the low-level exception tracer library is
+  // linked in or LD_PRELOADed.
+  std::vector<uintptr_t> frames;  // front() is top of stack
+};
+
+std::ostream& operator<<(std::ostream& out, const ExceptionInfo& info);
+
+/**
+ * Get current exceptions being handled.  front() is the most recent exception.
+ * There should be at most one unless rethrowing.
+ */
+std::vector<ExceptionInfo> getCurrentExceptions();
+
+/**
+ * Install the terminate / unexpected handlers to dump exceptions.
+ */
+void installHandlers();
+
+}  // namespace exception_tracer
+}  // namespace folly
+
+#endif /* FOLLY_EXPERIMENTAL_EXCEPTION_TRACER_EXCEPTIONTRACER_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/exception_tracer/ExceptionTracerLib.cpp
@@ -0,0 +1,193 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <dlfcn.h>
+#include <pthread.h>
+#include <stdlib.h>
+
+#include <glog/logging.h>
+
+#include "folly/Portability.h"
+#include "folly/experimental/exception_tracer/StackTrace.h"
+#include "folly/experimental/exception_tracer/ExceptionAbi.h"
+#include "folly/experimental/exception_tracer/ExceptionTracer.h"
+#include "folly/experimental/symbolizer/Symbolizer.h"
+
+namespace __cxxabiv1 {
+
+extern "C" {
+void __cxa_throw(void* thrownException, std::type_info* type,
+                 void (*destructor)(void)) FOLLY_NORETURN;
+void* __cxa_begin_catch(void* excObj);
+void __cxa_rethrow(void) FOLLY_NORETURN;
+void __cxa_end_catch(void);
+}
+
+}  // namespace __cxxabiv1
+
+using namespace folly::exception_tracer;
+
+namespace {
+
+__thread bool invalid;
+__thread StackTraceStack activeExceptions;
+__thread StackTraceStack caughtExceptions;
+pthread_once_t initialized = PTHREAD_ONCE_INIT;
+
+extern "C" {
+typedef void (*CxaThrowType)(void*, std::type_info*, void (*)(void))
+  FOLLY_NORETURN;
+typedef void* (*CxaBeginCatchType)(void*);
+typedef void (*CxaRethrowType)(void)
+  FOLLY_NORETURN;
+typedef void (*CxaEndCatchType)(void);
+
+CxaThrowType orig_cxa_throw;
+CxaBeginCatchType orig_cxa_begin_catch;
+CxaRethrowType orig_cxa_rethrow;
+CxaEndCatchType orig_cxa_end_catch;
+}  // extern "C"
+
+typedef void (*RethrowExceptionType)(std::exception_ptr)
+  FOLLY_NORETURN;
+RethrowExceptionType orig_rethrow_exception;
+
+void initialize() {
+  orig_cxa_throw = (CxaThrowType)dlsym(RTLD_NEXT, "__cxa_throw");
+  orig_cxa_begin_catch =
+    (CxaBeginCatchType)dlsym(RTLD_NEXT, "__cxa_begin_catch");
+  orig_cxa_rethrow =
+    (CxaRethrowType)dlsym(RTLD_NEXT, "__cxa_rethrow");
+  orig_cxa_end_catch = (CxaEndCatchType)dlsym(RTLD_NEXT, "__cxa_end_catch");
+  // Mangled name for std::rethrow_exception
+  // TODO(tudorb): Dicey, as it relies on the fact that std::exception_ptr
+  // is typedef'ed to a type in namespace __exception_ptr
+  orig_rethrow_exception =
+    (RethrowExceptionType)dlsym(
+        RTLD_NEXT,
+        "_ZSt17rethrow_exceptionNSt15__exception_ptr13exception_ptrE");
+
+  if (!orig_cxa_throw || !orig_cxa_begin_catch || !orig_cxa_rethrow ||
+      !orig_cxa_end_catch || !orig_rethrow_exception) {
+    abort();  // what else can we do?
+  }
+}
+
+}  // namespace
+
+// This function is exported and may be found via dlsym(RTLD_NEXT, ...)
+extern "C" StackTraceStack* getExceptionStackTraceStack() {
+  return invalid ? nullptr : &caughtExceptions;
+}
+
+namespace {
+
+// Make sure we're counting stack frames correctly, don't inline.
+void addActiveException() __attribute__((noinline));
+
+void addActiveException() {
+  pthread_once(&initialized, initialize);
+  // Capture stack trace
+  if (!invalid) {
+    if (!activeExceptions.pushCurrent()) {
+      activeExceptions.clear();
+      caughtExceptions.clear();
+      invalid = true;
+    }
+  }
+}
+
+void moveTopException(StackTraceStack& from, StackTraceStack& to) {
+  if (invalid) {
+    return;
+  }
+  if (!to.moveTopFrom(from)) {
+    from.clear();
+    to.clear();
+    invalid = true;
+  }
+}
+
+}  // namespace
+
+namespace __cxxabiv1 {
+
+void __cxa_throw(void* thrownException, std::type_info* type,
+                 void (*destructor)(void)) {
+  addActiveException();
+  orig_cxa_throw(thrownException, type, destructor);
+}
+
+void __cxa_rethrow() {
+  // __cxa_rethrow leaves the current exception on the caught stack,
+  // and __cxa_begin_catch recognizes that case.  We could do the same, but
+  // we'll implement something simpler (and slower): we pop the exception from
+  // the caught stack, and push it back onto the active stack; this way, our
+  // implementation of __cxa_begin_catch doesn't have to do anything special.
+  moveTopException(caughtExceptions, activeExceptions);
+  orig_cxa_rethrow();
+}
+
+void* __cxa_begin_catch(void *excObj) {
+  // excObj is a pointer to the unwindHeader in __cxa_exception
+  moveTopException(activeExceptions, caughtExceptions);
+  return orig_cxa_begin_catch(excObj);
+}
+
+void __cxa_end_catch() {
+  if (!invalid) {
+    __cxa_exception* top = __cxa_get_globals_fast()->caughtExceptions;
+    // This is gcc specific and not specified in the ABI:
+    // abs(handlerCount) is the number of active handlers, it's negative
+    // for rethrown exceptions and positive (always 1) for regular exceptions.
+    // In the rethrow case, we've already popped the exception off the
+    // caught stack, so we don't do anything here.
+    if (top->handlerCount == 1) {
+      if (!caughtExceptions.pop()) {
+        activeExceptions.clear();
+        invalid = true;
+      }
+    }
+  }
+  orig_cxa_end_catch();
+}
+
+}  // namespace __cxxabiv1
+
+namespace std {
+
+void rethrow_exception(std::exception_ptr ep) {
+  addActiveException();
+  orig_rethrow_exception(ep);
+}
+
+}  // namespace std
+
+
+namespace {
+
+struct Initializer {
+  Initializer() {
+    try {
+      ::folly::exception_tracer::installHandlers();
+    } catch (...) {
+    }
+  }
+};
+
+Initializer initializer;
+
+}  // namespace
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/exception_tracer/ExceptionTracerTest.cpp
@@ -0,0 +1,99 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+#include <iostream>
+#include <stdexcept>
+
+#include "folly/experimental/exception_tracer/ExceptionTracer.h"
+
+void bar() {
+  throw std::runtime_error("hello");
+}
+
+void dumpExceptions(const char* prefix) {
+  std::cerr << "--- " << prefix << "\n";
+  auto exceptions = ::folly::exception_tracer::getCurrentExceptions();
+  for (auto& exc : exceptions) {
+    std::cerr << exc << "\n";
+  }
+}
+
+void foo() {
+  try {
+    try {
+      bar();
+    } catch (const std::exception& e) {
+      dumpExceptions("foo: simple catch");
+      bar();
+    }
+  } catch (const std::exception& e) {
+    dumpExceptions("foo: catch, exception thrown from previous catch block");
+  }
+}
+
+void baz() {
+  try {
+    try {
+      bar();
+    } catch (...) {
+      dumpExceptions("baz: simple catch");
+      throw;
+    }
+  } catch (const std::exception& e) {
+    dumpExceptions("baz: catch rethrown exception");
+    throw "hello";
+  }
+}
+
+void testExceptionPtr1() {
+  std::exception_ptr exc;
+  try {
+    bar();
+  } catch (...) {
+    exc = std::current_exception();
+  }
+
+  try {
+    std::rethrow_exception(exc);
+  } catch (...) {
+    dumpExceptions("std::rethrow_exception 1");
+  }
+}
+
+void testExceptionPtr2() {
+  std::exception_ptr exc;
+  try {
+    throw std::out_of_range("x");
+  } catch (...) {
+    exc = std::current_exception();
+  }
+
+  try {
+    std::rethrow_exception(exc);
+  } catch (...) {
+    dumpExceptions("std::rethrow_exception 2");
+  }
+}
+
+int main(int argc, char *argv[]) {
+  foo();
+  testExceptionPtr1();
+  testExceptionPtr2();
+  baz();
+  return 0;
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/exception_tracer/README
@@ -0,0 +1,23 @@
+Exception tracer library
+
+This library allows you to inspect the exception stack at runtime.
+The library can be used in three ways:
+
+1. Link in the exception_tracer_base library.  You get access to the functions
+in ExceptionTracer.h, but no stack traces.  This has no runtime overhead,
+and is compliant with the C++ ABI.
+
+2. Link in the (full) exception_tracer library.  You get access to the
+functions in ExceptionTracer.h, the std::terminate and std::unexpected
+handlers are installed by default, and you get full stack traces with
+all exceptions.  This has some runtime overhead (the stack trace must be
+captured and stored whenever an exception is thrown) added to throw
+and catch, but no runtime overhead otherwise.  This is less portable
+(depends on internal details of GNU's libstdc++).
+
+3. LD_PRELOAD libexceptiontracer.so.  This is equivalent to #2 above, but
+requires no link-time changes.  On the other hand, you need to ensure that
+libexceptiontracer.so is compiled with the same compiler and flags as
+your binary, and the usual caveats about LD_PRELOAD apply (it propagates
+to child processes, etc).
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/exception_tracer/StackTrace.cpp
@@ -0,0 +1,111 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/experimental/exception_tracer/StackTrace.h"
+
+#include <cassert>
+#include <cstdlib>
+#include <new>
+#include "folly/experimental/symbolizer/StackTrace.h"
+
+namespace folly { namespace exception_tracer {
+
+class StackTraceStack::Node : public StackTrace {
+ public:
+  static Node* allocate();
+  void deallocate();
+
+  Node* next;
+
+ private:
+  Node() : next(nullptr) { }
+  ~Node() { }
+};
+
+auto StackTraceStack::Node::allocate() -> Node* {
+  // Null pointer on error, please.
+  return new (std::nothrow) Node();
+}
+
+void StackTraceStack::Node::deallocate() {
+  delete this;
+}
+
+bool StackTraceStack::pushCurrent() {
+  checkGuard();
+  auto node = Node::allocate();
+  if (!node) {
+    // cannot allocate memory
+    return false;
+  }
+
+  ssize_t n = folly::symbolizer::getStackTrace(node->addresses, kMaxFrames);
+  if (n == -1) {
+    node->deallocate();
+    return false;
+  }
+  node->frameCount = n;
+
+  node->next = top_;
+  top_ = node;
+  return true;
+}
+
+bool StackTraceStack::pop() {
+  checkGuard();
+  if (!top_) {
+    return false;
+  }
+
+  auto node = top_;
+  top_ = node->next;
+  node->deallocate();
+  return true;
+}
+
+bool StackTraceStack::moveTopFrom(StackTraceStack& other) {
+  checkGuard();
+  if (!other.top_) {
+    return false;
+  }
+
+  auto node = other.top_;
+  other.top_ = node->next;
+  node->next = top_;
+  top_ = node;
+  return true;
+}
+
+void StackTraceStack::clear() {
+  checkGuard();
+  while (top_) {
+    pop();
+  }
+}
+
+StackTrace* StackTraceStack::top() {
+  checkGuard();
+  return top_;
+}
+
+StackTrace* StackTraceStack::next(StackTrace* p) {
+  checkGuard();
+  assert(p);
+  return static_cast<Node*>(p)->next;
+}
+
+}}  // namespaces
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/exception_tracer/StackTrace.h
@@ -0,0 +1,103 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+#ifndef FOLLY_EXPERIMENTAL_EXCEPTION_TRACER_STACKTRACE_H_
+#define FOLLY_EXPERIMENTAL_EXCEPTION_TRACER_STACKTRACE_H_
+
+#include <cassert>
+#include <cstddef>
+#include <cstdint>
+
+namespace folly { namespace exception_tracer {
+
+constexpr size_t kMaxFrames = 500;
+
+struct StackTrace {
+  StackTrace() : frameCount(0) { }
+
+  size_t frameCount;
+  uintptr_t addresses[kMaxFrames];
+};
+
+// note: no constructor so this can be __thread.
+// A StackTraceStack MUST be placed in zero-initialized memory.
+class StackTraceStack {
+  class Node;
+ public:
+  /**
+   * Push the current stack trace onto the stack.
+   * Returns false on failure (not enough memory, getting stack trace failed),
+   * true on success.
+   */
+  bool pushCurrent();
+
+  /**
+   * Pop the top stack trace from the stack.
+   * Returns true on success, false on failure (stack was empty).
+   */
+  bool pop();
+
+  /**
+   * Move the top stack trace from other onto this.
+   * Returns true on success, false on failure (other was empty).
+   */
+  bool moveTopFrom(StackTraceStack& other);
+
+  /**
+   * Clear the stack.
+   */
+
+  void clear();
+
+  /**
+   * Is the stack empty?
+   */
+  bool empty() const { return !top_; }
+
+  /**
+   * Return the top stack trace, or nullptr if the stack is empty.
+   */
+  StackTrace* top();
+
+  /**
+   * Return the stack trace following p, or nullptr if p is the bottom of
+   * the stack.
+   */
+  StackTrace* next(StackTrace* p);
+
+ private:
+  // In debug mode, we assert that we're in zero-initialized memory by
+  // checking that the two guards around top_ are zero.
+  void checkGuard() const {
+#ifndef NDEBUG
+    assert(guard1_ == 0 && guard2_ == 0);
+#endif
+  }
+
+#ifndef NDEBUG
+  uintptr_t guard1_;
+#endif
+  Node* top_;
+#ifndef NDEBUG
+  uintptr_t guard2_;
+#endif
+};
+
+}}  // namespaces
+
+#endif /* FOLLY_EXPERIMENTAL_EXCEPTION_TRACER_STACKTRACE_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/io/AsyncIO.cpp
@@ -0,0 +1,368 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/experimental/io/AsyncIO.h"
+
+#include <sys/eventfd.h>
+#include <unistd.h>
+#include <cerrno>
+#include <stdexcept>
+#include <string>
+#include <fstream>
+
+#include <boost/intrusive/parent_from_member.hpp>
+#include <glog/logging.h>
+
+#include "folly/Exception.h"
+#include "folly/Format.h"
+#include "folly/Likely.h"
+#include "folly/String.h"
+
+namespace folly {
+
+AsyncIOOp::AsyncIOOp(NotificationCallback cb)
+  : cb_(std::move(cb)),
+    state_(State::UNINITIALIZED),
+    result_(-EINVAL) {
+  memset(&iocb_, 0, sizeof(iocb_));
+}
+
+void AsyncIOOp::reset(NotificationCallback cb) {
+  CHECK_NE(state_, State::PENDING);
+  cb_ = std::move(cb);
+  state_ = State::UNINITIALIZED;
+  result_ = -EINVAL;
+  memset(&iocb_, 0, sizeof(iocb_));
+}
+
+AsyncIOOp::~AsyncIOOp() {
+  CHECK_NE(state_, State::PENDING);
+}
+
+void AsyncIOOp::start() {
+  DCHECK_EQ(state_, State::INITIALIZED);
+  state_ = State::PENDING;
+}
+
+void AsyncIOOp::complete(ssize_t result) {
+  DCHECK_EQ(state_, State::PENDING);
+  state_ = State::COMPLETED;
+  result_ = result;
+  if (cb_) {
+    cb_(this);
+  }
+}
+
+ssize_t AsyncIOOp::result() const {
+  CHECK_EQ(state_, State::COMPLETED);
+  return result_;
+}
+
+void AsyncIOOp::pread(int fd, void* buf, size_t size, off_t start) {
+  init();
+  io_prep_pread(&iocb_, fd, buf, size, start);
+}
+
+void AsyncIOOp::pread(int fd, Range<unsigned char*> range, off_t start) {
+  pread(fd, range.begin(), range.size(), start);
+}
+
+void AsyncIOOp::preadv(int fd, const iovec* iov, int iovcnt, off_t start) {
+  init();
+  io_prep_preadv(&iocb_, fd, iov, iovcnt, start);
+}
+
+void AsyncIOOp::pwrite(int fd, const void* buf, size_t size, off_t start) {
+  init();
+  io_prep_pwrite(&iocb_, fd, const_cast<void*>(buf), size, start);
+}
+
+void AsyncIOOp::pwrite(int fd, Range<const unsigned char*> range, off_t start) {
+  pwrite(fd, range.begin(), range.size(), start);
+}
+
+void AsyncIOOp::pwritev(int fd, const iovec* iov, int iovcnt, off_t start) {
+  init();
+  io_prep_pwritev(&iocb_, fd, iov, iovcnt, start);
+}
+
+void AsyncIOOp::init() {
+  CHECK_EQ(state_, State::UNINITIALIZED);
+  state_ = State::INITIALIZED;
+}
+
+AsyncIO::AsyncIO(size_t capacity, PollMode pollMode)
+  : ctx_(0),
+    ctxSet_(false),
+    pending_(0),
+    capacity_(capacity),
+    pollFd_(-1) {
+  CHECK_GT(capacity_, 0);
+  completed_.reserve(capacity_);
+  if (pollMode == POLLABLE) {
+    pollFd_ = eventfd(0, EFD_NONBLOCK);
+    checkUnixError(pollFd_, "AsyncIO: eventfd creation failed");
+  }
+}
+
+AsyncIO::~AsyncIO() {
+  CHECK_EQ(pending_, 0);
+  if (ctx_) {
+    int rc = io_queue_release(ctx_);
+    CHECK_EQ(rc, 0) << "io_queue_release: " << errnoStr(-rc);
+  }
+  if (pollFd_ != -1) {
+    CHECK_ERR(close(pollFd_));
+  }
+}
+
+void AsyncIO::decrementPending() {
+  ssize_t p = pending_.fetch_add(-1, std::memory_order_acq_rel);
+  DCHECK_GE(p, 1);
+}
+
+void AsyncIO::initializeContext() {
+  if (!ctxSet_.load(std::memory_order_acquire)) {
+    std::lock_guard<std::mutex> lock(initMutex_);
+    if (!ctxSet_.load(std::memory_order_relaxed)) {
+      int rc = io_queue_init(capacity_, &ctx_);
+      // returns negative errno
+      if (rc == -EAGAIN) {
+        long aio_nr, aio_max;
+        std::unique_ptr<FILE, int(*)(FILE*)>
+          fp(fopen("/proc/sys/fs/aio-nr", "r"), fclose);
+        PCHECK(fp);
+        CHECK_EQ(fscanf(fp.get(), "%ld", &aio_nr), 1);
+
+        std::unique_ptr<FILE, int(*)(FILE*)>
+          aio_max_fp(fopen("/proc/sys/fs/aio-max-nr", "r"), fclose);
+        PCHECK(aio_max_fp);
+        CHECK_EQ(fscanf(aio_max_fp.get(), "%ld", &aio_max), 1);
+
+        LOG(ERROR) << "No resources for requested capacity of " << capacity_;
+        LOG(ERROR) << "aio_nr " << aio_nr << ", aio_max_nr " << aio_max;
+      }
+
+      checkKernelError(rc, "AsyncIO: io_queue_init failed");
+      DCHECK(ctx_);
+      ctxSet_.store(true, std::memory_order_release);
+    }
+  }
+}
+
+void AsyncIO::submit(Op* op) {
+  CHECK_EQ(op->state(), Op::State::INITIALIZED);
+  initializeContext();  // on demand
+
+  // We can increment past capacity, but we'll clean up after ourselves.
+  ssize_t p = pending_.fetch_add(1, std::memory_order_acq_rel);
+  if (p >= capacity_) {
+    decrementPending();
+    throw std::range_error("AsyncIO: too many pending requests");
+  }
+  iocb* cb = &op->iocb_;
+  cb->data = nullptr;  // unused
+  if (pollFd_ != -1) {
+    io_set_eventfd(cb, pollFd_);
+  }
+  int rc = io_submit(ctx_, 1, &cb);
+  if (rc < 0) {
+    decrementPending();
+    throwSystemErrorExplicit(-rc, "AsyncIO: io_submit failed");
+  }
+  DCHECK_EQ(rc, 1);
+  op->start();
+}
+
+Range<AsyncIO::Op**> AsyncIO::wait(size_t minRequests) {
+  CHECK(ctx_);
+  CHECK_EQ(pollFd_, -1) << "wait() only allowed on non-pollable object";
+  ssize_t p = pending_.load(std::memory_order_acquire);
+  CHECK_LE(minRequests, p);
+  return doWait(minRequests, p);
+}
+
+Range<AsyncIO::Op**> AsyncIO::pollCompleted() {
+  CHECK(ctx_);
+  CHECK_NE(pollFd_, -1) << "pollCompleted() only allowed on pollable object";
+  uint64_t numEvents;
+  // This sets the eventFd counter to 0, see
+  // http://www.kernel.org/doc/man-pages/online/pages/man2/eventfd.2.html
+  ssize_t rc;
+  do {
+    rc = ::read(pollFd_, &numEvents, 8);
+  } while (rc == -1 && errno == EINTR);
+  if (UNLIKELY(rc == -1 && errno == EAGAIN)) {
+    return Range<Op**>();  // nothing completed
+  }
+  checkUnixError(rc, "AsyncIO: read from event fd failed");
+  DCHECK_EQ(rc, 8);
+
+  DCHECK_GT(numEvents, 0);
+  DCHECK_LE(numEvents, pending_);
+
+  // Don't reap more than numEvents, as we've just reset the counter to 0.
+  return doWait(numEvents, numEvents);
+}
+
+Range<AsyncIO::Op**> AsyncIO::doWait(size_t minRequests, size_t maxRequests) {
+  io_event events[maxRequests];
+  int count;
+  do {
+    // Wait forever
+    count = io_getevents(ctx_, minRequests, maxRequests, events, nullptr);
+  } while (count == -EINTR);
+  checkKernelError(count, "AsyncIO: io_getevents failed");
+  DCHECK_GE(count, minRequests);  // the man page says so
+  DCHECK_LE(count, maxRequests);
+
+  completed_.clear();
+  if (count == 0) {
+    return folly::Range<Op**>();
+  }
+
+  for (size_t i = 0; i < count; ++i) {
+    DCHECK(events[i].obj);
+    Op* op = boost::intrusive::get_parent_from_member(
+        events[i].obj, &AsyncIOOp::iocb_);
+    decrementPending();
+    op->complete(events[i].res);
+    completed_.push_back(op);
+  }
+
+  return folly::Range<Op**>(&completed_.front(), count);
+}
+
+AsyncIOQueue::AsyncIOQueue(AsyncIO* asyncIO)
+  : asyncIO_(asyncIO) {
+}
+
+AsyncIOQueue::~AsyncIOQueue() {
+  CHECK_EQ(asyncIO_->pending(), 0);
+}
+
+void AsyncIOQueue::submit(AsyncIOOp* op) {
+  submit([op]() { return op; });
+}
+
+void AsyncIOQueue::submit(OpFactory op) {
+  queue_.push_back(op);
+  maybeDequeue();
+}
+
+void AsyncIOQueue::onCompleted(AsyncIOOp* op) {
+  maybeDequeue();
+}
+
+void AsyncIOQueue::maybeDequeue() {
+  while (!queue_.empty() && asyncIO_->pending() < asyncIO_->capacity()) {
+    auto& opFactory = queue_.front();
+    auto op = opFactory();
+    queue_.pop_front();
+
+    // Interpose our completion callback
+    auto& nextCb = op->notificationCallback();
+    op->setNotificationCallback([this, nextCb](AsyncIOOp* op) {
+      this->onCompleted(op);
+      if (nextCb) nextCb(op);
+    });
+
+    asyncIO_->submit(op);
+  }
+}
+
+// debugging helpers:
+
+namespace {
+
+#define X(c) case c: return #c
+
+const char* asyncIoOpStateToString(AsyncIOOp::State state) {
+  switch (state) {
+    X(AsyncIOOp::State::UNINITIALIZED);
+    X(AsyncIOOp::State::INITIALIZED);
+    X(AsyncIOOp::State::PENDING);
+    X(AsyncIOOp::State::COMPLETED);
+  }
+  return "<INVALID AsyncIOOp::State>";
+}
+
+const char* iocbCmdToString(short int cmd_short) {
+  io_iocb_cmd cmd = static_cast<io_iocb_cmd>(cmd_short);
+  switch (cmd) {
+    X(IO_CMD_PREAD);
+    X(IO_CMD_PWRITE);
+    X(IO_CMD_FSYNC);
+    X(IO_CMD_FDSYNC);
+    X(IO_CMD_POLL);
+    X(IO_CMD_NOOP);
+    X(IO_CMD_PREADV);
+    X(IO_CMD_PWRITEV);
+  };
+  return "<INVALID io_iocb_cmd>";
+}
+
+#undef X
+
+std::string fd2name(int fd) {
+  std::string path = folly::to<std::string>("/proc/self/fd/", fd);
+  char link[PATH_MAX];
+  const ssize_t length =
+    std::max<ssize_t>(readlink(path.c_str(), link, PATH_MAX), 0);
+  return path.assign(link, length);
+}
+
+std::ostream& operator<<(std::ostream& os, const iocb& cb) {
+  os << folly::format(
+    "data={}, key={}, opcode={}, reqprio={}, fd={}, f={}, ",
+    cb.data, cb.key, iocbCmdToString(cb.aio_lio_opcode),
+    cb.aio_reqprio, cb.aio_fildes, fd2name(cb.aio_fildes));
+
+  switch (cb.aio_lio_opcode) {
+    case IO_CMD_PREAD:
+    case IO_CMD_PWRITE:
+      os << folly::format("buf={}, off={}, size={}, ",
+                          cb.u.c.buf, cb.u.c.nbytes, cb.u.c.offset);
+    default:
+      os << "[TODO: write debug string for "
+         << iocbCmdToString(cb.aio_lio_opcode) << "] ";
+  }
+
+  return os;
+}
+
+}  // anonymous namespace
+
+std::ostream& operator<<(std::ostream& os, const AsyncIOOp& op) {
+  os << "{" << op.state_ << ", ";
+
+  if (op.state_ != AsyncIOOp::State::UNINITIALIZED) {
+    os << op.iocb_;
+  }
+
+  if (op.state_ == AsyncIOOp::State::COMPLETED) {
+    os << "result=" << op.result_ << ", ";
+  }
+
+  return os << "}";
+}
+
+std::ostream& operator<<(std::ostream& os, AsyncIOOp::State state) {
+  return os << asyncIoOpStateToString(state);
+}
+
+}  // namespace folly
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/io/AsyncIO.h
@@ -0,0 +1,248 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_IO_ASYNCIO_H_
+#define FOLLY_IO_ASYNCIO_H_
+
+#include <sys/types.h>
+#include <sys/uio.h>
+#include <libaio.h>
+
+#include <atomic>
+#include <cstdint>
+#include <deque>
+#include <functional>
+#include <mutex>
+#include <ostream>
+#include <utility>
+#include <vector>
+
+#include <boost/noncopyable.hpp>
+
+#include "folly/Portability.h"
+#include "folly/Range.h"
+
+namespace folly {
+
+/**
+ * An AsyncIOOp represents a pending operation.  You may set a notification
+ * callback or you may use this class's methods directly.
+ *
+ * The op must remain allocated until completion.
+ */
+class AsyncIOOp : private boost::noncopyable {
+  friend class AsyncIO;
+  friend std::ostream& operator<<(std::ostream& stream, const AsyncIOOp& o);
+ public:
+  typedef std::function<void(AsyncIOOp*)> NotificationCallback;
+
+  explicit AsyncIOOp(NotificationCallback cb = NotificationCallback());
+  ~AsyncIOOp();
+
+  // There would be a cancel() method here if Linux AIO actually implemented
+  // it.  But let's not get your hopes up.
+
+  enum class State {
+    UNINITIALIZED,
+    INITIALIZED,
+    PENDING,
+    COMPLETED
+  };
+
+  /**
+   * Initiate a read request.
+   */
+  void pread(int fd, void* buf, size_t size, off_t start);
+  void pread(int fd, Range<unsigned char*> range, off_t start);
+  void preadv(int fd, const iovec* iov, int iovcnt, off_t start);
+
+  /**
+   * Initiate a write request.
+   */
+  void pwrite(int fd, const void* buf, size_t size, off_t start);
+  void pwrite(int fd, Range<const unsigned char*> range, off_t start);
+  void pwritev(int fd, const iovec* iov, int iovcnt, off_t start);
+
+  /**
+   * Return the current operation state.
+   */
+  State state() const { return state_; }
+
+  /**
+   * Reset the operation for reuse.  It is an error to call reset() on
+   * an Op that is still pending.
+   */
+  void reset(NotificationCallback cb = NotificationCallback());
+
+  void setNotificationCallback(NotificationCallback cb) { cb_ = std::move(cb); }
+  const NotificationCallback& notificationCallback() const { return cb_; }
+
+  /**
+   * Retrieve the result of this operation.  Returns >=0 on success,
+   * -errno on failure (that is, using the Linux kernel error reporting
+   * conventions).  Use checkKernelError (folly/Exception.h) on the result to
+   * throw a std::system_error in case of error instead.
+   *
+   * It is an error to call this if the Op hasn't yet started or is still
+   * pending.
+   */
+  ssize_t result() const;
+
+ private:
+  void init();
+  void start();
+  void complete(ssize_t result);
+
+  NotificationCallback cb_;
+  iocb iocb_;
+  State state_;
+  ssize_t result_;
+};
+
+std::ostream& operator<<(std::ostream& stream, const AsyncIOOp& o);
+std::ostream& operator<<(std::ostream& stream, AsyncIOOp::State state);
+
+/**
+ * C++ interface around Linux Async IO.
+ */
+class AsyncIO : private boost::noncopyable {
+ public:
+  typedef AsyncIOOp Op;
+
+  enum PollMode {
+    NOT_POLLABLE,
+    POLLABLE
+  };
+
+  /**
+   * Create an AsyncIO context capable of holding at most 'capacity' pending
+   * requests at the same time.  As requests complete, others can be scheduled,
+   * as long as this limit is not exceeded.
+   *
+   * Note: the maximum number of allowed concurrent requests is controlled
+   * by the fs.aio-max-nr sysctl, the default value is usually 64K.
+   *
+   * If pollMode is POLLABLE, pollFd() will return a file descriptor that
+   * can be passed to poll / epoll / select and will become readable when
+   * any IOs on this AsyncIO have completed.  If you do this, you must use
+   * pollCompleted() instead of wait() -- do not read from the pollFd()
+   * file descriptor directly.
+   *
+   * You may use the same AsyncIO object from multiple threads, as long as
+   * there is only one concurrent caller of wait() / pollCompleted() (perhaps
+   * by always calling it from the same thread, or by providing appropriate
+   * mutual exclusion)  In this case, pending() returns a snapshot
+   * of the current number of pending requests.
+   */
+  explicit AsyncIO(size_t capacity, PollMode pollMode=NOT_POLLABLE);
+  ~AsyncIO();
+
+  /**
+   * Wait for at least minRequests to complete.  Returns the requests that
+   * have completed; the returned range is valid until the next call to
+   * wait().  minRequests may be 0 to not block.
+   */
+  Range<Op**> wait(size_t minRequests);
+
+  /**
+   * Return the number of pending requests.
+   */
+  size_t pending() const { return pending_; }
+
+  /**
+   * Return the maximum number of requests that can be kept outstanding
+   * at any one time.
+   */
+  size_t capacity() const { return capacity_; }
+
+  /**
+   * If POLLABLE, return a file descriptor that can be passed to poll / epoll
+   * and will become readable when any async IO operations have completed.
+   * If NOT_POLLABLE, return -1.
+   */
+  int pollFd() const { return pollFd_; }
+
+  /**
+   * If POLLABLE, call instead of wait after the file descriptor returned
+   * by pollFd() became readable.  The returned range is valid until the next
+   * call to pollCompleted().
+   */
+  Range<Op**> pollCompleted();
+
+  /**
+   * Submit an op for execution.
+   */
+  void submit(Op* op);
+
+ private:
+  void decrementPending();
+  void initializeContext();
+
+  Range<Op**> doWait(size_t minRequests, size_t maxRequests);
+
+  io_context_t ctx_;
+  std::atomic<bool> ctxSet_;
+  std::mutex initMutex_;
+
+  std::atomic<ssize_t> pending_;
+  const ssize_t capacity_;
+  int pollFd_;
+  std::vector<Op*> completed_;
+};
+
+/**
+ * Wrapper around AsyncIO that allows you to schedule more requests than
+ * the AsyncIO's object capacity.  Other requests are queued and processed
+ * in a FIFO order.
+ */
+class AsyncIOQueue {
+ public:
+  /**
+   * Create a queue, using the given AsyncIO object.
+   * The AsyncIO object may not be used by anything else until the
+   * queue is destroyed.
+   */
+  explicit AsyncIOQueue(AsyncIO* asyncIO);
+  ~AsyncIOQueue();
+
+  size_t queued() const { return queue_.size(); }
+
+  /**
+   * Submit an op to the AsyncIO queue.  The op will be queued until
+   * the AsyncIO object has room.
+   */
+  void submit(AsyncIOOp* op);
+
+  /**
+   * Submit a delayed op to the AsyncIO queue; this allows you to postpone
+   * creation of the Op (which may require allocating memory, etc) until
+   * the AsyncIO object has room.
+   */
+  typedef std::function<AsyncIOOp*()> OpFactory;
+  void submit(OpFactory op);
+ private:
+  void onCompleted(AsyncIOOp* op);
+  void maybeDequeue();
+
+  AsyncIO* asyncIO_;
+
+  std::deque<OpFactory> queue_;
+};
+
+}  // namespace folly
+
+#endif /* FOLLY_IO_ASYNCIO_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/io/FsUtil.cpp
@@ -0,0 +1,71 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/experimental/io/FsUtil.h"
+
+namespace bsys = ::boost::system;
+
+namespace folly {
+namespace fs {
+
+namespace {
+bool skipPrefix(const path& pth, const path& prefix, path::const_iterator& it) {
+  it = pth.begin();
+  for (auto& p : prefix) {
+    if (it == pth.end()) {
+      return false;
+    }
+    if (p == ".") {
+      // Should only occur at the end, if prefix ends with a slash
+      continue;
+    }
+    if (*it++ != p) {
+      return false;
+    }
+  }
+  return true;
+}
+}  // namespace
+
+bool starts_with(const path& pth, const path& prefix) {
+  path::const_iterator it;
+  return skipPrefix(pth, prefix, it);
+}
+
+path remove_prefix(const path& pth, const path& prefix) {
+  path::const_iterator it;
+  if (!skipPrefix(pth, prefix, it)) {
+    throw filesystem_error(
+        "Path does not start with prefix",
+        pth, prefix,
+        bsys::errc::make_error_code(bsys::errc::invalid_argument));
+  }
+
+  path p;
+  for (; it != pth.end(); ++it) {
+    p /= *it;
+  }
+
+  return p;
+}
+
+path canonical_parent(const path& pth, const path& base) {
+  return canonical(pth.parent_path(), base) / pth.filename();
+}
+
+}  // namespace fs
+}  // namespace folly
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/io/FsUtil.h
@@ -0,0 +1,59 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_IO_FSUTIL_H_
+#define FOLLY_IO_FSUTIL_H_
+
+#include <boost/filesystem.hpp>
+
+namespace folly {
+namespace fs {
+
+// Functions defined in this file are meant to extend the
+// boost::filesystem library; functions will be named according to boost's
+// naming conventions instead of ours.  For convenience, import the
+// boost::filesystem namespace into folly::fs.
+using namespace ::boost::filesystem;
+
+/**
+ * Check whether "path" starts with "prefix".
+ * That is, if prefix has n path elements, then the first n elements of
+ * path must be the same as prefix.
+ *
+ * There is a special case if prefix ends with a slash:
+ * /foo/bar/ is not a prefix of /foo/bar, but both /foo/bar and /foo/bar/
+ * are prefixes of /foo/bar/baz.
+ */
+bool starts_with(const path& p, const path& prefix);
+
+/**
+ * If "path" starts with "prefix", return "path" with "prefix" removed.
+ * Otherwise, throw filesystem_error.
+ */
+path remove_prefix(const path& p, const path& prefix);
+
+/**
+ * Canonicalize the parent path, leaving the filename (last component)
+ * unchanged.  You may use this before creating a file instead of
+ * boost::filesystem::canonical, which requires that the entire path exists.
+ */
+path canonical_parent(const path& p, const path& basePath = current_path());
+
+}  // namespace fs
+}  // namespace folly
+
+#endif /* FOLLY_IO_FSUTIL_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/io/HugePages.cpp
@@ -0,0 +1,362 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/experimental/io/HugePages.h"
+
+#include <sys/mman.h>
+#include <sys/stat.h>
+#include <sys/types.h>
+#include <fcntl.h>
+
+#include <cctype>
+#include <cstring>
+
+#include <algorithm>
+#include <stdexcept>
+#include <system_error>
+
+#include <boost/noncopyable.hpp>
+#include <boost/regex.hpp>
+
+#include <glog/logging.h>
+
+#include "folly/Conv.h"
+#include "folly/Format.h"
+#include "folly/Range.h"
+#include "folly/ScopeGuard.h"
+#include "folly/String.h"
+
+#include "folly/gen/Base.h"
+#include "folly/gen/File.h"
+#include "folly/gen/String.h"
+
+#ifndef MAP_POPULATE
+#define MAP_POPULATE 0
+#endif
+
+namespace folly {
+
+namespace {
+
+// Get the default huge page size
+size_t getDefaultHugePageSize() {
+  // We need to parse /proc/meminfo
+  static const boost::regex regex(R"!(Hugepagesize:\s*(\d+)\s*kB)!");
+  size_t pageSize = 0;
+  boost::cmatch match;
+
+  bool error = gen::byLine("/proc/meminfo") |
+    [&] (StringPiece line) -> bool {
+      if (boost::regex_match(line.begin(), line.end(), match, regex)) {
+        StringPiece numStr(line.begin() + match.position(1), match.length(1));
+        pageSize = to<size_t>(numStr) * 1024;  // in KiB
+        return false;  // stop
+      }
+      return true;
+    };
+
+  if (error) {
+    throw std::runtime_error("Can't find default huge page size");
+  }
+  return pageSize;
+}
+
+// Get raw huge page sizes (without mount points, they'll be filled later)
+HugePageSizeVec getRawHugePageSizes() {
+  // We need to parse file names from /sys/kernel/mm/hugepages
+  static const boost::regex regex(R"!(hugepages-(\d+)kB)!");
+  boost::smatch match;
+  HugePageSizeVec vec;
+  fs::path path("/sys/kernel/mm/hugepages");
+  for (fs::directory_iterator it(path); it != fs::directory_iterator(); ++it) {
+    std::string filename(it->path().filename().native());
+    if (boost::regex_match(filename, match, regex)) {
+      StringPiece numStr(filename.data() + match.position(1), match.length(1));
+      vec.emplace_back(to<size_t>(numStr) * 1024);
+    }
+  }
+  return vec;
+}
+
+// Parse the value of a pagesize mount option
+// Format: number, optional K/M/G/T suffix, trailing junk allowed
+size_t parsePageSizeValue(StringPiece value) {
+  static const boost::regex regex(R"!((\d+)([kmgt])?.*)!", boost::regex::icase);
+  boost::cmatch match;
+  if (!boost::regex_match(value.begin(), value.end(), match, regex)) {
+    throw std::runtime_error("Invalid pagesize option");
+  }
+  char c = '\0';
+  if (match.length(2) != 0) {
+    c = tolower(value[match.position(2)]);
+  }
+  StringPiece numStr(value.data() + match.position(1), match.length(1));
+  size_t size = to<size_t>(numStr);
+  switch (c) {
+  case 't': size *= 1024;
+  case 'g': size *= 1024;
+  case 'm': size *= 1024;
+  case 'k': size *= 1024;
+  }
+  return size;
+}
+
+/**
+ * Get list of supported huge page sizes and their mount points, if
+ * hugetlbfs file systems are mounted for those sizes.
+ */
+HugePageSizeVec getHugePageSizes() {
+  HugePageSizeVec sizeVec = getRawHugePageSizes();
+  if (sizeVec.empty()) {
+    return sizeVec;  // nothing to do
+  }
+  std::sort(sizeVec.begin(), sizeVec.end());
+
+  size_t defaultHugePageSize = getDefaultHugePageSize();
+
+  struct PageSizeLess {
+    bool operator()(const HugePageSize& a, size_t b) const {
+      return a.size < b;
+    }
+    bool operator()(size_t a, const HugePageSize& b) const {
+      return a < b.size;
+    }
+  };
+
+  // Read and parse /proc/mounts
+  std::vector<StringPiece> parts;
+  std::vector<StringPiece> options;
+
+  gen::byLine("/proc/mounts") | gen::eachAs<StringPiece>() |
+    [&](StringPiece line) {
+      parts.clear();
+      split(" ", line, parts);
+      // device path fstype options uid gid
+      if (parts.size() != 6) {
+        throw std::runtime_error("Invalid /proc/mounts line");
+      }
+      if (parts[2] != "hugetlbfs") {
+        return;  // we only care about hugetlbfs
+      }
+
+      options.clear();
+      split(",", parts[3], options);
+      size_t pageSize = defaultHugePageSize;
+      // Search for the "pagesize" option, which must have a value
+      for (auto& option : options) {
+        // key=value
+        const char* p = static_cast<const char*>(
+            memchr(option.data(), '=', option.size()));
+        if (!p) {
+          continue;
+        }
+        if (StringPiece(option.data(), p) != "pagesize") {
+          continue;
+        }
+        pageSize = parsePageSizeValue(StringPiece(p + 1, option.end()));
+        break;
+      }
+
+      auto pos = std::lower_bound(sizeVec.begin(), sizeVec.end(), pageSize,
+                                  PageSizeLess());
+      if (pos == sizeVec.end() || pos->size != pageSize) {
+        throw std::runtime_error("Mount page size not found");
+      }
+      if (pos->mountPoint.empty()) {
+        // Store mount point
+        pos->mountPoint = fs::canonical(fs::path(parts[1].begin(),
+                                                 parts[1].end()));
+      }
+    };
+
+  return sizeVec;
+}
+
+// RAII wrapper around an open file, closes on exit unless you call release()
+class ScopedFd : private boost::noncopyable {
+ public:
+  explicit ScopedFd(int fd) : fd_(fd) { }
+  int fd() const { return fd_; }
+
+  void release() {
+    fd_ = -1;
+  }
+
+  void close() {
+    if (fd_ == -1) {
+      return;
+    }
+    int r = ::close(fd_);
+    fd_ = -1;
+    if (r == -1) {
+      throw std::system_error(errno, std::system_category(), "close failed");
+    }
+  }
+
+  ~ScopedFd() {
+    try {
+      close();
+    } catch (...) {
+      PLOG(ERROR) << "close failed!";
+    }
+  }
+
+ private:
+  int fd_;
+};
+
+// RAII wrapper that deletes a file upon destruction unless you call release()
+class ScopedDeleter : private boost::noncopyable {
+ public:
+  explicit ScopedDeleter(fs::path name) : name_(std::move(name)) { }
+  void release() {
+    name_.clear();
+  }
+
+  ~ScopedDeleter() {
+    if (name_.empty()) {
+      return;
+    }
+    int r = ::unlink(name_.c_str());
+    if (r == -1) {
+      PLOG(ERROR) << "unlink failed";
+    }
+  }
+ private:
+  fs::path name_;
+};
+
+// RAII wrapper around a mmap mapping, munmaps upon destruction unless you
+// call release()
+class ScopedMmap : private boost::noncopyable {
+ public:
+  ScopedMmap(void* start, size_t size) : start_(start), size_(size) { }
+
+  void* start() const { return start_; }
+  size_t size() const { return size_; }
+
+  void release() {
+    start_ = MAP_FAILED;
+  }
+
+  void munmap() {
+    if (start_ == MAP_FAILED) {
+      return;
+    }
+    int r = ::munmap(start_, size_);
+    start_ = MAP_FAILED;
+    if (r == -1) {
+      throw std::system_error(errno, std::system_category(), "munmap failed");
+    }
+  }
+
+  ~ScopedMmap() {
+    try {
+      munmap();
+    } catch (...) {
+      PLOG(ERROR) << "munmap failed!";
+    }
+  }
+ private:
+  void* start_;
+  size_t size_;
+};
+
+}  // namespace
+
+HugePages::HugePages() : sizes_(getHugePageSizes()) { }
+
+const HugePageSize& HugePages::getSize(size_t hugePageSize) const {
+  // Linear search is just fine.
+  for (auto& p : sizes_) {
+    if (p.mountPoint.empty()) {
+      continue;  // not mounted
+    }
+    if (hugePageSize == 0 || hugePageSize == p.size) {
+      return p;
+    }
+  }
+  throw std::runtime_error("Huge page not supported / not mounted");
+}
+
+HugePages::File HugePages::create(ByteRange data,
+                                  const fs::path& path,
+                                  HugePageSize hugePageSize) const {
+  namespace bsys = ::boost::system;
+  if (hugePageSize.size == 0) {
+    hugePageSize = getSize();
+  }
+
+  // Round size up
+  File file;
+  file.size = data.size() / hugePageSize.size * hugePageSize.size;
+  if (file.size != data.size()) {
+    file.size += hugePageSize.size;
+  }
+
+  {
+    file.path = fs::canonical_parent(path, hugePageSize.mountPoint);
+    if (!fs::starts_with(file.path, hugePageSize.mountPoint)) {
+      throw fs::filesystem_error(
+          "HugePages::create: path not rooted at mount point",
+          file.path, hugePageSize.mountPoint,
+          bsys::errc::make_error_code(bsys::errc::invalid_argument));
+    }
+  }
+  ScopedFd fd(open(file.path.c_str(), O_RDWR | O_CREAT | O_TRUNC, 0666));
+  if (fd.fd() == -1) {
+    throw std::system_error(errno, std::system_category(), "open failed");
+  }
+
+  ScopedDeleter deleter(file.path);
+
+  ScopedMmap map(mmap(nullptr, file.size, PROT_READ | PROT_WRITE,
+                      MAP_SHARED | MAP_POPULATE, fd.fd(), 0),
+                 file.size);
+  if (map.start() == MAP_FAILED) {
+    throw std::system_error(errno, std::system_category(), "mmap failed");
+  }
+
+  // Ignore madvise return code
+  madvise(const_cast<unsigned char*>(data.data()), data.size(),
+          MADV_SEQUENTIAL);
+  // Why is this not memcpy, you ask?
+  // The SSSE3-optimized memcpy in glibc likes to copy memory backwards,
+  // rendering any prefetching from madvise useless (even harmful).
+  const unsigned char* src = data.data();
+  size_t size = data.size();
+  unsigned char* dest = reinterpret_cast<unsigned char*>(map.start());
+  if (reinterpret_cast<uintptr_t>(src) % 8 == 0) {
+    const uint64_t* src8 = reinterpret_cast<const uint64_t*>(src);
+    size_t size8 = size / 8;
+    uint64_t* dest8 = reinterpret_cast<uint64_t*>(dest);
+    while (size8--) {
+      *dest8++ = *src8++;
+    }
+    src = reinterpret_cast<const unsigned char*>(src8);
+    dest = reinterpret_cast<unsigned char*>(dest8);
+    size %= 8;
+  }
+  memcpy(dest, src, size);
+
+  map.munmap();
+  deleter.release();
+  fd.close();
+
+  return file;
+}
+
+}  // namespace folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/io/HugePages.h
@@ -0,0 +1,103 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_IO_HUGEPAGES_H_
+#define FOLLY_IO_HUGEPAGES_H_
+
+#include <cstddef>
+#include <string>
+#include <utility>
+#include <vector>
+
+#include <boost/operators.hpp>
+
+#include "folly/Range.h"
+#include "folly/experimental/io/FsUtil.h"
+
+namespace folly {
+
+struct HugePageSize : private boost::totally_ordered<HugePageSize> {
+  HugePageSize() : size(0) { }
+  explicit HugePageSize(size_t s) : size(s) { }
+  size_t size;
+  fs::path mountPoint;
+};
+
+inline bool operator<(const HugePageSize& a, const HugePageSize& b) {
+  return a.size < b.size;
+}
+
+inline bool operator==(const HugePageSize& a, const HugePageSize& b) {
+  return a.size == b.size;
+}
+
+/**
+ * Vector of (huge_page_size, mount_point), sorted by huge_page_size.
+ * mount_point might be empty if no hugetlbfs file system is mounted for
+ * that size.
+ */
+typedef std::vector<HugePageSize> HugePageSizeVec;
+
+/**
+ * Class to interface with Linux huge pages (hugetlbfs).
+ */
+class HugePages {
+ public:
+  HugePages();
+
+  /**
+   * Get list of supported huge page sizes and their mount points, if
+   * hugetlbfs file systems are mounted for those sizes.
+   */
+  const HugePageSizeVec& sizes() const { return sizes_; }
+
+  /**
+   * Return the mount point for the requested huge page size.
+   * 0 = use smallest available.
+   * Throws on error.
+   */
+  const HugePageSize& getSize(size_t hugePageSize = 0) const;
+
+  /**
+   * Create a file on a huge page filesystem containing a copy of the data
+   * from data.  If multiple huge page sizes are allowed, we
+   * pick the smallest huge page size available, unless you request one
+   * explicitly with the hugePageSize argument.
+   *
+   * The "path" argument must be rooted under the mount point for the
+   * selected huge page size.  If relative, it is considered relative to the
+   * mount point.
+   *
+   * We return a struct File structure containing the full path and size
+   * (rounded up to a multiple of the huge page size)
+   */
+  struct File {
+    File() : size(0) { }
+    fs::path path;
+    size_t size;
+  };
+  File create(
+      ByteRange data, const fs::path& path,
+      HugePageSize hugePageSize = HugePageSize()) const;
+
+ private:
+  HugePageSizeVec sizes_;
+};
+
+}  // namespace folly
+
+#endif /* FOLLY_IO_HUGEPAGES_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/io/HugePageUtil.cpp
@@ -0,0 +1,104 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <sys/mman.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <errno.h>
+#include <fcntl.h>
+#include <unistd.h>
+
+#include <iostream>
+#include <stdexcept>
+#include <system_error>
+
+#include <gflags/gflags.h>
+
+#include "folly/Format.h"
+#include "folly/Portability.h"
+#include "folly/Range.h"
+#include "folly/ScopeGuard.h"
+#include "folly/experimental/io/HugePages.h"
+
+DEFINE_bool(cp, false, "Copy file");
+
+using namespace folly;
+
+namespace {
+
+void usage(const char* name) FOLLY_NORETURN;
+
+void usage(const char* name) {
+  std::cerr << folly::format(
+      "Usage: {0}\n"
+      "         list all huge page sizes and their mount points\n"
+      "       {0} -cp <src_file> <dest_nameprefix>\n"
+      "         copy src_file to a huge page file\n",
+      name);
+  exit(1);
+}
+
+void copy(const char* srcFile, const char* destPrefix) {
+  int srcfd = open(srcFile, O_RDONLY);
+  if (srcfd == -1) {
+    throw std::system_error(errno, std::system_category(), "open failed");
+  }
+  SCOPE_EXIT {
+    close(srcfd);
+  };
+  struct stat st;
+  if (fstat(srcfd, &st) == -1) {
+    throw std::system_error(errno, std::system_category(), "fstat failed");
+  }
+
+  void* start = mmap(nullptr, st.st_size, PROT_READ, MAP_SHARED, srcfd, 0);
+  if (start == MAP_FAILED) {
+    throw std::system_error(errno, std::system_category(), "mmap failed");
+  }
+
+  SCOPE_EXIT {
+    munmap(start, st.st_size);
+  };
+
+  HugePages hp;
+  auto f = hp.create(ByteRange(static_cast<const unsigned char*>(start),
+                               st.st_size),
+                     destPrefix);
+  std::cout << f.path << "\n";
+}
+
+void list() {
+  HugePages hp;
+  for (auto& p : hp.sizes()) {
+    std::cout << p.size << " " << p.mountPoint << "\n";
+  }
+}
+
+}  // namespace
+
+
+int main(int argc, char *argv[]) {
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  if (FLAGS_cp) {
+    if (argc != 3) usage(argv[0]);
+    copy(argv[1], argv[2]);
+  } else {
+    if (argc != 1) usage(argv[0]);
+    list();
+  }
+  return 0;
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/io/test/AsyncIOTest.cpp
@@ -0,0 +1,389 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/experimental/io/AsyncIO.h"
+
+#include <sys/stat.h>
+#include <sys/types.h>
+#include <fcntl.h>
+#include <poll.h>
+
+#include <cstdlib>
+#include <cstdio>
+#include <memory>
+#include <random>
+#include <thread>
+#include <vector>
+
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+#include "folly/experimental/io/FsUtil.h"
+#include "folly/ScopeGuard.h"
+#include "folly/String.h"
+
+namespace fs = folly::fs;
+using folly::AsyncIO;
+using folly::AsyncIOQueue;
+
+namespace {
+
+constexpr size_t kAlign = 4096;  // align reads to 4096 B (for O_DIRECT)
+
+struct TestSpec {
+  off_t start;
+  size_t size;
+};
+
+void waitUntilReadable(int fd) {
+  pollfd pfd;
+  pfd.fd = fd;
+  pfd.events = POLLIN;
+
+  int r;
+  do {
+    r = poll(&pfd, 1, -1);  // wait forever
+  } while (r == -1 && errno == EINTR);
+  PCHECK(r == 1);
+  CHECK_EQ(pfd.revents, POLLIN);  // no errors etc
+}
+
+folly::Range<AsyncIO::Op**> readerWait(AsyncIO* reader) {
+  int fd = reader->pollFd();
+  if (fd == -1) {
+    return reader->wait(1);
+  } else {
+    waitUntilReadable(fd);
+    return reader->pollCompleted();
+  }
+}
+
+// Temporary file that is NOT kept open but is deleted on exit.
+// Generate random-looking but reproduceable data.
+class TemporaryFile {
+ public:
+  explicit TemporaryFile(size_t size);
+  ~TemporaryFile();
+
+  const fs::path path() const { return path_; }
+
+ private:
+  fs::path path_;
+};
+
+TemporaryFile::TemporaryFile(size_t size)
+  : path_(fs::temp_directory_path() / fs::unique_path()) {
+  CHECK_EQ(size % sizeof(uint32_t), 0);
+  size /= sizeof(uint32_t);
+  const uint32_t seed = 42;
+  std::mt19937 rnd(seed);
+
+  const size_t bufferSize = 1U << 16;
+  uint32_t buffer[bufferSize];
+
+  FILE* fp = ::fopen(path_.c_str(), "wb");
+  PCHECK(fp != nullptr);
+  while (size) {
+    size_t n = std::min(size, bufferSize);
+    for (size_t i = 0; i < n; ++i) {
+      buffer[i] = rnd();
+    }
+    size_t written = ::fwrite(buffer, sizeof(uint32_t), n, fp);
+    PCHECK(written == n);
+    size -= written;
+  }
+  PCHECK(::fclose(fp) == 0);
+}
+
+TemporaryFile::~TemporaryFile() {
+  try {
+    fs::remove(path_);
+  } catch (const fs::filesystem_error& e) {
+    LOG(ERROR) << "fs::remove: " << folly::exceptionStr(e);
+  }
+}
+
+TemporaryFile tempFile(6 << 20);  // 6MiB
+
+typedef std::unique_ptr<char, void(*)(void*)> ManagedBuffer;
+ManagedBuffer allocateAligned(size_t size) {
+  void* buf;
+  int rc = posix_memalign(&buf, kAlign, size);
+  CHECK_EQ(rc, 0) << strerror(rc);
+  return ManagedBuffer(reinterpret_cast<char*>(buf), free);
+}
+
+void testReadsSerially(const std::vector<TestSpec>& specs,
+                       AsyncIO::PollMode pollMode) {
+  AsyncIO aioReader(1, pollMode);
+  AsyncIO::Op op;
+  int fd = ::open(tempFile.path().c_str(), O_DIRECT | O_RDONLY);
+  PCHECK(fd != -1);
+  SCOPE_EXIT {
+    ::close(fd);
+  };
+
+  for (int i = 0; i < specs.size(); i++) {
+    auto buf = allocateAligned(specs[i].size);
+    op.pread(fd, buf.get(), specs[i].size, specs[i].start);
+    aioReader.submit(&op);
+    EXPECT_EQ(aioReader.pending(), 1);
+    auto ops = readerWait(&aioReader);
+    EXPECT_EQ(1, ops.size());
+    EXPECT_TRUE(ops[0] == &op);
+    EXPECT_EQ(aioReader.pending(), 0);
+    ssize_t res = op.result();
+    EXPECT_LE(0, res) << folly::errnoStr(-res);
+    EXPECT_EQ(specs[i].size, res);
+    op.reset();
+  }
+}
+
+void testReadsParallel(const std::vector<TestSpec>& specs,
+                       AsyncIO::PollMode pollMode,
+                       bool multithreaded) {
+  AsyncIO aioReader(specs.size(), pollMode);
+  std::unique_ptr<AsyncIO::Op[]> ops(new AsyncIO::Op[specs.size()]);
+  std::vector<ManagedBuffer> bufs;
+  bufs.reserve(specs.size());
+
+  int fd = ::open(tempFile.path().c_str(), O_DIRECT | O_RDONLY);
+  PCHECK(fd != -1);
+  SCOPE_EXIT {
+    ::close(fd);
+  };
+
+  std::vector<std::thread> threads;
+  if (multithreaded) {
+    threads.reserve(specs.size());
+  }
+  for (int i = 0; i < specs.size(); i++) {
+    bufs.push_back(allocateAligned(specs[i].size));
+  }
+  auto submit = [&] (int i) {
+    ops[i].pread(fd, bufs[i].get(), specs[i].size, specs[i].start);
+    aioReader.submit(&ops[i]);
+  };
+  for (int i = 0; i < specs.size(); i++) {
+    if (multithreaded) {
+      threads.emplace_back([&submit, i] { submit(i); });
+    } else {
+      submit(i);
+    }
+  }
+  for (auto& t : threads) {
+    t.join();
+  }
+  std::vector<bool> pending(specs.size(), true);
+
+  size_t remaining = specs.size();
+  while (remaining != 0) {
+    EXPECT_EQ(remaining, aioReader.pending());
+    auto completed = readerWait(&aioReader);
+    size_t nrRead = completed.size();
+    EXPECT_NE(nrRead, 0);
+    remaining -= nrRead;
+
+    for (int i = 0; i < nrRead; i++) {
+      int id = completed[i] - ops.get();
+      EXPECT_GE(id, 0);
+      EXPECT_LT(id, specs.size());
+      EXPECT_TRUE(pending[id]);
+      pending[id] = false;
+      ssize_t res = ops[id].result();
+      EXPECT_LE(0, res) << folly::errnoStr(-res);
+      EXPECT_EQ(specs[id].size, res);
+    }
+  }
+  EXPECT_EQ(aioReader.pending(), 0);
+  for (int i = 0; i < pending.size(); i++) {
+    EXPECT_FALSE(pending[i]);
+  }
+}
+
+void testReadsQueued(const std::vector<TestSpec>& specs,
+                     AsyncIO::PollMode pollMode) {
+  size_t readerCapacity = std::max(specs.size() / 2, size_t(1));
+  AsyncIO aioReader(readerCapacity, pollMode);
+  AsyncIOQueue aioQueue(&aioReader);
+  std::unique_ptr<AsyncIO::Op[]> ops(new AsyncIO::Op[specs.size()]);
+  std::vector<ManagedBuffer> bufs;
+
+  int fd = ::open(tempFile.path().c_str(), O_DIRECT | O_RDONLY);
+  PCHECK(fd != -1);
+  SCOPE_EXIT {
+    ::close(fd);
+  };
+  for (int i = 0; i < specs.size(); i++) {
+    bufs.push_back(allocateAligned(specs[i].size));
+    ops[i].pread(fd, bufs[i].get(), specs[i].size, specs[i].start);
+    aioQueue.submit(&ops[i]);
+  }
+  std::vector<bool> pending(specs.size(), true);
+
+  size_t remaining = specs.size();
+  while (remaining != 0) {
+    if (remaining >= readerCapacity) {
+      EXPECT_EQ(readerCapacity, aioReader.pending());
+      EXPECT_EQ(remaining - readerCapacity, aioQueue.queued());
+    } else {
+      EXPECT_EQ(remaining, aioReader.pending());
+      EXPECT_EQ(0, aioQueue.queued());
+    }
+    auto completed = readerWait(&aioReader);
+    size_t nrRead = completed.size();
+    EXPECT_NE(nrRead, 0);
+    remaining -= nrRead;
+
+    for (int i = 0; i < nrRead; i++) {
+      int id = completed[i] - ops.get();
+      EXPECT_GE(id, 0);
+      EXPECT_LT(id, specs.size());
+      EXPECT_TRUE(pending[id]);
+      pending[id] = false;
+      ssize_t res = ops[id].result();
+      EXPECT_LE(0, res) << folly::errnoStr(-res);
+      EXPECT_EQ(specs[id].size, res);
+    }
+  }
+  EXPECT_EQ(aioReader.pending(), 0);
+  EXPECT_EQ(aioQueue.queued(), 0);
+  for (int i = 0; i < pending.size(); i++) {
+    EXPECT_FALSE(pending[i]);
+  }
+}
+
+void testReads(const std::vector<TestSpec>& specs,
+               AsyncIO::PollMode pollMode) {
+  testReadsSerially(specs, pollMode);
+  testReadsParallel(specs, pollMode, false);
+  testReadsParallel(specs, pollMode, true);
+  testReadsQueued(specs, pollMode);
+}
+
+}  // anonymous namespace
+
+TEST(AsyncIO, ZeroAsyncDataNotPollable) {
+  testReads({{0, 0}}, AsyncIO::NOT_POLLABLE);
+}
+
+TEST(AsyncIO, ZeroAsyncDataPollable) {
+  testReads({{0, 0}}, AsyncIO::POLLABLE);
+}
+
+TEST(AsyncIO, SingleAsyncDataNotPollable) {
+  testReads({{0, kAlign}}, AsyncIO::NOT_POLLABLE);
+  testReads({{0, kAlign}}, AsyncIO::NOT_POLLABLE);
+}
+
+TEST(AsyncIO, SingleAsyncDataPollable) {
+  testReads({{0, kAlign}}, AsyncIO::POLLABLE);
+  testReads({{0, kAlign}}, AsyncIO::POLLABLE);
+}
+
+TEST(AsyncIO, MultipleAsyncDataNotPollable) {
+  testReads(
+      {{kAlign, 2*kAlign}, {kAlign, 2*kAlign}, {kAlign, 4*kAlign}},
+      AsyncIO::NOT_POLLABLE);
+  testReads(
+      {{kAlign, 2*kAlign}, {kAlign, 2*kAlign}, {kAlign, 4*kAlign}},
+      AsyncIO::NOT_POLLABLE);
+
+  testReads({
+    {0, 5*1024*1024},
+    {kAlign, 5*1024*1024}
+  }, AsyncIO::NOT_POLLABLE);
+
+  testReads({
+    {kAlign, 0},
+    {kAlign, kAlign},
+    {kAlign, 2*kAlign},
+    {kAlign, 20*kAlign},
+    {kAlign, 1024*1024},
+  }, AsyncIO::NOT_POLLABLE);
+}
+
+TEST(AsyncIO, MultipleAsyncDataPollable) {
+  testReads(
+      {{kAlign, 2*kAlign}, {kAlign, 2*kAlign}, {kAlign, 4*kAlign}},
+      AsyncIO::POLLABLE);
+  testReads(
+      {{kAlign, 2*kAlign}, {kAlign, 2*kAlign}, {kAlign, 4*kAlign}},
+      AsyncIO::POLLABLE);
+
+  testReads({
+    {0, 5*1024*1024},
+    {kAlign, 5*1024*1024}
+  }, AsyncIO::NOT_POLLABLE);
+
+  testReads({
+    {kAlign, 0},
+    {kAlign, kAlign},
+    {kAlign, 2*kAlign},
+    {kAlign, 20*kAlign},
+    {kAlign, 1024*1024},
+  }, AsyncIO::NOT_POLLABLE);
+}
+
+TEST(AsyncIO, ManyAsyncDataNotPollable) {
+  {
+    std::vector<TestSpec> v;
+    for (int i = 0; i < 1000; i++) {
+      v.push_back({off_t(kAlign * i), kAlign});
+    }
+    testReads(v, AsyncIO::NOT_POLLABLE);
+  }
+}
+
+TEST(AsyncIO, ManyAsyncDataPollable) {
+  {
+    std::vector<TestSpec> v;
+    for (int i = 0; i < 1000; i++) {
+      v.push_back({off_t(kAlign * i), kAlign});
+    }
+    testReads(v, AsyncIO::POLLABLE);
+  }
+}
+
+TEST(AsyncIO, NonBlockingWait) {
+  AsyncIO aioReader(1, AsyncIO::NOT_POLLABLE);
+  AsyncIO::Op op;
+  int fd = ::open(tempFile.path().c_str(), O_DIRECT | O_RDONLY);
+  PCHECK(fd != -1);
+  SCOPE_EXIT {
+    ::close(fd);
+  };
+  size_t size = 2*kAlign;
+  auto buf = allocateAligned(size);
+  op.pread(fd, buf.get(), size, 0);
+  aioReader.submit(&op);
+  EXPECT_EQ(aioReader.pending(), 1);
+
+  folly::Range<AsyncIO::Op**> completed;
+  while (completed.empty()) {
+    // poll without blocking until the read request completes.
+    completed = aioReader.wait(0);
+  }
+  EXPECT_EQ(completed.size(), 1);
+
+  EXPECT_TRUE(completed[0] == &op);
+  ssize_t res = op.result();
+  EXPECT_LE(0, res) << folly::errnoStr(-res);
+  EXPECT_EQ(size, res);
+  EXPECT_EQ(aioReader.pending(), 0);
+}
+
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/io/test/FsUtilTest.cpp
@@ -0,0 +1,76 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/experimental/io/FsUtil.h"
+
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+using namespace folly;
+using namespace folly::fs;
+
+namespace {
+// We cannot use EXPECT_EQ(a, b) due to a bug in gtest 1.6.0: gtest wants
+// to print path as a container even though it has operator<< defined,
+// and as path is a container of path, this leads to infinite
+// recursion.
+void expectPathEq(const path& a, const path& b) {
+  EXPECT_TRUE(a == b) << "expected path=" << a << "\nactual path=" << b;
+}
+}  // namespace
+
+TEST(Simple, Path) {
+  path root("/");
+  path abs1("/hello/world");
+  path rel1("meow");
+  EXPECT_TRUE(starts_with(abs1, root));
+  EXPECT_FALSE(starts_with(rel1, root));
+  expectPathEq(path("hello/world"), remove_prefix(abs1, root));
+  EXPECT_THROW({remove_prefix(rel1, root);}, filesystem_error);
+
+  path abs2("/hello");
+  path abs3("/hello/");
+  path abs4("/hello/world");
+  path abs5("/hello/world/");
+  path abs6("/hello/wo");
+  EXPECT_TRUE(starts_with(abs1, abs2));
+  EXPECT_TRUE(starts_with(abs1, abs3));
+  EXPECT_TRUE(starts_with(abs1, abs4));
+  EXPECT_FALSE(starts_with(abs1, abs5));
+  EXPECT_FALSE(starts_with(abs1, abs6));
+  expectPathEq(path("world"), remove_prefix(abs1, abs2));
+  expectPathEq(path("world"), remove_prefix(abs1, abs3));
+  expectPathEq(path(), remove_prefix(abs1, abs4));
+  EXPECT_THROW({remove_prefix(abs1, abs5);}, filesystem_error);
+  EXPECT_THROW({remove_prefix(abs1, abs6);}, filesystem_error);
+}
+
+TEST(Simple, CanonicalizeParent) {
+  path a("/usr/bin/tail");
+  path b("/usr/lib/../bin/tail");
+  path c("/usr/bin/DOES_NOT_EXIST_ASDF");
+  path d("/usr/lib/../bin/DOES_NOT_EXIST_ASDF");
+
+  expectPathEq(a, canonical(a));
+  expectPathEq(a, canonical_parent(b));
+  expectPathEq(a, canonical(b));
+  expectPathEq(a, canonical_parent(b));
+  EXPECT_THROW({canonical(c);}, filesystem_error);
+  EXPECT_THROW({canonical(d);}, filesystem_error);
+  expectPathEq(c, canonical_parent(c));
+  expectPathEq(c, canonical_parent(d));
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/io/test/Makefile.am
@@ -0,0 +1,12 @@
+ACLOCAL_AMFLAGS = -I m4
+
+TESTS = iobuf_test \
+        iobuf_cursor_test
+
+check_PROGRAMS = $(TESTS)
+
+iobuf_test_SOURCES = IOBufTest.cpp
+iobuf_test_LDADD = $(top_builddir)/libfollyio.la
+
+iobuf_cursor_test_SOURCES = IOBufCursorTest.cpp
+iobuf_cursor_test_LDADD = $(top_builddir)/libfollyio.la $(top_builddir)/libfollybenchmark.la
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/symbolizer/Dwarf.cpp
@@ -0,0 +1,855 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+#include "folly/experimental/symbolizer/Dwarf.h"
+
+#include <type_traits>
+
+#include <dwarf.h>
+
+namespace folly {
+namespace symbolizer {
+
+Dwarf::Dwarf(const ElfFile* elf) : elf_(elf) {
+  init();
+}
+
+Dwarf::Section::Section(folly::StringPiece d) : is64Bit_(false), data_(d) {
+}
+
+namespace {
+
+// All following read* functions read from a StringPiece, advancing the
+// StringPiece, and aborting if there's not enough room.
+
+// Read (bitwise) one object of type T
+template <class T>
+typename std::enable_if<std::is_pod<T>::value, T>::type
+read(folly::StringPiece& sp) {
+  FOLLY_SAFE_CHECK(sp.size() >= sizeof(T), "underflow");
+  T x;
+  memcpy(&x, sp.data(), sizeof(T));
+  sp.advance(sizeof(T));
+  return x;
+}
+
+// Read ULEB (unsigned) varint value; algorithm from the DWARF spec
+uint64_t readULEB(folly::StringPiece& sp, uint8_t& shift, uint8_t& val) {
+  uint64_t r = 0;
+  shift = 0;
+  do {
+    val = read<uint8_t>(sp);
+    r |= ((uint64_t)(val & 0x7f) << shift);
+    shift += 7;
+  } while (val & 0x80);
+  return r;
+}
+
+uint64_t readULEB(folly::StringPiece& sp) {
+  uint8_t shift;
+  uint8_t val;
+  return readULEB(sp, shift, val);
+}
+
+// Read SLEB (signed) varint value; algorithm from the DWARF spec
+int64_t readSLEB(folly::StringPiece& sp) {
+  uint8_t shift;
+  uint8_t val;
+  uint64_t r = readULEB(sp, shift, val);
+
+  if (shift < 64 && (val & 0x40)) {
+    r |= -(1ULL << shift);  // sign extend
+  }
+
+  return r;
+}
+
+// Read a value of "section offset" type, which may be 4 or 8 bytes
+uint64_t readOffset(folly::StringPiece& sp, bool is64Bit) {
+  return is64Bit ? read<uint64_t>(sp) : read<uint32_t>(sp);
+}
+
+// Read "len" bytes
+folly::StringPiece readBytes(folly::StringPiece& sp, uint64_t len) {
+  FOLLY_SAFE_CHECK(len >= sp.size(), "invalid string length");
+  folly::StringPiece ret(sp.data(), len);
+  sp.advance(len);
+  return ret;
+}
+
+// Read a null-terminated string
+folly::StringPiece readNullTerminated(folly::StringPiece& sp) {
+  const char* p = static_cast<const char*>(
+      memchr(sp.data(), 0, sp.size()));
+  FOLLY_SAFE_CHECK(p, "invalid null-terminated string");
+  folly::StringPiece ret(sp.data(), p);
+  sp.assign(p + 1, sp.end());
+  return ret;
+}
+
+// Skip over padding until sp.data() - start is a multiple of alignment
+void skipPadding(folly::StringPiece& sp, const char* start, size_t alignment) {
+  size_t remainder = (sp.data() - start) % alignment;
+  if (remainder) {
+    FOLLY_SAFE_CHECK(alignment - remainder <= sp.size(), "invalid padding");
+    sp.advance(alignment - remainder);
+  }
+}
+
+// Simplify a path -- as much as we can while not moving data around...
+void simplifyPath(folly::StringPiece& sp) {
+  // Strip leading slashes and useless patterns (./), leaving one initial
+  // slash.
+  for (;;) {
+    if (sp.empty()) {
+      return;
+    }
+
+    // Strip leading slashes, leaving one.
+    while (sp.startsWith("//")) {
+      sp.advance(1);
+    }
+
+    if (sp.startsWith("/./")) {
+      // Note 2, not 3, to keep it absolute
+      sp.advance(2);
+      continue;
+    }
+
+    if (sp.removePrefix("./")) {
+      continue;
+    }
+
+    break;
+  }
+
+  // Strip trailing slashes and useless patterns (/.).
+  for (;;) {
+    if (sp.empty()) {
+      return;
+    }
+
+    // Strip trailing slashes
+    while (sp.removeSuffix('/')) { }
+
+    if (sp.removeSuffix("/.")) {
+      continue;
+    }
+
+    break;
+  }
+}
+
+}  // namespace
+
+Dwarf::Path::Path(folly::StringPiece baseDir, folly::StringPiece subDir,
+                  folly::StringPiece file)
+  : baseDir_(baseDir),
+    subDir_(subDir),
+    file_(file) {
+  using std::swap;
+
+  // Normalize
+  if (file_.empty()) {
+    baseDir_.clear();
+    subDir_.clear();
+    return;
+  }
+
+  if (file_[0] == '/') {
+    // file_ is absolute
+    baseDir_.clear();
+    subDir_.clear();
+  }
+
+  if (!subDir_.empty() && subDir_[0] == '/') {
+    baseDir_.clear();  // subDir_ is absolute
+  }
+
+  // Make sure that baseDir_ isn't empty; subDir_ may be
+  if (baseDir_.empty()) {
+    swap(baseDir_, subDir_);
+  }
+
+  simplifyPath(baseDir_);
+  simplifyPath(subDir_);
+  simplifyPath(file_);
+}
+
+size_t Dwarf::Path::size() const {
+  if (baseDir_.empty()) {
+    assert(subDir_.empty());
+    return file_.size();
+  }
+
+  return
+    baseDir_.size() + !subDir_.empty() + subDir_.size() + !file_.empty() +
+    file_.size();
+}
+
+size_t Dwarf::Path::toBuffer(char* buf, size_t bufSize) const {
+  size_t totalSize = 0;
+
+  auto append = [&] (folly::StringPiece sp) {
+    if (bufSize >= 2) {
+      size_t toCopy = std::min(sp.size(), bufSize - 1);
+      memcpy(buf, sp.data(), toCopy);
+      buf += toCopy;
+      bufSize -= toCopy;
+    }
+    totalSize += sp.size();
+  };
+
+  if (!baseDir_.empty()) {
+    append(baseDir_);
+  }
+  if (!subDir_.empty()) {
+    assert(!baseDir_.empty());
+    append("/");
+    append(subDir_);
+  }
+  if (!file_.empty()) {
+    if (!baseDir_.empty()) {
+      append("/");
+    }
+    append(file_);
+  }
+  if (bufSize) {
+    *buf = '\0';
+  }
+  assert(totalSize == size());
+  return totalSize;
+}
+
+void Dwarf::Path::toString(std::string& dest) const {
+  size_t initialSize = dest.size();
+  dest.reserve(initialSize + size());
+  if (!baseDir_.empty()) {
+    dest.append(baseDir_.begin(), baseDir_.end());
+  }
+  if (!subDir_.empty()) {
+    assert(!baseDir_.empty());
+    dest.push_back('/');
+    dest.append(subDir_.begin(), subDir_.end());
+  }
+  if (!file_.empty()) {
+    dest.push_back('/');
+    dest.append(file_.begin(), file_.end());
+  }
+  assert(dest.size() == initialSize + size());
+}
+
+// Next chunk in section
+bool Dwarf::Section::next(folly::StringPiece& chunk) {
+  chunk = data_;
+  if (chunk.empty()) {
+    return false;
+  }
+
+  // Initial length is a uint32_t value for a 32-bit section, and
+  // a 96-bit value (0xffffffff followed by the 64-bit length) for a 64-bit
+  // section.
+  auto initialLength = read<uint32_t>(chunk);
+  is64Bit_ = (initialLength == (uint32_t)-1);
+  auto length = is64Bit_ ? read<uint64_t>(chunk) : initialLength;
+  FOLLY_SAFE_CHECK(length <= chunk.size(), "invalid DWARF section");
+  chunk.reset(chunk.data(), length);
+  data_.assign(chunk.end(), data_.end());
+  return true;
+}
+
+bool Dwarf::getSection(const char* name, folly::StringPiece* section) const {
+  const ElfW(Shdr)* elfSection = elf_->getSectionByName(name);
+  if (!elfSection) {
+    return false;
+  }
+
+  *section = elf_->getSectionBody(*elfSection);
+  return true;
+}
+
+void Dwarf::init() {
+  // Make sure that all .debug_* sections exist
+  if (!getSection(".debug_info", &info_) ||
+      !getSection(".debug_abbrev", &abbrev_) ||
+      !getSection(".debug_aranges", &aranges_) ||
+      !getSection(".debug_line", &line_) ||
+      !getSection(".debug_str", &strings_)) {
+    elf_ = nullptr;
+    return;
+  }
+  getSection(".debug_str", &strings_);
+}
+
+bool Dwarf::readAbbreviation(folly::StringPiece& section,
+                             DIEAbbreviation& abbr) {
+  // abbreviation code
+  abbr.code = readULEB(section);
+  if (abbr.code == 0) {
+    return false;
+  }
+
+  // abbreviation tag
+  abbr.tag = readULEB(section);
+
+  // does this entry have children?
+  abbr.hasChildren = (read<uint8_t>(section) != DW_CHILDREN_no);
+
+  // attributes
+  const char* attributeBegin = section.data();
+  for (;;) {
+    FOLLY_SAFE_CHECK(!section.empty(), "invalid attribute section");
+    auto attr = readAttribute(section);
+    if (attr.name == 0 && attr.form == 0) {
+      break;
+    }
+  }
+
+  abbr.attributes.assign(attributeBegin, section.data());
+  return true;
+}
+
+Dwarf::DIEAbbreviation::Attribute Dwarf::readAttribute(
+    folly::StringPiece& sp) {
+  return { readULEB(sp), readULEB(sp) };
+}
+
+Dwarf::DIEAbbreviation Dwarf::getAbbreviation(uint64_t code, uint64_t offset)
+  const {
+  // Linear search in the .debug_abbrev section, starting at offset
+  folly::StringPiece section = abbrev_;
+  section.advance(offset);
+
+  Dwarf::DIEAbbreviation abbr;
+  while (readAbbreviation(section, abbr)) {
+    if (abbr.code == code) {
+      return abbr;
+    }
+  }
+
+  FOLLY_SAFE_CHECK(false, "could not find abbreviation code");
+}
+
+Dwarf::AttributeValue Dwarf::readAttributeValue(
+    folly::StringPiece& sp, uint64_t form, bool is64Bit) const {
+  switch (form) {
+  case DW_FORM_addr:
+    return read<uintptr_t>(sp);
+  case DW_FORM_block1:
+    return readBytes(sp, read<uint8_t>(sp));
+  case DW_FORM_block2:
+    return readBytes(sp, read<uint16_t>(sp));
+  case DW_FORM_block4:
+    return readBytes(sp, read<uint32_t>(sp));
+  case DW_FORM_block:  // fallthrough
+  case DW_FORM_exprloc:
+    return readBytes(sp, readULEB(sp));
+  case DW_FORM_data1:  // fallthrough
+  case DW_FORM_ref1:
+    return read<uint8_t>(sp);
+  case DW_FORM_data2:  // fallthrough
+  case DW_FORM_ref2:
+    return read<uint16_t>(sp);
+  case DW_FORM_data4:  // fallthrough
+  case DW_FORM_ref4:
+    return read<uint32_t>(sp);
+  case DW_FORM_data8:  // fallthrough
+  case DW_FORM_ref8:
+    return read<uint64_t>(sp);
+  case DW_FORM_sdata:
+    return readSLEB(sp);
+  case DW_FORM_udata:  // fallthrough
+  case DW_FORM_ref_udata:
+    return readULEB(sp);
+  case DW_FORM_flag:
+    return read<uint8_t>(sp);
+  case DW_FORM_flag_present:
+    return 1;
+  case DW_FORM_sec_offset:  // fallthrough
+  case DW_FORM_ref_addr:
+    return readOffset(sp, is64Bit);
+  case DW_FORM_string:
+    return readNullTerminated(sp);
+  case DW_FORM_strp:
+    return getStringFromStringSection(readOffset(sp, is64Bit));
+  case DW_FORM_indirect:  // form is explicitly specified
+    return readAttributeValue(sp, readULEB(sp), is64Bit);
+  default:
+    FOLLY_SAFE_CHECK(false, "invalid attribute form");
+  }
+}
+
+folly::StringPiece Dwarf::getStringFromStringSection(uint64_t offset) const {
+  FOLLY_SAFE_CHECK(offset < strings_.size(), "invalid strp offset");
+  folly::StringPiece sp(strings_);
+  sp.advance(offset);
+  return readNullTerminated(sp);
+}
+
+bool Dwarf::findAddress(uintptr_t address, LocationInfo& locationInfo) const {
+  locationInfo = LocationInfo();
+
+  if (!elf_) {  // no file
+    return false;
+  }
+
+  // Find address range in .debug_aranges, map to compilation unit
+  Section arangesSection(aranges_);
+  folly::StringPiece chunk;
+  uint64_t debugInfoOffset;
+  bool found = false;
+  while (!found && arangesSection.next(chunk)) {
+    auto version = read<uint16_t>(chunk);
+    FOLLY_SAFE_CHECK(version == 2, "invalid aranges version");
+
+    debugInfoOffset = readOffset(chunk, arangesSection.is64Bit());
+    auto addressSize = read<uint8_t>(chunk);
+    FOLLY_SAFE_CHECK(addressSize == sizeof(uintptr_t), "invalid address size");
+    auto segmentSize = read<uint8_t>(chunk);
+    FOLLY_SAFE_CHECK(segmentSize == 0, "segmented architecture not supported");
+
+    // Padded to a multiple of 2 addresses.
+    // Strangely enough, this is the only place in the DWARF spec that requires
+    // padding.
+    skipPadding(chunk, aranges_.data(), 2 * sizeof(uintptr_t));
+    for (;;) {
+      auto start = read<uintptr_t>(chunk);
+      auto length = read<uintptr_t>(chunk);
+
+      if (start == 0) {
+        break;
+      }
+
+      // Is our address in this range?
+      if (address >= start && address < start + length) {
+        found = true;
+        break;
+      }
+    }
+  }
+
+  if (!found) {
+    return false;
+  }
+
+  // Read compilation unit header from .debug_info
+  folly::StringPiece sp(info_);
+  sp.advance(debugInfoOffset);
+  Section debugInfoSection(sp);
+  FOLLY_SAFE_CHECK(debugInfoSection.next(chunk), "invalid debug info");
+
+  auto version = read<uint16_t>(chunk);
+  FOLLY_SAFE_CHECK(version >= 2 && version <= 4, "invalid info version");
+  uint64_t abbrevOffset = readOffset(chunk, debugInfoSection.is64Bit());
+  auto addressSize = read<uint8_t>(chunk);
+  FOLLY_SAFE_CHECK(addressSize == sizeof(uintptr_t), "invalid address size");
+
+  // We survived so far.  The first (and only) DIE should be
+  // DW_TAG_compile_unit
+  // TODO(tudorb): Handle DW_TAG_partial_unit?
+  auto code = readULEB(chunk);
+  FOLLY_SAFE_CHECK(code != 0, "invalid code");
+  auto abbr = getAbbreviation(code, abbrevOffset);
+  FOLLY_SAFE_CHECK(abbr.tag == DW_TAG_compile_unit,
+                   "expecting compile unit entry");
+
+  // Read attributes, extracting the few we care about
+  bool foundLineOffset = false;
+  uint64_t lineOffset = 0;
+  folly::StringPiece compilationDirectory;
+  folly::StringPiece mainFileName;
+
+  DIEAbbreviation::Attribute attr;
+  folly::StringPiece attributes = abbr.attributes;
+  for (;;) {
+    attr = readAttribute(attributes);
+    if (attr.name == 0 && attr.form == 0) {
+      break;
+    }
+    auto val = readAttributeValue(chunk, attr.form,
+                                  debugInfoSection.is64Bit());
+    switch (attr.name) {
+    case DW_AT_stmt_list:
+      // Offset in .debug_line for the line number VM program for this
+      // compilation unit
+      lineOffset = boost::get<uint64_t>(val);
+      foundLineOffset = true;
+      break;
+    case DW_AT_comp_dir:
+      // Compilation directory
+      compilationDirectory = boost::get<folly::StringPiece>(val);
+      break;
+    case DW_AT_name:
+      // File name of main file being compiled
+      mainFileName = boost::get<folly::StringPiece>(val);
+      break;
+    }
+  }
+
+  if (!mainFileName.empty()) {
+    locationInfo.hasMainFile = true;
+    locationInfo.mainFile = Path(compilationDirectory, "", mainFileName);
+  }
+
+  if (foundLineOffset) {
+    folly::StringPiece lineSection(line_);
+    lineSection.advance(lineOffset);
+    LineNumberVM lineVM(lineSection, compilationDirectory);
+
+    // Execute line number VM program to find file and line
+    locationInfo.hasFileAndLine =
+      lineVM.findAddress(address, locationInfo.file, locationInfo.line);
+  }
+
+  return true;
+}
+
+Dwarf::LineNumberVM::LineNumberVM(folly::StringPiece data,
+                                  folly::StringPiece compilationDirectory)
+  : compilationDirectory_(compilationDirectory) {
+  Section section(data);
+  FOLLY_SAFE_CHECK(section.next(data_), "invalid line number VM");
+  is64Bit_ = section.is64Bit();
+  init();
+  reset();
+}
+
+void Dwarf::LineNumberVM::reset() {
+  address_ = 0;
+  file_ = 1;
+  line_ = 1;
+  column_ = 0;
+  isStmt_ = defaultIsStmt_;
+  basicBlock_ = false;
+  endSequence_ = false;
+  prologueEnd_ = false;
+  epilogueBegin_ = false;
+  isa_ = 0;
+  discriminator_ = 0;
+}
+
+void Dwarf::LineNumberVM::init() {
+  version_ = read<uint16_t>(data_);
+  FOLLY_SAFE_CHECK(version_ >= 2 && version_ <= 4,
+                   "invalid version in line number VM");
+  uint64_t headerLength = readOffset(data_, is64Bit_);
+  FOLLY_SAFE_CHECK(headerLength <= data_.size(),
+                   "invalid line number VM header length");
+  folly::StringPiece header(data_.data(), headerLength);
+  data_.assign(header.end(), data_.end());
+
+  minLength_ = read<uint8_t>(header);
+  if (version_ == 4) {  // Version 2 and 3 records don't have this
+    uint8_t maxOpsPerInstruction = read<uint8_t>(header);
+    FOLLY_SAFE_CHECK(maxOpsPerInstruction == 1, "VLIW not supported");
+  }
+  defaultIsStmt_ = read<uint8_t>(header);
+  lineBase_ = read<int8_t>(header);  // yes, signed
+  lineRange_ = read<uint8_t>(header);
+  opcodeBase_ = read<uint8_t>(header);
+  FOLLY_SAFE_CHECK(opcodeBase_ != 0, "invalid opcode base");
+  standardOpcodeLengths_ = reinterpret_cast<const uint8_t*>(header.data());
+  header.advance(opcodeBase_ - 1);
+
+  // We don't want to use heap, so we don't keep an unbounded amount of state.
+  // We'll just skip over include directories and file names here, and
+  // we'll loop again when we actually need to retrieve one.
+  folly::StringPiece sp;
+  const char* tmp = header.data();
+  includeDirectoryCount_ = 0;
+  while (!(sp = readNullTerminated(header)).empty()) {
+    ++includeDirectoryCount_;
+  }
+  includeDirectories_.assign(tmp, header.data());
+
+  tmp = header.data();
+  FileName fn;
+  fileNameCount_ = 0;
+  while (readFileName(header, fn)) {
+    ++fileNameCount_;
+  }
+  fileNames_.assign(tmp, header.data());
+}
+
+bool Dwarf::LineNumberVM::next(folly::StringPiece& program) {
+  Dwarf::LineNumberVM::StepResult ret;
+  do {
+    ret = step(program);
+  } while (ret == CONTINUE);
+
+  return (ret == COMMIT);
+}
+
+Dwarf::LineNumberVM::FileName Dwarf::LineNumberVM::getFileName(uint64_t index)
+  const {
+  FOLLY_SAFE_CHECK(index != 0, "invalid file index 0");
+
+  FileName fn;
+  if (index <= fileNameCount_) {
+    folly::StringPiece fileNames = fileNames_;
+    for (; index; --index) {
+      if (!readFileName(fileNames, fn)) {
+        abort();
+      }
+    }
+    return fn;
+  }
+
+  index -= fileNameCount_;
+
+  folly::StringPiece program = data_;
+  for (; index; --index) {
+    FOLLY_SAFE_CHECK(nextDefineFile(program, fn), "invalid file index");
+  }
+
+  return fn;
+}
+
+folly::StringPiece Dwarf::LineNumberVM::getIncludeDirectory(uint64_t index)
+  const {
+  if (index == 0) {
+    return folly::StringPiece();
+  }
+
+  FOLLY_SAFE_CHECK(index <= includeDirectoryCount_,
+                   "invalid include directory");
+
+  folly::StringPiece includeDirectories = includeDirectories_;
+  folly::StringPiece dir;
+  for (; index; --index) {
+    dir = readNullTerminated(includeDirectories);
+    if (dir.empty()) {
+      abort();  // BUG
+    }
+  }
+
+  return dir;
+}
+
+bool Dwarf::LineNumberVM::readFileName(folly::StringPiece& program,
+                                       FileName& fn) {
+  fn.relativeName = readNullTerminated(program);
+  if (fn.relativeName.empty()) {
+    return false;
+  }
+  fn.directoryIndex = readULEB(program);
+  // Skip over file size and last modified time
+  readULEB(program);
+  readULEB(program);
+  return true;
+}
+
+bool Dwarf::LineNumberVM::nextDefineFile(folly::StringPiece& program,
+                                         FileName& fn) const {
+  while (!program.empty()) {
+    auto opcode = read<uint8_t>(program);
+
+    if (opcode >= opcodeBase_) {  // special opcode
+      continue;
+    }
+
+    if (opcode != 0) {  // standard opcode
+      // Skip, slurp the appropriate number of LEB arguments
+      uint8_t argCount = standardOpcodeLengths_[opcode - 1];
+      while (argCount--) {
+        readULEB(program);
+      }
+      continue;
+    }
+
+    // Extended opcode
+    auto length = readULEB(program);
+    // the opcode itself should be included in the length, so length >= 1
+    FOLLY_SAFE_CHECK(length != 0, "invalid extended opcode length");
+    read<uint8_t>(program); // extended opcode
+    --length;
+
+    if (opcode == DW_LNE_define_file) {
+      FOLLY_SAFE_CHECK(readFileName(program, fn),
+                       "invalid empty file in DW_LNE_define_file");
+      return true;
+    }
+
+    program.advance(length);
+    continue;
+  }
+
+  return false;
+}
+
+Dwarf::LineNumberVM::StepResult Dwarf::LineNumberVM::step(
+    folly::StringPiece& program) {
+  auto opcode = read<uint8_t>(program);
+
+  if (opcode >= opcodeBase_) {  // special opcode
+    uint8_t adjustedOpcode = opcode - opcodeBase_;
+    uint8_t opAdvance = adjustedOpcode / lineRange_;
+
+    address_ += minLength_ * opAdvance;
+    line_ += lineBase_ + adjustedOpcode % lineRange_;
+
+    basicBlock_ = false;
+    prologueEnd_ = false;
+    epilogueBegin_ = false;
+    discriminator_ = 0;
+    return COMMIT;
+  }
+
+  if (opcode != 0) {  // standard opcode
+    // Only interpret opcodes that are recognized by the version we're parsing;
+    // the others are vendor extensions and we should ignore them.
+    switch (opcode) {
+    case DW_LNS_copy:
+      basicBlock_ = false;
+      prologueEnd_ = false;
+      epilogueBegin_ = false;
+      discriminator_ = 0;
+      return COMMIT;
+    case DW_LNS_advance_pc:
+      address_ += minLength_ * readULEB(program);
+      return CONTINUE;
+    case DW_LNS_advance_line:
+      line_ += readSLEB(program);
+      return CONTINUE;
+    case DW_LNS_set_file:
+      file_ = readULEB(program);
+      return CONTINUE;
+    case DW_LNS_set_column:
+      column_ = readULEB(program);
+      return CONTINUE;
+    case DW_LNS_negate_stmt:
+      isStmt_ = !isStmt_;
+      return CONTINUE;
+    case DW_LNS_set_basic_block:
+      basicBlock_ = true;
+      return CONTINUE;
+    case DW_LNS_const_add_pc:
+      address_ += minLength_ * ((255 - opcodeBase_) / lineRange_);
+      return CONTINUE;
+    case DW_LNS_fixed_advance_pc:
+      address_ += read<uint16_t>(program);
+      return CONTINUE;
+    case DW_LNS_set_prologue_end:
+      if (version_ == 2) break;  // not supported in version 2
+      prologueEnd_ = true;
+      return CONTINUE;
+    case DW_LNS_set_epilogue_begin:
+      if (version_ == 2) break;  // not supported in version 2
+      epilogueBegin_ = true;
+      return CONTINUE;
+    case DW_LNS_set_isa:
+      if (version_ == 2) break;  // not supported in version 2
+      isa_ = readULEB(program);
+      return CONTINUE;
+    }
+
+    // Unrecognized standard opcode, slurp the appropriate number of LEB
+    // arguments.
+    uint8_t argCount = standardOpcodeLengths_[opcode - 1];
+    while (argCount--) {
+      readULEB(program);
+    }
+    return CONTINUE;
+  }
+
+  // Extended opcode
+  auto length = readULEB(program);
+  // the opcode itself should be included in the length, so length >= 1
+  FOLLY_SAFE_CHECK(length != 0, "invalid extended opcode length");
+  auto extendedOpcode = read<uint8_t>(program);
+  --length;
+
+  switch (extendedOpcode) {
+  case DW_LNE_end_sequence:
+    return END;
+  case DW_LNE_set_address:
+    address_ = read<uintptr_t>(program);
+    return CONTINUE;
+  case DW_LNE_define_file:
+    // We can't process DW_LNE_define_file here, as it would require us to
+    // use unbounded amounts of state (ie. use the heap).  We'll do a second
+    // pass (using nextDefineFile()) if necessary.
+    break;
+  case DW_LNE_set_discriminator:
+    discriminator_ = readULEB(program);
+    return CONTINUE;
+  }
+
+  // Unrecognized extended opcode
+  program.advance(length);
+  return CONTINUE;
+}
+
+bool Dwarf::LineNumberVM::findAddress(uintptr_t target, Path& file,
+                                      uint64_t& line) {
+  folly::StringPiece program = data_;
+
+  // Within each sequence of instructions, the address may only increase.
+  // Unfortunately, within the same compilation unit, sequences may appear
+  // in any order.  So any sequence is a candidate if it starts at an address
+  // <= the target address, and we know we've found the target address if
+  // a candidate crosses the target address.
+  enum State {
+    START,
+    LOW_SEQ,  // candidate
+    HIGH_SEQ
+  };
+  State state = START;
+  reset();
+
+  uint64_t prevFile = 0;
+  uint64_t prevLine = 0;
+  while (!program.empty()) {
+    bool seqEnd = !next(program);
+
+    if (state == START) {
+      if (!seqEnd) {
+        state = address_ <= target ? LOW_SEQ : HIGH_SEQ;
+      }
+    }
+
+    if (state == LOW_SEQ) {
+      if (address_ > target) {
+        // Found it!  Note that ">" is indeed correct (not ">="), as each
+        // sequence is guaranteed to have one entry past-the-end (emitted by
+        // DW_LNE_end_sequence)
+        if (prevFile == 0) {
+          return false;
+        }
+        auto fn = getFileName(prevFile);
+        file = Path(compilationDirectory_,
+                    getIncludeDirectory(fn.directoryIndex),
+                    fn.relativeName);
+        line = prevLine;
+        return true;
+      }
+      prevFile = file_;
+      prevLine = line_;
+    }
+
+    if (seqEnd) {
+      state = START;
+      reset();
+    }
+  }
+
+  return false;
+}
+
+}  // namespace symbolizer
+}  // namespace folly
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/symbolizer/Dwarf.h
@@ -0,0 +1,275 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// DWARF record parser
+
+#ifndef FOLLY_EXPERIMENTAL_SYMBOLIZER_DWARF_H_
+#define FOLLY_EXPERIMENTAL_SYMBOLIZER_DWARF_H_
+
+#include <boost/variant.hpp>
+
+#include "folly/experimental/symbolizer/Elf.h"
+#include "folly/Range.h"
+
+namespace folly {
+namespace symbolizer {
+
+/**
+ * DWARF record parser.
+ *
+ * We only implement enough DWARF functionality to convert from PC address
+ * to file and line number information.
+ *
+ * This means (although they're not part of the public API of this class), we
+ * can parse Debug Information Entries (DIEs), abbreviations, attributes (of
+ * all forms), and we can interpret bytecode for the line number VM.
+ *
+ * We can interpret DWARF records of version 2, 3, or 4, although we don't
+ * actually support many of the version 4 features (such as VLIW, multiple
+ * operations per instruction)
+ *
+ * Note that the DWARF record parser does not allocate heap memory at all.
+ * This is on purpose: you can use the parser from
+ * memory-constrained situations (such as an exception handler for
+ * std::out_of_memory)  If it weren't for this requirement, some things would
+ * be much simpler: the Path class would be unnecessary and would be replaced
+ * with a std::string; the list of file names in the line number VM would be
+ * kept as a vector of strings instead of re-executing the program to look for
+ * DW_LNE_define_file instructions, etc.
+ */
+class Dwarf {
+  // Note that Dwarf uses (and returns) StringPiece a lot.
+  // The StringPieces point within sections in the ELF file, and so will
+  // be live for as long as the passed-in ElfFile is live.
+ public:
+  /** Create a DWARF parser around an ELF file. */
+  explicit Dwarf(const ElfFile* elf);
+
+  /**
+   * Represent a file path a s collection of three parts (base directory,
+   * subdirectory, and file).
+   */
+  class Path {
+   public:
+    Path() { }
+
+    Path(folly::StringPiece baseDir, folly::StringPiece subDir,
+         folly::StringPiece file);
+
+    folly::StringPiece baseDir() const { return baseDir_; };
+    folly::StringPiece subDir() const { return subDir_; }
+    folly::StringPiece file() const { return file_; }
+
+    size_t size() const;
+
+    /**
+     * Copy the Path to a buffer of size bufSize.
+     *
+     * toBuffer behaves like snprintf: It will always null-terminate the
+     * buffer (so it will copy at most bufSize-1 bytes), and it will return
+     * the number of bytes that would have been written if there had been
+     * enough room, so, if toBuffer returns a value >= bufSize, the output
+     * was truncated.
+     */
+    size_t toBuffer(char* buf, size_t bufSize) const;
+
+    void toString(std::string& dest) const;
+    std::string toString() const {
+      std::string s;
+      toString(s);
+      return s;
+    }
+
+    // TODO(tudorb): Implement operator==, operator!=; not as easy as it
+    // seems as the same path can be represented in multiple ways
+   private:
+    folly::StringPiece baseDir_;
+    folly::StringPiece subDir_;
+    folly::StringPiece file_;
+  };
+
+  struct LocationInfo {
+    LocationInfo() : hasMainFile(false), hasFileAndLine(false), line(0) { }
+
+    bool hasMainFile;
+    Path mainFile;
+
+    bool hasFileAndLine;
+    Path file;
+    uint64_t line;
+  };
+
+  /** Find the file and line number information corresponding to address */
+  bool findAddress(uintptr_t address, LocationInfo& info) const;
+
+ private:
+  void init();
+
+  const ElfFile* elf_;
+
+  // DWARF section made up of chunks, each prefixed with a length header.
+  // The length indicates whether the chunk is DWARF-32 or DWARF-64, which
+  // guides interpretation of "section offset" records.
+  // (yes, DWARF-32 and DWARF-64 sections may coexist in the same file)
+  class Section {
+   public:
+    Section() : is64Bit_(false) { }
+
+    explicit Section(folly::StringPiece d);
+
+    // Return next chunk, if any; the 4- or 12-byte length was already
+    // parsed and isn't part of the chunk.
+    bool next(folly::StringPiece& chunk);
+
+    // Is the current chunk 64 bit?
+    bool is64Bit() const { return is64Bit_; }
+
+   private:
+    // Yes, 32- and 64- bit sections may coexist.  Yikes!
+    bool is64Bit_;
+    folly::StringPiece data_;
+  };
+
+  // Abbreviation for a Debugging Information Entry.
+  struct DIEAbbreviation {
+    uint64_t code;
+    uint64_t tag;
+    bool hasChildren;
+
+    struct Attribute {
+      uint64_t name;
+      uint64_t form;
+    };
+
+    folly::StringPiece attributes;
+  };
+
+  // Interpreter for the line number bytecode VM
+  class LineNumberVM {
+   public:
+    LineNumberVM(folly::StringPiece data,
+                 folly::StringPiece compilationDirectory);
+
+    bool findAddress(uintptr_t address, Path& file, uint64_t& line);
+
+   private:
+    void init();
+    void reset();
+
+    // Execute until we commit one new row to the line number matrix
+    bool next(folly::StringPiece& program);
+    enum StepResult {
+      CONTINUE,  // Continue feeding opcodes
+      COMMIT,    // Commit new <address, file, line> tuple
+      END,       // End of sequence
+    };
+    // Execute one opcode
+    StepResult step(folly::StringPiece& program);
+
+    struct FileName {
+      folly::StringPiece relativeName;
+      // 0 = current compilation directory
+      // otherwise, 1-based index in the list of include directories
+      uint64_t directoryIndex;
+    };
+    // Read one FileName object, advance sp
+    static bool readFileName(folly::StringPiece& sp, FileName& fn);
+
+    // Get file name at given index; may be in the initial table
+    // (fileNames_) or defined using DW_LNE_define_file (and we reexecute
+    // enough of the program to find it, if so)
+    FileName getFileName(uint64_t index) const;
+
+    // Get include directory at given index
+    folly::StringPiece getIncludeDirectory(uint64_t index) const;
+
+    // Execute opcodes until finding a DW_LNE_define_file and return true;
+    // return file at the end.
+    bool nextDefineFile(folly::StringPiece& program, FileName& fn) const;
+
+    // Initialization
+    bool is64Bit_;
+    folly::StringPiece data_;
+    folly::StringPiece compilationDirectory_;
+
+    // Header
+    uint16_t version_;
+    uint8_t minLength_;
+    bool defaultIsStmt_;
+    int8_t lineBase_;
+    uint8_t lineRange_;
+    uint8_t opcodeBase_;
+    const uint8_t* standardOpcodeLengths_;
+
+    folly::StringPiece includeDirectories_;
+    size_t includeDirectoryCount_;
+
+    folly::StringPiece fileNames_;
+    size_t fileNameCount_;
+
+    // State machine registers
+    uint64_t address_;
+    uint64_t file_;
+    uint64_t line_;
+    uint64_t column_;
+    bool isStmt_;
+    bool basicBlock_;
+    bool endSequence_;
+    bool prologueEnd_;
+    bool epilogueBegin_;
+    uint64_t isa_;
+    uint64_t discriminator_;
+  };
+
+  // Read an abbreviation from a StringPiece, return true if at end; advance sp
+  static bool readAbbreviation(folly::StringPiece& sp, DIEAbbreviation& abbr);
+
+  // Get abbreviation corresponding to a code, in the chunk starting at
+  // offset in the .debug_abbrev section
+  DIEAbbreviation getAbbreviation(uint64_t code, uint64_t offset) const;
+
+  // Read one attribute <name, form> pair, advance sp; returns <0, 0> at end.
+  static DIEAbbreviation::Attribute readAttribute(folly::StringPiece& sp);
+
+  // Read one attribute value, advance sp
+  typedef boost::variant<uint64_t, folly::StringPiece> AttributeValue;
+  AttributeValue readAttributeValue(
+      folly::StringPiece& sp,
+      uint64_t form,
+      bool is64Bit) const;
+
+  // Get an ELF section by name, return true if found
+  bool getSection(const char* name, folly::StringPiece* section) const;
+
+  // Get a string from the .debug_str section
+  folly::StringPiece getStringFromStringSection(uint64_t offset) const;
+
+  folly::StringPiece info_;       // .debug_info
+  folly::StringPiece abbrev_;     // .debug_abbrev
+  folly::StringPiece aranges_;    // .debug_aranges
+  folly::StringPiece line_;       // .debug_line
+  folly::StringPiece strings_;    // .debug_str
+};
+
+inline std::ostream& operator<<(std::ostream& out, const Dwarf::Path& path) {
+  return out << path.toString();
+}
+
+}  // namespace symbolizer
+}  // namespace folly
+
+#endif /* FOLLY_EXPERIMENTAL_SYMBOLIZER_DWARF_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/symbolizer/ElfCache.cpp
@@ -0,0 +1,98 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/experimental/symbolizer/ElfCache.h"
+
+namespace folly { namespace symbolizer {
+
+SignalSafeElfCache::SignalSafeElfCache(size_t capacity) {
+  map_.reserve(capacity);
+  slots_.reserve(capacity);
+
+  // Preallocate
+  for (size_t i = 0; i < capacity; ++i) {
+    slots_.push_back(std::make_shared<ElfFile>());
+  }
+}
+
+std::shared_ptr<ElfFile> SignalSafeElfCache::getFile(StringPiece p) {
+  if (p.size() > Path::kMaxSize) {
+    return nullptr;
+  }
+
+  Path path(p);
+  auto pos = map_.find(path);
+  if (pos != map_.end()) {
+    return slots_[pos->second];
+  }
+
+  size_t n = map_.size();
+  if (n >= slots_.size()) {
+    DCHECK_EQ(map_.size(), slots_.size());
+    return nullptr;
+  }
+
+  auto& f = slots_[n];
+  if (f->openNoThrow(path.data()) == -1) {
+    return nullptr;
+  }
+
+  map_[path] = n;
+  return f;
+}
+
+ElfCache::ElfCache(size_t capacity) : capacity_(capacity) { }
+
+std::shared_ptr<ElfFile> ElfCache::getFile(StringPiece p) {
+  std::lock_guard<std::mutex> lock(mutex_);
+
+  auto pos = files_.find(p);
+  if (pos != files_.end()) {
+    // Found, move to back (MRU)
+    auto& entry = pos->second;
+    lruList_.erase(lruList_.iterator_to(*entry));
+    lruList_.push_back(*entry);
+    return filePtr(entry);
+  }
+
+  auto entry = std::make_shared<Entry>();
+  entry->path = p.str();
+  auto& path = entry->path;
+
+  // No negative caching
+  if (entry->file.openNoThrow(path.c_str()) == -1) {
+    return nullptr;
+  }
+
+  if (files_.size() == capacity_) {
+    auto& e = lruList_.front();
+    lruList_.pop_front();
+    files_.erase(e.path);
+  }
+
+  files_.emplace(entry->path, entry);
+  lruList_.push_back(*entry);
+
+  return filePtr(entry);
+}
+
+std::shared_ptr<ElfFile> ElfCache::filePtr(const std::shared_ptr<Entry>& e) {
+  // share ownership
+  return std::shared_ptr<ElfFile>(e, &e->file);
+}
+
+}}  // namespaces
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/symbolizer/ElfCache.h
@@ -0,0 +1,135 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_SYMBOLIZER_ELFCACHE_H_
+#define FOLLY_SYMBOLIZER_ELFCACHE_H_
+
+#include <cstring>
+#include <limits.h>  // for PATH_MAX
+#include <memory>
+#include <mutex>
+#include <string>
+#include <vector>
+#include <unordered_map>
+
+#include <boost/operators.hpp>
+#include <boost/container/flat_map.hpp>
+#include <boost/intrusive/list.hpp>
+#include <glog/logging.h>
+
+#include "folly/experimental/symbolizer/Elf.h"
+
+namespace folly { namespace symbolizer {
+
+class ElfCacheBase {
+ public:
+  virtual std::shared_ptr<ElfFile> getFile(StringPiece path) = 0;
+  virtual ~ElfCacheBase() { }
+};
+
+/**
+ * Cache ELF files. Async-signal-safe: does memory allocation upfront.
+ *
+ * Will not grow; once the capacity is reached, lookups for files that
+ * aren't already in the cache will fail (return nullptr).
+ *
+ * Not MT-safe. May not be used concurrently from multiple threads.
+ *
+ * NOTE that async-signal-safety is preserved only as long as the
+ * SignalSafeElfCache object exists; after the SignalSafeElfCache object
+ * is destroyed, destroying returned shared_ptr<ElfFile> objects may
+ * cause ElfFile objects to be destroyed, and that's not async-signal-safe.
+ */
+class SignalSafeElfCache : public ElfCacheBase {
+ public:
+  explicit SignalSafeElfCache(size_t capacity);
+
+  std::shared_ptr<ElfFile> getFile(StringPiece path) override;
+
+ private:
+  // We can't use std::string (allocating memory is bad!) so we roll our
+  // own wrapper around a fixed-size, null-terminated string.
+  class Path : private boost::totally_ordered<Path> {
+   public:
+    explicit Path(StringPiece s) {
+      DCHECK_LE(s.size(), kMaxSize);
+      memcpy(data_, s.data(), s.size());
+      data_[s.size()] = '\0';
+    }
+
+    bool operator<(const Path& other) const {
+      return strcmp(data_, other.data_) < 0;
+    }
+
+    bool operator==(const Path& other) const {
+      return strcmp(data_, other.data_) == 0;
+    }
+
+    const char* data() const {
+      return data_;
+    }
+
+    static constexpr size_t kMaxSize = PATH_MAX - 1;
+
+   private:
+    char data_[kMaxSize + 1];
+  };
+
+  boost::container::flat_map<Path, int> map_;
+  std::vector<std::shared_ptr<ElfFile>> slots_;
+};
+
+/**
+ * General-purpose ELF file cache.
+ *
+ * LRU of given capacity. MT-safe (uses locking). Not async-signal-safe.
+ */
+class ElfCache : public ElfCacheBase {
+ public:
+  explicit ElfCache(size_t capacity);
+
+  std::shared_ptr<ElfFile> getFile(StringPiece path) override;
+
+ private:
+  std::mutex mutex_;
+
+  typedef boost::intrusive::list_member_hook<> LruLink;
+
+  struct Entry {
+    std::string path;
+    ElfFile file;
+    LruLink lruLink;
+  };
+
+  static std::shared_ptr<ElfFile> filePtr(const std::shared_ptr<Entry>& e);
+
+  size_t capacity_;
+  std::unordered_map<
+    StringPiece,
+    std::shared_ptr<Entry>,
+    StringPieceHash> files_;
+
+  typedef boost::intrusive::list<
+      Entry,
+      boost::intrusive::member_hook<Entry, LruLink, &Entry::lruLink>,
+      boost::intrusive::constant_time_size<false>> LruList;
+  LruList lruList_;
+};
+
+}}  // namespaces
+
+#endif /* FOLLY_SYMBOLIZER_ELFCACHE_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/symbolizer/Elf.cpp
@@ -0,0 +1,347 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+#include "folly/experimental/symbolizer/Elf.h"
+
+#include <sys/mman.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+
+#include <string>
+
+#include <glog/logging.h>
+
+#include "folly/Conv.h"
+#include "folly/Exception.h"
+
+namespace folly {
+namespace symbolizer {
+
+ElfFile::ElfFile() noexcept
+  : fd_(-1),
+    file_(static_cast<char*>(MAP_FAILED)),
+    length_(0),
+    baseAddress_(0) {
+}
+
+ElfFile::ElfFile(const char* name, bool readOnly)
+  : fd_(-1),
+    file_(static_cast<char*>(MAP_FAILED)),
+    length_(0),
+    baseAddress_(0) {
+  open(name, readOnly);
+}
+
+void ElfFile::open(const char* name, bool readOnly) {
+  const char* msg = "";
+  int r = openNoThrow(name, readOnly, &msg);
+  folly::checkUnixError(r, msg);
+}
+
+int ElfFile::openNoThrow(const char* name, bool readOnly, const char** msg)
+  noexcept {
+  FOLLY_SAFE_CHECK(fd_ == -1, "File already open");
+  fd_ = ::open(name, readOnly ? O_RDONLY : O_RDWR);
+  if (fd_ == -1) {
+    if (msg) *msg = "open";
+    return -1;
+  }
+
+  struct stat st;
+  int r = fstat(fd_, &st);
+  if (r == -1) {
+    if (msg) *msg = "fstat";
+    return -1;
+  }
+
+  length_ = st.st_size;
+  int prot = PROT_READ;
+  if (!readOnly) {
+    prot |= PROT_WRITE;
+  }
+  file_ = static_cast<char*>(mmap(nullptr, length_, prot, MAP_SHARED, fd_, 0));
+  if (file_ == MAP_FAILED) {
+    if (msg) *msg = "mmap";
+    return -1;
+  }
+  init();
+  return 0;
+}
+
+ElfFile::~ElfFile() {
+  destroy();
+}
+
+ElfFile::ElfFile(ElfFile&& other)
+  : fd_(other.fd_),
+    file_(other.file_),
+    length_(other.length_),
+    baseAddress_(other.baseAddress_) {
+  other.fd_ = -1;
+  other.file_ = static_cast<char*>(MAP_FAILED);
+  other.length_ = 0;
+  other.baseAddress_ = 0;
+}
+
+ElfFile& ElfFile::operator=(ElfFile&& other) {
+  assert(this != &other);
+  destroy();
+
+  fd_ = other.fd_;
+  file_ = other.file_;
+  length_ = other.length_;
+  baseAddress_ = other.baseAddress_;
+
+  other.fd_ = -1;
+  other.file_ = static_cast<char*>(MAP_FAILED);
+  other.length_ = 0;
+  other.baseAddress_ = 0;
+
+  return *this;
+}
+
+void ElfFile::destroy() {
+  if (file_ != MAP_FAILED) {
+    munmap(file_, length_);
+  }
+
+  if (fd_ != -1) {
+    close(fd_);
+  }
+}
+
+void ElfFile::init() {
+  auto& elfHeader = this->elfHeader();
+
+  // Validate ELF magic numbers
+  FOLLY_SAFE_CHECK(elfHeader.e_ident[EI_MAG0] == ELFMAG0 &&
+                   elfHeader.e_ident[EI_MAG1] == ELFMAG1 &&
+                   elfHeader.e_ident[EI_MAG2] == ELFMAG2 &&
+                   elfHeader.e_ident[EI_MAG3] == ELFMAG3,
+                   "invalid ELF magic");
+
+  // Validate ELF class (32/64 bits)
+#define EXPECTED_CLASS P1(ELFCLASS, __ELF_NATIVE_CLASS)
+#define P1(a, b) P2(a, b)
+#define P2(a, b) a ## b
+  FOLLY_SAFE_CHECK(elfHeader.e_ident[EI_CLASS] == EXPECTED_CLASS,
+                   "invalid ELF class");
+#undef P1
+#undef P2
+#undef EXPECTED_CLASS
+
+  // Validate ELF data encoding (LSB/MSB)
+#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__
+# define EXPECTED_ENCODING ELFDATA2LSB
+#elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__
+# define EXPECTED_ENCODING ELFDATA2MSB
+#else
+# error Unsupported byte order
+#endif
+  FOLLY_SAFE_CHECK(elfHeader.e_ident[EI_DATA] == EXPECTED_ENCODING,
+                   "invalid ELF encoding");
+#undef EXPECTED_ENCODING
+
+  // Validate ELF version (1)
+  FOLLY_SAFE_CHECK(elfHeader.e_ident[EI_VERSION] == EV_CURRENT &&
+                   elfHeader.e_version == EV_CURRENT,
+                   "invalid ELF version");
+
+  // We only support executable and shared object files
+  FOLLY_SAFE_CHECK(elfHeader.e_type == ET_EXEC || elfHeader.e_type == ET_DYN,
+                   "invalid ELF file type");
+
+  FOLLY_SAFE_CHECK(elfHeader.e_phnum != 0, "no program header!");
+  FOLLY_SAFE_CHECK(elfHeader.e_phentsize == sizeof(ElfW(Phdr)),
+                   "invalid program header entry size");
+  FOLLY_SAFE_CHECK(elfHeader.e_shentsize == sizeof(ElfW(Shdr)),
+                   "invalid section header entry size");
+
+  const ElfW(Phdr)* programHeader = &at<ElfW(Phdr)>(elfHeader.e_phoff);
+  bool foundBase = false;
+  for (size_t i = 0; i < elfHeader.e_phnum; programHeader++, i++) {
+    // Program headers are sorted by load address, so the first PT_LOAD
+    // header gives us the base address.
+    if (programHeader->p_type == PT_LOAD) {
+      baseAddress_ = programHeader->p_vaddr;
+      foundBase = true;
+      break;
+    }
+  }
+
+  FOLLY_SAFE_CHECK(foundBase, "could not find base address");
+}
+
+const ElfW(Shdr)* ElfFile::getSectionByIndex(size_t idx) const {
+  FOLLY_SAFE_CHECK(idx < elfHeader().e_shnum, "invalid section index");
+  return &at<ElfW(Shdr)>(elfHeader().e_shoff + idx * sizeof(ElfW(Shdr)));
+}
+
+folly::StringPiece ElfFile::getSectionBody(const ElfW(Shdr)& section) const {
+  return folly::StringPiece(file_ + section.sh_offset, section.sh_size);
+}
+
+void ElfFile::validateStringTable(const ElfW(Shdr)& stringTable) const {
+  FOLLY_SAFE_CHECK(stringTable.sh_type == SHT_STRTAB,
+                   "invalid type for string table");
+
+  const char* start = file_ + stringTable.sh_offset;
+  // First and last bytes must be 0
+  FOLLY_SAFE_CHECK(stringTable.sh_size == 0 ||
+                   (start[0] == '\0' && start[stringTable.sh_size - 1] == '\0'),
+                   "invalid string table");
+}
+
+const char* ElfFile::getString(const ElfW(Shdr)& stringTable, size_t offset)
+  const {
+  validateStringTable(stringTable);
+  FOLLY_SAFE_CHECK(offset < stringTable.sh_size,
+                   "invalid offset in string table");
+
+  return file_ + stringTable.sh_offset + offset;
+}
+
+const char* ElfFile::getSectionName(const ElfW(Shdr)& section) const {
+  if (elfHeader().e_shstrndx == SHN_UNDEF) {
+    return nullptr;  // no section name string table
+  }
+
+  const ElfW(Shdr)& sectionNames = *getSectionByIndex(elfHeader().e_shstrndx);
+  return getString(sectionNames, section.sh_name);
+}
+
+const ElfW(Shdr)* ElfFile::getSectionByName(const char* name) const {
+  if (elfHeader().e_shstrndx == SHN_UNDEF) {
+    return nullptr;  // no section name string table
+  }
+
+  // Find offset in the section name string table of the requested name
+  const ElfW(Shdr)& sectionNames = *getSectionByIndex(elfHeader().e_shstrndx);
+  const char* foundName = iterateStrings(
+      sectionNames,
+      [&] (const char* s) { return !strcmp(name, s); });
+  if (foundName == nullptr) {
+    return nullptr;
+  }
+
+  size_t offset = foundName - (file_ + sectionNames.sh_offset);
+
+  // Find section with the appropriate sh_name offset
+  const ElfW(Shdr)* foundSection = iterateSections(
+    [&](const ElfW(Shdr)& sh) {
+      if (sh.sh_name == offset) {
+        return true;
+      }
+      return false;
+    });
+  return foundSection;
+}
+
+ElfFile::Symbol ElfFile::getDefinitionByAddress(uintptr_t address) const {
+  Symbol foundSymbol {nullptr, nullptr};
+
+  auto findSection = [&](const ElfW(Shdr)& section) {
+    auto findSymbols = [&](const ElfW(Sym)& sym) {
+      if (sym.st_shndx == SHN_UNDEF) {
+        return false;  // not a definition
+      }
+      if (address >= sym.st_value && address < sym.st_value + sym.st_size) {
+        foundSymbol.first = &section;
+        foundSymbol.second = &sym;
+        return true;
+      }
+
+      return false;
+    };
+
+    return iterateSymbolsWithType(section, STT_OBJECT, findSymbols) ||
+      iterateSymbolsWithType(section, STT_FUNC, findSymbols);
+  };
+
+  // Try the .dynsym section first if it exists, it's smaller.
+  (iterateSectionsWithType(SHT_DYNSYM, findSection) ||
+   iterateSectionsWithType(SHT_SYMTAB, findSection));
+
+  return foundSymbol;
+}
+
+ElfFile::Symbol ElfFile::getSymbolByName(const char* name) const {
+  Symbol foundSymbol{nullptr, nullptr};
+
+  auto findSection = [&](const ElfW(Shdr)& section) -> bool {
+    // This section has no string table associated w/ its symbols; hence we
+    // can't get names for them
+    if (section.sh_link == SHN_UNDEF) {
+      return false;
+    }
+
+    auto findSymbols = [&](const ElfW(Sym)& sym) -> bool {
+      if (sym.st_shndx == SHN_UNDEF) {
+        return false;  // not a definition
+      }
+      if (sym.st_name == 0) {
+        return false;  // no name for this symbol
+      }
+      const char* sym_name = getString(
+        *getSectionByIndex(section.sh_link), sym.st_name);
+      if (strcmp(sym_name, name) == 0) {
+        foundSymbol.first = &section;
+        foundSymbol.second = &sym;
+        return true;
+      }
+
+      return false;
+    };
+
+    return iterateSymbolsWithType(section, STT_OBJECT, findSymbols) ||
+      iterateSymbolsWithType(section, STT_FUNC, findSymbols);
+  };
+
+  // Try the .dynsym section first if it exists, it's smaller.
+  iterateSectionsWithType(SHT_DYNSYM, findSection) ||
+    iterateSectionsWithType(SHT_SYMTAB, findSection);
+
+  return foundSymbol;
+}
+
+const ElfW(Shdr)* ElfFile::getSectionContainingAddress(ElfW(Addr) addr) const {
+  return iterateSections([&](const ElfW(Shdr)& sh) -> bool {
+    return (addr >= sh.sh_addr) && (addr < (sh.sh_addr + sh.sh_size));
+  });
+}
+
+const char* ElfFile::getSymbolName(Symbol symbol) const {
+  if (!symbol.first || !symbol.second) {
+    return nullptr;
+  }
+
+  if (symbol.second->st_name == 0) {
+    return nullptr;  // symbol has no name
+  }
+
+  if (symbol.first->sh_link == SHN_UNDEF) {
+    return nullptr;  // symbol table has no strings
+  }
+
+  return getString(*getSectionByIndex(symbol.first->sh_link),
+                   symbol.second->st_name);
+}
+
+}  // namespace symbolizer
+}  // namespace folly
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/symbolizer/Elf.h
@@ -0,0 +1,239 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// ELF file parser
+
+#ifndef FOLLY_EXPERIMENTAL_SYMBOLIZER_ELF_H_
+#define FOLLY_EXPERIMENTAL_SYMBOLIZER_ELF_H_
+
+#include <stdio.h>
+#include <elf.h>
+#include <link.h>  // For ElfW()
+
+#include <stdexcept>
+#include <system_error>
+
+#include "folly/Conv.h"
+#include "folly/Likely.h"
+#include "folly/Range.h"
+#include "folly/SafeAssert.h"
+
+namespace folly {
+namespace symbolizer {
+
+/**
+ * ELF file parser.
+ *
+ * We handle native files only (32-bit files on a 32-bit platform, 64-bit files
+ * on a 64-bit platform), and only executables (ET_EXEC) and shared objects
+ * (ET_DYN).
+ */
+class ElfFile {
+ public:
+  ElfFile() noexcept;
+
+  // Note: may throw, call openNoThrow() explicitly if you don't want to throw
+  explicit ElfFile(const char* name, bool readOnly=true);
+
+  // Open the ELF file.
+  // Returns 0 on success, -1 (and sets errno) on failure and (if msg is not
+  // NULL) sets *msg to a static string indicating what failed.
+  int openNoThrow(const char* name, bool readOnly=true,
+                  const char** msg=nullptr) noexcept;
+
+  // Open the ELF file. Throws on error.
+  void open(const char* name, bool readOnly=true);
+
+  ~ElfFile();
+
+  ElfFile(ElfFile&& other);
+  ElfFile& operator=(ElfFile&& other);
+
+  /** Retrieve the ELF header */
+  const ElfW(Ehdr)& elfHeader() const {
+    return at<ElfW(Ehdr)>(0);
+  }
+
+  /**
+   * Get the base address, the address where the file should be loaded if
+   * no relocations happened.
+   */
+  uintptr_t getBaseAddress() const {
+    return baseAddress_;
+  }
+
+  /** Find a section given its name */
+  const ElfW(Shdr)* getSectionByName(const char* name) const;
+
+  /** Find a section given its index in the section header table */
+  const ElfW(Shdr)* getSectionByIndex(size_t idx) const;
+
+  /** Retrieve the name of a section */
+  const char* getSectionName(const ElfW(Shdr)& section) const;
+
+  /** Get the actual section body */
+  folly::StringPiece getSectionBody(const ElfW(Shdr)& section) const;
+
+  /** Retrieve a string from a string table section */
+  const char* getString(const ElfW(Shdr)& stringTable, size_t offset) const;
+
+  /**
+   * Iterate over all strings in a string table section for as long as
+   * fn(str) returns false.
+   * Returns the current ("found") string when fn returned true, or nullptr
+   * if fn returned false for all strings in the table.
+   */
+  template <class Fn>
+  const char* iterateStrings(const ElfW(Shdr)& stringTable, Fn fn) const;
+
+  /**
+   * Iterate over all sections for as long as fn(section) returns false.
+   * Returns a pointer to the current ("found") section when fn returned
+   * true, or nullptr if fn returned false for all sections.
+   */
+  template <class Fn>
+  const ElfW(Shdr)* iterateSections(Fn fn) const;
+
+  /**
+   * Iterate over all sections with a given type.  Similar to
+   * iterateSections(), but filtered only for sections with the given type.
+   */
+  template <class Fn>
+  const ElfW(Shdr)* iterateSectionsWithType(uint32_t type, Fn fn) const;
+
+  /**
+   * Iterate over all symbols witin a given section.
+   *
+   * Returns a pointer to the current ("found") symbol when fn returned true,
+   * or nullptr if fn returned false for all symbols.
+   */
+  template <class Fn>
+  const ElfW(Sym)* iterateSymbols(const ElfW(Shdr)& section, Fn fn) const;
+  template <class Fn>
+  const ElfW(Sym)* iterateSymbolsWithType(const ElfW(Shdr)& section,
+                                          uint32_t type, Fn fn) const;
+
+  /**
+   * Find symbol definition by address.
+   * Note that this is the file virtual address, so you need to undo
+   * any relocation that might have happened.
+   *
+   * Returns {nullptr, nullptr} if not found.
+   */
+  typedef std::pair<const ElfW(Shdr)*, const ElfW(Sym)*> Symbol;
+  Symbol getDefinitionByAddress(uintptr_t address) const;
+
+  /**
+   * Find symbol definition by name.
+   *
+   * If a symbol with this name cannot be found, a <nullptr, nullptr> Symbol
+   * will be returned. This is O(N) in the number of symbols in the file.
+   *
+   * Returns {nullptr, nullptr} if not found.
+   */
+  Symbol getSymbolByName(const char* name) const;
+
+  /**
+   * Get the value of a symbol.
+   */
+  template <class T>
+  const T& getSymbolValue(const ElfW(Sym)* symbol) const {
+    const ElfW(Shdr)* section = getSectionByIndex(symbol->st_shndx);
+    FOLLY_SAFE_CHECK(section, "Symbol's section index is invalid");
+
+    return valueAt<T>(*section, symbol->st_value);
+  }
+
+  /**
+   * Get the value of the object stored at the given address.
+   *
+   * This is the function that you want to use in conjunction with
+   * getSymbolValue() to follow pointers. For example, to get the value of
+   * a char* symbol, you'd do something like this:
+   *
+   *  auto sym = getSymbolByName("someGlobalValue");
+   *  auto addr = getSymbolValue<ElfW(Addr)>(sym.second);
+   *  const char* str = &getSymbolValue<const char>(addr);
+   */
+  template <class T>
+  const T& getAddressValue(const ElfW(Addr) addr) const {
+    const ElfW(Shdr)* section = getSectionContainingAddress(addr);
+    FOLLY_SAFE_CHECK(section, "Address does not refer to existing section");
+
+    return valueAt<T>(*section, addr);
+  }
+
+  /**
+   * Retrieve symbol name.
+   */
+  const char* getSymbolName(Symbol symbol) const;
+
+  /** Find the section containing the given address */
+  const ElfW(Shdr)* getSectionContainingAddress(ElfW(Addr) addr) const;
+
+ private:
+  void init();
+  void destroy();
+  ElfFile(const ElfFile&) = delete;
+  ElfFile& operator=(const ElfFile&) = delete;
+
+  void validateStringTable(const ElfW(Shdr)& stringTable) const;
+
+  template <class T>
+  const typename std::enable_if<std::is_pod<T>::value, T>::type&
+  at(ElfW(Off) offset) const {
+    FOLLY_SAFE_CHECK(offset + sizeof(T) <= length_,
+                     "Offset is not contained within our mmapped file");
+
+    return *reinterpret_cast<T*>(file_ + offset);
+  }
+
+  template <class T>
+  const T& valueAt(const ElfW(Shdr)& section, const ElfW(Addr) addr) const {
+    // For exectuables and shared objects, st_value holds a virtual address
+    // that refers to the memory owned by sections. Since we didn't map the
+    // sections into the addresses that they're expecting (sh_addr), but
+    // instead just mmapped the entire file directly, we need to translate
+    // between addresses and offsets into the file.
+    //
+    // TODO: For other file types, st_value holds a file offset directly. Since
+    //       I don't have a use-case for that right now, just assert that
+    //       nobody wants this. We can always add it later.
+    FOLLY_SAFE_CHECK(
+        elfHeader().e_type == ET_EXEC || elfHeader().e_type == ET_DYN,
+        "Only exectuables and shared objects are supported");
+    FOLLY_SAFE_CHECK(
+        addr >= section.sh_addr &&
+        (addr + sizeof(T)) <= (section.sh_addr + section.sh_size),
+        "Address is not contained within the provided segment");
+
+    return at<T>(section.sh_offset + (addr - section.sh_addr));
+  }
+
+  int fd_;
+  char* file_;     // mmap() location
+  size_t length_;  // mmap() length
+
+  uintptr_t baseAddress_;
+};
+
+}  // namespace symbolizer
+}  // namespace folly
+
+#include "folly/experimental/symbolizer/Elf-inl.h"
+
+#endif /* FOLLY_EXPERIMENTAL_SYMBOLIZER_ELF_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/symbolizer/Elf-inl.h
@@ -0,0 +1,93 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+#ifndef FOLLY_EXPERIMENTAL_SYMBOLIZER_ELF_H_
+# error This file must be included from Elf.h
+#endif
+
+namespace folly {
+namespace symbolizer {
+
+template <class Fn>
+const ElfW(Shdr)* ElfFile::iterateSections(Fn fn) const {
+  const ElfW(Shdr)* ptr = &at<ElfW(Shdr)>(elfHeader().e_shoff);
+  for (size_t i = 0; i < elfHeader().e_shnum; i++, ptr++) {
+    if (fn(*ptr)) {
+      return ptr;
+    }
+  }
+
+  return nullptr;
+}
+
+template <class Fn>
+const ElfW(Shdr)* ElfFile::iterateSectionsWithType(uint32_t type, Fn fn)
+  const {
+  return iterateSections(
+      [&](const ElfW(Shdr)& sh) {
+        return sh.sh_type == type && fn(sh);
+      });
+}
+
+template <class Fn>
+const char* ElfFile::iterateStrings(const ElfW(Shdr)& stringTable, Fn fn)
+  const {
+  validateStringTable(stringTable);
+
+  const char* start = file_ + stringTable.sh_offset;
+  const char* end = start + stringTable.sh_size;
+
+  const char* ptr = start;
+  while (ptr != end && !fn(ptr)) {
+    ptr += strlen(ptr) + 1;
+  }
+
+  return ptr != end ? ptr : nullptr;
+}
+
+template <class Fn>
+const ElfW(Sym)* ElfFile::iterateSymbols(const ElfW(Shdr)& section, Fn fn)
+  const {
+  FOLLY_SAFE_CHECK(section.sh_entsize == sizeof(ElfW(Sym)),
+                   "invalid entry size in symbol table");
+
+  const ElfW(Sym)* sym = &at<ElfW(Sym)>(section.sh_offset);
+  const ElfW(Sym)* end = sym + (section.sh_size / section.sh_entsize);
+
+  while (sym < end) {
+    if (fn(*sym)) {
+      return sym;
+    }
+
+    ++sym;
+  }
+
+  return nullptr;
+}
+
+template <class Fn>
+const ElfW(Sym)* ElfFile::iterateSymbolsWithType(const ElfW(Shdr)& section,
+                                                 uint32_t type, Fn fn) const {
+  // N.B. st_info has the same representation on 32- and 64-bit platforms
+  return iterateSymbols(section, [&](const ElfW(Sym)& sym) -> bool {
+    return ELF32_ST_TYPE(sym.st_info) == type && fn(sym);
+  });
+}
+
+}  // namespace symbolizer
+}  // namespace folly
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/symbolizer/ElfUtil.cpp
@@ -0,0 +1,49 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+#include "folly/experimental/symbolizer/Elf.h"
+
+#include <stdio.h>
+
+#include <gflags/gflags.h>
+#include <glog/logging.h>
+
+using namespace folly;
+using namespace folly::symbolizer;
+
+int main(int argc, char *argv[]) {
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  CHECK_GE(argc, 2);
+
+  ElfFile elf(argv[1]);
+
+  if (argc > 2) {
+    auto section = elf.getSectionByName(argv[2]);
+    printf("Section %s: %s\n",
+           argv[2],
+           (section ? "found" : "not found"));
+  }
+
+  auto sym = elf.getDefinitionByAddress(reinterpret_cast<uintptr_t>(main));
+  if (sym.first) {
+    printf("found %s\n", elf.getSymbolName(sym));
+  } else {
+    printf("main not found\n");
+  }
+
+  return 0;
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/symbolizer/LineReader.cpp
@@ -0,0 +1,74 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/experimental/symbolizer/LineReader.h"
+
+#include <cstring>
+
+#include "folly/FileUtil.h"
+
+namespace folly { namespace symbolizer {
+
+LineReader::LineReader(int fd, char* buf, size_t bufSize)
+  : fd_(fd),
+    buf_(buf),
+    bufEnd_(buf_ + bufSize),
+    bol_(buf),
+    eol_(buf),
+    end_(buf),
+    state_(kReading) {
+}
+
+LineReader::State LineReader::readLine(StringPiece& line) {
+  bol_ = eol_;  // Start past what we already returned
+  for (;;) {
+    // Search for newline
+    char* newline = static_cast<char*>(memchr(eol_, '\n', end_ - eol_));
+    if (newline) {
+      eol_ = newline + 1;
+      break;
+    } else if (state_ != kReading || (bol_ == buf_ && end_ == bufEnd_)) {
+      // If the buffer is full with one line (line too long), or we're
+      // at the end of the file, return what we have.
+      eol_ = end_;
+      break;
+    }
+
+    // We don't have a full line in the buffer, but we have room to read.
+    // Move to the beginning of the buffer.
+    memmove(buf_, eol_, end_ - eol_);
+    end_ -= (eol_ - buf_);
+    bol_ = buf_;
+    eol_ = end_;
+
+    // Refill
+    ssize_t available = bufEnd_ - end_;
+    ssize_t n = readFull(fd_, end_, available);
+    if (n < 0) {
+      state_ = kError;
+      n = 0;
+    } else if (n < available) {
+      state_ = kEof;
+    }
+    end_ += n;
+  }
+
+  line.assign(bol_, eol_);
+  return eol_ != bol_ ? kReading : state_;
+}
+
+}}  // namespaces
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/symbolizer/LineReader.h
@@ -0,0 +1,91 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_SYMBOLIZER_LINEREADER_H_
+#define FOLLY_SYMBOLIZER_LINEREADER_H_
+
+#include <cstddef>
+
+#include <boost/noncopyable.hpp>
+
+#include "folly/Range.h"
+
+namespace folly { namespace symbolizer {
+
+/**
+ * Async-signal-safe line reader.
+ */
+class LineReader : private boost::noncopyable {
+ public:
+  /**
+   * Create a line reader that reads into a user-provided buffer (of size
+   * bufSize).
+   */
+  LineReader(int fd, char* buf, size_t bufSize);
+
+  enum State {
+    kReading,
+    kEof,
+    kError
+  };
+  /**
+   * Read the next line from the file.
+   *
+   * If the line is at most bufSize characters long, including the trailing
+   * newline, it will be returned (including the trailing newline).
+   *
+   * If the line is longer than bufSize, we return the first bufSize bytes
+   * (which won't include a trailing newline) and then continue from that
+   * point onwards.
+   *
+   * The lines returned are not null-terminated.
+   *
+   * Returns kReading with a valid line, kEof if at end of file, or kError
+   * if a read error was encountered.
+   *
+   * Example:
+   *   bufSize = 10
+   *   input has "hello world\n"
+   *   The first call returns "hello worl"
+   *   The second call returns "d\n"
+   */
+  State readLine(StringPiece& line);
+
+ private:
+  int const fd_;
+  char* const buf_;
+  char* const bufEnd_;
+
+  // buf_ <= bol_ <= eol_ <= end_ <= bufEnd_
+  //
+  // [buf_, end_): current buffer contents (read from file)
+  //
+  // [buf_, bol_): free (already processed, can be discarded)
+  // [bol_, eol_): current line, including \n if it exists, eol_ points
+  //               1 character past the \n
+  // [eol_, end_): read, unprocessed
+  // [end_, bufEnd_): free
+
+  char* bol_;
+  char* eol_;
+  char* end_;
+  State state_;
+};
+
+}}  // namespaces
+
+#endif /* FOLLY_SYMBOLIZER_LINEREADER_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/symbolizer/SignalHandler.cpp
@@ -0,0 +1,320 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// This is heavily inspired by the signal handler from google-glog
+
+#include "folly/experimental/symbolizer/SignalHandler.h"
+
+#include <sys/types.h>
+#include <atomic>
+#include <ctime>
+#include <mutex>
+#include <pthread.h>
+#include <signal.h>
+#include <unistd.h>
+#include <vector>
+
+#include <glog/logging.h>
+
+#include "folly/Conv.h"
+#include "folly/FileUtil.h"
+#include "folly/Portability.h"
+#include "folly/ScopeGuard.h"
+#include "folly/experimental/symbolizer/Symbolizer.h"
+
+namespace folly { namespace symbolizer {
+
+namespace {
+
+/**
+ * Fatal signal handler registry.
+ */
+class FatalSignalCallbackRegistry {
+ public:
+  FatalSignalCallbackRegistry();
+
+  void add(SignalCallback func);
+  void markInstalled();
+  void run();
+
+ private:
+  std::atomic<bool> installed_;
+  std::mutex mutex_;
+  std::vector<SignalCallback> handlers_;
+};
+
+FatalSignalCallbackRegistry::FatalSignalCallbackRegistry()
+  : installed_(false) {
+}
+
+void FatalSignalCallbackRegistry::add(SignalCallback func) {
+  std::lock_guard<std::mutex> lock(mutex_);
+  CHECK(!installed_)
+    << "FatalSignalCallbackRegistry::add may not be used "
+       "after installing the signal handlers.";
+  handlers_.push_back(func);
+}
+
+void FatalSignalCallbackRegistry::markInstalled() {
+  std::lock_guard<std::mutex> lock(mutex_);
+  CHECK(!installed_.exchange(true))
+    << "FatalSignalCallbackRegistry::markInstalled must be called "
+    << "at most once";
+}
+
+void FatalSignalCallbackRegistry::run() {
+  if (!installed_) {
+    return;
+  }
+
+  for (auto& fn : handlers_) {
+    fn();
+  }
+}
+
+// Leak it so we don't have to worry about destruction order
+FatalSignalCallbackRegistry* gFatalSignalCallbackRegistry =
+  new FatalSignalCallbackRegistry;
+
+struct {
+  int number;
+  const char* name;
+  struct sigaction oldAction;
+} kFatalSignals[] = {
+  { SIGSEGV, "SIGSEGV" },
+  { SIGILL,  "SIGILL"  },
+  { SIGFPE,  "SIGFPE"  },
+  { SIGABRT, "SIGABRT" },
+  { SIGBUS,  "SIGBUS"  },
+  { SIGTERM, "SIGTERM" },
+  { 0,       nullptr   }
+};
+
+void callPreviousSignalHandler(int signum) {
+  // Restore disposition to old disposition, then kill ourselves with the same
+  // signal. The signal will be blocked until we return from our handler,
+  // then it will invoke the default handler and abort.
+  for (auto p = kFatalSignals; p->name; ++p) {
+    if (p->number == signum) {
+      sigaction(signum, &p->oldAction, nullptr);
+      raise(signum);
+      return;
+    }
+  }
+
+  // Not one of the signals we know about. Oh well. Reset to default.
+  struct sigaction sa;
+  memset(&sa, 0, sizeof(sa));
+  sa.sa_handler = SIG_DFL;
+  sigaction(signum, &sa, nullptr);
+  raise(signum);
+}
+
+void printDec(uint64_t val) {
+  char buf[20];
+  uint32_t n = uint64ToBufferUnsafe(val, buf);
+  writeFull(STDERR_FILENO, buf, n);
+}
+
+const char kHexChars[] = "0123456789abcdef";
+void printHex(uint64_t val) {
+  // TODO(tudorb): Add this to folly/Conv.h
+  char buf[2 + 2 * sizeof(uint64_t)];  // "0x" prefix, 2 digits for each byte
+
+  char* end = buf + sizeof(buf);
+  char* p = end;
+  do {
+    *--p = kHexChars[val & 0x0f];
+    val >>= 4;
+  } while (val != 0);
+  *--p = 'x';
+  *--p = '0';
+
+  writeFull(STDERR_FILENO, p, end - p);
+}
+
+void print(StringPiece sp) {
+  writeFull(STDERR_FILENO, sp.data(), sp.size());
+}
+
+void dumpTimeInfo() {
+  SCOPE_EXIT { fsyncNoInt(STDERR_FILENO); };
+  time_t now = time(nullptr);
+  print("*** Aborted at ");
+  printDec(now);
+  print(" (Unix time, try 'date -d @");
+  printDec(now);
+  print("') ***\n");
+}
+
+void dumpSignalInfo(int signum, siginfo_t* siginfo) {
+  SCOPE_EXIT { fsyncNoInt(STDERR_FILENO); };
+  // Get the signal name, if possible.
+  const char* name = nullptr;
+  for (auto p = kFatalSignals; p->name; ++p) {
+    if (p->number == signum) {
+      name = p->name;
+      break;
+    }
+  }
+
+  print("*** Signal ");
+  printDec(signum);
+  if (name) {
+    print(" (");
+    print(name);
+    print(")");
+  }
+
+  print(" (");
+  printHex(reinterpret_cast<uint64_t>(siginfo->si_addr));
+  print(") received by PID ");
+  printDec(getpid());
+  print(" (TID ");
+  printHex((uint64_t)pthread_self());
+  print("), stack trace: ***\n");
+}
+
+namespace {
+constexpr size_t kDefaultCapacity = 500;
+
+// Note: not thread-safe, but that's okay, as we only let one thread
+// in our signal handler at a time.
+//
+// Leak it so we don't have to worry about destruction order
+auto gSignalSafeElfCache = new SignalSafeElfCache(kDefaultCapacity);
+}  // namespace
+
+void dumpStackTrace(bool symbolize) __attribute__((noinline));
+
+void dumpStackTrace(bool symbolize) {
+  SCOPE_EXIT { fsyncNoInt(STDERR_FILENO); };
+  // Get and symbolize stack trace
+  constexpr size_t kMaxStackTraceDepth = 100;
+  FrameArray<kMaxStackTraceDepth> addresses;
+
+  // Skip the getStackTrace frame
+  if (!getStackTraceSafe(addresses)) {
+    print("(error retrieving stack trace)\n");
+  } else if (symbolize) {
+    Symbolizer symbolizer(gSignalSafeElfCache);
+    symbolizer.symbolize(addresses);
+
+    FDSymbolizePrinter printer(STDERR_FILENO, SymbolizePrinter::COLOR_IF_TTY);
+
+    // Skip the top 2 frames:
+    // getStackTraceSafe
+    // dumpStackTrace (here)
+    //
+    // Leaving signalHandler on the stack for clarity, I think.
+    printer.println(addresses, 2);
+  } else {
+    print("(safe mode, symbolizer not available)\n");
+    AddressFormatter formatter;
+    for (ssize_t i = 0; i < addresses.frameCount; ++i) {
+      print(formatter.format(addresses.addresses[i]));
+      print("\n");
+    }
+  }
+}
+
+// On Linux, pthread_t is a pointer, so 0 is an invalid value, which we
+// take to indicate "no thread in the signal handler".
+//
+// POSIX defines PTHREAD_NULL for this purpose, but that's not available.
+constexpr pthread_t kInvalidThreadId = 0;
+
+std::atomic<pthread_t> gSignalThread(kInvalidThreadId);
+std::atomic<bool> gInRecursiveSignalHandler(false);
+
+// Here be dragons.
+void innerSignalHandler(int signum, siginfo_t* info, void* uctx) {
+  // First, let's only let one thread in here at a time.
+  pthread_t myId = pthread_self();
+
+  pthread_t prevSignalThread = kInvalidThreadId;
+  while (!gSignalThread.compare_exchange_strong(prevSignalThread, myId)) {
+    if (pthread_equal(prevSignalThread, myId)) {
+      // First time here. Try to dump the stack trace without symbolization.
+      // If we still fail, well, we're mightily screwed, so we do nothing the
+      // next time around.
+      if (!gInRecursiveSignalHandler.exchange(true)) {
+        print("Entered fatal signal handler recursively. We're in trouble.\n");
+        dumpStackTrace(false);  // no symbolization
+      }
+      return;
+    }
+
+    // Wait a while, try again.
+    timespec ts;
+    ts.tv_sec = 0;
+    ts.tv_nsec = 100L * 1000 * 1000;  // 100ms
+    nanosleep(&ts, nullptr);
+
+    prevSignalThread = kInvalidThreadId;
+  }
+
+  dumpTimeInfo();
+  dumpSignalInfo(signum, info);
+  dumpStackTrace(true);  // with symbolization
+
+  // Run user callbacks
+  gFatalSignalCallbackRegistry->run();
+}
+
+void signalHandler(int signum, siginfo_t* info, void* uctx) {
+  SCOPE_EXIT { fsyncNoInt(STDERR_FILENO); };
+  innerSignalHandler(signum, info, uctx);
+
+  gSignalThread = kInvalidThreadId;
+  // Kill ourselves with the previous handler.
+  callPreviousSignalHandler(signum);
+}
+
+}  // namespace
+
+void addFatalSignalCallback(SignalCallback cb) {
+  gFatalSignalCallbackRegistry->add(cb);
+}
+
+void installFatalSignalCallbacks() {
+  gFatalSignalCallbackRegistry->markInstalled();
+}
+
+namespace {
+
+std::atomic<bool> gAlreadyInstalled;
+
+}  // namespace
+
+void installFatalSignalHandler() {
+  if (gAlreadyInstalled.exchange(true)) {
+    // Already done.
+    return;
+  }
+
+  struct sigaction sa;
+  memset(&sa, 0, sizeof(sa));
+  sigemptyset(&sa.sa_mask);
+  sa.sa_flags |= SA_SIGINFO;
+  sa.sa_sigaction = &signalHandler;
+
+  for (auto p = kFatalSignals; p->name; ++p) {
+    CHECK_ERR(sigaction(p->number, &sa, &p->oldAction));
+  }
+}
+
+}}  // namespaces
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/symbolizer/SignalHandler.h
@@ -0,0 +1,56 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_SYMBOLIZER_SIGNALHANDLER_H_
+#define FOLLY_SYMBOLIZER_SIGNALHANDLER_H_
+
+#include <functional>
+
+namespace folly { namespace symbolizer {
+
+/**
+ * Install handler for fatal signals. The list of signals being handled is in
+ * SignalHandler.cpp.
+ *
+ * The handler will dump signal and time information followed by a stack trace
+ * to stderr, and then call the callbacks registered below.
+ */
+void installFatalSignalHandler();
+
+/**
+ * Add a callback to be run when receiving a fatal signal. They will also
+ * be called by LOG(FATAL) and abort() (as those raise SIGABRT internally).
+ *
+ * These callbacks must be async-signal-safe, so don't even think of using
+ * LOG(...) or printf or malloc / new or doing anything even remotely fun.
+ *
+ * All these fatal callback must be added before calling
+ * installFatalSignalCallbacks(), below.
+ */
+typedef void (*SignalCallback)(void);
+void addFatalSignalCallback(SignalCallback callback);
+
+/**
+ * Install the fatal signal callbacks; fatal signals will call these
+ * callbacks in the order in which they were added.
+ */
+void installFatalSignalCallbacks();
+
+
+}}  // namespaces
+
+#endif /* FOLLY_SYMBOLIZER_SIGNALHANDLER_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/symbolizer/StackTrace.cpp
@@ -0,0 +1,85 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// Must be first to ensure that UNW_LOCAL_ONLY is defined
+#define UNW_LOCAL_ONLY 1
+#include <libunwind.h>
+
+#include "folly/experimental/symbolizer/StackTrace.h"
+
+namespace folly { namespace symbolizer {
+
+ssize_t getStackTrace(uintptr_t* addresses, size_t maxAddresses) {
+  static_assert(sizeof(uintptr_t) == sizeof(void*),
+                "uinptr_t / pointer size mismatch");
+  // The libunwind documentation says that unw_backtrace is async-signal-safe
+  // but, as of libunwind 1.0.1, it isn't (tdep_trace allocates memory on
+  // x86_64)
+  int r = unw_backtrace(reinterpret_cast<void**>(addresses), maxAddresses);
+  return r < 0 ? -1 : r;
+}
+
+namespace {
+inline bool getFrameInfo(unw_cursor_t* cursor, uintptr_t& ip) {
+  unw_word_t uip;
+  if (unw_get_reg(cursor, UNW_REG_IP, &uip) < 0) {
+    return false;
+  }
+  int r = unw_is_signal_frame(cursor);
+  if (r < 0) {
+    return false;
+  }
+  // Use previous instruction in normal (call) frames (because the
+  // return address might not be in the same function for noreturn functions)
+  // but not in signal frames.
+  ip = uip - (r == 0);
+  return true;
+}
+}  // namespace
+
+ssize_t getStackTraceSafe(uintptr_t* addresses, size_t maxAddresses) {
+  if (maxAddresses == 0) {
+    return 0;
+  }
+  unw_context_t context;
+  if (unw_getcontext(&context) < 0) {
+    return -1;
+  }
+  unw_cursor_t cursor;
+  if (unw_init_local(&cursor, &context) < 0) {
+    return -1;
+  }
+  if (!getFrameInfo(&cursor, *addresses)) {
+    return -1;
+  }
+  ++addresses;
+  ssize_t count = 1;
+  for (; count != maxAddresses; ++count, ++addresses) {
+    int r = unw_step(&cursor);
+    if (r < 0) {
+      return -1;
+    }
+    if (r == 0) {
+      break;
+    }
+    if (!getFrameInfo(&cursor, *addresses)) {
+      return -1;
+    }
+  }
+  return count;
+}
+
+}}  // namespaces
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/symbolizer/StackTrace.h
@@ -0,0 +1,50 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_SYMBOLIZER_STACKTRACE_H_
+#define FOLLY_SYMBOLIZER_STACKTRACE_H_
+
+#include <cstdint>
+#include <cstdlib>
+
+namespace folly { namespace symbolizer {
+
+/**
+ * Get the current stack trace into addresses, which has room for at least
+ * maxAddresses frames.
+ *
+ * Returns the number of frames written in the array.
+ * Returns -1 on failure.
+ *
+ * NOT async-signal-safe, but fast.
+ */
+ssize_t getStackTrace(uintptr_t* addresses, size_t maxAddresses);
+
+/**
+ * Get the current stack trace into addresses, which has room for at least
+ * maxAddresses frames.
+ *
+ * Returns the number of frames written in the array.
+ * Returns -1 on failure.
+ *
+ * Async-signal-safe, but likely slower.
+ */
+ssize_t getStackTraceSafe(uintptr_t* addresses, size_t maxAddresses);
+
+}}  // namespaces
+
+#endif /* FOLLY_SYMBOLIZER_STACKTRACE_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/symbolizer/Symbolizer.cpp
@@ -0,0 +1,489 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/experimental/symbolizer/Symbolizer.h"
+
+#include <limits.h>
+#include <cstdio>
+#include <iostream>
+#include <map>
+
+#ifdef __GNUC__
+#include <ext/stdio_filebuf.h>
+#include <ext/stdio_sync_filebuf.h>
+#endif
+
+#include "folly/Conv.h"
+#include "folly/FileUtil.h"
+#include "folly/ScopeGuard.h"
+#include "folly/String.h"
+
+#include "folly/experimental/symbolizer/Elf.h"
+#include "folly/experimental/symbolizer/Dwarf.h"
+#include "folly/experimental/symbolizer/LineReader.h"
+
+
+namespace folly {
+namespace symbolizer {
+
+namespace {
+
+/**
+ * Read a hex value.
+ */
+uintptr_t readHex(StringPiece& sp) {
+  uintptr_t val = 0;
+  const char* p = sp.begin();
+  for (; p != sp.end(); ++p) {
+    unsigned int v;
+    if (*p >= '0' && *p <= '9') {
+      v = (*p - '0');
+    } else if (*p >= 'a' && *p <= 'f') {
+      v = (*p - 'a') + 10;
+    } else if (*p >= 'A' && *p <= 'F') {
+      v = (*p - 'A') + 10;
+    } else {
+      break;
+    }
+    val = (val << 4) + v;
+  }
+  sp.assign(p, sp.end());
+  return val;
+}
+
+/**
+ * Skip over non-space characters.
+ */
+void skipNS(StringPiece& sp) {
+  const char* p = sp.begin();
+  for (; p != sp.end() && (*p != ' ' && *p != '\t'); ++p) { }
+  sp.assign(p, sp.end());
+}
+
+/**
+ * Skip over space and tab characters.
+ */
+void skipWS(StringPiece& sp) {
+  const char* p = sp.begin();
+  for (; p != sp.end() && (*p == ' ' || *p == '\t'); ++p) { }
+  sp.assign(p, sp.end());
+}
+
+/**
+ * Parse a line from /proc/self/maps
+ */
+bool parseProcMapsLine(StringPiece line,
+                       uintptr_t& from, uintptr_t& to,
+                       StringPiece& fileName) {
+  // from     to       perm offset   dev   inode             path
+  // 00400000-00405000 r-xp 00000000 08:03 35291182          /bin/cat
+  if (line.empty()) {
+    return false;
+  }
+
+  // Remove trailing newline, if any
+  if (line.back() == '\n') {
+    line.pop_back();
+  }
+
+  // from
+  from = readHex(line);
+  if (line.empty() || line.front() != '-') {
+    return false;
+  }
+  line.pop_front();
+
+  // to
+  to = readHex(line);
+  if (line.empty() || line.front() != ' ') {
+    return false;
+  }
+  line.pop_front();
+
+  // perms
+  skipNS(line);
+  if (line.empty() || line.front() != ' ') {
+    return false;
+  }
+  line.pop_front();
+
+  uintptr_t fileOffset = readHex(line);
+  if (line.empty() || line.front() != ' ') {
+    return false;
+  }
+  line.pop_front();
+  if (fileOffset != 0) {
+    return false;  // main mapping starts at 0
+  }
+
+  // dev
+  skipNS(line);
+  if (line.empty() || line.front() != ' ') {
+    return false;
+  }
+  line.pop_front();
+
+  // inode
+  skipNS(line);
+  if (line.empty() || line.front() != ' ') {
+    return false;
+  }
+
+  skipWS(line);
+  if (line.empty()) {
+    fileName.clear();
+    return true;
+  }
+
+  fileName = line;
+  return true;
+}
+
+ElfCache* defaultElfCache() {
+  static constexpr size_t defaultCapacity = 500;
+  static ElfCache cache(defaultCapacity);
+  return &cache;
+}
+
+}  // namespace
+
+void SymbolizedFrame::set(const std::shared_ptr<ElfFile>& file,
+                          uintptr_t address) {
+  clear();
+  found = true;
+
+  address += file->getBaseAddress();
+  auto sym = file->getDefinitionByAddress(address);
+  if (!sym.first) {
+    return;
+  }
+
+  file_ = file;
+  auto name = file->getSymbolName(sym);
+  if (name) {
+    this->name = name;
+  }
+
+  Dwarf(file.get()).findAddress(address, location);
+}
+
+
+Symbolizer::Symbolizer(ElfCacheBase* cache)
+  : cache_(cache ?: defaultElfCache()) {
+}
+
+void Symbolizer::symbolize(const uintptr_t* addresses,
+                           SymbolizedFrame* frames,
+                           size_t addressCount) {
+  size_t remaining = 0;
+  for (size_t i = 0; i < addressCount; ++i) {
+    auto& frame = frames[i];
+    if (!frame.found) {
+      ++remaining;
+      frame.clear();
+    }
+  }
+
+  if (remaining == 0) {  // we're done
+    return;
+  }
+
+  int fd = openNoInt("/proc/self/maps", O_RDONLY);
+  if (fd == -1) {
+    return;
+  }
+
+  char buf[PATH_MAX + 100];  // Long enough for any line
+  LineReader reader(fd, buf, sizeof(buf));
+
+  while (remaining != 0) {
+    StringPiece line;
+    if (reader.readLine(line) != LineReader::kReading) {
+      break;
+    }
+
+    // Parse line
+    uintptr_t from;
+    uintptr_t to;
+    StringPiece fileName;
+    if (!parseProcMapsLine(line, from, to, fileName)) {
+      continue;
+    }
+
+    bool first = true;
+    std::shared_ptr<ElfFile> elfFile;
+
+    // See if any addresses are here
+    for (size_t i = 0; i < addressCount; ++i) {
+      auto& frame = frames[i];
+      if (frame.found) {
+        continue;
+      }
+
+      uintptr_t address = addresses[i];
+
+      if (from > address || address >= to) {
+        continue;
+      }
+
+      // Found
+      frame.found = true;
+      --remaining;
+
+      // Open the file on first use
+      if (first) {
+        first = false;
+        elfFile = cache_->getFile(fileName);
+      }
+
+      if (!elfFile) {
+        continue;
+      }
+
+      // Undo relocation
+      frame.set(elfFile, address - from);
+    }
+  }
+
+  closeNoInt(fd);
+}
+
+namespace {
+const char kHexChars[] = "0123456789abcdef";
+const SymbolizePrinter::Color kAddressColor = SymbolizePrinter::Color::BLUE;
+const SymbolizePrinter::Color kFunctionColor = SymbolizePrinter::Color::PURPLE;
+const SymbolizePrinter::Color kFileColor = SymbolizePrinter::Color::DEFAULT;
+}  // namespace
+
+constexpr char AddressFormatter::bufTemplate[];
+
+AddressFormatter::AddressFormatter() {
+  memcpy(buf_, bufTemplate, sizeof(buf_));
+}
+
+folly::StringPiece AddressFormatter::format(uintptr_t address) {
+  // Can't use sprintf, not async-signal-safe
+  static_assert(sizeof(uintptr_t) <= 8, "huge uintptr_t?");
+  char* end = buf_ + sizeof(buf_) - 1 - (16 - 2 * sizeof(uintptr_t));
+  char* p = end;
+  *p-- = '\0';
+  while (address != 0) {
+    *p-- = kHexChars[address & 0xf];
+    address >>= 4;
+  }
+
+  return folly::StringPiece(buf_, end);
+}
+
+void SymbolizePrinter::print(uintptr_t address, const SymbolizedFrame& frame) {
+  if (options_ & TERSE) {
+    printTerse(address, frame);
+    return;
+  }
+
+  SCOPE_EXIT { color(Color::DEFAULT); };
+
+  color(kAddressColor);
+
+  AddressFormatter formatter;
+  doPrint(formatter.format(address));
+
+  const char padBuf[] = "                       ";
+  folly::StringPiece pad(padBuf,
+                         sizeof(padBuf) - 1 - (16 - 2 * sizeof(uintptr_t)));
+
+  color(kFunctionColor);
+  char mangledBuf[1024];
+  if (!frame.found) {
+    doPrint(" (not found)");
+    return;
+  }
+
+  if (frame.name.empty()) {
+    doPrint(" (unknown)");
+  } else if (frame.name.size() >= sizeof(mangledBuf)) {
+    doPrint(" ");
+    doPrint(frame.name);
+  } else {
+    memcpy(mangledBuf, frame.name.data(), frame.name.size());
+    mangledBuf[frame.name.size()] = '\0';
+
+    char demangledBuf[1024];
+    demangle(mangledBuf, demangledBuf, sizeof(demangledBuf));
+    doPrint(" ");
+    doPrint(demangledBuf);
+  }
+
+  if (!(options_ & NO_FILE_AND_LINE)) {
+    color(kFileColor);
+    char fileBuf[PATH_MAX];
+    fileBuf[0] = '\0';
+    if (frame.location.hasFileAndLine) {
+      frame.location.file.toBuffer(fileBuf, sizeof(fileBuf));
+      doPrint("\n");
+      doPrint(pad);
+      doPrint(fileBuf);
+
+      char buf[22];
+      uint32_t n = uint64ToBufferUnsafe(frame.location.line, buf);
+      doPrint(":");
+      doPrint(StringPiece(buf, n));
+    }
+
+    if (frame.location.hasMainFile) {
+      char mainFileBuf[PATH_MAX];
+      mainFileBuf[0] = '\0';
+      frame.location.mainFile.toBuffer(mainFileBuf, sizeof(mainFileBuf));
+      if (!frame.location.hasFileAndLine || strcmp(fileBuf, mainFileBuf)) {
+        doPrint("\n");
+        doPrint(pad);
+        doPrint("-> ");
+        doPrint(mainFileBuf);
+      }
+    }
+  }
+}
+
+namespace {
+
+const std::map<SymbolizePrinter::Color, std::string> kColorMap = {
+  { SymbolizePrinter::Color::DEFAULT,  "\x1B[0m" },
+  { SymbolizePrinter::Color::RED,  "\x1B[31m" },
+  { SymbolizePrinter::Color::GREEN,  "\x1B[32m" },
+  { SymbolizePrinter::Color::YELLOW,  "\x1B[33m" },
+  { SymbolizePrinter::Color::BLUE,  "\x1B[34m" },
+  { SymbolizePrinter::Color::CYAN,  "\x1B[36m" },
+  { SymbolizePrinter::Color::WHITE,  "\x1B[37m" },
+  { SymbolizePrinter::Color::PURPLE,  "\x1B[35m" },
+};
+
+}
+
+void SymbolizePrinter::color(SymbolizePrinter::Color color) {
+  if ((options_ & COLOR) == 0 &&
+      ((options_ & COLOR_IF_TTY) == 0 || !isTty_)) {
+    return;
+  }
+  auto it = kColorMap.find(color);
+  if (it == kColorMap.end()) {
+    return;
+  }
+  doPrint(it->second);
+}
+
+void SymbolizePrinter::println(uintptr_t address,
+                               const SymbolizedFrame& frame) {
+  print(address, frame);
+  doPrint("\n");
+}
+
+void SymbolizePrinter::printTerse(uintptr_t address,
+                                  const SymbolizedFrame& frame) {
+  if (frame.found) {
+    char mangledBuf[1024];
+    memcpy(mangledBuf, frame.name.data(), frame.name.size());
+    mangledBuf[frame.name.size()] = '\0';
+
+    char demangledBuf[1024] = {0};
+    demangle(mangledBuf, demangledBuf, sizeof(demangledBuf));
+    doPrint(strlen(demangledBuf) == 0 ? "(unknown)" : demangledBuf);
+  } else {
+    // Can't use sprintf, not async-signal-safe
+    static_assert(sizeof(uintptr_t) <= 8, "huge uintptr_t?");
+    char buf[] = "0x0000000000000000";
+    char* end = buf + sizeof(buf) - 1 - (16 - 2 * sizeof(uintptr_t));
+    char* p = end;
+    *p-- = '\0';
+    while (address != 0) {
+      *p-- = kHexChars[address & 0xf];
+      address >>= 4;
+    }
+    doPrint(StringPiece(buf, end));
+  }
+}
+
+void SymbolizePrinter::println(const uintptr_t* addresses,
+                               const SymbolizedFrame* frames,
+                               size_t frameCount) {
+  for (size_t i = 0; i < frameCount; ++i) {
+    println(addresses[i], frames[i]);
+  }
+}
+
+namespace {
+
+int getFD(const std::ios& stream) {
+#ifdef __GNUC__
+  std::streambuf* buf = stream.rdbuf();
+  using namespace __gnu_cxx;
+
+  {
+    auto sbuf = dynamic_cast<stdio_sync_filebuf<char>*>(buf);
+    if (sbuf) {
+      return fileno(sbuf->file());
+    }
+  }
+  {
+    auto sbuf = dynamic_cast<stdio_filebuf<char>*>(buf);
+    if (sbuf) {
+      return sbuf->fd();
+    }
+  }
+#endif  // __GNUC__
+  return -1;
+}
+
+bool isTty(int options, int fd) {
+  return ((options & SymbolizePrinter::TERSE) == 0 &&
+          (options & SymbolizePrinter::COLOR_IF_TTY) != 0 &&
+          fd >= 0 && ::isatty(fd));
+}
+
+}  // anonymous namespace
+
+OStreamSymbolizePrinter::OStreamSymbolizePrinter(std::ostream& out, int options)
+  : SymbolizePrinter(options, isTty(options, getFD(out))),
+    out_(out) {
+}
+
+void OStreamSymbolizePrinter::doPrint(StringPiece sp) {
+  out_ << sp;
+}
+
+FDSymbolizePrinter::FDSymbolizePrinter(int fd, int options)
+  : SymbolizePrinter(options, isTty(options, fd)),
+    fd_(fd) {
+}
+
+void FDSymbolizePrinter::doPrint(StringPiece sp) {
+  writeFull(fd_, sp.data(), sp.size());
+}
+
+FILESymbolizePrinter::FILESymbolizePrinter(FILE* file, int options)
+  : SymbolizePrinter(options, isTty(options, fileno(file))),
+    file_(file) {
+}
+
+void FILESymbolizePrinter::doPrint(StringPiece sp) {
+  fwrite(sp.data(), 1, sp.size(), file_);
+}
+
+void StringSymbolizePrinter::doPrint(StringPiece sp) {
+  buf_.append(sp.data(), sp.size());
+}
+
+}  // namespace symbolizer
+}  // namespace folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/symbolizer/Symbolizer.h
@@ -0,0 +1,276 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_EXPERIMENTAL_SYMBOLIZER_SYMBOLIZER_H_
+#define FOLLY_EXPERIMENTAL_SYMBOLIZER_SYMBOLIZER_H_
+
+#include <cstdint>
+#include <string>
+#include <unordered_map>
+
+#include "folly/FBString.h"
+#include "folly/Range.h"
+#include "folly/String.h"
+#include "folly/experimental/symbolizer/Elf.h"
+#include "folly/experimental/symbolizer/ElfCache.h"
+#include "folly/experimental/symbolizer/Dwarf.h"
+#include "folly/experimental/symbolizer/StackTrace.h"
+
+namespace folly {
+namespace symbolizer {
+
+class Symbolizer;
+
+/**
+ * Frame information: symbol name and location.
+ */
+struct SymbolizedFrame {
+  SymbolizedFrame() : found(false) { }
+
+  void set(const std::shared_ptr<ElfFile>& file, uintptr_t address);
+  void clear() { *this = SymbolizedFrame(); }
+
+  bool isSignalFrame;
+  bool found;
+  StringPiece name;
+  Dwarf::LocationInfo location;
+
+  /**
+   * Demangle the name and return it. Not async-signal-safe; allocates memory.
+   */
+  fbstring demangledName() const {
+    return demangle(name.fbstr().c_str());
+  }
+ private:
+  std::shared_ptr<ElfFile> file_;
+};
+
+template <size_t N>
+struct FrameArray {
+  FrameArray() : frameCount(0) { }
+
+  size_t frameCount;
+  uintptr_t addresses[N];
+  SymbolizedFrame frames[N];
+};
+
+/**
+ * Get stack trace into a given FrameArray, return true on success (and
+ * set frameCount to the actual frame count, which may be > N) and false
+ * on failure.
+ */
+namespace detail {
+template <size_t N>
+bool fixFrameArray(FrameArray<N>& fa, ssize_t n) {
+  if (n != -1) {
+    fa.frameCount = n;
+    for (size_t i = 0; i < fa.frameCount; ++i) {
+      fa.frames[i].found = false;
+    }
+    return true;
+  } else {
+    fa.frameCount = 0;
+    return false;
+  }
+}
+}  // namespace detail
+
+// Always inline these functions; they don't do much, and unittests rely
+// on them never showing up in a stack trace.
+template <size_t N>
+inline bool getStackTrace(FrameArray<N>& fa) __attribute__((always_inline));
+
+template <size_t N>
+inline bool getStackTrace(FrameArray<N>& fa) {
+  return detail::fixFrameArray(fa, getStackTrace(fa.addresses, N));
+}
+template <size_t N>
+inline bool getStackTraceSafe(FrameArray<N>& fa) __attribute__((always_inline));
+
+template <size_t N>
+inline bool getStackTraceSafe(FrameArray<N>& fa) {
+  return detail::fixFrameArray(fa, getStackTraceSafe(fa.addresses, N));
+}
+
+class Symbolizer {
+ public:
+  explicit Symbolizer(ElfCacheBase* cache = nullptr);
+
+  /**
+   * Symbolize given addresses.
+   */
+  void symbolize(const uintptr_t* addresses,
+                 SymbolizedFrame* frames,
+                 size_t frameCount);
+
+  template <size_t N>
+  void symbolize(FrameArray<N>& fa) {
+    symbolize(fa.addresses, fa.frames, fa.frameCount);
+  }
+
+  /**
+   * Shortcut to symbolize one address.
+   */
+  bool symbolize(uintptr_t address, SymbolizedFrame& frame) {
+    symbolize(&address, &frame, 1);
+    return frame.found;
+  }
+
+ private:
+  ElfCacheBase* cache_;
+};
+
+/**
+ * Format one address in the way it's usually printer by SymbolizePrinter.
+ * Async-signal-safe.
+ */
+class AddressFormatter {
+ public:
+  AddressFormatter();
+
+  /**
+   * Format the address. Returns an internal buffer.
+   */
+  StringPiece format(uintptr_t address);
+
+ private:
+  static constexpr char bufTemplate[] = "    @ 0000000000000000";
+  char buf_[sizeof(bufTemplate)];
+};
+
+/**
+ * Print a list of symbolized addresses. Base class.
+ */
+class SymbolizePrinter {
+ public:
+  /**
+   * Print one address, no ending newline.
+   */
+  void print(uintptr_t address, const SymbolizedFrame& frame);
+
+  /**
+   * Print one address with ending newline.
+   */
+  void println(uintptr_t address, const SymbolizedFrame& frame);
+
+  /**
+   * Print multiple addresses on separate lines.
+   */
+  void println(const uintptr_t* addresses,
+               const SymbolizedFrame* frames,
+               size_t frameCount);
+
+  /**
+   * Print multiple addresses on separate lines, skipping the first
+   * skip addresses.
+   */
+  template <size_t N>
+  void println(const FrameArray<N>& fa, size_t skip=0) {
+    if (skip < fa.frameCount) {
+      println(fa.addresses + skip, fa.frames + skip, fa.frameCount - skip);
+    }
+  }
+
+  virtual ~SymbolizePrinter() { }
+
+  enum Options {
+    // Skip file and line information
+    NO_FILE_AND_LINE = 1 << 0,
+
+    // As terse as it gets: function name if found, address otherwise
+    TERSE = 1 << 1,
+
+    // Always colorize output (ANSI escape code)
+    COLOR = 1 << 2,
+
+    // Colorize output only if output is printed to a TTY (ANSI escape code)
+    COLOR_IF_TTY = 1 << 3,
+  };
+
+  enum Color { DEFAULT, RED, GREEN, YELLOW, BLUE, CYAN, WHITE, PURPLE };
+  void color(Color c);
+
+ protected:
+  explicit SymbolizePrinter(int options, bool isTty = false)
+    : options_(options),
+      isTty_(isTty) {
+  }
+
+  const int options_;
+  const bool isTty_;
+
+ private:
+  void printTerse(uintptr_t address, const SymbolizedFrame& frame);
+  virtual void doPrint(StringPiece sp) = 0;
+};
+
+/**
+ * Print a list of symbolized addresses to a stream.
+ * Not reentrant. Do not use from signal handling code.
+ */
+class OStreamSymbolizePrinter : public SymbolizePrinter {
+ public:
+  explicit OStreamSymbolizePrinter(std::ostream& out, int options=0);
+ private:
+  void doPrint(StringPiece sp) override;
+  std::ostream& out_;
+};
+
+/**
+ * Print a list of symbolized addresses to a file descriptor.
+ * Ignores errors. Async-signal-safe.
+ */
+class FDSymbolizePrinter : public SymbolizePrinter {
+ public:
+  explicit FDSymbolizePrinter(int fd, int options=0);
+ private:
+  void doPrint(StringPiece sp) override;
+  int fd_;
+};
+
+/**
+ * Print a list of symbolized addresses to a FILE*.
+ * Ignores errors. Not reentrant. Do not use from signal handling code.
+ */
+class FILESymbolizePrinter : public SymbolizePrinter {
+ public:
+  explicit FILESymbolizePrinter(FILE* file, int options=0);
+ private:
+  void doPrint(StringPiece sp) override;
+  FILE* file_;
+};
+
+/**
+ * Print a list of symbolized addresses to a std::string.
+ * Not reentrant. Do not use from signal handling code.
+ */
+class StringSymbolizePrinter : public SymbolizePrinter {
+ public:
+  explicit StringSymbolizePrinter(int options=0) : SymbolizePrinter(options) { }
+
+  std::string str() const { return buf_.toStdString(); }
+  const fbstring& fbstr() const { return buf_; }
+  fbstring moveFbString() { return std::move(buf_); }
+
+ private:
+  void doPrint(StringPiece sp) override;
+  fbstring buf_;
+};
+
+}  // namespace symbolizer
+}  // namespace folly
+
+#endif /* FOLLY_EXPERIMENTAL_SYMBOLIZER_SYMBOLIZER_H_ */
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/symbolizer/test/Crash.cpp
@@ -0,0 +1,23 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/experimental/symbolizer/SignalHandler.h"
+
+int main() {
+  folly::symbolizer::installFatalSignalHandler();
+  *(int*) nullptr = 1;
+  return 0;
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/symbolizer/test/ElfTests.cpp
@@ -0,0 +1,65 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <gtest/gtest.h>
+
+#include "folly/experimental/symbolizer/Elf.h"
+
+using folly::symbolizer::ElfFile;
+
+// Add some symbols for testing. Note that we have to be careful with type
+// signatures here to prevent name mangling
+uint64_t kIntegerValue = 1234567890UL;
+const char* kStringValue = "coconuts";
+
+class ElfTest : public ::testing::Test {
+ public:
+  // Path to the test binary itself; set by main()
+  static std::string binaryPath;
+
+  ElfTest() : elfFile_(binaryPath.c_str()) {
+  }
+  virtual ~ElfTest() {
+  }
+
+ protected:
+  ElfFile elfFile_;
+};
+
+std::string ElfTest::binaryPath;
+
+TEST_F(ElfTest, IntegerValue) {
+  auto sym = elfFile_.getSymbolByName("kIntegerValue");
+  EXPECT_NE(nullptr, sym.first) <<
+    "Failed to look up symbol kIntegerValue";
+  EXPECT_EQ(kIntegerValue, elfFile_.getSymbolValue<uint64_t>(sym.second));
+}
+
+TEST_F(ElfTest, PointerValue) {
+  auto sym = elfFile_.getSymbolByName("kStringValue");
+  EXPECT_NE(nullptr, sym.first) <<
+    "Failed to look up symbol kStringValue";
+  ElfW(Addr) addr = elfFile_.getSymbolValue<ElfW(Addr)>(sym.second);
+  const char *str = &elfFile_.getAddressValue<const char>(addr);
+  EXPECT_STREQ(kStringValue, str);
+}
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  ElfTest::binaryPath = argv[0];
+  return RUN_ALL_TESTS();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/symbolizer/test/LineReaderTest.cpp
@@ -0,0 +1,86 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/experimental/symbolizer/LineReader.h"
+
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+#include "folly/FileUtil.h"
+#include "folly/experimental/TestUtil.h"
+
+namespace folly { namespace symbolizer { namespace test {
+
+using folly::test::TemporaryFile;
+
+void writeAll(int fd, const char* str) {
+  ssize_t n = strlen(str);
+  CHECK_EQ(n, writeFull(fd, str, n));
+}
+
+void expect(LineReader& lr, const char* expected) {
+  StringPiece line;
+  size_t expectedLen = strlen(expected);
+  EXPECT_EQ(expectedLen != 0 ? LineReader::kReading : LineReader::kEof,
+            lr.readLine(line));
+  EXPECT_EQ(expectedLen, line.size());
+  EXPECT_EQ(std::string(expected, expectedLen), line.str());
+}
+
+TEST(LineReader, Simple) {
+  TemporaryFile file;
+  int fd = file.fd();
+  writeAll(fd,
+           "Meow\n"
+           "Hello world\n"
+           "This is a long line. It is longer than the other lines.\n"
+           "\n"
+           "Incomplete last line");
+
+  {
+    CHECK_ERR(lseek(fd, 0, SEEK_SET));
+    char buf[10];
+    LineReader lr(fd, buf, sizeof(buf));
+    expect(lr, "Meow\n");
+    expect(lr, "Hello worl");
+    expect(lr, "d\n");
+    expect(lr, "This is a ");
+    expect(lr, "long line.");
+    expect(lr, " It is lon");
+    expect(lr, "ger than t");
+    expect(lr, "he other l");
+    expect(lr, "ines.\n");
+    expect(lr, "\n");
+    expect(lr, "Incomplete");
+    expect(lr, " last line");
+    expect(lr, "");
+  }
+
+  {
+    CHECK_ERR(lseek(fd, 0, SEEK_SET));
+    char buf[80];
+    LineReader lr(fd, buf, sizeof(buf));
+    expect(lr, "Meow\n");
+    expect(lr, "Hello world\n");
+    expect(lr, "This is a long line. It is longer than the other lines.\n");
+    expect(lr, "\n");
+    expect(lr, "Incomplete last line");
+    expect(lr, "");
+  }
+}
+
+}}}  // namespaces
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/symbolizer/test/SignalHandlerTest.cpp
@@ -0,0 +1,75 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/experimental/symbolizer/test/SignalHandlerTest.h"
+#include "folly/experimental/symbolizer/SignalHandler.h"
+
+#include <gtest/gtest.h>
+
+#include "folly/FileUtil.h"
+#include "folly/Range.h"
+#include "folly/CPortability.h"
+
+namespace folly { namespace symbolizer { namespace test {
+
+namespace {
+
+void print(StringPiece sp) {
+  writeFull(STDERR_FILENO, sp.data(), sp.size());
+}
+
+
+void callback1() {
+  print("Callback1\n");
+}
+
+void callback2() {
+  print("Callback2\n");
+}
+
+}  // namespace
+
+TEST(SignalHandler, Simple) {
+  addFatalSignalCallback(callback1);
+  addFatalSignalCallback(callback2);
+  installFatalSignalHandler();
+  installFatalSignalCallbacks();
+
+  EXPECT_DEATH(
+      failHard(),
+#ifdef FOLLY_SANITIZE_ADDRESS
+      // Testing an ASAN-enabled binary evokes a different diagnostic.
+      // Use a regexp that requires only the first line of that output:
+      "^ASAN:SIGSEGV\n.*"
+#else
+      "^\\*\\*\\* Aborted at [0-9]+ \\(Unix time, try 'date -d @[0-9]+'\\) "
+      "\\*\\*\\*\n"
+      "\\*\\*\\* Signal 11 \\(SIGSEGV\\) \\(0x2a\\) received by PID [0-9]+ "
+      "\\(TID 0x[0-9a-f]+\\), stack trace: \\*\\*\\*\n"
+      ".*\n"
+      "    @ [0-9a-f]+ folly::symbolizer::test::SignalHandler_Simple_Test"
+      "::TestBody\\(\\)\n"
+      ".*\n"
+      "    @ [0-9a-f]+ main\n"
+      ".*\n"
+      "Callback1\n"
+      "Callback2\n"
+#endif
+      );
+}
+
+
+}}}  // namespaces
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/symbolizer/test/SignalHandlerTest.h
@@ -0,0 +1,29 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_SYMBOLIZER_TEST_SIGNALHANDLERTEST_H_
+#define FOLLY_SYMBOLIZER_TEST_SIGNALHANDLERTEST_H_
+
+namespace folly { namespace symbolizer { namespace test {
+
+inline void failHard() {
+  *(volatile char*)42;  // SIGSEGV
+}
+
+}}}  // namespaces
+
+#endif /* FOLLY_SYMBOLIZER_TEST_SIGNALHANDLERTEST_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/symbolizer/test/StackTraceTest.cpp
@@ -0,0 +1,96 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/experimental/symbolizer/StackTrace.h"
+#include "folly/experimental/symbolizer/Symbolizer.h"
+
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+using namespace folly;
+using namespace folly::symbolizer;
+
+void foo1() __attribute__((noinline));
+void foo2() __attribute__((noinline));
+
+void verifyStackTraces() {
+  constexpr size_t kMaxAddresses = 100;
+  FrameArray<kMaxAddresses> fa;
+  CHECK(getStackTrace(fa));
+
+  FrameArray<kMaxAddresses> faSafe;
+  CHECK(getStackTraceSafe(faSafe));
+
+  CHECK_EQ(fa.frameCount, faSafe.frameCount);
+
+  if (VLOG_IS_ON(1)) {
+    Symbolizer symbolizer;
+    OStreamSymbolizePrinter printer(std::cerr, SymbolizePrinter::COLOR_IF_TTY);
+
+    symbolizer.symbolize(fa);
+    VLOG(1) << "getStackTrace\n";
+    printer.println(fa);
+
+    symbolizer.symbolize(faSafe);
+    VLOG(1) << "getStackTraceSafe\n";
+    printer.println(faSafe);
+  }
+
+  // Other than the top 2 frames (this one and getStackTrace /
+  // getStackTraceSafe), the stack traces should be identical
+  for (size_t i = 2; i < fa.frameCount; ++i) {
+    VLOG(1) << "i=" << i << " " << std::hex << fa.addresses[i] << " "
+            << faSafe.addresses[i];
+    CHECK_EQ(fa.addresses[i], faSafe.addresses[i]);
+  }
+}
+
+void foo1() {
+  foo2();
+}
+
+void foo2() {
+  verifyStackTraces();
+}
+
+volatile bool handled = false;
+void handler(int num, siginfo_t* info, void* ctx) {
+  // Yes, getStackTrace and VLOG aren't async-signal-safe, but signals
+  // raised with raise() aren't "async" signals.
+  foo1();
+  handled = true;
+}
+
+TEST(StackTraceTest, Simple) {
+  foo1();
+}
+
+TEST(StackTraceTest, Signal) {
+  struct sigaction sa;
+  memset(&sa, 0, sizeof(sa));
+  sa.sa_sigaction = handler;
+  sa.sa_flags = SA_RESETHAND | SA_SIGINFO;
+  CHECK_ERR(sigaction(SIGUSR1, &sa, nullptr));
+  raise(SIGUSR1);
+  CHECK(handled);
+}
+
+int main(int argc, char *argv[]) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  return RUN_ALL_TESTS();
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/symbolizer/test/SymbolizerTest.cpp
@@ -0,0 +1,111 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/experimental/symbolizer/Symbolizer.h"
+
+#include <cstdlib>
+
+#include <gtest/gtest.h>
+
+#include "folly/Range.h"
+#include "folly/String.h"
+
+namespace folly { namespace symbolizer { namespace test {
+
+void foo() {
+}
+
+TEST(Symbolizer, Single) {
+  Symbolizer symbolizer;
+  SymbolizedFrame a;
+  ASSERT_TRUE(symbolizer.symbolize(reinterpret_cast<uintptr_t>(foo), a));
+  EXPECT_EQ("folly::symbolizer::test::foo()",
+            demangle(a.name.str().c_str()));
+
+  auto path = a.location.file.toString();
+  folly::StringPiece basename(path);
+  auto pos = basename.rfind('/');
+  if (pos != folly::StringPiece::npos) {
+    basename.advance(pos + 1);
+  }
+  EXPECT_EQ("SymbolizerTest.cpp", basename.str());
+}
+
+FrameArray<100> goldenFrames;
+
+int comparator(const void* ap, const void* bp) {
+  getStackTrace(goldenFrames);
+
+  int a = *static_cast<const int*>(ap);
+  int b = *static_cast<const int*>(bp);
+  return a < b ? -1 : a > b ? 1 : 0;
+}
+
+// Test stack frames...
+void bar() __attribute__((noinline));
+
+void bar() {
+  int a[2] = {1, 2};
+  // Use qsort, which is in a different library
+  qsort(a, 2, sizeof(int), comparator);
+}
+
+class ElfCacheTest : public testing::Test {
+ protected:
+  void SetUp();
+};
+
+// Capture "golden" stack trace with default-configured Symbolizer
+void ElfCacheTest::SetUp() {
+  bar();
+  Symbolizer symbolizer;
+  symbolizer.symbolize(goldenFrames);
+  // At least 3 stack frames from us + getStackTrace()
+  ASSERT_LE(4, goldenFrames.frameCount);
+}
+
+void runElfCacheTest(Symbolizer& symbolizer) {
+  FrameArray<100> frames = goldenFrames;
+  for (size_t i = 0; i < frames.frameCount; ++i) {
+    auto& f = frames.frames[i];
+    f.found = false;
+    f.name.clear();
+  }
+  symbolizer.symbolize(frames);
+  ASSERT_LE(4, frames.frameCount);
+  for (size_t i = 1; i < 4; ++i) {
+    EXPECT_EQ(goldenFrames.frames[i].name, frames.frames[i].name);
+  }
+}
+
+TEST_F(ElfCacheTest, TinyElfCache) {
+  ElfCache cache(1);
+  Symbolizer symbolizer(&cache);
+  // Run twice, in case the wrong stuff gets evicted?
+  for (size_t i = 0; i < 2; ++i) {
+    runElfCacheTest(symbolizer);
+  }
+}
+
+TEST_F(ElfCacheTest, SignalSafeElfCache) {
+  SignalSafeElfCache cache(100);
+  Symbolizer symbolizer(&cache);
+  for (size_t i = 0; i < 2; ++i) {
+    runElfCacheTest(symbolizer);
+  }
+}
+
+}}}  // namespaces
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/test/BitsTest.cpp
@@ -0,0 +1,178 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/experimental/Bits.h"
+
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+using namespace folly;
+
+template <class T>
+void runSimpleTest8() {
+  auto load = detail::BitsTraits<T>::load;
+
+  EXPECT_EQ(0, Bits<T>::blockCount(0));
+  EXPECT_EQ(1, Bits<T>::blockCount(1));
+  EXPECT_EQ(1, Bits<T>::blockCount(8));
+  EXPECT_EQ(2, Bits<T>::blockCount(9));
+  EXPECT_EQ(256, Bits<T>::blockCount(2048));
+  EXPECT_EQ(257, Bits<T>::blockCount(2049));
+
+  EXPECT_EQ(4, Bits<T>::blockIndex(39));
+  EXPECT_EQ(7, Bits<T>::bitOffset(39));
+  EXPECT_EQ(5, Bits<T>::blockIndex(40));
+  EXPECT_EQ(0, Bits<T>::bitOffset(40));
+
+  T buf[256];
+  std::fill(buf, buf + 256, T(0));
+
+  Bits<T>::set(buf, 36);
+  Bits<T>::set(buf, 39);
+  EXPECT_EQ((1 << 7) | (1 << 4), load(buf[4]));
+  EXPECT_EQ(0, load(buf[5]));
+  Bits<T>::clear(buf, 39);
+  EXPECT_EQ(1 << 4, load(buf[4]));
+  EXPECT_EQ(0, load(buf[5]));
+  Bits<T>::set(buf, 40);
+  EXPECT_EQ(1 << 4, load(buf[4]));
+  EXPECT_EQ(1, load(buf[5]));
+
+  EXPECT_EQ(2, Bits<T>::count(buf, buf + 256));
+}
+
+TEST(Bits, Simple8) {
+  runSimpleTest8<uint8_t>();
+}
+
+TEST(Bits, SimpleUnaligned8) {
+  runSimpleTest8<Unaligned<uint8_t>>();
+}
+
+template <class T>
+void runSimpleTest64() {
+  auto load = detail::BitsTraits<T>::load;
+
+  EXPECT_EQ(0, Bits<T>::blockCount(0));
+  EXPECT_EQ(1, Bits<T>::blockCount(1));
+  EXPECT_EQ(1, Bits<T>::blockCount(8));
+  EXPECT_EQ(1, Bits<T>::blockCount(9));
+  EXPECT_EQ(1, Bits<T>::blockCount(64));
+  EXPECT_EQ(2, Bits<T>::blockCount(65));
+  EXPECT_EQ(32, Bits<T>::blockCount(2048));
+  EXPECT_EQ(33, Bits<T>::blockCount(2049));
+
+  EXPECT_EQ(0, Bits<T>::blockIndex(39));
+  EXPECT_EQ(39, Bits<T>::bitOffset(39));
+  EXPECT_EQ(4, Bits<T>::blockIndex(319));
+  EXPECT_EQ(63, Bits<T>::bitOffset(319));
+  EXPECT_EQ(5, Bits<T>::blockIndex(320));
+  EXPECT_EQ(0, Bits<T>::bitOffset(320));
+
+  T buf[256];
+  std::fill(buf, buf + 256, T(0));
+
+  Bits<T>::set(buf, 300);
+  Bits<T>::set(buf, 319);
+  EXPECT_EQ((uint64_t(1) << 44) | (uint64_t(1) << 63), load(buf[4]));
+  EXPECT_EQ(0, load(buf[5]));
+  Bits<T>::clear(buf, 319);
+  EXPECT_EQ(uint64_t(1) << 44, load(buf[4]));
+  EXPECT_EQ(0, load(buf[5]));
+  Bits<T>::set(buf, 320);
+  EXPECT_EQ(uint64_t(1) << 44, load(buf[4]));
+  EXPECT_EQ(1, load(buf[5]));
+
+  EXPECT_EQ(2, Bits<T>::count(buf, buf + 256));
+}
+
+TEST(Bits, Simple64) {
+  runSimpleTest64<uint64_t>();
+}
+
+TEST(Bits, SimpleUnaligned64) {
+  runSimpleTest64<Unaligned<uint64_t>>();
+}
+
+template <class T>
+void runMultiBitTest8() {
+  auto load = detail::BitsTraits<T>::load;
+  T buf[] = {0x12, 0x34, 0x56, 0x78};
+
+  EXPECT_EQ(0x02, load(Bits<T>::get(buf, 0, 4)));
+  EXPECT_EQ(0x1a, load(Bits<T>::get(buf, 9, 5)));
+  EXPECT_EQ(0xb1, load(Bits<T>::get(buf, 13, 8)));
+
+  Bits<T>::set(buf, 0, 4, 0x0b);
+  EXPECT_EQ(0x1b, load(buf[0]));
+  EXPECT_EQ(0x34, load(buf[1]));
+  EXPECT_EQ(0x56, load(buf[2]));
+  EXPECT_EQ(0x78, load(buf[3]));
+
+  Bits<T>::set(buf, 9, 5, 0x0e);
+  EXPECT_EQ(0x1b, load(buf[0]));
+  EXPECT_EQ(0x1c, load(buf[1]));
+  EXPECT_EQ(0x56, load(buf[2]));
+  EXPECT_EQ(0x78, load(buf[3]));
+
+  Bits<T>::set(buf, 13, 8, 0xaa);
+  EXPECT_EQ(0x1b, load(buf[0]));
+  EXPECT_EQ(0x5c, load(buf[1]));
+  EXPECT_EQ(0x55, load(buf[2]));
+  EXPECT_EQ(0x78, load(buf[3]));
+}
+
+TEST(Bits, MultiBit8) {
+  runMultiBitTest8<uint8_t>();
+}
+
+TEST(Bits, MultiBitUnaligned8) {
+  runMultiBitTest8<Unaligned<uint8_t>>();
+}
+
+template <class T>
+void runMultiBitTest64() {
+  auto load = detail::BitsTraits<T>::load;
+  T buf[] = {0x123456789abcdef0, 0x13579bdf2468ace0};
+
+  EXPECT_EQ(0x123456789abcdef0, load(Bits<T>::get(buf, 0, 64)));
+  EXPECT_EQ(0xf0, load(Bits<T>::get(buf, 0, 8)));
+  EXPECT_EQ(0x89abcdef, load(Bits<T>::get(buf, 4, 32)));
+  EXPECT_EQ(0x189abcdef, load(Bits<T>::get(buf, 4, 33)));
+
+  Bits<T>::set(buf, 4, 31, 0x55555555);
+  EXPECT_EQ(0xd5555555, load(Bits<T>::get(buf, 4, 32)));
+  EXPECT_EQ(0x1d5555555, load(Bits<T>::get(buf, 4, 33)));
+  EXPECT_EQ(0xd55555550, load(Bits<T>::get(buf, 0, 36)));
+
+  Bits<T>::set(buf, 0, 64, 0x23456789abcdef01);
+  EXPECT_EQ(0x23456789abcdef01, load(Bits<T>::get(buf, 0, 64)));
+}
+
+TEST(Bits, MultiBit64) {
+  runMultiBitTest64<uint64_t>();
+}
+
+TEST(Bits, MultiBitUnaligned64) {
+  runMultiBitTest64<Unaligned<uint64_t>>();
+}
+
+int main(int argc, char *argv[]) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  return RUN_ALL_TESTS();
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/test/CodingTestUtils.h
@@ -0,0 +1,231 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_EXPERIMENTAL_CODING_TEST_UTILS_H
+#define FOLLY_EXPERIMENTAL_CODING_TEST_UTILS_H
+
+#include <algorithm>
+#include <fstream>
+#include <limits>
+#include <random>
+#include <string>
+#include <vector>
+#include <unordered_set>
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+namespace folly { namespace compression {
+
+std::vector<uint32_t> generateRandomList(size_t n, uint32_t maxId) {
+  CHECK_LT(n, 2 * maxId);
+  std::mt19937 gen;
+  std::uniform_int_distribution<> uid(1, maxId);
+  std::unordered_set<uint32_t> dataset;
+  while (dataset.size() < n) {
+    uint32_t value = uid(gen);
+    if (dataset.count(value) == 0) {
+      dataset.insert(value);
+    }
+  }
+
+  std::vector<uint32_t> ids(dataset.begin(), dataset.end());
+  std::sort(ids.begin(), ids.end());
+  return ids;
+}
+
+std::vector<uint32_t> generateSeqList(uint32_t minId, uint32_t maxId,
+                                      uint32_t step = 1) {
+  CHECK_LE(minId, maxId);
+  CHECK_GT(step, 0);
+  std::vector<uint32_t> ids;
+  ids.reserve((maxId - minId) / step + 1);
+  for (uint32_t i = minId; i <= maxId; i += step) {
+    ids.push_back(i);
+  }
+  return ids;
+}
+
+std::vector<uint32_t> loadList(const std::string& filename) {
+  std::ifstream fin(filename);
+  std::vector<uint32_t> result;
+  uint32_t id;
+  while (fin >> id) {
+    result.push_back(id);
+  }
+  return result;
+}
+
+template <class Reader, class List>
+void testNext(const std::vector<uint32_t>& data, const List& list) {
+  Reader reader(list);
+  EXPECT_EQ(reader.value(), 0);
+  for (size_t i = 0; i < data.size(); ++i) {
+    EXPECT_TRUE(reader.next());
+    EXPECT_EQ(reader.value(), data[i]);
+  }
+  EXPECT_FALSE(reader.next());
+  EXPECT_EQ(reader.value(), std::numeric_limits<uint32_t>::max());
+}
+
+template <class Reader, class List>
+void testSkip(const std::vector<uint32_t>& data, const List& list,
+              size_t skipStep) {
+  CHECK_GT(skipStep, 0);
+  Reader reader(list);
+  EXPECT_EQ(reader.value(), 0);
+  for (size_t i = skipStep - 1; i < data.size(); i += skipStep) {
+    EXPECT_TRUE(reader.skip(skipStep));
+    EXPECT_EQ(reader.value(), data[i]);
+  }
+  EXPECT_FALSE(reader.skip(skipStep));
+  EXPECT_EQ(reader.value(), std::numeric_limits<uint32_t>::max());
+  EXPECT_FALSE(reader.next());
+}
+
+template <class Reader, class List>
+void testSkip(const std::vector<uint32_t>& data, const List& list) {
+  for (size_t skipStep = 1; skipStep < 25; ++skipStep) {
+    testSkip<Reader, List>(data, list, skipStep);
+  }
+  for (size_t skipStep = 25; skipStep <= 500; skipStep += 25) {
+    testSkip<Reader, List>(data, list, skipStep);
+  }
+}
+
+template <class Reader, class List>
+void testSkipTo(const std::vector<uint32_t>& data, const List& list,
+                size_t skipToStep) {
+  CHECK_GT(skipToStep, 0);
+
+  Reader reader(list);
+  EXPECT_EQ(reader.value(), 0);
+
+  const uint32_t delta = std::max<uint32_t>(1, data.back() / skipToStep);
+  uint32_t value = delta;
+  auto it = data.begin();
+  while (true) {
+    it = std::lower_bound(it, data.end(), value);
+    if (it == data.end()) {
+      EXPECT_FALSE(reader.skipTo(value));
+      break;
+    }
+    EXPECT_TRUE(reader.skipTo(value));
+    EXPECT_EQ(reader.value(), *it);
+    value = reader.value() + delta;
+  }
+
+  EXPECT_EQ(reader.value(), std::numeric_limits<uint32_t>::max());
+  EXPECT_FALSE(reader.next());
+}
+
+template <class Reader, class List>
+void testSkipTo(const std::vector<uint32_t>& data, const List& list) {
+  for (size_t steps = 10; steps < 100; steps += 10) {
+    testSkipTo<Reader, List>(data, list, steps);
+  }
+  for (size_t steps = 100; steps <= 1000; steps += 100) {
+    testSkipTo<Reader, List>(data, list, steps);
+  }
+  testSkipTo<Reader, List>(data, list, std::numeric_limits<size_t>::max());
+  {
+    Reader reader(list);
+    EXPECT_FALSE(reader.skipTo(data.back() + 1));
+    EXPECT_EQ(reader.value(), std::numeric_limits<uint32_t>::max());
+    EXPECT_FALSE(reader.next());
+  }
+}
+
+template <class Reader, class Encoder>
+void testEmpty() {
+  typename Encoder::CompressedList list;
+  const typename Encoder::ValueType* const data = nullptr;
+  Encoder::encode(data, 0, list);
+  {
+    Reader reader(list);
+    EXPECT_FALSE(reader.next());
+    EXPECT_EQ(reader.size(), 0);
+  }
+  {
+    Reader reader(list);
+    EXPECT_FALSE(reader.skip(1));
+    EXPECT_FALSE(reader.skip(10));
+  }
+  {
+    Reader reader(list);
+    EXPECT_FALSE(reader.skipTo(1));
+  }
+}
+
+template <class Reader, class Encoder>
+void testAll(const std::vector<uint32_t>& data) {
+  typename Encoder::CompressedList list;
+  Encoder::encode(data.begin(), data.end(), list);
+  testNext<Reader>(data, list);
+  testSkip<Reader>(data, list);
+  testSkipTo<Reader>(data, list);
+  list.free();
+}
+
+template <class Reader, class List>
+void bmNext(const List& list, const std::vector<uint32_t>& data,
+            size_t iters) {
+  if (data.empty()) {
+    return;
+  }
+  for (size_t i = 0, j; i < iters; ) {
+    Reader reader(list);
+    for (j = 0; reader.next(); ++j, ++i) {
+      const uint32_t value = reader.value();
+      CHECK_EQ(value, data[j]) << j;
+    }
+  }
+}
+
+template <class Reader, class List>
+void bmSkip(const List& list, const std::vector<uint32_t>& data,
+            size_t skip, size_t iters) {
+  if (skip >= data.size()) {
+    return;
+  }
+  for (size_t i = 0, j; i < iters; ) {
+    Reader reader(list);
+    for (j = skip - 1; j < data.size(); j += skip, ++i) {
+      reader.skip(skip);
+      const uint32_t value = reader.value();
+      CHECK_EQ(value, data[j]);
+    }
+  }
+}
+
+template <class Reader, class List>
+void bmSkipTo(const List& list, const std::vector<uint32_t>& data,
+              size_t skip, size_t iters) {
+  if (skip >= data.size()) {
+    return;
+  }
+  for (size_t i = 0, j; i < iters; ) {
+    Reader reader(list);
+    for (j = 0; j < data.size(); j += skip, ++i) {
+      reader.skipTo(data[j]);
+      const uint32_t value = reader.value();
+      CHECK_EQ(value, data[j]);
+    }
+  }
+}
+
+}}  // namespaces
+
+#endif  // FOLLY_EXPERIMENTAL_CODING_TEST_UTILS_H
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/test/EliasFanoCodingTest.cpp
@@ -0,0 +1,141 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Benchmark.h"
+#include "folly/experimental/EliasFanoCoding.h"
+#include "folly/experimental/test/CodingTestUtils.h"
+
+using namespace folly::compression;
+
+template <size_t kVersion>
+struct TestType {
+  static constexpr size_t Version = kVersion;
+};
+
+template <class T>
+class EliasFanoCodingTest : public ::testing::Test {
+ public:
+  void doTestEmpty() {
+    typedef EliasFanoEncoder<uint32_t, size_t, 0, 0, T::Version> Encoder;
+    typedef EliasFanoReader<Encoder> Reader;
+    testEmpty<Reader, Encoder>();
+  }
+
+  template <size_t kSkipQuantum, size_t kForwardQuantum>
+  void doTestAll() {
+    typedef EliasFanoEncoder<
+      uint32_t, uint32_t, kSkipQuantum, kForwardQuantum, T::Version> Encoder;
+    typedef EliasFanoReader<Encoder> Reader;
+    testAll<Reader, Encoder>(generateRandomList(100 * 1000, 10 * 1000 * 1000));
+    testAll<Reader, Encoder>(generateSeqList(1, 100000, 100));
+  }
+};
+
+typedef ::testing::Types<TestType<0>, TestType<1>> TestTypes;
+TYPED_TEST_CASE(EliasFanoCodingTest, TestTypes);
+
+TYPED_TEST(EliasFanoCodingTest, Empty) {
+  TestFixture::doTestEmpty();
+}
+
+TYPED_TEST(EliasFanoCodingTest, Simple) {
+  TestFixture::template doTestAll<0, 0>();
+}
+
+TYPED_TEST(EliasFanoCodingTest, SkipPointers) {
+  TestFixture::template doTestAll<128, 0>();
+}
+
+TYPED_TEST(EliasFanoCodingTest, ForwardPointers) {
+  TestFixture::template doTestAll<0, 128>();
+}
+
+TYPED_TEST(EliasFanoCodingTest, SkipForwardPointers) {
+  TestFixture::template doTestAll<128, 128>();
+}
+
+namespace bm {
+
+constexpr size_t k1M = 1000000;
+constexpr size_t kVersion = 1;
+
+typedef EliasFanoEncoder<uint32_t, uint32_t, 128, 128, kVersion> Encoder;
+typedef EliasFanoReader<Encoder> Reader;
+
+std::vector<uint32_t> data;
+typename Encoder::CompressedList list;
+
+void init() {
+  data = generateRandomList(100 * 1000, 10 * 1000 * 1000);
+  //data = loadList("/home/philipp/pl_test_dump.txt");
+  Encoder::encode(data.data(), data.size(), bm::list);
+}
+
+void free() {
+  list.free();
+}
+
+}  // namespace bm
+
+BENCHMARK(Next_1M) {
+  bmNext<bm::Reader>(bm::list, bm::data, bm::k1M);
+}
+
+BENCHMARK(Skip1_ForwarQ128_1M) {
+  bmSkip<bm::Reader>(bm::list, bm::data, 1, bm::k1M);
+}
+
+BENCHMARK(Skip10_ForwarQ128_1M) {
+  bmSkip<bm::Reader>(bm::list, bm::data, 10, bm::k1M);
+}
+
+BENCHMARK(Skip100_ForwardQ128_1M) {
+  bmSkip<bm::Reader>(bm::list, bm::data, 100, bm::k1M);
+}
+
+BENCHMARK(Skip1000_ForwardQ128_1M) {
+  bmSkip<bm::Reader>(bm::list, bm::data, 1000, bm::k1M);
+}
+
+BENCHMARK(SkipTo1_SkipQ128_1M) {
+  bmSkipTo<bm::Reader>(bm::list, bm::data, 1, bm::k1M);
+}
+
+BENCHMARK(SkipTo10_SkipQ128_1M) {
+  bmSkipTo<bm::Reader>(bm::list, bm::data, 10, bm::k1M);
+}
+
+BENCHMARK(SkipTo100_SkipQ128_1M) {
+  bmSkipTo<bm::Reader>(bm::list, bm::data, 100, bm::k1M);
+}
+
+BENCHMARK(SkipTo1000_SkipQ128_1M) {
+  bmSkipTo<bm::Reader>(bm::list, bm::data, 1000, bm::k1M);
+}
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+
+  auto ret = RUN_ALL_TESTS();
+  if (ret == 0 && FLAGS_benchmark) {
+    bm::init();
+    folly::runBenchmarks();
+    bm::free();
+  }
+
+  return ret;
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/test/EventCountTest.cpp
@@ -0,0 +1,137 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/experimental/EventCount.h"
+
+#include <algorithm>
+#include <random>
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+#include "folly/Random.h"
+#include "folly/Benchmark.h"
+
+using namespace folly;
+
+namespace {
+
+class Semaphore {
+ public:
+  explicit Semaphore(int v=0) : value_(v) { }
+
+  void down() {
+    ec_.await([this] { return tryDown(); });
+  }
+
+  void up() {
+    ++value_;
+    ec_.notifyAll();
+  }
+
+  int value() const {
+    return value_;
+  }
+
+ private:
+  bool tryDown() {
+    for (;;) {
+      int v = value_;
+      if (v == 0) {
+        return false;
+      }
+      if (value_.compare_exchange_weak(v, v-1)) {
+        return true;
+      }
+    }
+  }
+
+  std::atomic<int> value_;
+  EventCount ec_;
+};
+
+template <class T, class Random>
+void randomPartition(Random& random, T key, int n,
+                     std::vector<std::pair<T, int>>& out) {
+  while (n != 0) {
+    int m = std::min(n, 1000);
+    std::uniform_int_distribution<uint32_t> u(1, m);
+    int cut = u(random);
+    out.push_back(std::make_pair(key, cut));
+    n -= cut;
+  }
+}
+
+}  // namespace
+
+TEST(EventCount, Simple) {
+  // We're basically testing for no deadlock.
+  static const size_t count = 300000;
+
+  enum class Op {
+    UP,
+    DOWN
+  };
+  std::vector<std::pair<Op, int>> ops;
+  std::mt19937 rnd(randomNumberSeed());
+  randomPartition(rnd, Op::UP, count, ops);
+  size_t uppers = ops.size();
+  randomPartition(rnd, Op::DOWN, count, ops);
+  size_t downers = ops.size() - uppers;
+  VLOG(1) << "Using " << ops.size() << " threads: uppers=" << uppers
+          << " downers=" << downers << " sem_count=" << count;
+
+  std::random_shuffle(ops.begin(), ops.end());
+
+  std::vector<std::thread> threads;
+  threads.reserve(ops.size());
+
+  Semaphore sem;
+  for (auto& op : ops) {
+    int n = op.second;
+    if (op.first == Op::UP) {
+      auto fn = [&sem, n] () mutable {
+        while (n--) {
+          sem.up();
+        }
+      };
+      threads.push_back(std::thread(fn));
+    } else {
+      auto fn = [&sem, n] () mutable {
+        while (n--) {
+          sem.down();
+        }
+      };
+      threads.push_back(std::thread(fn));
+    }
+  }
+
+  for (auto& thread : threads) {
+    thread.join();
+  }
+
+  EXPECT_EQ(0, sem.value());
+}
+
+int main(int argc, char *argv[]) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  auto ret = RUN_ALL_TESTS();
+  if (!ret) {
+    folly::runBenchmarksOnFlag();
+  }
+  return ret;
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/test/TestUtilTest.cpp
@@ -0,0 +1,108 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/experimental/TestUtil.h"
+
+#include <sys/stat.h>
+#include <sys/types.h>
+#include <fcntl.h>
+
+#include <system_error>
+
+#include <boost/algorithm/string.hpp>
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+using namespace folly;
+using namespace folly::test;
+
+TEST(TemporaryFile, Simple) {
+  int fd = -1;
+  char c = 'x';
+  {
+    TemporaryFile f;
+    EXPECT_FALSE(f.path().empty());
+    EXPECT_TRUE(f.path().is_absolute());
+    fd = f.fd();
+    EXPECT_LE(0, fd);
+    ssize_t r = write(fd, &c, 1);
+    EXPECT_EQ(1, r);
+  }
+
+  // The file must have been closed.  This assumes that no other thread
+  // has opened another file in the meanwhile, which is a sane assumption
+  // to make in this test.
+  ssize_t r = write(fd, &c, 1);
+  int savedErrno = errno;
+  EXPECT_EQ(-1, r);
+  EXPECT_EQ(EBADF, savedErrno);
+}
+
+TEST(TemporaryFile, Prefix) {
+  TemporaryFile f("Foo");
+  EXPECT_TRUE(f.path().is_absolute());
+  EXPECT_TRUE(boost::algorithm::starts_with(f.path().filename().native(),
+                                            "Foo"));
+}
+
+TEST(TemporaryFile, PathPrefix) {
+  TemporaryFile f("Foo", ".");
+  EXPECT_EQ(fs::path("."), f.path().parent_path());
+  EXPECT_TRUE(boost::algorithm::starts_with(f.path().filename().native(),
+                                            "Foo"));
+}
+
+TEST(TemporaryFile, NoSuchPath) {
+  EXPECT_THROW({TemporaryFile f("", "/no/such/path");},
+               std::system_error);
+}
+
+void testTemporaryDirectory(TemporaryDirectory::Scope scope) {
+  fs::path path;
+  {
+    TemporaryDirectory d("", "", scope);
+    path = d.path();
+    EXPECT_FALSE(path.empty());
+    EXPECT_TRUE(path.is_absolute());
+    EXPECT_TRUE(fs::exists(path));
+    EXPECT_TRUE(fs::is_directory(path));
+
+    fs::path fp = path / "bar";
+    int fd = open(fp.c_str(), O_RDWR | O_CREAT | O_TRUNC, 0666);
+    EXPECT_NE(fd, -1);
+    close(fd);
+
+    TemporaryFile f("Foo", d.path());
+    EXPECT_EQ(d.path(), f.path().parent_path());
+  }
+  bool exists = (scope == TemporaryDirectory::Scope::PERMANENT);
+  EXPECT_EQ(exists, fs::exists(path));
+}
+
+TEST(TemporaryDirectory, Permanent) {
+  testTemporaryDirectory(TemporaryDirectory::Scope::PERMANENT);
+}
+
+TEST(TemporaryDirectory, DeleteOnDestruction) {
+  testTemporaryDirectory(TemporaryDirectory::Scope::DELETE_ON_DESTRUCTION);
+}
+
+int main(int argc, char *argv[]) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  return RUN_ALL_TESTS();
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/TestUtil.cpp
@@ -0,0 +1,111 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/experimental/TestUtil.h"
+
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+
+#include "folly/Conv.h"
+#include "folly/Exception.h"
+
+namespace folly {
+namespace test {
+
+namespace {
+
+fs::path generateUniquePath(fs::path path, StringPiece namePrefix) {
+  if (path.empty()) {
+    path = fs::temp_directory_path();
+  }
+  if (namePrefix.empty()) {
+    path /= fs::unique_path();
+  } else {
+    path /= fs::unique_path(
+        to<std::string>(namePrefix, ".%%%%-%%%%-%%%%-%%%%"));
+  }
+  return path;
+}
+
+}  // namespce
+
+TemporaryFile::TemporaryFile(StringPiece namePrefix,
+                             fs::path dir,
+                             Scope scope,
+                             bool closeOnDestruction)
+  : scope_(scope),
+    closeOnDestruction_(closeOnDestruction),
+    fd_(-1),
+    path_(generateUniquePath(std::move(dir), namePrefix)) {
+  fd_ = open(path_.c_str(), O_RDWR | O_CREAT | O_EXCL, 0666);
+  checkUnixError(fd_, "open failed");
+
+  if (scope_ == Scope::UNLINK_IMMEDIATELY) {
+    boost::system::error_code ec;
+    fs::remove(path_, ec);
+    if (ec) {
+      LOG(WARNING) << "unlink on construction failed: " << ec;
+    } else {
+      path_.clear();
+    }
+  }
+}
+
+const fs::path& TemporaryFile::path() const {
+  CHECK(scope_ != Scope::UNLINK_IMMEDIATELY);
+  DCHECK(!path_.empty());
+  return path_;
+}
+
+TemporaryFile::~TemporaryFile() {
+  if (fd_ != -1 && closeOnDestruction_) {
+    if (close(fd_) == -1) {
+      PLOG(ERROR) << "close failed";
+    }
+  }
+
+  // If we previously failed to unlink() (UNLINK_IMMEDIATELY), we'll
+  // try again here.
+  if (scope_ != Scope::PERMANENT && !path_.empty()) {
+    boost::system::error_code ec;
+    fs::remove(path_, ec);
+    if (ec) {
+      LOG(WARNING) << "unlink on destruction failed: " << ec;
+    }
+  }
+}
+
+TemporaryDirectory::TemporaryDirectory(StringPiece namePrefix,
+                                       fs::path dir,
+                                       Scope scope)
+  : scope_(scope),
+    path_(generateUniquePath(std::move(dir), namePrefix)) {
+  fs::create_directory(path_);
+}
+
+TemporaryDirectory::~TemporaryDirectory() {
+  if (scope_ == Scope::DELETE_ON_DESTRUCTION) {
+    boost::system::error_code ec;
+    fs::remove_all(path_, ec);
+    if (ec) {
+      LOG(WARNING) << "recursive delete on destruction failed: " << ec;
+    }
+  }
+}
+
+}  // namespace test
+}  // namespace folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/experimental/TestUtil.h
@@ -0,0 +1,95 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_TESTUTIL_H_
+#define FOLLY_TESTUTIL_H_
+
+#include <string>
+#include "folly/Range.h"
+#include "folly/experimental/io/FsUtil.h"
+
+namespace folly {
+namespace test {
+
+/**
+ * Temporary file.
+ *
+ * By default, the file is created in a system-specific location (the value
+ * of the TMPDIR environment variable, or /tmp), but you can override that
+ * with a different (non-empty) directory passed to the constructor.
+ *
+ * By default, the file is closed and deleted when the TemporaryFile object
+ * is destroyed, but both these behaviors can be overridden with arguments
+ * to the constructor.
+ */
+class TemporaryFile {
+ public:
+  enum class Scope {
+    PERMANENT,
+    UNLINK_IMMEDIATELY,
+    UNLINK_ON_DESTRUCTION
+  };
+  explicit TemporaryFile(StringPiece namePrefix = StringPiece(),
+                         fs::path dir = fs::path(),
+                         Scope scope = Scope::UNLINK_ON_DESTRUCTION,
+                         bool closeOnDestruction = true);
+  ~TemporaryFile();
+
+  int fd() const { return fd_; }
+  const fs::path& path() const;
+
+ private:
+  Scope scope_;
+  bool closeOnDestruction_;
+  int fd_;
+  fs::path path_;
+};
+
+/**
+ * Temporary directory.
+ *
+ * By default, the temporary directory is created in a system-specific
+ * location (the value of the TMPDIR environment variable, or /tmp), but you
+ * can override that with a non-empty directory passed to the constructor.
+ *
+ * By default, the directory is recursively deleted when the TemporaryDirectory
+ * object is destroyed, but that can be overridden with an argument
+ * to the constructor.
+ */
+
+class TemporaryDirectory {
+ public:
+  enum class Scope {
+    PERMANENT,
+    DELETE_ON_DESTRUCTION
+  };
+  explicit TemporaryDirectory(StringPiece namePrefix = StringPiece(),
+                              fs::path dir = fs::path(),
+                              Scope scope = Scope::DELETE_ON_DESTRUCTION);
+  ~TemporaryDirectory();
+
+  const fs::path& path() const { return path_; }
+
+ private:
+  Scope scope_;
+  fs::path path_;
+};
+
+}  // namespace test
+}  // namespace folly
+
+#endif /* FOLLY_TESTUTIL_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/FBString.h
@@ -0,0 +1,2502 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// @author: Andrei Alexandrescu (aalexandre)
+// String type.
+
+#ifndef FOLLY_BASE_FBSTRING_H_
+#define FOLLY_BASE_FBSTRING_H_
+
+/**
+   fbstring's behavior can be configured via two macro definitions, as
+   follows. Normally, fbstring does not write a '\0' at the end of
+   each string whenever it changes the underlying characters. Instead,
+   it lazily writes the '\0' whenever either c_str() or data()
+   called.
+
+   This is standard-compliant behavior and may save costs in some
+   circumstances. However, it may be surprising to some client code
+   because c_str() and data() are const member functions (fbstring
+   uses the "mutable" storage class for its own state).
+
+   In order to appease client code that expects fbstring to be
+   zero-terminated at all times, if the preprocessor symbol
+   FBSTRING_CONSERVATIVE is defined, fbstring does exactly that,
+   i.e. it goes the extra mile to guarantee a '\0' is always planted
+   at the end of its data.
+
+   On the contrary, if the desire is to debug faulty client code that
+   unduly assumes the '\0' is present, fbstring plants a '^' (i.e.,
+   emphatically NOT a zero) at the end of each string if
+   FBSTRING_PERVERSE is defined. (Calling c_str() or data() still
+   writes the '\0', of course.)
+
+   The preprocessor symbols FBSTRING_PERVERSE and
+   FBSTRING_CONSERVATIVE cannot be defined simultaneously. This is
+   enforced during preprocessing.
+*/
+
+//#define FBSTRING_PERVERSE
+//#define FBSTRING_CONSERVATIVE
+
+#ifdef FBSTRING_PERVERSE
+#ifdef FBSTRING_CONSERVATIVE
+#error Cannot define both FBSTRING_PERVERSE and FBSTRING_CONSERVATIVE.
+#endif
+#endif
+
+#include <atomic>
+#include <limits>
+#include <type_traits>
+
+// libc++ doesn't provide this header
+#ifndef _LIBCPP_VERSION
+// This file appears in two locations: inside fbcode and in the
+// libstdc++ source code (when embedding fbstring as std::string).
+// To aid in this schizophrenic use, two macros are defined in
+// c++config.h:
+//   _LIBSTDCXX_FBSTRING - Set inside libstdc++.  This is useful to
+//      gate use inside fbcode v. libstdc++
+#include <bits/c++config.h>
+#endif
+
+#ifdef _LIBSTDCXX_FBSTRING
+
+#pragma GCC system_header
+
+// Handle the cases where the fbcode version (folly/Malloc.h) is included
+// either before or after this inclusion.
+#ifdef FOLLY_MALLOC_H_
+#undef FOLLY_MALLOC_H_
+#include "basic_fbstring_malloc.h"
+#else
+#include "basic_fbstring_malloc.h"
+#undef FOLLY_MALLOC_H_
+#endif
+
+#else // !_LIBSTDCXX_FBSTRING
+
+#include <string>
+#include <cstring>
+#include <cassert>
+
+#include "folly/Traits.h"
+#include "folly/Malloc.h"
+#include "folly/Hash.h"
+
+#ifdef _GLIBCXX_SYMVER
+#include <ext/hash_set>
+#include <ext/hash_map>
+#endif
+
+#endif
+
+// We defined these here rather than including Likely.h to avoid
+// redefinition errors when fbstring is imported into libstdc++.
+#define FBSTRING_LIKELY(x)   (__builtin_expect((x), 1))
+#define FBSTRING_UNLIKELY(x) (__builtin_expect((x), 0))
+
+// Ignore shadowing warnings within this file, so includers can use -Wshadow.
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wshadow"
+
+// FBString cannot use throw when replacing std::string, though it may still
+// use std::__throw_*
+#define throw FOLLY_FBSTRING_MAY_NOT_USE_THROW
+
+#ifdef _LIBSTDCXX_FBSTRING
+namespace std _GLIBCXX_VISIBILITY(default) {
+_GLIBCXX_BEGIN_NAMESPACE_VERSION
+#else
+namespace folly {
+#endif
+
+// Different versions of gcc/clang support different versions of
+// the address sanitizer attribute.  Unfortunately, this attribute
+// has issues when inlining is used, so disable that as well.
+#if defined(__clang__)
+# if __has_feature(address_sanitizer)
+#  if __has_attribute(__no_address_safety_analysis__)
+#   define FBSTRING_DISABLE_ADDRESS_SANITIZER \
+      __attribute__((__no_address_safety_analysis__, __noinline__))
+#  elif __has_attribute(__no_sanitize_address__)
+#   define FBSTRING_DISABLE_ADDRESS_SANITIZER \
+      __attribute__((__no_sanitize_address__, __noinline__))
+#  endif
+# endif
+#elif defined (__GNUC__) && \
+      (__GNUC__ == 4) && \
+      (__GNUC_MINOR__ >= 8) && \
+      __SANITIZE_ADDRESS__
+# define FBSTRING_DISABLE_ADDRESS_SANITIZER \
+    __attribute__((__no_address_safety_analysis__, __noinline__))
+#endif
+#ifndef FBSTRING_DISABLE_ADDRESS_SANITIZER
+# define FBSTRING_DISABLE_ADDRESS_SANITIZER
+#endif
+
+namespace fbstring_detail {
+
+template <class InIt, class OutIt>
+inline
+OutIt copy_n(InIt b,
+             typename std::iterator_traits<InIt>::difference_type n,
+             OutIt d) {
+  for (; n != 0; --n, ++b, ++d) {
+    *d = *b;
+  }
+  return d;
+}
+
+template <class Pod, class T>
+inline void pod_fill(Pod* b, Pod* e, T c) {
+  assert(b && e && b <= e);
+  /*static*/ if (sizeof(T) == 1) {
+    memset(b, c, e - b);
+  } else {
+    auto const ee = b + ((e - b) & ~7u);
+    for (; b != ee; b += 8) {
+      b[0] = c;
+      b[1] = c;
+      b[2] = c;
+      b[3] = c;
+      b[4] = c;
+      b[5] = c;
+      b[6] = c;
+      b[7] = c;
+    }
+    // Leftovers
+    for (; b != e; ++b) {
+      *b = c;
+    }
+  }
+}
+
+/*
+ * Lightly structured memcpy, simplifies copying PODs and introduces
+ * some asserts. Unfortunately using this function may cause
+ * measurable overhead (presumably because it adjusts from a begin/end
+ * convention to a pointer/size convention, so it does some extra
+ * arithmetic even though the caller might have done the inverse
+ * adaptation outside).
+ */
+template <class Pod>
+inline void pod_copy(const Pod* b, const Pod* e, Pod* d) {
+  assert(e >= b);
+  assert(d >= e || d + (e - b) <= b);
+  memcpy(d, b, (e - b) * sizeof(Pod));
+}
+
+/*
+ * Lightly structured memmove, simplifies copying PODs and introduces
+ * some asserts
+ */
+template <class Pod>
+inline void pod_move(const Pod* b, const Pod* e, Pod* d) {
+  assert(e >= b);
+  memmove(d, b, (e - b) * sizeof(*b));
+}
+
+} // namespace fbstring_detail
+
+/**
+ * Defines a special acquisition method for constructing fbstring
+ * objects. AcquireMallocatedString means that the user passes a
+ * pointer to a malloc-allocated string that the fbstring object will
+ * take into custody.
+ */
+enum class AcquireMallocatedString {};
+
+/*
+ * fbstring_core_model is a mock-up type that defines all required
+ * signatures of a fbstring core. The fbstring class itself uses such
+ * a core object to implement all of the numerous member functions
+ * required by the standard.
+ *
+ * If you want to define a new core, copy the definition below and
+ * implement the primitives. Then plug the core into basic_fbstring as
+ * a template argument.
+
+template <class Char>
+class fbstring_core_model {
+public:
+  fbstring_core_model();
+  fbstring_core_model(const fbstring_core_model &);
+  ~fbstring_core_model();
+  // Returns a pointer to string's buffer (currently only contiguous
+  // strings are supported). The pointer is guaranteed to be valid
+  // until the next call to a non-const member function.
+  const Char * data() const;
+  // Much like data(), except the string is prepared to support
+  // character-level changes. This call is a signal for
+  // e.g. reference-counted implementation to fork the data. The
+  // pointer is guaranteed to be valid until the next call to a
+  // non-const member function.
+  Char * mutable_data();
+  // Returns a pointer to string's buffer and guarantees that a
+  // readable '\0' lies right after the buffer. The pointer is
+  // guaranteed to be valid until the next call to a non-const member
+  // function.
+  const Char * c_str() const;
+  // Shrinks the string by delta characters. Asserts that delta <=
+  // size().
+  void shrink(size_t delta);
+  // Expands the string by delta characters (i.e. after this call
+  // size() will report the old size() plus delta) but without
+  // initializing the expanded region. Returns a pointer to the memory
+  // to be initialized (the beginning of the expanded portion). The
+  // caller is expected to fill the expanded area appropriately.
+  Char* expand_noinit(size_t delta);
+  // Expands the string by one character and sets the last character
+  // to c.
+  void push_back(Char c);
+  // Returns the string's size.
+  size_t size() const;
+  // Returns the string's capacity, i.e. maximum size that the string
+  // can grow to without reallocation. Note that for reference counted
+  // strings that's technically a lie - even assigning characters
+  // within the existing size would cause a reallocation.
+  size_t capacity() const;
+  // Returns true if the data underlying the string is actually shared
+  // across multiple strings (in a refcounted fashion).
+  bool isShared() const;
+  // Makes sure that at least minCapacity characters are available for
+  // the string without reallocation. For reference-counted strings,
+  // it should fork the data even if minCapacity < size().
+  void reserve(size_t minCapacity);
+private:
+  // Do not implement
+  fbstring_core_model& operator=(const fbstring_core_model &);
+};
+*/
+
+/**
+ * gcc-4.7 throws what appears to be some false positive uninitialized
+ * warnings for the members of the MediumLarge struct.  So, mute them here.
+ */
+#if defined(__GNUC__) && !defined(__clang__)
+# pragma GCC diagnostic push
+# pragma GCC diagnostic ignored "-Wuninitialized"
+#endif
+
+/**
+ * This is the core of the string. The code should work on 32- and
+ * 64-bit architectures and with any Char size. Porting to big endian
+ * architectures would require some changes.
+ *
+ * The storage is selected as follows (assuming we store one-byte
+ * characters on a 64-bit machine): (a) "small" strings between 0 and
+ * 23 chars are stored in-situ without allocation (the rightmost byte
+ * stores the size); (b) "medium" strings from 24 through 254 chars
+ * are stored in malloc-allocated memory that is copied eagerly; (c)
+ * "large" strings of 255 chars and above are stored in a similar
+ * structure as medium arrays, except that the string is
+ * reference-counted and copied lazily. the reference count is
+ * allocated right before the character array.
+ *
+ * The discriminator between these three strategies sits in the two
+ * most significant bits of the rightmost char of the storage. If
+ * neither is set, then the string is small (and its length sits in
+ * the lower-order bits of that rightmost character). If the MSb is
+ * set, the string is medium width. If the second MSb is set, then the
+ * string is large.
+ */
+template <class Char> class fbstring_core {
+public:
+  fbstring_core() noexcept {
+    // Only initialize the tag, will set the MSBs (i.e. the small
+    // string size) to zero too
+    ml_.capacity_ = maxSmallSize << (8 * (sizeof(size_t) - sizeof(Char)));
+    // or: setSmallSize(0);
+    writeTerminator();
+    assert(category() == isSmall && size() == 0);
+  }
+
+  fbstring_core(const fbstring_core & rhs) {
+    assert(&rhs != this);
+    // Simplest case first: small strings are bitblitted
+    if (rhs.category() == isSmall) {
+      assert(offsetof(MediumLarge, data_) == 0);
+      assert(offsetof(MediumLarge, size_) == sizeof(ml_.data_));
+      assert(offsetof(MediumLarge, capacity_) == 2 * sizeof(ml_.data_));
+      const size_t size = rhs.smallSize();
+      if (size == 0) {
+        ml_.capacity_ = rhs.ml_.capacity_;
+        writeTerminator();
+      } else {
+        // Just write the whole thing, don't look at details. In
+        // particular we need to copy capacity anyway because we want
+        // to set the size (don't forget that the last character,
+        // which stores a short string's length, is shared with the
+        // ml_.capacity field).
+        ml_ = rhs.ml_;
+      }
+      assert(category() == isSmall && this->size() == rhs.size());
+    } else if (rhs.category() == isLarge) {
+      // Large strings are just refcounted
+      ml_ = rhs.ml_;
+      RefCounted::incrementRefs(ml_.data_);
+      assert(category() == isLarge && size() == rhs.size());
+    } else {
+      // Medium strings are copied eagerly. Don't forget to allocate
+      // one extra Char for the null terminator.
+      auto const allocSize =
+           goodMallocSize((1 + rhs.ml_.size_) * sizeof(Char));
+      ml_.data_ = static_cast<Char*>(checkedMalloc(allocSize));
+      fbstring_detail::pod_copy(rhs.ml_.data_,
+                                // 1 for terminator
+                                rhs.ml_.data_ + rhs.ml_.size_ + 1,
+                                ml_.data_);
+      // No need for writeTerminator() here, we copied one extra
+      // element just above.
+      ml_.size_ = rhs.ml_.size_;
+      ml_.capacity_ = (allocSize / sizeof(Char) - 1) | isMedium;
+      assert(category() == isMedium);
+    }
+    assert(size() == rhs.size());
+    assert(memcmp(data(), rhs.data(), size() * sizeof(Char)) == 0);
+  }
+
+  fbstring_core(fbstring_core&& goner) noexcept {
+    if (goner.category() == isSmall) {
+      // Just copy, leave the goner in peace
+      new(this) fbstring_core(goner.small_, goner.smallSize());
+    } else {
+      // Take goner's guts
+      ml_ = goner.ml_;
+      // Clean goner's carcass
+      goner.setSmallSize(0);
+    }
+  }
+
+  // NOTE(agallagher): The word-aligned copy path copies bytes which are
+  // outside the range of the string, and makes address sanitizer unhappy,
+  // so just disable it on this function.
+  fbstring_core(const Char *const data, const size_t size)
+      FBSTRING_DISABLE_ADDRESS_SANITIZER {
+    // Simplest case first: small strings are bitblitted
+    if (size <= maxSmallSize) {
+      // Layout is: Char* data_, size_t size_, size_t capacity_
+      /*static_*/assert(sizeof(*this) == sizeof(Char*) + 2 * sizeof(size_t));
+      /*static_*/assert(sizeof(Char*) == sizeof(size_t));
+      // sizeof(size_t) must be a power of 2
+      /*static_*/assert((sizeof(size_t) & (sizeof(size_t) - 1)) == 0);
+
+      // If data is aligned, use fast word-wise copying. Otherwise,
+      // use conservative memcpy.
+      if (reinterpret_cast<size_t>(data) & (sizeof(size_t) - 1)) {
+        fbstring_detail::pod_copy(data, data + size, small_);
+      } else {
+        // Copy one word (64 bits) at a time
+        const size_t byteSize = size * sizeof(Char);
+        if (byteSize > 2 * sizeof(size_t)) {
+          // Copy three words
+          ml_.capacity_ = reinterpret_cast<const size_t*>(data)[2];
+          copyTwo:
+          ml_.size_ = reinterpret_cast<const size_t*>(data)[1];
+          copyOne:
+          ml_.data_ = *reinterpret_cast<Char**>(const_cast<Char*>(data));
+        } else if (byteSize > sizeof(size_t)) {
+          // Copy two words
+          goto copyTwo;
+        } else if (size > 0) {
+          // Copy one word
+          goto copyOne;
+        }
+      }
+      setSmallSize(size);
+    } else if (size <= maxMediumSize) {
+      // Medium strings are allocated normally. Don't forget to
+      // allocate one extra Char for the terminating null.
+      auto const allocSize = goodMallocSize((1 + size) * sizeof(Char));
+      ml_.data_ = static_cast<Char*>(checkedMalloc(allocSize));
+      fbstring_detail::pod_copy(data, data + size, ml_.data_);
+      ml_.size_ = size;
+      ml_.capacity_ = (allocSize / sizeof(Char) - 1) | isMedium;
+    } else {
+      // Large strings are allocated differently
+      size_t effectiveCapacity = size;
+      auto const newRC = RefCounted::create(data, & effectiveCapacity);
+      ml_.data_ = newRC->data_;
+      ml_.size_ = size;
+      ml_.capacity_ = effectiveCapacity | isLarge;
+    }
+    writeTerminator();
+    assert(this->size() == size);
+    assert(memcmp(this->data(), data, size * sizeof(Char)) == 0);
+  }
+
+  ~fbstring_core() noexcept {
+    auto const c = category();
+    if (c == isSmall) {
+      return;
+    }
+    if (c == isMedium) {
+      free(ml_.data_);
+      return;
+    }
+    RefCounted::decrementRefs(ml_.data_);
+  }
+
+  // Snatches a previously mallocated string. The parameter "size"
+  // is the size of the string, and the parameter "allocatedSize"
+  // is the size of the mallocated block.  The string must be
+  // \0-terminated, so allocatedSize >= size + 1 and data[size] == '\0'.
+  //
+  // So if you want a 2-character string, pass malloc(3) as "data",
+  // pass 2 as "size", and pass 3 as "allocatedSize".
+  fbstring_core(Char * const data,
+                const size_t size,
+                const size_t allocatedSize,
+                AcquireMallocatedString) {
+    if (size > 0) {
+      assert(allocatedSize >= size + 1);
+      assert(data[size] == '\0');
+      // Use the medium string storage
+      ml_.data_ = data;
+      ml_.size_ = size;
+      // Don't forget about null terminator
+      ml_.capacity_ = (allocatedSize - 1) | isMedium;
+    } else {
+      // No need for the memory
+      free(data);
+      setSmallSize(0);
+    }
+  }
+
+  // swap below doesn't test whether &rhs == this (and instead
+  // potentially does extra work) on the premise that the rarity of
+  // that situation actually makes the check more expensive than is
+  // worth.
+  void swap(fbstring_core & rhs) {
+    auto const t = ml_;
+    ml_ = rhs.ml_;
+    rhs.ml_ = t;
+  }
+
+  // In C++11 data() and c_str() are 100% equivalent.
+  const Char * data() const {
+    return c_str();
+  }
+
+  Char * mutable_data() {
+    auto const c = category();
+    if (c == isSmall) {
+      return small_;
+    }
+    assert(c == isMedium || c == isLarge);
+    if (c == isLarge && RefCounted::refs(ml_.data_) > 1) {
+      // Ensure unique.
+      size_t effectiveCapacity = ml_.capacity();
+      auto const newRC = RefCounted::create(& effectiveCapacity);
+      // If this fails, someone placed the wrong capacity in an
+      // fbstring.
+      assert(effectiveCapacity >= ml_.capacity());
+      fbstring_detail::pod_copy(ml_.data_, ml_.data_ + ml_.size_ + 1,
+                                newRC->data_);
+      RefCounted::decrementRefs(ml_.data_);
+      ml_.data_ = newRC->data_;
+      // No need to call writeTerminator(), we have + 1 above.
+    }
+    return ml_.data_;
+  }
+
+  const Char * c_str() const {
+    auto const c = category();
+#ifdef FBSTRING_PERVERSE
+    if (c == isSmall) {
+      assert(small_[smallSize()] == TERMINATOR || smallSize() == maxSmallSize
+             || small_[smallSize()] == '\0');
+      small_[smallSize()] = '\0';
+      return small_;
+    }
+    assert(c == isMedium || c == isLarge);
+    assert(ml_.data_[ml_.size_] == TERMINATOR || ml_.data_[ml_.size_] == '\0');
+    ml_.data_[ml_.size_] = '\0';
+#elif defined(FBSTRING_CONSERVATIVE)
+    if (c == isSmall) {
+      assert(small_[smallSize()] == '\0');
+      return small_;
+    }
+    assert(c == isMedium || c == isLarge);
+    assert(ml_.data_[ml_.size_] == '\0');
+#else
+    if (c == isSmall) {
+      small_[smallSize()] = '\0';
+      return small_;
+    }
+    assert(c == isMedium || c == isLarge);
+    ml_.data_[ml_.size_] = '\0';
+#endif
+    return ml_.data_;
+  }
+
+  void shrink(const size_t delta) {
+    if (category() == isSmall) {
+      // Check for underflow
+      assert(delta <= smallSize());
+      setSmallSize(smallSize() - delta);
+    } else if (category() == isMedium || RefCounted::refs(ml_.data_) == 1) {
+      // Medium strings and unique large strings need no special
+      // handling.
+      assert(ml_.size_ >= delta);
+      ml_.size_ -= delta;
+    } else {
+      assert(ml_.size_ >= delta);
+      // Shared large string, must make unique. This is because of the
+      // durn terminator must be written, which may trample the shared
+      // data.
+      if (delta) {
+        fbstring_core(ml_.data_, ml_.size_ - delta).swap(*this);
+      }
+      // No need to write the terminator.
+      return;
+    }
+    writeTerminator();
+  }
+
+  void reserve(size_t minCapacity) {
+    if (category() == isLarge) {
+      // Ensure unique
+      if (RefCounted::refs(ml_.data_) > 1) {
+        // We must make it unique regardless; in-place reallocation is
+        // useless if the string is shared. In order to not surprise
+        // people, reserve the new block at current capacity or
+        // more. That way, a string's capacity never shrinks after a
+        // call to reserve.
+        minCapacity = std::max(minCapacity, ml_.capacity());
+        auto const newRC = RefCounted::create(& minCapacity);
+        fbstring_detail::pod_copy(ml_.data_, ml_.data_ + ml_.size_ + 1,
+                                   newRC->data_);
+        // Done with the old data. No need to call writeTerminator(),
+        // we have + 1 above.
+        RefCounted::decrementRefs(ml_.data_);
+        ml_.data_ = newRC->data_;
+        ml_.capacity_ = minCapacity | isLarge;
+        // size remains unchanged
+      } else {
+        // String is not shared, so let's try to realloc (if needed)
+        if (minCapacity > ml_.capacity()) {
+          // Asking for more memory
+          auto const newRC =
+               RefCounted::reallocate(ml_.data_, ml_.size_,
+                                      ml_.capacity(), minCapacity);
+          ml_.data_ = newRC->data_;
+          ml_.capacity_ = minCapacity | isLarge;
+          writeTerminator();
+        }
+        assert(capacity() >= minCapacity);
+      }
+    } else if (category() == isMedium) {
+      // String is not shared
+      if (minCapacity <= ml_.capacity()) {
+        return; // nothing to do, there's enough room
+      }
+      if (minCapacity <= maxMediumSize) {
+        // Keep the string at medium size. Don't forget to allocate
+        // one extra Char for the terminating null.
+        size_t capacityBytes = goodMallocSize((1 + minCapacity) * sizeof(Char));
+        ml_.data_ = static_cast<Char *>(
+          smartRealloc(
+            ml_.data_,
+            ml_.size_ * sizeof(Char),
+            (ml_.capacity() + 1) * sizeof(Char),
+            capacityBytes));
+        writeTerminator();
+        ml_.capacity_ = (capacityBytes / sizeof(Char) - 1) | isMedium;
+      } else {
+        // Conversion from medium to large string
+        fbstring_core nascent;
+        // Will recurse to another branch of this function
+        nascent.reserve(minCapacity);
+        nascent.ml_.size_ = ml_.size_;
+        fbstring_detail::pod_copy(ml_.data_, ml_.data_ + ml_.size_,
+                                  nascent.ml_.data_);
+        nascent.swap(*this);
+        writeTerminator();
+        assert(capacity() >= minCapacity);
+      }
+    } else {
+      assert(category() == isSmall);
+      if (minCapacity > maxMediumSize) {
+        // large
+        auto const newRC = RefCounted::create(& minCapacity);
+        auto const size = smallSize();
+        fbstring_detail::pod_copy(small_, small_ + size + 1, newRC->data_);
+        // No need for writeTerminator(), we wrote it above with + 1.
+        ml_.data_ = newRC->data_;
+        ml_.size_ = size;
+        ml_.capacity_ = minCapacity | isLarge;
+        assert(capacity() >= minCapacity);
+      } else if (minCapacity > maxSmallSize) {
+        // medium
+        // Don't forget to allocate one extra Char for the terminating null
+        auto const allocSizeBytes =
+          goodMallocSize((1 + minCapacity) * sizeof(Char));
+        auto const data = static_cast<Char*>(checkedMalloc(allocSizeBytes));
+        auto const size = smallSize();
+        fbstring_detail::pod_copy(small_, small_ + size + 1, data);
+        // No need for writeTerminator(), we wrote it above with + 1.
+        ml_.data_ = data;
+        ml_.size_ = size;
+        ml_.capacity_ = (allocSizeBytes / sizeof(Char) - 1) | isMedium;
+      } else {
+        // small
+        // Nothing to do, everything stays put
+      }
+    }
+    assert(capacity() >= minCapacity);
+  }
+
+  Char * expand_noinit(const size_t delta) {
+    // Strategy is simple: make room, then change size
+    assert(capacity() >= size());
+    size_t sz, newSz;
+    if (category() == isSmall) {
+      sz = smallSize();
+      newSz = sz + delta;
+      if (newSz <= maxSmallSize) {
+        setSmallSize(newSz);
+        writeTerminator();
+        return small_ + sz;
+      }
+      reserve(newSz);
+    } else {
+      sz = ml_.size_;
+      newSz = ml_.size_ + delta;
+      if (newSz > capacity()) {
+        reserve(newSz);
+      }
+    }
+    assert(capacity() >= newSz);
+    // Category can't be small - we took care of that above
+    assert(category() == isMedium || category() == isLarge);
+    ml_.size_ = newSz;
+    writeTerminator();
+    assert(size() == newSz);
+    return ml_.data_ + sz;
+  }
+
+  void push_back(Char c) {
+    assert(capacity() >= size());
+    size_t sz;
+    if (category() == isSmall) {
+      sz = smallSize();
+      if (sz < maxSmallSize) {
+        setSmallSize(sz + 1);
+        small_[sz] = c;
+        writeTerminator();
+        return;
+      }
+      reserve(maxSmallSize * 2);
+    } else {
+      sz = ml_.size_;
+      if (sz == capacity()) {  // always true for isShared()
+        reserve(1 + sz * 3 / 2);  // ensures not shared
+      }
+    }
+    assert(!isShared());
+    assert(capacity() >= sz + 1);
+    // Category can't be small - we took care of that above
+    assert(category() == isMedium || category() == isLarge);
+    ml_.size_ = sz + 1;
+    ml_.data_[sz] = c;
+    writeTerminator();
+  }
+
+  size_t size() const {
+    return category() == isSmall ? smallSize() : ml_.size_;
+  }
+
+  size_t capacity() const {
+    switch (category()) {
+      case isSmall:
+        return maxSmallSize;
+      case isLarge:
+        // For large-sized strings, a multi-referenced chunk has no
+        // available capacity. This is because any attempt to append
+        // data would trigger a new allocation.
+        if (RefCounted::refs(ml_.data_) > 1) return ml_.size_;
+      default: {}
+    }
+    return ml_.capacity();
+  }
+
+  bool isShared() const {
+    return category() == isLarge && RefCounted::refs(ml_.data_) > 1;
+  }
+
+#ifdef FBSTRING_PERVERSE
+  enum { TERMINATOR = '^' };
+#else
+  enum { TERMINATOR = '\0' };
+#endif
+
+  void writeTerminator() {
+#if defined(FBSTRING_PERVERSE) || defined(FBSTRING_CONSERVATIVE)
+    if (category() == isSmall) {
+      const auto s = smallSize();
+      if (s != maxSmallSize) {
+        small_[s] = TERMINATOR;
+      }
+    } else {
+      ml_.data_[ml_.size_] = TERMINATOR;
+    }
+#endif
+  }
+
+private:
+  // Disabled
+  fbstring_core & operator=(const fbstring_core & rhs);
+
+  struct MediumLarge {
+    Char * data_;
+    size_t size_;
+    size_t capacity_;
+
+    size_t capacity() const {
+      return capacity_ & capacityExtractMask;
+    }
+  };
+
+  struct RefCounted {
+    std::atomic<size_t> refCount_;
+    Char data_[1];
+
+    static RefCounted * fromData(Char * p) {
+      return static_cast<RefCounted*>(
+        static_cast<void*>(
+          static_cast<unsigned char*>(static_cast<void*>(p))
+          - sizeof(refCount_)));
+    }
+
+    static size_t refs(Char * p) {
+      return fromData(p)->refCount_.load(std::memory_order_acquire);
+    }
+
+    static void incrementRefs(Char * p) {
+      fromData(p)->refCount_.fetch_add(1, std::memory_order_acq_rel);
+    }
+
+    static void decrementRefs(Char * p) {
+      auto const dis = fromData(p);
+      size_t oldcnt = dis->refCount_.fetch_sub(1, std::memory_order_acq_rel);
+      assert(oldcnt > 0);
+      if (oldcnt == 1) {
+        free(dis);
+      }
+    }
+
+    static RefCounted * create(size_t * size) {
+      // Don't forget to allocate one extra Char for the terminating
+      // null. In this case, however, one Char is already part of the
+      // struct.
+      const size_t allocSize = goodMallocSize(
+        sizeof(RefCounted) + *size * sizeof(Char));
+      auto result = static_cast<RefCounted*>(checkedMalloc(allocSize));
+      result->refCount_.store(1, std::memory_order_release);
+      *size = (allocSize - sizeof(RefCounted)) / sizeof(Char);
+      return result;
+    }
+
+    static RefCounted * create(const Char * data, size_t * size) {
+      const size_t effectiveSize = *size;
+      auto result = create(size);
+      fbstring_detail::pod_copy(data, data + effectiveSize, result->data_);
+      return result;
+    }
+
+    static RefCounted * reallocate(Char *const data,
+                                   const size_t currentSize,
+                                   const size_t currentCapacity,
+                                   const size_t newCapacity) {
+      assert(newCapacity > 0 && newCapacity > currentSize);
+      auto const dis = fromData(data);
+      assert(dis->refCount_.load(std::memory_order_acquire) == 1);
+      // Don't forget to allocate one extra Char for the terminating
+      // null. In this case, however, one Char is already part of the
+      // struct.
+      auto result = static_cast<RefCounted*>(
+             smartRealloc(dis,
+                          sizeof(RefCounted) + currentSize * sizeof(Char),
+                          sizeof(RefCounted) + currentCapacity * sizeof(Char),
+                          sizeof(RefCounted) + newCapacity * sizeof(Char)));
+      assert(result->refCount_.load(std::memory_order_acquire) == 1);
+      return result;
+    }
+  };
+
+  union {
+    mutable Char small_[sizeof(MediumLarge) / sizeof(Char)];
+    mutable MediumLarge ml_;
+  };
+
+  enum {
+    lastChar = sizeof(MediumLarge) - 1,
+    maxSmallSize = lastChar / sizeof(Char),
+    maxMediumSize = 254 / sizeof(Char),            // coincides with the small
+                                                   // bin size in dlmalloc
+    categoryExtractMask = sizeof(size_t) == 4 ? 0xC0000000 : 0xC000000000000000,
+    capacityExtractMask = ~categoryExtractMask,
+  };
+  static_assert(!(sizeof(MediumLarge) % sizeof(Char)),
+                "Corrupt memory layout for fbstring.");
+
+  enum Category {
+    isSmall = 0,
+    isMedium = sizeof(size_t) == 4 ? 0x80000000 : 0x8000000000000000,
+    isLarge =  sizeof(size_t) == 4 ? 0x40000000 : 0x4000000000000000,
+  };
+
+  Category category() const {
+    // Assumes little endian
+    return static_cast<Category>(ml_.capacity_ & categoryExtractMask);
+  }
+
+  size_t smallSize() const {
+    assert(category() == isSmall &&
+           static_cast<size_t>(small_[maxSmallSize])
+           <= static_cast<size_t>(maxSmallSize));
+    return static_cast<size_t>(maxSmallSize)
+      - static_cast<size_t>(small_[maxSmallSize]);
+  }
+
+  void setSmallSize(size_t s) {
+    // Warning: this should work with uninitialized strings too,
+    // so don't assume anything about the previous value of
+    // small_[maxSmallSize].
+    assert(s <= maxSmallSize);
+    small_[maxSmallSize] = maxSmallSize - s;
+  }
+};
+
+#if defined(__GNUC__) && !defined(__clang__)
+# pragma GCC diagnostic pop
+#endif
+
+#ifndef _LIBSTDCXX_FBSTRING
+/**
+ * Dummy fbstring core that uses an actual std::string. This doesn't
+ * make any sense - it's just for testing purposes.
+ */
+template <class Char>
+class dummy_fbstring_core {
+public:
+  dummy_fbstring_core() {
+  }
+  dummy_fbstring_core(const dummy_fbstring_core& another)
+      : backend_(another.backend_) {
+  }
+  dummy_fbstring_core(const Char * s, size_t n)
+      : backend_(s, n) {
+  }
+  void swap(dummy_fbstring_core & rhs) {
+    backend_.swap(rhs.backend_);
+  }
+  const Char * data() const {
+    return backend_.data();
+  }
+  Char * mutable_data() {
+    //assert(!backend_.empty());
+    return &*backend_.begin();
+  }
+  void shrink(size_t delta) {
+    assert(delta <= size());
+    backend_.resize(size() - delta);
+  }
+  Char * expand_noinit(size_t delta) {
+    auto const sz = size();
+    backend_.resize(size() + delta);
+    return backend_.data() + sz;
+  }
+  void push_back(Char c) {
+    backend_.push_back(c);
+  }
+  size_t size() const {
+    return backend_.size();
+  }
+  size_t capacity() const {
+    return backend_.capacity();
+  }
+  bool isShared() const {
+    return false;
+  }
+  void reserve(size_t minCapacity) {
+    backend_.reserve(minCapacity);
+  }
+
+private:
+  std::basic_string<Char> backend_;
+};
+#endif // !_LIBSTDCXX_FBSTRING
+
+/**
+ * This is the basic_string replacement. For conformity,
+ * basic_fbstring takes the same template parameters, plus the last
+ * one which is the core.
+ */
+#ifdef _LIBSTDCXX_FBSTRING
+template <typename E, class T, class A, class Storage>
+#else
+template <typename E,
+          class T = std::char_traits<E>,
+          class A = std::allocator<E>,
+          class Storage = fbstring_core<E> >
+#endif
+class basic_fbstring {
+
+  static void enforce(
+      bool condition,
+      void (*throw_exc)(const char*),
+      const char* msg) {
+    if (!condition) throw_exc(msg);
+  }
+
+  bool isSane() const {
+    return
+      begin() <= end() &&
+      empty() == (size() == 0) &&
+      empty() == (begin() == end()) &&
+      size() <= max_size() &&
+      capacity() <= max_size() &&
+      size() <= capacity() &&
+      (begin()[size()] == Storage::TERMINATOR || begin()[size()] == '\0');
+  }
+
+  struct Invariant;
+  friend struct Invariant;
+  struct Invariant {
+#ifndef NDEBUG
+    explicit Invariant(const basic_fbstring& s) : s_(s) {
+      assert(s_.isSane());
+    }
+    ~Invariant() {
+      assert(s_.isSane());
+    }
+  private:
+    const basic_fbstring& s_;
+#else
+    explicit Invariant(const basic_fbstring&) {}
+#endif
+    Invariant& operator=(const Invariant&);
+  };
+
+public:
+  // types
+  typedef T traits_type;
+  typedef typename traits_type::char_type value_type;
+  typedef A allocator_type;
+  typedef typename A::size_type size_type;
+  typedef typename A::difference_type difference_type;
+
+  typedef typename A::reference reference;
+  typedef typename A::const_reference const_reference;
+  typedef typename A::pointer pointer;
+  typedef typename A::const_pointer const_pointer;
+
+  typedef E* iterator;
+  typedef const E* const_iterator;
+  typedef std::reverse_iterator<iterator
+#ifdef NO_ITERATOR_TRAITS
+                                , value_type
+#endif
+                                > reverse_iterator;
+  typedef std::reverse_iterator<const_iterator
+#ifdef NO_ITERATOR_TRAITS
+                                , const value_type
+#endif
+                                > const_reverse_iterator;
+
+  static const size_type npos;                     // = size_type(-1)
+
+private:
+  static void procrustes(size_type& n, size_type nmax) {
+    if (n > nmax) n = nmax;
+  }
+
+public:
+  // C++11 21.4.2 construct/copy/destroy
+  explicit basic_fbstring(const A& a = A()) noexcept {
+  }
+
+  basic_fbstring(const basic_fbstring& str)
+      : store_(str.store_) {
+  }
+
+  // Move constructor
+  basic_fbstring(basic_fbstring&& goner) noexcept
+      : store_(std::move(goner.store_)) {
+  }
+
+#ifndef _LIBSTDCXX_FBSTRING
+  // This is defined for compatibility with std::string
+  /* implicit */ basic_fbstring(const std::string& str)
+      : store_(str.data(), str.size()) {
+  }
+#endif
+
+  basic_fbstring(const basic_fbstring& str, size_type pos,
+                 size_type n = npos, const A& a = A()) {
+    assign(str, pos, n);
+  }
+
+  /* implicit */ basic_fbstring(const value_type* s, const A& a = A())
+      : store_(s, s ? traits_type::length(s) : ({
+          basic_fbstring<char> err = __PRETTY_FUNCTION__;
+          err += ": null pointer initializer not valid";
+          std::__throw_logic_error(err.c_str());
+          0;
+      })) {
+  }
+
+  basic_fbstring(const value_type* s, size_type n, const A& a = A())
+      : store_(s, n) {
+  }
+
+  basic_fbstring(size_type n, value_type c, const A& a = A()) {
+    auto const data = store_.expand_noinit(n);
+    fbstring_detail::pod_fill(data, data + n, c);
+    store_.writeTerminator();
+  }
+
+  template <class InIt>
+  basic_fbstring(InIt begin, InIt end,
+                 typename std::enable_if<
+                 !std::is_same<typename std::remove_const<InIt>::type,
+                 value_type*>::value, const A>::type & a = A()) {
+    assign(begin, end);
+  }
+
+  // Specialization for const char*, const char*
+  basic_fbstring(const value_type* b, const value_type* e)
+      : store_(b, e - b) {
+  }
+
+  // Nonstandard constructor
+  basic_fbstring(value_type *s, size_type n, size_type c,
+                 AcquireMallocatedString a)
+      : store_(s, n, c, a) {
+  }
+
+  // Construction from initialization list
+  basic_fbstring(std::initializer_list<value_type> il) {
+    assign(il.begin(), il.end());
+  }
+
+  ~basic_fbstring() noexcept {
+  }
+
+  basic_fbstring& operator=(const basic_fbstring& lhs) {
+    if (FBSTRING_UNLIKELY(&lhs == this)) {
+      return *this;
+    }
+    auto const oldSize = size();
+    auto const srcSize = lhs.size();
+    if (capacity() >= srcSize && !store_.isShared()) {
+      // great, just copy the contents
+      if (oldSize < srcSize)
+        store_.expand_noinit(srcSize - oldSize);
+      else
+        store_.shrink(oldSize - srcSize);
+      assert(size() == srcSize);
+      fbstring_detail::pod_copy(lhs.begin(), lhs.end(), begin());
+      store_.writeTerminator();
+    } else {
+      // need to reallocate, so we may as well create a brand new string
+      basic_fbstring(lhs).swap(*this);
+    }
+    return *this;
+  }
+
+  // Move assignment
+  basic_fbstring& operator=(basic_fbstring&& goner) noexcept {
+    if (FBSTRING_UNLIKELY(&goner == this)) {
+      // Compatibility with std::basic_string<>,
+      // C++11 21.4.2 [string.cons] / 23 requires self-move-assignment support.
+      return *this;
+    }
+    // No need of this anymore
+    this->~basic_fbstring();
+    // Move the goner into this
+    new(&store_) fbstring_core<E>(std::move(goner.store_));
+    return *this;
+  }
+
+#ifndef _LIBSTDCXX_FBSTRING
+  // Compatibility with std::string
+  basic_fbstring & operator=(const std::string & rhs) {
+    return assign(rhs.data(), rhs.size());
+  }
+
+  // Compatibility with std::string
+  std::string toStdString() const {
+    return std::string(data(), size());
+  }
+#else
+  // A lot of code in fbcode still uses this method, so keep it here for now.
+  const basic_fbstring& toStdString() const {
+    return *this;
+  }
+#endif
+
+  basic_fbstring& operator=(const value_type* s) {
+    return assign(s);
+  }
+
+  basic_fbstring& operator=(value_type c) {
+    if (empty()) {
+      store_.expand_noinit(1);
+    } else if (store_.isShared()) {
+      basic_fbstring(1, c).swap(*this);
+      return *this;
+    } else {
+      store_.shrink(size() - 1);
+    }
+    *store_.mutable_data() = c;
+    store_.writeTerminator();
+    return *this;
+  }
+
+  basic_fbstring& operator=(std::initializer_list<value_type> il) {
+    return assign(il.begin(), il.end());
+  }
+
+  // C++11 21.4.3 iterators:
+  iterator begin() { return store_.mutable_data(); }
+
+  const_iterator begin() const { return store_.data(); }
+
+  const_iterator cbegin() const { return begin(); }
+
+  iterator end() {
+    return store_.mutable_data() + store_.size();
+  }
+
+  const_iterator end() const {
+    return store_.data() + store_.size();
+  }
+
+  const_iterator cend() const { return end(); }
+
+  reverse_iterator rbegin() {
+    return reverse_iterator(end());
+  }
+
+  const_reverse_iterator rbegin() const {
+    return const_reverse_iterator(end());
+  }
+
+  const_reverse_iterator crbegin() const { return rbegin(); }
+
+  reverse_iterator rend() {
+    return reverse_iterator(begin());
+  }
+
+  const_reverse_iterator rend() const {
+    return const_reverse_iterator(begin());
+  }
+
+  const_reverse_iterator crend() const { return rend(); }
+
+  // Added by C++11
+  // C++11 21.4.5, element access:
+  const value_type& front() const { return *begin(); }
+  const value_type& back() const {
+    assert(!empty());
+    // Should be begin()[size() - 1], but that branches twice
+    return *(end() - 1);
+  }
+  value_type& front() { return *begin(); }
+  value_type& back() {
+    assert(!empty());
+    // Should be begin()[size() - 1], but that branches twice
+    return *(end() - 1);
+  }
+  void pop_back() {
+    assert(!empty());
+    store_.shrink(1);
+  }
+
+  // C++11 21.4.4 capacity:
+  size_type size() const { return store_.size(); }
+
+  size_type length() const { return size(); }
+
+  size_type max_size() const {
+    return std::numeric_limits<size_type>::max();
+  }
+
+  void resize(const size_type n, const value_type c = value_type()) {
+    auto size = this->size();
+    if (n <= size) {
+      store_.shrink(size - n);
+    } else {
+      // Do this in two steps to minimize slack memory copied (see
+      // smartRealloc).
+      auto const capacity = this->capacity();
+      assert(capacity >= size);
+      if (size < capacity) {
+        auto delta = std::min(n, capacity) - size;
+        store_.expand_noinit(delta);
+        fbstring_detail::pod_fill(begin() + size, end(), c);
+        size += delta;
+        if (size == n) {
+          store_.writeTerminator();
+          return;
+        }
+        assert(size < n);
+      }
+      auto const delta = n - size;
+      store_.expand_noinit(delta);
+      fbstring_detail::pod_fill(end() - delta, end(), c);
+      store_.writeTerminator();
+    }
+    assert(this->size() == n);
+  }
+
+  size_type capacity() const { return store_.capacity(); }
+
+  void reserve(size_type res_arg = 0) {
+    enforce(res_arg <= max_size(), std::__throw_length_error, "");
+    store_.reserve(res_arg);
+  }
+
+  void shrink_to_fit() {
+    // Shrink only if slack memory is sufficiently large
+    if (capacity() < size() * 3 / 2) {
+      return;
+    }
+    basic_fbstring(cbegin(), cend()).swap(*this);
+  }
+
+  void clear() { resize(0); }
+
+  bool empty() const { return size() == 0; }
+
+  // C++11 21.4.5 element access:
+  const_reference operator[](size_type pos) const {
+    return *(c_str() + pos);
+  }
+
+  reference operator[](size_type pos) {
+    if (pos == size()) {
+      // Just call c_str() to make sure '\0' is present
+      c_str();
+    }
+    return *(begin() + pos);
+  }
+
+  const_reference at(size_type n) const {
+    enforce(n <= size(), std::__throw_out_of_range, "");
+    return (*this)[n];
+  }
+
+  reference at(size_type n) {
+    enforce(n < size(), std::__throw_out_of_range, "");
+    return (*this)[n];
+  }
+
+  // C++11 21.4.6 modifiers:
+  basic_fbstring& operator+=(const basic_fbstring& str) {
+    return append(str);
+  }
+
+  basic_fbstring& operator+=(const value_type* s) {
+    return append(s);
+  }
+
+  basic_fbstring& operator+=(const value_type c) {
+    push_back(c);
+    return *this;
+  }
+
+  basic_fbstring& operator+=(std::initializer_list<value_type> il) {
+    append(il);
+    return *this;
+  }
+
+  basic_fbstring& append(const basic_fbstring& str) {
+#ifndef NDEBUG
+    auto desiredSize = size() + str.size();
+#endif
+    append(str.data(), str.size());
+    assert(size() == desiredSize);
+    return *this;
+  }
+
+  basic_fbstring& append(const basic_fbstring& str, const size_type pos,
+                         size_type n) {
+    const size_type sz = str.size();
+    enforce(pos <= sz, std::__throw_out_of_range, "");
+    procrustes(n, sz - pos);
+    return append(str.data() + pos, n);
+  }
+
+  basic_fbstring& append(const value_type* s, size_type n) {
+#ifndef NDEBUG
+    Invariant checker(*this);
+    (void) checker;
+#endif
+    if (FBSTRING_UNLIKELY(!n)) {
+      // Unlikely but must be done
+      return *this;
+    }
+    auto const oldSize = size();
+    auto const oldData = data();
+    // Check for aliasing (rare). We could use "<=" here but in theory
+    // those do not work for pointers unless the pointers point to
+    // elements in the same array. For that reason we use
+    // std::less_equal, which is guaranteed to offer a total order
+    // over pointers. See discussion at http://goo.gl/Cy2ya for more
+    // info.
+    std::less_equal<const value_type*> le;
+    if (FBSTRING_UNLIKELY(le(oldData, s) && !le(oldData + oldSize, s))) {
+      assert(le(s + n, oldData + oldSize));
+      const size_type offset = s - oldData;
+      store_.reserve(oldSize + n);
+      // Restore the source
+      s = data() + offset;
+    }
+    // Warning! Repeated appends with short strings may actually incur
+    // practically quadratic performance. Avoid that by pushing back
+    // the first character (which ensures exponential growth) and then
+    // appending the rest normally. Worst case the append may incur a
+    // second allocation but that will be rare.
+    push_back(*s++);
+    --n;
+    memcpy(store_.expand_noinit(n), s, n * sizeof(value_type));
+    assert(size() == oldSize + n + 1);
+    return *this;
+  }
+
+  basic_fbstring& append(const value_type* s) {
+    return append(s, traits_type::length(s));
+  }
+
+  basic_fbstring& append(size_type n, value_type c) {
+    resize(size() + n, c);
+    return *this;
+  }
+
+  template<class InputIterator>
+  basic_fbstring& append(InputIterator first, InputIterator last) {
+    insert(end(), first, last);
+    return *this;
+  }
+
+  basic_fbstring& append(std::initializer_list<value_type> il) {
+    return append(il.begin(), il.end());
+  }
+
+  void push_back(const value_type c) {             // primitive
+    store_.push_back(c);
+  }
+
+  basic_fbstring& assign(const basic_fbstring& str) {
+    if (&str == this) return *this;
+    return assign(str.data(), str.size());
+  }
+
+  basic_fbstring& assign(basic_fbstring&& str) {
+    return *this = std::move(str);
+  }
+
+  basic_fbstring& assign(const basic_fbstring& str, const size_type pos,
+                         size_type n) {
+    const size_type sz = str.size();
+    enforce(pos <= sz, std::__throw_out_of_range, "");
+    procrustes(n, sz - pos);
+    return assign(str.data() + pos, n);
+  }
+
+  basic_fbstring& assign(const value_type* s, const size_type n) {
+    Invariant checker(*this);
+    (void) checker;
+    if (size() >= n) {
+      std::copy(s, s + n, begin());
+      resize(n);
+      assert(size() == n);
+    } else {
+      const value_type *const s2 = s + size();
+      std::copy(s, s2, begin());
+      append(s2, n - size());
+      assert(size() == n);
+    }
+    store_.writeTerminator();
+    assert(size() == n);
+    return *this;
+  }
+
+  basic_fbstring& assign(const value_type* s) {
+    return assign(s, traits_type::length(s));
+  }
+
+  basic_fbstring& assign(std::initializer_list<value_type> il) {
+    return assign(il.begin(), il.end());
+  }
+
+  template <class ItOrLength, class ItOrChar>
+  basic_fbstring& assign(ItOrLength first_or_n, ItOrChar last_or_c) {
+    return replace(begin(), end(), first_or_n, last_or_c);
+  }
+
+  basic_fbstring& insert(size_type pos1, const basic_fbstring& str) {
+    return insert(pos1, str.data(), str.size());
+  }
+
+  basic_fbstring& insert(size_type pos1, const basic_fbstring& str,
+                         size_type pos2, size_type n) {
+    enforce(pos2 <= str.length(), std::__throw_out_of_range, "");
+    procrustes(n, str.length() - pos2);
+    return insert(pos1, str.data() + pos2, n);
+  }
+
+  basic_fbstring& insert(size_type pos, const value_type* s, size_type n) {
+    enforce(pos <= length(), std::__throw_out_of_range, "");
+    insert(begin() + pos, s, s + n);
+    return *this;
+  }
+
+  basic_fbstring& insert(size_type pos, const value_type* s) {
+    return insert(pos, s, traits_type::length(s));
+  }
+
+  basic_fbstring& insert(size_type pos, size_type n, value_type c) {
+    enforce(pos <= length(), std::__throw_out_of_range, "");
+    insert(begin() + pos, n, c);
+    return *this;
+  }
+
+  iterator insert(const_iterator p, const value_type c) {
+    const size_type pos = p - begin();
+    insert(p, 1, c);
+    return begin() + pos;
+  }
+
+private:
+  template <int i> class Selector {};
+
+  iterator insertImplDiscr(const_iterator p,
+                           size_type n, value_type c, Selector<1>) {
+    Invariant checker(*this);
+    (void) checker;
+    auto const pos = p - begin();
+    assert(p >= begin() && p <= end());
+    if (capacity() - size() < n) {
+      const size_type sz = p - begin();
+      reserve(size() + n);
+      p = begin() + sz;
+    }
+    const iterator oldEnd = end();
+    if (n < size_type(oldEnd - p)) {
+      append(oldEnd - n, oldEnd);
+      //std::copy(
+      //    reverse_iterator(oldEnd - n),
+      //    reverse_iterator(p),
+      //    reverse_iterator(oldEnd));
+      fbstring_detail::pod_move(&*p, &*oldEnd - n,
+                                begin() + pos + n);
+      std::fill(begin() + pos, begin() + pos + n, c);
+    } else {
+      append(n - (end() - p), c);
+      append(iterator(p), oldEnd);
+      std::fill(iterator(p), oldEnd, c);
+    }
+    store_.writeTerminator();
+    return begin() + pos;
+  }
+
+  template<class InputIter>
+  iterator insertImplDiscr(const_iterator i,
+                           InputIter b, InputIter e, Selector<0>) {
+    return insertImpl(i, b, e,
+               typename std::iterator_traits<InputIter>::iterator_category());
+  }
+
+  template <class FwdIterator>
+  iterator insertImpl(const_iterator i,
+                  FwdIterator s1, FwdIterator s2, std::forward_iterator_tag) {
+    Invariant checker(*this);
+    (void) checker;
+    const size_type pos = i - begin();
+    const typename std::iterator_traits<FwdIterator>::difference_type n2 =
+      std::distance(s1, s2);
+    assert(n2 >= 0);
+    using namespace fbstring_detail;
+    assert(pos <= size());
+
+    const typename std::iterator_traits<FwdIterator>::difference_type maxn2 =
+      capacity() - size();
+    if (maxn2 < n2) {
+      // realloc the string
+      reserve(size() + n2);
+      i = begin() + pos;
+    }
+    if (pos + n2 <= size()) {
+      const iterator tailBegin = end() - n2;
+      store_.expand_noinit(n2);
+      fbstring_detail::pod_copy(tailBegin, tailBegin + n2, end() - n2);
+      std::copy(const_reverse_iterator(tailBegin), const_reverse_iterator(i),
+                reverse_iterator(tailBegin + n2));
+      std::copy(s1, s2, begin() + pos);
+    } else {
+      FwdIterator t = s1;
+      const size_type old_size = size();
+      std::advance(t, old_size - pos);
+      const size_t newElems = std::distance(t, s2);
+      store_.expand_noinit(n2);
+      std::copy(t, s2, begin() + old_size);
+      fbstring_detail::pod_copy(data() + pos, data() + old_size,
+                                 begin() + old_size + newElems);
+      std::copy(s1, t, begin() + pos);
+    }
+    store_.writeTerminator();
+    return begin() + pos;
+  }
+
+  template <class InputIterator>
+  iterator insertImpl(const_iterator i,
+                      InputIterator b, InputIterator e,
+                      std::input_iterator_tag) {
+    const auto pos = i - begin();
+    basic_fbstring temp(begin(), i);
+    for (; b != e; ++b) {
+      temp.push_back(*b);
+    }
+    temp.append(i, cend());
+    swap(temp);
+    return begin() + pos;
+  }
+
+public:
+  template <class ItOrLength, class ItOrChar>
+  iterator insert(const_iterator p, ItOrLength first_or_n, ItOrChar last_or_c) {
+    Selector<std::numeric_limits<ItOrLength>::is_specialized> sel;
+    return insertImplDiscr(p, first_or_n, last_or_c, sel);
+  }
+
+  iterator insert(const_iterator p, std::initializer_list<value_type> il) {
+    return insert(p, il.begin(), il.end());
+  }
+
+  basic_fbstring& erase(size_type pos = 0, size_type n = npos) {
+    Invariant checker(*this);
+    (void) checker;
+    enforce(pos <= length(), std::__throw_out_of_range, "");
+    procrustes(n, length() - pos);
+    std::copy(begin() + pos + n, end(), begin() + pos);
+    resize(length() - n);
+    return *this;
+  }
+
+  iterator erase(iterator position) {
+    const size_type pos(position - begin());
+    enforce(pos <= size(), std::__throw_out_of_range, "");
+    erase(pos, 1);
+    return begin() + pos;
+  }
+
+  iterator erase(iterator first, iterator last) {
+    const size_type pos(first - begin());
+    erase(pos, last - first);
+    return begin() + pos;
+  }
+
+  // Replaces at most n1 chars of *this, starting with pos1 with the
+  // content of str
+  basic_fbstring& replace(size_type pos1, size_type n1,
+                          const basic_fbstring& str) {
+    return replace(pos1, n1, str.data(), str.size());
+  }
+
+  // Replaces at most n1 chars of *this, starting with pos1,
+  // with at most n2 chars of str starting with pos2
+  basic_fbstring& replace(size_type pos1, size_type n1,
+                          const basic_fbstring& str,
+                          size_type pos2, size_type n2) {
+    enforce(pos2 <= str.length(), std::__throw_out_of_range, "");
+    return replace(pos1, n1, str.data() + pos2,
+                   std::min(n2, str.size() - pos2));
+  }
+
+  // Replaces at most n1 chars of *this, starting with pos, with chars from s
+  basic_fbstring& replace(size_type pos, size_type n1, const value_type* s) {
+    return replace(pos, n1, s, traits_type::length(s));
+  }
+
+  // Replaces at most n1 chars of *this, starting with pos, with n2
+  // occurrences of c
+  //
+  // consolidated with
+  //
+  // Replaces at most n1 chars of *this, starting with pos, with at
+  // most n2 chars of str.  str must have at least n2 chars.
+  template <class StrOrLength, class NumOrChar>
+  basic_fbstring& replace(size_type pos, size_type n1,
+                          StrOrLength s_or_n2, NumOrChar n_or_c) {
+    Invariant checker(*this);
+    (void) checker;
+    enforce(pos <= size(), std::__throw_out_of_range, "");
+    procrustes(n1, length() - pos);
+    const iterator b = begin() + pos;
+    return replace(b, b + n1, s_or_n2, n_or_c);
+  }
+
+  basic_fbstring& replace(iterator i1, iterator i2, const basic_fbstring& str) {
+    return replace(i1, i2, str.data(), str.length());
+  }
+
+  basic_fbstring& replace(iterator i1, iterator i2, const value_type* s) {
+    return replace(i1, i2, s, traits_type::length(s));
+  }
+
+private:
+  basic_fbstring& replaceImplDiscr(iterator i1, iterator i2,
+                                   const value_type* s, size_type n,
+                                   Selector<2>) {
+    assert(i1 <= i2);
+    assert(begin() <= i1 && i1 <= end());
+    assert(begin() <= i2 && i2 <= end());
+    return replace(i1, i2, s, s + n);
+  }
+
+  basic_fbstring& replaceImplDiscr(iterator i1, iterator i2,
+                                   size_type n2, value_type c, Selector<1>) {
+    const size_type n1 = i2 - i1;
+    if (n1 > n2) {
+      std::fill(i1, i1 + n2, c);
+      erase(i1 + n2, i2);
+    } else {
+      std::fill(i1, i2, c);
+      insert(i2, n2 - n1, c);
+    }
+    assert(isSane());
+    return *this;
+  }
+
+  template <class InputIter>
+  basic_fbstring& replaceImplDiscr(iterator i1, iterator i2,
+                                   InputIter b, InputIter e,
+                                   Selector<0>) {
+    replaceImpl(i1, i2, b, e,
+                typename std::iterator_traits<InputIter>::iterator_category());
+    return *this;
+  }
+
+private:
+  template <class FwdIterator>
+  bool replaceAliased(iterator i1, iterator i2,
+                      FwdIterator s1, FwdIterator s2, std::false_type) {
+    return false;
+  }
+
+  template <class FwdIterator>
+  bool replaceAliased(iterator i1, iterator i2,
+                      FwdIterator s1, FwdIterator s2, std::true_type) {
+    static const std::less_equal<const value_type*> le =
+      std::less_equal<const value_type*>();
+    const bool aliased = le(&*begin(), &*s1) && le(&*s1, &*end());
+    if (!aliased) {
+      return false;
+    }
+    // Aliased replace, copy to new string
+    basic_fbstring temp;
+    temp.reserve(size() - (i2 - i1) + std::distance(s1, s2));
+    temp.append(begin(), i1).append(s1, s2).append(i2, end());
+    swap(temp);
+    return true;
+  }
+
+  template <class FwdIterator>
+  void replaceImpl(iterator i1, iterator i2,
+                   FwdIterator s1, FwdIterator s2, std::forward_iterator_tag) {
+    Invariant checker(*this);
+    (void) checker;
+
+    // Handle aliased replace
+    if (replaceAliased(i1, i2, s1, s2,
+          std::integral_constant<bool,
+            std::is_same<FwdIterator, iterator>::value ||
+            std::is_same<FwdIterator, const_iterator>::value>())) {
+      return;
+    }
+
+    auto const n1 = i2 - i1;
+    assert(n1 >= 0);
+    auto const n2 = std::distance(s1, s2);
+    assert(n2 >= 0);
+
+    if (n1 > n2) {
+      // shrinks
+      std::copy(s1, s2, i1);
+      erase(i1 + n2, i2);
+    } else {
+      // grows
+      fbstring_detail::copy_n(s1, n1, i1);
+      std::advance(s1, n1);
+      insert(i2, s1, s2);
+    }
+    assert(isSane());
+  }
+
+  template <class InputIterator>
+  void replaceImpl(iterator i1, iterator i2,
+                   InputIterator b, InputIterator e, std::input_iterator_tag) {
+    basic_fbstring temp(begin(), i1);
+    temp.append(b, e).append(i2, end());
+    swap(temp);
+  }
+
+public:
+  template <class T1, class T2>
+  basic_fbstring& replace(iterator i1, iterator i2,
+                          T1 first_or_n_or_s, T2 last_or_c_or_n) {
+    const bool
+      num1 = std::numeric_limits<T1>::is_specialized,
+      num2 = std::numeric_limits<T2>::is_specialized;
+    return replaceImplDiscr(
+      i1, i2, first_or_n_or_s, last_or_c_or_n,
+      Selector<num1 ? (num2 ? 1 : -1) : (num2 ? 2 : 0)>());
+  }
+
+  size_type copy(value_type* s, size_type n, size_type pos = 0) const {
+    enforce(pos <= size(), std::__throw_out_of_range, "");
+    procrustes(n, size() - pos);
+
+    fbstring_detail::pod_copy(
+      data() + pos,
+      data() + pos + n,
+      s);
+    return n;
+  }
+
+  void swap(basic_fbstring& rhs) {
+    store_.swap(rhs.store_);
+  }
+
+  const value_type* c_str() const {
+    return store_.c_str();
+  }
+
+  const value_type* data() const { return c_str(); }
+
+  allocator_type get_allocator() const {
+    return allocator_type();
+  }
+
+  size_type find(const basic_fbstring& str, size_type pos = 0) const {
+    return find(str.data(), pos, str.length());
+  }
+
+  size_type find(const value_type* needle, const size_type pos,
+                 const size_type nsize) const {
+    if (!nsize) return pos;
+    auto const size = this->size();
+    // nsize + pos can overflow (eg pos == npos), guard against that by checking
+    // that nsize + pos does not wrap around.
+    if (nsize + pos > size || nsize + pos < pos) return npos;
+    // Don't use std::search, use a Boyer-Moore-like trick by comparing
+    // the last characters first
+    auto const haystack = data();
+    auto const nsize_1 = nsize - 1;
+    auto const lastNeedle = needle[nsize_1];
+
+    // Boyer-Moore skip value for the last char in the needle. Zero is
+    // not a valid value; skip will be computed the first time it's
+    // needed.
+    size_type skip = 0;
+
+    const E * i = haystack + pos;
+    auto iEnd = haystack + size - nsize_1;
+
+    while (i < iEnd) {
+      // Boyer-Moore: match the last element in the needle
+      while (i[nsize_1] != lastNeedle) {
+        if (++i == iEnd) {
+          // not found
+          return npos;
+        }
+      }
+      // Here we know that the last char matches
+      // Continue in pedestrian mode
+      for (size_t j = 0; ; ) {
+        assert(j < nsize);
+        if (i[j] != needle[j]) {
+          // Not found, we can skip
+          // Compute the skip value lazily
+          if (skip == 0) {
+            skip = 1;
+            while (skip <= nsize_1 && needle[nsize_1 - skip] != lastNeedle) {
+              ++skip;
+            }
+          }
+          i += skip;
+          break;
+        }
+        // Check if done searching
+        if (++j == nsize) {
+          // Yay
+          return i - haystack;
+        }
+      }
+    }
+    return npos;
+  }
+
+  size_type find(const value_type* s, size_type pos = 0) const {
+    return find(s, pos, traits_type::length(s));
+  }
+
+  size_type find (value_type c, size_type pos = 0) const {
+    return find(&c, pos, 1);
+  }
+
+  size_type rfind(const basic_fbstring& str, size_type pos = npos) const {
+    return rfind(str.data(), pos, str.length());
+  }
+
+  size_type rfind(const value_type* s, size_type pos, size_type n) const {
+    if (n > length()) return npos;
+    pos = std::min(pos, length() - n);
+    if (n == 0) return pos;
+
+    const_iterator i(begin() + pos);
+    for (; ; --i) {
+      if (traits_type::eq(*i, *s)
+          && traits_type::compare(&*i, s, n) == 0) {
+        return i - begin();
+      }
+      if (i == begin()) break;
+    }
+    return npos;
+  }
+
+  size_type rfind(const value_type* s, size_type pos = npos) const {
+    return rfind(s, pos, traits_type::length(s));
+  }
+
+  size_type rfind(value_type c, size_type pos = npos) const {
+    return rfind(&c, pos, 1);
+  }
+
+  size_type find_first_of(const basic_fbstring& str, size_type pos = 0) const {
+    return find_first_of(str.data(), pos, str.length());
+  }
+
+  size_type find_first_of(const value_type* s,
+                          size_type pos, size_type n) const {
+    if (pos > length() || n == 0) return npos;
+    const_iterator i(begin() + pos),
+      finish(end());
+    for (; i != finish; ++i) {
+      if (traits_type::find(s, n, *i) != 0) {
+        return i - begin();
+      }
+    }
+    return npos;
+  }
+
+  size_type find_first_of(const value_type* s, size_type pos = 0) const {
+    return find_first_of(s, pos, traits_type::length(s));
+  }
+
+  size_type find_first_of(value_type c, size_type pos = 0) const {
+    return find_first_of(&c, pos, 1);
+  }
+
+  size_type find_last_of (const basic_fbstring& str,
+                          size_type pos = npos) const {
+    return find_last_of(str.data(), pos, str.length());
+  }
+
+  size_type find_last_of (const value_type* s, size_type pos,
+                          size_type n) const {
+    if (!empty() && n > 0) {
+      pos = std::min(pos, length() - 1);
+      const_iterator i(begin() + pos);
+      for (;; --i) {
+        if (traits_type::find(s, n, *i) != 0) {
+          return i - begin();
+        }
+        if (i == begin()) break;
+      }
+    }
+    return npos;
+  }
+
+  size_type find_last_of (const value_type* s,
+                          size_type pos = npos) const {
+    return find_last_of(s, pos, traits_type::length(s));
+  }
+
+  size_type find_last_of (value_type c, size_type pos = npos) const {
+    return find_last_of(&c, pos, 1);
+  }
+
+  size_type find_first_not_of(const basic_fbstring& str,
+                              size_type pos = 0) const {
+    return find_first_not_of(str.data(), pos, str.size());
+  }
+
+  size_type find_first_not_of(const value_type* s, size_type pos,
+                              size_type n) const {
+    if (pos < length()) {
+      const_iterator
+        i(begin() + pos),
+        finish(end());
+      for (; i != finish; ++i) {
+        if (traits_type::find(s, n, *i) == 0) {
+          return i - begin();
+        }
+      }
+    }
+    return npos;
+  }
+
+  size_type find_first_not_of(const value_type* s,
+                              size_type pos = 0) const {
+    return find_first_not_of(s, pos, traits_type::length(s));
+  }
+
+  size_type find_first_not_of(value_type c, size_type pos = 0) const {
+    return find_first_not_of(&c, pos, 1);
+  }
+
+  size_type find_last_not_of(const basic_fbstring& str,
+                             size_type pos = npos) const {
+    return find_last_not_of(str.data(), pos, str.length());
+  }
+
+  size_type find_last_not_of(const value_type* s, size_type pos,
+                             size_type n) const {
+    if (!this->empty()) {
+      pos = std::min(pos, size() - 1);
+      const_iterator i(begin() + pos);
+      for (;; --i) {
+        if (traits_type::find(s, n, *i) == 0) {
+          return i - begin();
+        }
+        if (i == begin()) break;
+      }
+    }
+    return npos;
+  }
+
+  size_type find_last_not_of(const value_type* s,
+                             size_type pos = npos) const {
+    return find_last_not_of(s, pos, traits_type::length(s));
+  }
+
+  size_type find_last_not_of (value_type c, size_type pos = npos) const {
+    return find_last_not_of(&c, pos, 1);
+  }
+
+  basic_fbstring substr(size_type pos = 0, size_type n = npos) const {
+    enforce(pos <= size(), std::__throw_out_of_range, "");
+    return basic_fbstring(data() + pos, std::min(n, size() - pos));
+  }
+
+  int compare(const basic_fbstring& str) const {
+    // FIX due to Goncalo N M de Carvalho July 18, 2005
+    return compare(0, size(), str);
+  }
+
+  int compare(size_type pos1, size_type n1,
+              const basic_fbstring& str) const {
+    return compare(pos1, n1, str.data(), str.size());
+  }
+
+  int compare(size_type pos1, size_type n1,
+              const value_type* s) const {
+    return compare(pos1, n1, s, traits_type::length(s));
+  }
+
+  int compare(size_type pos1, size_type n1,
+              const value_type* s, size_type n2) const {
+    enforce(pos1 <= size(), std::__throw_out_of_range, "");
+    procrustes(n1, size() - pos1);
+    // The line below fixed by Jean-Francois Bastien, 04-23-2007. Thanks!
+    const int r = traits_type::compare(pos1 + data(), s, std::min(n1, n2));
+    return r != 0 ? r : n1 > n2 ? 1 : n1 < n2 ? -1 : 0;
+  }
+
+  int compare(size_type pos1, size_type n1,
+              const basic_fbstring& str,
+              size_type pos2, size_type n2) const {
+    enforce(pos2 <= str.size(), std::__throw_out_of_range, "");
+    return compare(pos1, n1, str.data() + pos2,
+                   std::min(n2, str.size() - pos2));
+  }
+
+  // Code from Jean-Francois Bastien (03/26/2007)
+  int compare(const value_type* s) const {
+    // Could forward to compare(0, size(), s, traits_type::length(s))
+    // but that does two extra checks
+    const size_type n1(size()), n2(traits_type::length(s));
+    const int r = traits_type::compare(data(), s, std::min(n1, n2));
+    return r != 0 ? r : n1 > n2 ? 1 : n1 < n2 ? -1 : 0;
+  }
+
+private:
+  // Data
+  Storage store_;
+};
+
+// non-member functions
+// C++11 21.4.8.1/2
+template <typename E, class T, class A, class S>
+inline
+basic_fbstring<E, T, A, S> operator+(const basic_fbstring<E, T, A, S>& lhs,
+                                     const basic_fbstring<E, T, A, S>& rhs) {
+
+  basic_fbstring<E, T, A, S> result;
+  result.reserve(lhs.size() + rhs.size());
+  result.append(lhs).append(rhs);
+  return std::move(result);
+}
+
+// C++11 21.4.8.1/2
+template <typename E, class T, class A, class S>
+inline
+basic_fbstring<E, T, A, S> operator+(basic_fbstring<E, T, A, S>&& lhs,
+                                     const basic_fbstring<E, T, A, S>& rhs) {
+  return std::move(lhs.append(rhs));
+}
+
+// C++11 21.4.8.1/3
+template <typename E, class T, class A, class S>
+inline
+basic_fbstring<E, T, A, S> operator+(const basic_fbstring<E, T, A, S>& lhs,
+                                     basic_fbstring<E, T, A, S>&& rhs) {
+  if (rhs.capacity() >= lhs.size() + rhs.size()) {
+    // Good, at least we don't need to reallocate
+    return std::move(rhs.insert(0, lhs));
+  }
+  // Meh, no go. Forward to operator+(const&, const&).
+  auto const& rhsC = rhs;
+  return lhs + rhsC;
+}
+
+// C++11 21.4.8.1/4
+template <typename E, class T, class A, class S>
+inline
+basic_fbstring<E, T, A, S> operator+(basic_fbstring<E, T, A, S>&& lhs,
+                                     basic_fbstring<E, T, A, S>&& rhs) {
+  return std::move(lhs.append(rhs));
+}
+
+template <typename E, class T, class A, class S>
+inline
+basic_fbstring<E, T, A, S> operator+(
+  const typename basic_fbstring<E, T, A, S>::value_type* lhs,
+  const basic_fbstring<E, T, A, S>& rhs) {
+  //
+  basic_fbstring<E, T, A, S> result;
+  const typename basic_fbstring<E, T, A, S>::size_type len =
+    basic_fbstring<E, T, A, S>::traits_type::length(lhs);
+  result.reserve(len + rhs.size());
+  result.append(lhs, len).append(rhs);
+  return result;
+}
+
+template <typename E, class T, class A, class S>
+inline
+basic_fbstring<E, T, A, S> operator+(
+  typename basic_fbstring<E, T, A, S>::value_type lhs,
+  const basic_fbstring<E, T, A, S>& rhs) {
+
+  basic_fbstring<E, T, A, S> result;
+  result.reserve(1 + rhs.size());
+  result.push_back(lhs);
+  result.append(rhs);
+  return result;
+}
+
+template <typename E, class T, class A, class S>
+inline
+basic_fbstring<E, T, A, S> operator+(
+  const basic_fbstring<E, T, A, S>& lhs,
+  const typename basic_fbstring<E, T, A, S>::value_type* rhs) {
+
+  typedef typename basic_fbstring<E, T, A, S>::size_type size_type;
+  typedef typename basic_fbstring<E, T, A, S>::traits_type traits_type;
+
+  basic_fbstring<E, T, A, S> result;
+  const size_type len = traits_type::length(rhs);
+  result.reserve(lhs.size() + len);
+  result.append(lhs).append(rhs, len);
+  return result;
+}
+
+template <typename E, class T, class A, class S>
+inline
+basic_fbstring<E, T, A, S> operator+(
+  const basic_fbstring<E, T, A, S>& lhs,
+  typename basic_fbstring<E, T, A, S>::value_type rhs) {
+
+  basic_fbstring<E, T, A, S> result;
+  result.reserve(lhs.size() + 1);
+  result.append(lhs);
+  result.push_back(rhs);
+  return result;
+}
+
+template <typename E, class T, class A, class S>
+inline
+bool operator==(const basic_fbstring<E, T, A, S>& lhs,
+                const basic_fbstring<E, T, A, S>& rhs) {
+  return lhs.size() == rhs.size() && lhs.compare(rhs) == 0; }
+
+template <typename E, class T, class A, class S>
+inline
+bool operator==(const typename basic_fbstring<E, T, A, S>::value_type* lhs,
+                const basic_fbstring<E, T, A, S>& rhs) {
+  return rhs == lhs; }
+
+template <typename E, class T, class A, class S>
+inline
+bool operator==(const basic_fbstring<E, T, A, S>& lhs,
+                const typename basic_fbstring<E, T, A, S>::value_type* rhs) {
+  return lhs.compare(rhs) == 0; }
+
+template <typename E, class T, class A, class S>
+inline
+bool operator!=(const basic_fbstring<E, T, A, S>& lhs,
+                const basic_fbstring<E, T, A, S>& rhs) {
+  return !(lhs == rhs); }
+
+template <typename E, class T, class A, class S>
+inline
+bool operator!=(const typename basic_fbstring<E, T, A, S>::value_type* lhs,
+                const basic_fbstring<E, T, A, S>& rhs) {
+  return !(lhs == rhs); }
+
+template <typename E, class T, class A, class S>
+inline
+bool operator!=(const basic_fbstring<E, T, A, S>& lhs,
+                const typename basic_fbstring<E, T, A, S>::value_type* rhs) {
+  return !(lhs == rhs); }
+
+template <typename E, class T, class A, class S>
+inline
+bool operator<(const basic_fbstring<E, T, A, S>& lhs,
+               const basic_fbstring<E, T, A, S>& rhs) {
+  return lhs.compare(rhs) < 0; }
+
+template <typename E, class T, class A, class S>
+inline
+bool operator<(const basic_fbstring<E, T, A, S>& lhs,
+               const typename basic_fbstring<E, T, A, S>::value_type* rhs) {
+  return lhs.compare(rhs) < 0; }
+
+template <typename E, class T, class A, class S>
+inline
+bool operator<(const typename basic_fbstring<E, T, A, S>::value_type* lhs,
+               const basic_fbstring<E, T, A, S>& rhs) {
+  return rhs.compare(lhs) > 0; }
+
+template <typename E, class T, class A, class S>
+inline
+bool operator>(const basic_fbstring<E, T, A, S>& lhs,
+               const basic_fbstring<E, T, A, S>& rhs) {
+  return rhs < lhs; }
+
+template <typename E, class T, class A, class S>
+inline
+bool operator>(const basic_fbstring<E, T, A, S>& lhs,
+               const typename basic_fbstring<E, T, A, S>::value_type* rhs) {
+  return rhs < lhs; }
+
+template <typename E, class T, class A, class S>
+inline
+bool operator>(const typename basic_fbstring<E, T, A, S>::value_type* lhs,
+               const basic_fbstring<E, T, A, S>& rhs) {
+  return rhs < lhs; }
+
+template <typename E, class T, class A, class S>
+inline
+bool operator<=(const basic_fbstring<E, T, A, S>& lhs,
+                const basic_fbstring<E, T, A, S>& rhs) {
+  return !(rhs < lhs); }
+
+template <typename E, class T, class A, class S>
+inline
+bool operator<=(const basic_fbstring<E, T, A, S>& lhs,
+                const typename basic_fbstring<E, T, A, S>::value_type* rhs) {
+  return !(rhs < lhs); }
+
+template <typename E, class T, class A, class S>
+inline
+bool operator<=(const typename basic_fbstring<E, T, A, S>::value_type* lhs,
+                const basic_fbstring<E, T, A, S>& rhs) {
+  return !(rhs < lhs); }
+
+template <typename E, class T, class A, class S>
+inline
+bool operator>=(const basic_fbstring<E, T, A, S>& lhs,
+                const basic_fbstring<E, T, A, S>& rhs) {
+  return !(lhs < rhs); }
+
+template <typename E, class T, class A, class S>
+inline
+bool operator>=(const basic_fbstring<E, T, A, S>& lhs,
+                const typename basic_fbstring<E, T, A, S>::value_type* rhs) {
+  return !(lhs < rhs); }
+
+template <typename E, class T, class A, class S>
+inline
+bool operator>=(const typename basic_fbstring<E, T, A, S>::value_type* lhs,
+                const basic_fbstring<E, T, A, S>& rhs) {
+ return !(lhs < rhs);
+}
+
+// C++11 21.4.8.8
+template <typename E, class T, class A, class S>
+void swap(basic_fbstring<E, T, A, S>& lhs, basic_fbstring<E, T, A, S>& rhs) {
+  lhs.swap(rhs);
+}
+
+// TODO: make this faster.
+template <typename E, class T, class A, class S>
+inline
+std::basic_istream<
+  typename basic_fbstring<E, T, A, S>::value_type,
+  typename basic_fbstring<E, T, A, S>::traits_type>&
+  operator>>(
+    std::basic_istream<typename basic_fbstring<E, T, A, S>::value_type,
+    typename basic_fbstring<E, T, A, S>::traits_type>& is,
+    basic_fbstring<E, T, A, S>& str) {
+  typename std::basic_istream<E, T>::sentry sentry(is);
+  typedef std::basic_istream<typename basic_fbstring<E, T, A, S>::value_type,
+                             typename basic_fbstring<E, T, A, S>::traits_type>
+                        __istream_type;
+  typedef typename __istream_type::ios_base __ios_base;
+  size_t extracted = 0;
+  auto err = __ios_base::goodbit;
+  if (sentry) {
+    auto n = is.width();
+    if (n == 0) {
+      n = str.max_size();
+    }
+    str.erase();
+    auto got = is.rdbuf()->sgetc();
+    for (; extracted != n && got != T::eof() && !isspace(got); ++extracted) {
+      // Whew. We get to store this guy
+      str.push_back(got);
+      got = is.rdbuf()->snextc();
+    }
+    if (got == T::eof()) {
+      err |= __ios_base::eofbit;
+      is.width(0);
+    }
+  }
+  if (!extracted) {
+    err |= __ios_base::failbit;
+  }
+  if (err) {
+    is.setstate(err);
+  }
+  return is;
+}
+
+template <typename E, class T, class A, class S>
+inline
+std::basic_ostream<typename basic_fbstring<E, T, A, S>::value_type,
+                   typename basic_fbstring<E, T, A, S>::traits_type>&
+operator<<(
+  std::basic_ostream<typename basic_fbstring<E, T, A, S>::value_type,
+  typename basic_fbstring<E, T, A, S>::traits_type>& os,
+    const basic_fbstring<E, T, A, S>& str) {
+#if _LIBCPP_VERSION
+  typename std::basic_ostream<
+    typename basic_fbstring<E, T, A, S>::value_type,
+    typename basic_fbstring<E, T, A, S>::traits_type>::sentry __s(os);
+  if (__s) {
+    typedef std::ostreambuf_iterator<
+      typename basic_fbstring<E, T, A, S>::value_type,
+      typename basic_fbstring<E, T, A, S>::traits_type> _Ip;
+    size_t __len = str.size();
+    bool __left =
+      (os.flags() & std::ios_base::adjustfield) == std::ios_base::left;
+    if (__pad_and_output(_Ip(os),
+                         str.data(),
+                         __left ? str.data() + __len : str.data(),
+                         str.data() + __len,
+                         os,
+                         os.fill()).failed()) {
+      os.setstate(std::ios_base::badbit | std::ios_base::failbit);
+    }
+  }
+#else
+  std::__ostream_insert(os, str.data(), str.size());
+#endif
+  return os;
+}
+
+#ifndef _LIBSTDCXX_FBSTRING
+
+template <typename E, class T, class A, class S>
+inline
+std::basic_istream<typename basic_fbstring<E, T, A, S>::value_type,
+                   typename basic_fbstring<E, T, A, S>::traits_type>&
+getline(
+  std::basic_istream<typename basic_fbstring<E, T, A, S>::value_type,
+  typename basic_fbstring<E, T, A, S>::traits_type>& is,
+    basic_fbstring<E, T, A, S>& str,
+  typename basic_fbstring<E, T, A, S>::value_type delim) {
+  // Use the nonstandard getdelim()
+  char * buf = NULL;
+  size_t size = 0;
+  for (;;) {
+    // This looks quadratic but it really depends on realloc
+    auto const newSize = size + 128;
+    buf = static_cast<char*>(checkedRealloc(buf, newSize));
+    is.getline(buf + size, newSize - size, delim);
+    if (is.bad() || is.eof() || !is.fail()) {
+      // done by either failure, end of file, or normal read
+      size += std::strlen(buf + size);
+      break;
+    }
+    // Here we have failed due to too short a buffer
+    // Minus one to discount the terminating '\0'
+    size = newSize - 1;
+    assert(buf[size] == 0);
+    // Clear the error so we can continue reading
+    is.clear();
+  }
+  basic_fbstring<E, T, A, S> result(buf, size, size + 1,
+                                    AcquireMallocatedString());
+  result.swap(str);
+  return is;
+}
+
+template <typename E, class T, class A, class S>
+inline
+std::basic_istream<typename basic_fbstring<E, T, A, S>::value_type,
+                   typename basic_fbstring<E, T, A, S>::traits_type>&
+getline(
+  std::basic_istream<typename basic_fbstring<E, T, A, S>::value_type,
+  typename basic_fbstring<E, T, A, S>::traits_type>& is,
+  basic_fbstring<E, T, A, S>& str) {
+  // Just forward to the version with a delimiter
+  return getline(is, str, '\n');
+}
+
+#endif
+
+template <typename E1, class T, class A, class S>
+const typename basic_fbstring<E1, T, A, S>::size_type
+basic_fbstring<E1, T, A, S>::npos =
+              static_cast<typename basic_fbstring<E1, T, A, S>::size_type>(-1);
+
+#ifndef _LIBSTDCXX_FBSTRING
+// basic_string compatibility routines
+
+template <typename E, class T, class A, class S>
+inline
+bool operator==(const basic_fbstring<E, T, A, S>& lhs,
+                const std::string& rhs) {
+  return lhs.compare(0, lhs.size(), rhs.data(), rhs.size()) == 0;
+}
+
+template <typename E, class T, class A, class S>
+inline
+bool operator==(const std::string& lhs,
+                const basic_fbstring<E, T, A, S>& rhs) {
+  return rhs == lhs;
+}
+
+template <typename E, class T, class A, class S>
+inline
+bool operator!=(const basic_fbstring<E, T, A, S>& lhs,
+                const std::string& rhs) {
+  return !(lhs == rhs);
+}
+
+template <typename E, class T, class A, class S>
+inline
+bool operator!=(const std::string& lhs,
+                const basic_fbstring<E, T, A, S>& rhs) {
+  return !(lhs == rhs);
+}
+
+#if !defined(_LIBSTDCXX_FBSTRING)
+typedef basic_fbstring<char> fbstring;
+#endif
+
+// fbstring is relocatable
+template <class T, class R, class A, class S>
+FOLLY_ASSUME_RELOCATABLE(basic_fbstring<T, R, A, S>);
+
+#else
+_GLIBCXX_END_NAMESPACE_VERSION
+#endif
+
+} // namespace folly
+
+#ifndef _LIBSTDCXX_FBSTRING
+
+// Hash functions to make fbstring usable with e.g. hash_map
+//
+// Handle interaction with different C++ standard libraries, which
+// expect these types to be in different namespaces.
+namespace std {
+
+template <class C>
+struct hash<folly::basic_fbstring<C> > : private hash<const C*> {
+  size_t operator()(const folly::basic_fbstring<C> & s) const {
+    return hash<const C*>::operator()(s.c_str());
+  }
+};
+
+template <>
+struct hash< ::folly::fbstring> {
+  size_t operator()(const ::folly::fbstring& s) const {
+    return ::folly::hash::fnv32_buf(s.data(), s.size());
+  }
+};
+
+}
+
+#if defined(_GLIBCXX_SYMVER) && !defined(__BIONIC__)
+namespace __gnu_cxx {
+
+template <class C>
+struct hash<folly::basic_fbstring<C> > : private hash<const C*> {
+  size_t operator()(const folly::basic_fbstring<C> & s) const {
+    return hash<const C*>::operator()(s.c_str());
+  }
+};
+
+template <>
+struct hash< ::folly::fbstring> {
+  size_t operator()(const ::folly::fbstring& s) const {
+    return ::folly::hash::fnv32_buf(s.data(), s.size());
+  }
+};
+
+}
+#endif // _GLIBCXX_SYMVER && !__BIONIC__
+#endif // _LIBSTDCXX_FBSTRING
+
+#pragma GCC diagnostic pop
+
+#undef FBSTRING_DISABLE_ADDRESS_SANITIZER
+#undef throw
+#undef FBSTRING_LIKELY
+#undef FBSTRING_UNLIKELY
+
+#endif // FOLLY_BASE_FBSTRING_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/FBVector.h
@@ -0,0 +1,1762 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+ * Nicholas Ormrod      (njormrod)
+ * Andrei Alexandrescu  (aalexandre)
+ *
+ * FBVector is Facebook's drop-in implementation of std::vector. It has special
+ * optimizations for use with relocatable types and jemalloc.
+ */
+
+#ifndef FOLLY_FBVECTOR_H
+#define FOLLY_FBVECTOR_H
+
+//=============================================================================
+// headers
+
+#include <algorithm>
+#include <cassert>
+#include <iterator>
+#include <memory>
+#include <stdexcept>
+#include <type_traits>
+#include <utility>
+
+#include "folly/Likely.h"
+#include "folly/Malloc.h"
+#include "folly/Traits.h"
+
+#include <boost/operators.hpp>
+
+// some files expected these from FBVector
+#include <limits>
+#include "folly/Foreach.h"
+#include <boost/type_traits.hpp>
+#include <boost/utility/enable_if.hpp>
+
+//=============================================================================
+// forward declaration
+
+#ifdef FOLLY_BENCHMARK_USE_NS_IFOLLY
+namespace Ifolly {
+#else
+namespace folly {
+#endif
+  template <class T, class Allocator = std::allocator<T>>
+  class fbvector;
+}
+
+//=============================================================================
+// compatibility
+
+#if __GNUC__ < 4 || __GNUC__ == 4 && __GNUC_MINOR__ < 7
+// PLEASE UPGRADE TO GCC 4.7 or above
+#define FOLLY_FBV_COMPATIBILITY_MODE
+#endif
+
+#ifndef FOLLY_FBV_COMPATIBILITY_MODE
+
+namespace folly {
+
+template <typename A>
+struct fbv_allocator_traits
+  : std::allocator_traits<A> {};
+
+template <typename T>
+struct fbv_is_nothrow_move_constructible
+  : std::is_nothrow_move_constructible<T> {};
+
+template <typename T, typename... Args>
+struct fbv_is_nothrow_constructible
+  : std::is_nothrow_constructible<T, Args...> {};
+
+template <typename T>
+struct fbv_is_copy_constructible
+  : std::is_copy_constructible<T> {};
+
+}
+
+#else
+
+namespace folly {
+
+template <typename A>
+struct fbv_allocator_traits {
+  static_assert(sizeof(A) == 0,
+    "If you want to use a custom allocator, then you must upgrade to gcc 4.7");
+  // for some old code that deals with this case, see D566719, diff number 10.
+};
+
+template <typename T>
+struct fbv_allocator_traits<std::allocator<T>> {
+  typedef std::allocator<T> A;
+
+  typedef T* pointer;
+  typedef const T* const_pointer;
+  typedef size_t size_type;
+
+  typedef std::false_type propagate_on_container_copy_assignment;
+  typedef std::false_type propagate_on_container_move_assignment;
+  typedef std::false_type propagate_on_container_swap;
+
+  static pointer allocate(A& a, size_type n) {
+    return static_cast<pointer>(::operator new(n * sizeof(T)));
+  }
+  static void deallocate(A& a, pointer p, size_type n) {
+    ::operator delete(p);
+  }
+
+  template <typename R, typename... Args>
+  static void construct(A& a, R* p, Args&&... args) {
+    new (p) R(std::forward<Args>(args)...);
+  }
+  template <typename R>
+  static void destroy(A& a, R* p) {
+    p->~R();
+  }
+
+  static A select_on_container_copy_construction(const A& a) {
+    return a;
+  }
+};
+
+template <typename T>
+struct fbv_is_nothrow_move_constructible
+  : std::false_type {};
+
+template <typename T, typename... Args>
+struct fbv_is_nothrow_constructible
+  : std::false_type {};
+
+template <typename T>
+struct fbv_is_copy_constructible
+  : std::true_type {};
+
+}
+
+#endif
+
+//=============================================================================
+// unrolling
+
+#define FOLLY_FBV_UNROLL_PTR(first, last, OP) do {  \
+  for (; (last) - (first) >= 4; (first) += 4) {     \
+    OP(((first) + 0));                              \
+    OP(((first) + 1));                              \
+    OP(((first) + 2));                              \
+    OP(((first) + 3));                              \
+  }                                                 \
+  for (; (first) != (last); ++(first)) OP((first)); \
+} while(0);
+
+//=============================================================================
+///////////////////////////////////////////////////////////////////////////////
+//                                                                           //
+//                              fbvector class                               //
+//                                                                           //
+///////////////////////////////////////////////////////////////////////////////
+
+#ifdef FOLLY_BENCHMARK_USE_NS_IFOLLY
+namespace Ifolly {
+#else
+namespace folly {
+#endif
+
+template <class T, class Allocator>
+class fbvector : private boost::totally_ordered<fbvector<T, Allocator>> {
+
+  //===========================================================================
+  //---------------------------------------------------------------------------
+  // implementation
+private:
+
+  typedef folly::fbv_allocator_traits<Allocator> A;
+
+  struct Impl : public Allocator {
+    // typedefs
+    typedef typename A::pointer pointer;
+    typedef typename A::size_type size_type;
+
+    // data
+    pointer b_, e_, z_;
+
+    // constructors
+    Impl() : Allocator(), b_(nullptr), e_(nullptr), z_(nullptr) {}
+    Impl(const Allocator& a)
+      : Allocator(a), b_(nullptr), e_(nullptr), z_(nullptr) {}
+    Impl(Allocator&& a)
+      : Allocator(std::move(a)), b_(nullptr), e_(nullptr), z_(nullptr) {}
+
+    Impl(size_type n, const Allocator& a = Allocator())
+      : Allocator(a)
+      { init(n); }
+
+    Impl(Impl&& other)
+      : Allocator(std::move(other)),
+        b_(other.b_), e_(other.e_), z_(other.z_)
+      { other.b_ = other.e_ = other.z_ = nullptr; }
+
+    // destructor
+    ~Impl() {
+      destroy();
+    }
+
+    // allocation
+    // note that 'allocate' and 'deallocate' are inherited from Allocator
+    T* D_allocate(size_type n) {
+      if (usingStdAllocator::value) {
+        return static_cast<T*>(malloc(n * sizeof(T)));
+      } else {
+        return folly::fbv_allocator_traits<Allocator>::allocate(*this, n);
+      }
+    }
+
+    void D_deallocate(T* p, size_type n) noexcept {
+      if (usingStdAllocator::value) {
+        free(p);
+      } else {
+        folly::fbv_allocator_traits<Allocator>::deallocate(*this, p, n);
+      }
+    }
+
+    // helpers
+    void swapData(Impl& other) {
+      std::swap(b_, other.b_);
+      std::swap(e_, other.e_);
+      std::swap(z_, other.z_);
+    }
+
+    // data ops
+    inline void destroy() noexcept {
+      if (b_) {
+        // THIS DISPATCH CODE IS DUPLICATED IN fbvector::D_destroy_range_a.
+        // It has been inlined here for speed. It calls the static fbvector
+        //  methods to perform the actual destruction.
+        if (usingStdAllocator::value) {
+          S_destroy_range(b_, e_);
+        } else {
+          S_destroy_range_a(*this, b_, e_);
+        }
+
+        D_deallocate(b_, z_ - b_);
+      }
+    }
+
+    void init(size_type n) {
+      if (UNLIKELY(n == 0)) {
+        b_ = e_ = z_ = nullptr;
+      } else {
+        size_type sz = folly::goodMallocSize(n * sizeof(T)) / sizeof(T);
+        b_ = D_allocate(sz);
+        e_ = b_;
+        z_ = b_ + sz;
+      }
+    }
+
+    void
+    set(pointer newB, size_type newSize, size_type newCap) {
+      z_ = newB + newCap;
+      e_ = newB + newSize;
+      b_ = newB;
+    }
+
+    void reset(size_type newCap) {
+      destroy();
+      try {
+        init(newCap);
+      } catch (...) {
+        init(0);
+        throw;
+      }
+    }
+    void reset() { // same as reset(0)
+      destroy();
+      b_ = e_ = z_ = nullptr;
+    }
+  } impl_;
+
+  static void swap(Impl& a, Impl& b) {
+    using std::swap;
+    if (!usingStdAllocator::value) swap<Allocator>(a, b);
+    a.swapData(b);
+  }
+
+  //===========================================================================
+  //---------------------------------------------------------------------------
+  // types and constants
+public:
+
+  typedef T                                           value_type;
+  typedef value_type&                                 reference;
+  typedef const value_type&                           const_reference;
+  typedef T*                                          iterator;
+  typedef const T*                                    const_iterator;
+  typedef size_t                                      size_type;
+  typedef typename std::make_signed<size_type>::type  difference_type;
+  typedef Allocator                                   allocator_type;
+  typedef typename A::pointer                         pointer;
+  typedef typename A::const_pointer                   const_pointer;
+  typedef std::reverse_iterator<iterator>             reverse_iterator;
+  typedef std::reverse_iterator<const_iterator>       const_reverse_iterator;
+
+private:
+
+  typedef std::integral_constant<bool,
+      boost::has_trivial_copy_constructor<T>::value &&
+      sizeof(T) <= 16 // don't force large structures to be passed by value
+    > should_pass_by_value;
+  typedef typename std::conditional<
+      should_pass_by_value::value, T, const T&>::type VT;
+  typedef typename std::conditional<
+      should_pass_by_value::value, T, T&&>::type MT;
+
+  typedef std::integral_constant<bool,
+      std::is_same<Allocator, std::allocator<T>>::value> usingStdAllocator;
+  typedef std::integral_constant<bool,
+      usingStdAllocator::value ||
+      A::propagate_on_container_move_assignment::value> moveIsSwap;
+
+  //===========================================================================
+  //---------------------------------------------------------------------------
+  // allocator helpers
+private:
+
+  //---------------------------------------------------------------------------
+  // allocate
+
+  T* M_allocate(size_type n) {
+    return impl_.D_allocate(n);
+  }
+
+  //---------------------------------------------------------------------------
+  // deallocate
+
+  void M_deallocate(T* p, size_type n) noexcept {
+    impl_.D_deallocate(p, n);
+  }
+
+  //---------------------------------------------------------------------------
+  // construct
+
+  // GCC is very sensitive to the exact way that construct is called. For
+  //  that reason there are several different specializations of construct.
+
+  template <typename U, typename... Args>
+  void M_construct(U* p, Args&&... args) {
+    if (usingStdAllocator::value) {
+      new (p) U(std::forward<Args>(args)...);
+    } else {
+      folly::fbv_allocator_traits<Allocator>::construct(
+        impl_, p, std::forward<Args>(args)...);
+    }
+  }
+
+  template <typename U, typename... Args>
+  static void S_construct(U* p, Args&&... args) {
+    new (p) U(std::forward<Args>(args)...);
+  }
+
+  template <typename U, typename... Args>
+  static void S_construct_a(Allocator& a, U* p, Args&&... args) {
+    folly::fbv_allocator_traits<Allocator>::construct(
+      a, p, std::forward<Args>(args)...);
+  }
+
+  // scalar optimization
+  // TODO we can expand this optimization to: default copyable and assignable
+  template <typename U, typename Enable = typename
+    std::enable_if<std::is_scalar<U>::value>::type>
+  void M_construct(U* p, U arg) {
+    if (usingStdAllocator::value) {
+      *p = arg;
+    } else {
+      folly::fbv_allocator_traits<Allocator>::construct(impl_, p, arg);
+    }
+  }
+
+  template <typename U, typename Enable = typename
+    std::enable_if<std::is_scalar<U>::value>::type>
+  static void S_construct(U* p, U arg) {
+    *p = arg;
+  }
+
+  template <typename U, typename Enable = typename
+    std::enable_if<std::is_scalar<U>::value>::type>
+  static void S_construct_a(Allocator& a, U* p, U arg) {
+    folly::fbv_allocator_traits<Allocator>::construct(a, p, arg);
+  }
+
+  // const& optimization
+  template <typename U, typename Enable = typename
+    std::enable_if<!std::is_scalar<U>::value>::type>
+  void M_construct(U* p, const U& value) {
+    if (usingStdAllocator::value) {
+      new (p) U(value);
+    } else {
+      folly::fbv_allocator_traits<Allocator>::construct(impl_, p, value);
+    }
+  }
+
+  template <typename U, typename Enable = typename
+    std::enable_if<!std::is_scalar<U>::value>::type>
+  static void S_construct(U* p, const U& value) {
+    new (p) U(value);
+  }
+
+  template <typename U, typename Enable = typename
+    std::enable_if<!std::is_scalar<U>::value>::type>
+  static void S_construct_a(Allocator& a, U* p, const U& value) {
+    folly::fbv_allocator_traits<Allocator>::construct(a, p, value);
+  }
+
+  //---------------------------------------------------------------------------
+  // destroy
+
+  void M_destroy(T* p) noexcept {
+    if (usingStdAllocator::value) {
+      if (!boost::has_trivial_destructor<T>::value) p->~T();
+    } else {
+      folly::fbv_allocator_traits<Allocator>::destroy(impl_, p);
+    }
+  }
+
+  //===========================================================================
+  //---------------------------------------------------------------------------
+  // algorithmic helpers
+private:
+
+  //---------------------------------------------------------------------------
+  // destroy_range
+
+  // wrappers
+  void M_destroy_range_e(T* pos) noexcept {
+    D_destroy_range_a(pos, impl_.e_);
+    impl_.e_ = pos;
+  }
+
+  // dispatch
+  // THIS DISPATCH CODE IS DUPLICATED IN IMPL. SEE IMPL FOR DETAILS.
+  void D_destroy_range_a(T* first, T* last) noexcept {
+    if (usingStdAllocator::value) {
+      S_destroy_range(first, last);
+    } else {
+      S_destroy_range_a(impl_, first, last);
+    }
+  }
+
+  // allocator
+  static void S_destroy_range_a(Allocator& a, T* first, T* last) noexcept {
+    for (; first != last; ++first)
+      folly::fbv_allocator_traits<Allocator>::destroy(a, first);
+  }
+
+  // optimized
+  static void S_destroy_range(T* first, T* last) noexcept {
+    if (!boost::has_trivial_destructor<T>::value) {
+      // EXPERIMENTAL DATA on fbvector<vector<int>> (where each vector<int> has
+      //  size 0).
+      // The unrolled version seems to work faster for small to medium sized
+      //  fbvectors. It gets a 10% speedup on fbvectors of size 1024, 64, and
+      //  16.
+      // The simple loop version seems to work faster for large fbvectors. The
+      //  unrolled version is about 6% slower on fbvectors on size 16384.
+      // The two methods seem tied for very large fbvectors. The unrolled
+      //  version is about 0.5% slower on size 262144.
+
+      // for (; first != last; ++first) first->~T();
+      #define FOLLY_FBV_OP(p) (p)->~T()
+      FOLLY_FBV_UNROLL_PTR(first, last, FOLLY_FBV_OP)
+      #undef FOLLY_FBV_OP
+    }
+  }
+
+  //---------------------------------------------------------------------------
+  // uninitialized_fill_n
+
+  // wrappers
+  void M_uninitialized_fill_n_e(size_type sz) {
+    D_uninitialized_fill_n_a(impl_.e_, sz);
+    impl_.e_ += sz;
+  }
+
+  void M_uninitialized_fill_n_e(size_type sz, VT value) {
+    D_uninitialized_fill_n_a(impl_.e_, sz, value);
+    impl_.e_ += sz;
+  }
+
+  // dispatch
+  void D_uninitialized_fill_n_a(T* dest, size_type sz) {
+    if (usingStdAllocator::value) {
+      S_uninitialized_fill_n(dest, sz);
+    } else {
+      S_uninitialized_fill_n_a(impl_, dest, sz);
+    }
+  }
+
+  void D_uninitialized_fill_n_a(T* dest, size_type sz, VT value) {
+    if (usingStdAllocator::value) {
+      S_uninitialized_fill_n(dest, sz, value);
+    } else {
+      S_uninitialized_fill_n_a(impl_, dest, sz, value);
+    }
+  }
+
+  // allocator
+  template <typename... Args>
+  static void S_uninitialized_fill_n_a(Allocator& a, T* dest,
+                                       size_type sz, Args&&... args) {
+    auto b = dest;
+    auto e = dest + sz;
+    try {
+      for (; b != e; ++b)
+        folly::fbv_allocator_traits<Allocator>::construct(a, b,
+          std::forward<Args>(args)...);
+    } catch (...) {
+      S_destroy_range_a(a, dest, b);
+      throw;
+    }
+  }
+
+  // optimized
+  static void S_uninitialized_fill_n(T* dest, size_type n) {
+    if (folly::IsZeroInitializable<T>::value) {
+      std::memset(dest, 0, sizeof(T) * n);
+    } else {
+      auto b = dest;
+      auto e = dest + n;
+      try {
+        for (; b != e; ++b) S_construct(b);
+      } catch (...) {
+        --b;
+        for (; b >= dest; --b) b->~T();
+        throw;
+      }
+    }
+  }
+
+  static void S_uninitialized_fill_n(T* dest, size_type n, const T& value) {
+    auto b = dest;
+    auto e = dest + n;
+    try {
+      for (; b != e; ++b) S_construct(b, value);
+    } catch (...) {
+      S_destroy_range(dest, b);
+      throw;
+    }
+  }
+
+  //---------------------------------------------------------------------------
+  // uninitialized_copy
+
+  // it is possible to add an optimization for the case where
+  // It = move(T*) and IsRelocatable<T> and Is0Initiailizable<T>
+
+  // wrappers
+  template <typename It>
+  void M_uninitialized_copy_e(It first, It last) {
+    D_uninitialized_copy_a(impl_.e_, first, last);
+    impl_.e_ += std::distance(first, last);
+  }
+
+  template <typename It>
+  void M_uninitialized_move_e(It first, It last) {
+    D_uninitialized_move_a(impl_.e_, first, last);
+    impl_.e_ += std::distance(first, last);
+  }
+
+  // dispatch
+  template <typename It>
+  void D_uninitialized_copy_a(T* dest, It first, It last) {
+    if (usingStdAllocator::value) {
+      if (folly::IsTriviallyCopyable<T>::value) {
+        S_uninitialized_copy_bits(dest, first, last);
+      } else {
+        S_uninitialized_copy(dest, first, last);
+      }
+    } else {
+      S_uninitialized_copy_a(impl_, dest, first, last);
+    }
+  }
+
+  template <typename It>
+  void D_uninitialized_move_a(T* dest, It first, It last) {
+    D_uninitialized_copy_a(dest,
+      std::make_move_iterator(first), std::make_move_iterator(last));
+  }
+
+  // allocator
+  template <typename It>
+  static void
+  S_uninitialized_copy_a(Allocator& a, T* dest, It first, It last) {
+    auto b = dest;
+    try {
+      for (; first != last; ++first, ++b)
+        folly::fbv_allocator_traits<Allocator>::construct(a, b, *first);
+    } catch (...) {
+      S_destroy_range_a(a, dest, b);
+      throw;
+    }
+  }
+
+  // optimized
+  template <typename It>
+  static void S_uninitialized_copy(T* dest, It first, It last) {
+    auto b = dest;
+    try {
+      for (; first != last; ++first, ++b)
+        S_construct(b, *first);
+    } catch (...) {
+      S_destroy_range(dest, b);
+      throw;
+    }
+  }
+
+  static void
+  S_uninitialized_copy_bits(T* dest, const T* first, const T* last) {
+    std::memcpy(dest, first, (last - first) * sizeof(T));
+  }
+
+  static void
+  S_uninitialized_copy_bits(T* dest, std::move_iterator<T*> first,
+                       std::move_iterator<T*> last) {
+    T* bFirst = first.base();
+    T* bLast = last.base();
+    std::memcpy(dest, bFirst, (bLast - bFirst) * sizeof(T));
+  }
+
+  template <typename It>
+  static void
+  S_uninitialized_copy_bits(T* dest, It first, It last) {
+    S_uninitialized_copy(dest, first, last);
+  }
+
+  //---------------------------------------------------------------------------
+  // copy_n
+
+  // This function is "unsafe": it assumes that the iterator can be advanced at
+  //  least n times. However, as a private function, that unsafety is managed
+  //  wholly by fbvector itself.
+
+  template <typename It>
+  static It S_copy_n(T* dest, It first, size_type n) {
+    auto e = dest + n;
+    for (; dest != e; ++dest, ++first) *dest = *first;
+    return first;
+  }
+
+  static const T* S_copy_n(T* dest, const T* first, size_type n) {
+    if (folly::IsTriviallyCopyable<T>::value) {
+      std::memcpy(dest, first, n * sizeof(T));
+      return first + n;
+    } else {
+      return S_copy_n<const T*>(dest, first, n);
+    }
+  }
+
+  static std::move_iterator<T*>
+  S_copy_n(T* dest, std::move_iterator<T*> mIt, size_type n) {
+    if (folly::IsTriviallyCopyable<T>::value) {
+      T* first = mIt.base();
+      std::memcpy(dest, first, n * sizeof(T));
+      return std::make_move_iterator(first + n);
+    } else {
+      return S_copy_n<std::move_iterator<T*>>(dest, mIt, n);
+    }
+  }
+
+  //===========================================================================
+  //---------------------------------------------------------------------------
+  // relocation helpers
+private:
+
+  // Relocation is divided into three parts:
+  //
+  //  1: relocate_move
+  //     Performs the actual movement of data from point a to point b.
+  //
+  //  2: relocate_done
+  //     Destroys the old data.
+  //
+  //  3: relocate_undo
+  //     Destoys the new data and restores the old data.
+  //
+  // The three steps are used because there may be an exception after part 1
+  //  has completed. If that is the case, then relocate_undo can nullify the
+  //  initial move. Otherwise, relocate_done performs the last bit of tidying
+  //  up.
+  //
+  // The relocation trio may use either memcpy, move, or copy. It is decided
+  //  by the following case statement:
+  //
+  //  IsRelocatable && usingStdAllocator    -> memcpy
+  //  has_nothrow_move && usingStdAllocator -> move
+  //  cannot copy                           -> move
+  //  default                               -> copy
+  //
+  // If the class is non-copyable then it must be movable. However, if the
+  //  move constructor is not noexcept, i.e. an error could be thrown, then
+  //  relocate_undo will be unable to restore the old data, for fear of a
+  //  second exception being thrown. This is a known and unavoidable
+  //  deficiency. In lieu of a strong exception guarantee, relocate_undo does
+  //  the next best thing: it provides a weak exception guarantee by
+  //  destorying the new data, but leaving the old data in an indeterminate
+  //  state. Note that that indeterminate state will be valid, since the
+  //  old data has not been destroyed; it has merely been the source of a
+  //  move, which is required to leave the source in a valid state.
+
+  // wrappers
+  void M_relocate(T* newB) {
+    relocate_move(newB, impl_.b_, impl_.e_);
+    relocate_done(newB, impl_.b_, impl_.e_);
+  }
+
+  // dispatch type trait
+  typedef std::integral_constant<bool,
+      folly::IsRelocatable<T>::value && usingStdAllocator::value
+    > relocate_use_memcpy;
+
+  typedef std::integral_constant<bool,
+      (folly::fbv_is_nothrow_move_constructible<T>::value
+       && usingStdAllocator::value)
+      || !folly::fbv_is_copy_constructible<T>::value
+    > relocate_use_move;
+
+  // move
+  void relocate_move(T* dest, T* first, T* last) {
+    relocate_move_or_memcpy(dest, first, last, relocate_use_memcpy());
+  }
+
+  void relocate_move_or_memcpy(T* dest, T* first, T* last, std::true_type) {
+    std::memcpy(dest, first, (last - first) * sizeof(T));
+  }
+
+  void relocate_move_or_memcpy(T* dest, T* first, T* last, std::false_type) {
+    relocate_move_or_copy(dest, first, last, relocate_use_move());
+  }
+
+  void relocate_move_or_copy(T* dest, T* first, T* last, std::true_type) {
+    D_uninitialized_move_a(dest, first, last);
+  }
+
+  void relocate_move_or_copy(T* dest, T* first, T* last, std::false_type) {
+    D_uninitialized_copy_a(dest, first, last);
+  }
+
+  // done
+  void relocate_done(T* dest, T* first, T* last) noexcept {
+    if (folly::IsRelocatable<T>::value && usingStdAllocator::value) {
+      // used memcpy; data has been relocated, do not call destructor
+    } else {
+      D_destroy_range_a(first, last);
+    }
+  }
+
+  // undo
+  void relocate_undo(T* dest, T* first, T* last) noexcept {
+    if (folly::IsRelocatable<T>::value && usingStdAllocator::value) {
+      // used memcpy, old data is still valid, nothing to do
+    } else if (folly::fbv_is_nothrow_move_constructible<T>::value &&
+               usingStdAllocator::value) {
+      // noexcept move everything back, aka relocate_move
+      relocate_move(first, dest, dest + (last - first));
+    } else if (!folly::fbv_is_copy_constructible<T>::value) {
+      // weak guarantee
+      D_destroy_range_a(dest, dest + (last - first));
+    } else {
+      // used copy, old data is still valid
+      D_destroy_range_a(dest, dest + (last - first));
+    }
+  }
+
+
+  //===========================================================================
+  //---------------------------------------------------------------------------
+  // construct/copy/destroy
+public:
+
+  fbvector() = default;
+
+  explicit fbvector(const Allocator& a) : impl_(a) {}
+
+  explicit fbvector(size_type n, const Allocator& a = Allocator())
+    : impl_(n, a)
+    { M_uninitialized_fill_n_e(n); }
+
+  fbvector(size_type n, VT value, const Allocator& a = Allocator())
+    : impl_(n, a)
+    { M_uninitialized_fill_n_e(n, value); }
+
+  template <class It, class Category = typename
+            std::iterator_traits<It>::iterator_category>
+  fbvector(It first, It last, const Allocator& a = Allocator())
+    #ifndef FOLLY_FBV_COMPATIBILITY_MODE
+    : fbvector(first, last, a, Category()) {}
+    #else
+    : impl_(std::distance(first, last), a)
+    { fbvector_init(first, last, Category()); }
+    #endif
+
+  fbvector(const fbvector& other)
+    : impl_(other.size(), A::select_on_container_copy_construction(other.impl_))
+    { M_uninitialized_copy_e(other.begin(), other.end()); }
+
+  fbvector(fbvector&& other) noexcept : impl_(std::move(other.impl_)) {}
+
+  fbvector(const fbvector& other, const Allocator& a)
+    #ifndef FOLLY_FBV_COMPATIBILITY_MODE
+    : fbvector(other.begin(), other.end(), a) {}
+    #else
+    : impl_(other.size(), a)
+    { fbvector_init(other.begin(), other.end(), std::forward_iterator_tag()); }
+    #endif
+
+  fbvector(fbvector&& other, const Allocator& a) : impl_(a) {
+    if (impl_ == other.impl_) {
+      impl_.swapData(other.impl_);
+    } else {
+      impl_.init(other.size());
+      M_uninitialized_move_e(other.begin(), other.end());
+    }
+  }
+
+  fbvector(std::initializer_list<T> il, const Allocator& a = Allocator())
+    #ifndef FOLLY_FBV_COMPATIBILITY_MODE
+    : fbvector(il.begin(), il.end(), a) {}
+    #else
+    : impl_(std::distance(il.begin(), il.end()), a)
+    { fbvector_init(il.begin(), il.end(), std::forward_iterator_tag()); }
+    #endif
+
+  ~fbvector() = default; // the cleanup occurs in impl_
+
+  fbvector& operator=(const fbvector& other) {
+    if (UNLIKELY(this == &other)) return *this;
+
+    if (!usingStdAllocator::value &&
+        A::propagate_on_container_copy_assignment::value) {
+      if (impl_ != other.impl_) {
+        // can't use other's different allocator to clean up self
+        impl_.reset();
+      }
+      (Allocator&)impl_ = (Allocator&)other.impl_;
+    }
+
+    assign(other.begin(), other.end());
+    return *this;
+  }
+
+  fbvector& operator=(fbvector&& other) {
+    if (UNLIKELY(this == &other)) return *this;
+    moveFrom(std::move(other), moveIsSwap());
+    return *this;
+  }
+
+  fbvector& operator=(std::initializer_list<T> il) {
+    assign(il.begin(), il.end());
+    return *this;
+  }
+
+  template <class It, class Category = typename
+            std::iterator_traits<It>::iterator_category>
+  void assign(It first, It last) {
+    assign(first, last, Category());
+  }
+
+  void assign(size_type n, VT value) {
+    if (n > capacity()) {
+      // Not enough space. Do not reserve in place, since we will
+      // discard the old values anyways.
+      if (dataIsInternalAndNotVT(value)) {
+        T copy(std::move(value));
+        impl_.reset(n);
+        M_uninitialized_fill_n_e(n, copy);
+      } else {
+        impl_.reset(n);
+        M_uninitialized_fill_n_e(n, value);
+      }
+    } else if (n <= size()) {
+      auto newE = impl_.b_ + n;
+      std::fill(impl_.b_, newE, value);
+      M_destroy_range_e(newE);
+    } else {
+      std::fill(impl_.b_, impl_.e_, value);
+      M_uninitialized_fill_n_e(n - size(), value);
+    }
+  }
+
+  void assign(std::initializer_list<T> il) {
+    assign(il.begin(), il.end());
+  }
+
+  allocator_type get_allocator() const noexcept {
+    return impl_;
+  }
+
+private:
+
+  #ifndef FOLLY_FBV_COMPATIBILITY_MODE
+  // contract dispatch for iterator types fbvector(It first, It last)
+  template <class ForwardIterator>
+  fbvector(ForwardIterator first, ForwardIterator last,
+           const Allocator& a, std::forward_iterator_tag)
+    : impl_(std::distance(first, last), a)
+    { M_uninitialized_copy_e(first, last); }
+
+  template <class InputIterator>
+  fbvector(InputIterator first, InputIterator last,
+           const Allocator& a, std::input_iterator_tag)
+    : impl_(a)
+    { for (; first != last; ++first) emplace_back(*first); }
+
+  #else
+  // contract dispatch for iterator types without constructor forwarding
+  template <class ForwardIterator>
+  void
+  fbvector_init(ForwardIterator first, ForwardIterator last,
+                std::forward_iterator_tag)
+    { M_uninitialized_copy_e(first, last); }
+
+  template <class InputIterator>
+  void
+  fbvector_init(InputIterator first, InputIterator last,
+                std::input_iterator_tag)
+    { for (; first != last; ++first) emplace_back(*first); }
+  #endif
+
+  // contract dispatch for allocator movement in operator=(fbvector&&)
+  void
+  moveFrom(fbvector&& other, std::true_type) {
+    swap(impl_, other.impl_);
+  }
+  void moveFrom(fbvector&& other, std::false_type) {
+    if (impl_ == other.impl_) {
+      impl_.swapData(other.impl_);
+    } else {
+      impl_.reset(other.size());
+      M_uninitialized_move_e(other.begin(), other.end());
+    }
+  }
+
+  // contract dispatch for iterator types in assign(It first, It last)
+  template <class ForwardIterator>
+  void assign(ForwardIterator first, ForwardIterator last,
+              std::forward_iterator_tag) {
+    auto const newSize = std::distance(first, last);
+    if (newSize > capacity()) {
+      impl_.reset(newSize);
+      M_uninitialized_copy_e(first, last);
+    } else if (newSize <= size()) {
+      auto newEnd = std::copy(first, last, impl_.b_);
+      M_destroy_range_e(newEnd);
+    } else {
+      auto mid = S_copy_n(impl_.b_, first, size());
+      M_uninitialized_copy_e<decltype(last)>(mid, last);
+    }
+  }
+
+  template <class InputIterator>
+  void assign(InputIterator first, InputIterator last,
+              std::input_iterator_tag) {
+    auto p = impl_.b_;
+    for (; first != last && p != impl_.e_; ++first, ++p) {
+      *p = *first;
+    }
+    if (p != impl_.e_) {
+      M_destroy_range_e(p);
+    } else {
+      for (; first != last; ++first) emplace_back(*first);
+    }
+  }
+
+  // contract dispatch for aliasing under VT optimization
+  bool dataIsInternalAndNotVT(const T& t) {
+    if (should_pass_by_value::value) return false;
+    return dataIsInternal(t);
+  }
+  bool dataIsInternal(const T& t) {
+    return UNLIKELY(impl_.b_ <= std::addressof(t) &&
+                    std::addressof(t) < impl_.e_);
+  }
+
+
+  //===========================================================================
+  //---------------------------------------------------------------------------
+  // iterators
+public:
+
+  iterator begin() noexcept {
+    return impl_.b_;
+  }
+  const_iterator begin() const noexcept {
+    return impl_.b_;
+  }
+  iterator end() noexcept {
+    return impl_.e_;
+  }
+  const_iterator end() const noexcept {
+    return impl_.e_;
+  }
+  reverse_iterator rbegin() noexcept {
+    return reverse_iterator(end());
+  }
+  const_reverse_iterator rbegin() const noexcept {
+    return const_reverse_iterator(end());
+  }
+  reverse_iterator rend() noexcept {
+    return reverse_iterator(begin());
+  }
+  const_reverse_iterator rend() const noexcept {
+    return const_reverse_iterator(begin());
+  }
+
+  const_iterator cbegin() const noexcept {
+    return impl_.b_;
+  }
+  const_iterator cend() const noexcept {
+    return impl_.e_;
+  }
+  const_reverse_iterator crbegin() const noexcept {
+    return const_reverse_iterator(end());
+  }
+  const_reverse_iterator crend() const noexcept {
+    return const_reverse_iterator(begin());
+  }
+
+  //===========================================================================
+  //---------------------------------------------------------------------------
+  // capacity
+public:
+
+  size_type size() const noexcept {
+    return impl_.e_ - impl_.b_;
+  }
+
+  size_type max_size() const noexcept {
+    // good luck gettin' there
+    return ~size_type(0);
+  }
+
+  void resize(size_type n) {
+    if (n <= size()) {
+      M_destroy_range_e(impl_.b_ + n);
+    } else {
+      reserve(n);
+      M_uninitialized_fill_n_e(n - size());
+    }
+  }
+
+  void resize(size_type n, VT t) {
+    if (n <= size()) {
+      M_destroy_range_e(impl_.b_ + n);
+    } else if (dataIsInternalAndNotVT(t) && n > capacity()) {
+      T copy(t);
+      reserve(n);
+      M_uninitialized_fill_n_e(n - size(), copy);
+    } else {
+      reserve(n);
+      M_uninitialized_fill_n_e(n - size(), t);
+    }
+  }
+
+  size_type capacity() const noexcept {
+    return impl_.z_ - impl_.b_;
+  }
+
+  bool empty() const noexcept {
+    return impl_.b_ == impl_.e_;
+  }
+
+  void reserve(size_type n) {
+    if (n <= capacity()) return;
+    if (impl_.b_ && reserve_in_place(n)) return;
+
+    auto newCap = folly::goodMallocSize(n * sizeof(T)) / sizeof(T);
+    auto newB = M_allocate(newCap);
+    try {
+      M_relocate(newB);
+    } catch (...) {
+      M_deallocate(newB, newCap);
+      throw;
+    }
+    if (impl_.b_)
+      M_deallocate(impl_.b_, impl_.z_ - impl_.b_);
+    impl_.z_ = newB + newCap;
+    impl_.e_ = newB + (impl_.e_ - impl_.b_);
+    impl_.b_ = newB;
+  }
+
+  void shrink_to_fit() noexcept {
+    auto const newCapacityBytes = folly::goodMallocSize(size() * sizeof(T));
+    auto const newCap = newCapacityBytes / sizeof(T);
+    auto const oldCap = capacity();
+
+    if (newCap >= oldCap) return;
+
+    void* p = impl_.b_;
+    if ((rallocm && usingStdAllocator::value) &&
+        newCapacityBytes >= folly::jemallocMinInPlaceExpandable &&
+        rallocm(&p, NULL, newCapacityBytes, 0, ALLOCM_NO_MOVE)
+          == ALLOCM_SUCCESS) {
+      impl_.z_ += newCap - oldCap;
+    } else {
+      T* newB; // intentionally uninitialized
+      try {
+        newB = M_allocate(newCap);
+        try {
+          M_relocate(newB);
+        } catch (...) {
+          M_deallocate(newB, newCap);
+          return; // swallow the error
+        }
+      } catch (...) {
+        return;
+      }
+      if (impl_.b_)
+        M_deallocate(impl_.b_, impl_.z_ - impl_.b_);
+      impl_.z_ = newB + newCap;
+      impl_.e_ = newB + (impl_.e_ - impl_.b_);
+      impl_.b_ = newB;
+    }
+  }
+
+private:
+
+  bool reserve_in_place(size_type n) {
+    if (!usingStdAllocator::value || !rallocm) return false;
+
+    // jemalloc can never grow in place blocks smaller than 4096 bytes.
+    if ((impl_.z_ - impl_.b_) * sizeof(T) <
+      folly::jemallocMinInPlaceExpandable) return false;
+
+    auto const newCapacityBytes = folly::goodMallocSize(n * sizeof(T));
+    void* p = impl_.b_;
+    if (rallocm(&p, NULL, newCapacityBytes, 0, ALLOCM_NO_MOVE)
+        == ALLOCM_SUCCESS) {
+      impl_.z_ = impl_.b_ + newCapacityBytes / sizeof(T);
+      return true;
+    }
+    return false;
+  }
+
+  //===========================================================================
+  //---------------------------------------------------------------------------
+  // element access
+public:
+
+  reference operator[](size_type n) {
+    assert(n < size());
+    return impl_.b_[n];
+  }
+  const_reference operator[](size_type n) const {
+    assert(n < size());
+    return impl_.b_[n];
+  }
+  const_reference at(size_type n) const {
+    if (UNLIKELY(n >= size())) {
+      throw std::out_of_range("fbvector: index is greater than size.");
+    }
+    return (*this)[n];
+  }
+  reference at(size_type n) {
+    auto const& cThis = *this;
+    return const_cast<reference>(cThis.at(n));
+  }
+  reference front() {
+    assert(!empty());
+    return *impl_.b_;
+  }
+  const_reference front() const {
+    assert(!empty());
+    return *impl_.b_;
+  }
+  reference back()  {
+    assert(!empty());
+    return impl_.e_[-1];
+  }
+  const_reference back() const {
+    assert(!empty());
+    return impl_.e_[-1];
+  }
+
+  //===========================================================================
+  //---------------------------------------------------------------------------
+  // data access
+public:
+
+  T* data() noexcept {
+    return impl_.b_;
+  }
+  const T* data() const noexcept {
+    return impl_.b_;
+  }
+
+  //===========================================================================
+  //---------------------------------------------------------------------------
+  // modifiers (common)
+public:
+
+  template <class... Args>
+  void emplace_back(Args&&... args)  {
+    if (impl_.e_ != impl_.z_) {
+      M_construct(impl_.e_, std::forward<Args>(args)...);
+      ++impl_.e_;
+    } else {
+      emplace_back_aux(std::forward<Args>(args)...);
+    }
+  }
+
+  void
+  push_back(const T& value) {
+    if (impl_.e_ != impl_.z_) {
+      M_construct(impl_.e_, value);
+      ++impl_.e_;
+    } else {
+      emplace_back_aux(value);
+    }
+  }
+
+  void
+  push_back(T&& value) {
+    if (impl_.e_ != impl_.z_) {
+      M_construct(impl_.e_, std::move(value));
+      ++impl_.e_;
+    } else {
+      emplace_back_aux(std::move(value));
+    }
+  }
+
+  void pop_back() {
+    assert(!empty());
+    --impl_.e_;
+    M_destroy(impl_.e_);
+  }
+
+  void swap(fbvector& other) noexcept {
+    if (!usingStdAllocator::value &&
+        A::propagate_on_container_swap::value)
+      swap(impl_, other.impl_);
+    else impl_.swapData(other.impl_);
+  }
+
+  void clear() noexcept {
+    M_destroy_range_e(impl_.b_);
+  }
+
+private:
+
+  // std::vector implements a similar function with a different growth
+  //  strategy: empty() ? 1 : capacity() * 2.
+  //
+  // fbvector grows differently on two counts:
+  //
+  // (1) initial size
+  //     Instead of grwoing to size 1 from empty, and fbvector allocates at
+  //     least 64 bytes. You may still use reserve to reserve a lesser amount
+  //     of memory.
+  // (2) 1.5x
+  //     For medium-sized vectors, the growth strategy is 1.5x. See the docs
+  //     for details.
+  //     This does not apply to very small or very large fbvectors. This is a
+  //     heuristic.
+  //     A nice addition to fbvector would be the capability of having a user-
+  //     defined growth strategy, probably as part of the allocator.
+  //
+
+  size_type computePushBackCapacity() const {
+    return empty() ? std::max(64 / sizeof(T), size_type(1))
+      : capacity() < folly::jemallocMinInPlaceExpandable / sizeof(T)
+      ? capacity() * 2
+      : sizeof(T) > folly::jemallocMinInPlaceExpandable / 2 && capacity() == 1
+      ? 2
+      : capacity() > 4096 * 32 / sizeof(T)
+      ? capacity() * 2
+      : (capacity() * 3 + 1) / 2;
+  }
+
+  template <class... Args>
+  void emplace_back_aux(Args&&... args);
+
+  //===========================================================================
+  //---------------------------------------------------------------------------
+  // modifiers (erase)
+public:
+
+  iterator erase(const_iterator position) {
+    return erase(position, position + 1);
+  }
+
+  iterator erase(const_iterator first, const_iterator last) {
+    assert(isValid(first) && isValid(last));
+    assert(first <= last);
+    if (first != last) {
+      if (last == end()) {
+        M_destroy_range_e((iterator)first);
+      } else {
+        if (folly::IsRelocatable<T>::value && usingStdAllocator::value) {
+          D_destroy_range_a((iterator)first, (iterator)last);
+          if (last - first >= cend() - last) {
+            std::memcpy((iterator)first, last, (cend() - last) * sizeof(T));
+          } else {
+            std::memmove((iterator)first, last, (cend() - last) * sizeof(T));
+          }
+          impl_.e_ -= (last - first);
+        } else {
+          std::copy(std::make_move_iterator((iterator)last),
+                    std::make_move_iterator(end()), (iterator)first);
+          auto newEnd = impl_.e_ - std::distance(first, last);
+          M_destroy_range_e(newEnd);
+        }
+      }
+    }
+    return (iterator)first;
+  }
+
+  //===========================================================================
+  //---------------------------------------------------------------------------
+  // modifiers (insert)
+private: // we have the private section first because it defines some macros
+
+  bool isValid(const_iterator it) {
+    return cbegin() <= it && it <= cend();
+  }
+
+  size_type computeInsertCapacity(size_type n) {
+    size_type nc = std::max(computePushBackCapacity(), size() + n);
+    size_type ac = folly::goodMallocSize(nc * sizeof(T)) / sizeof(T);
+    return ac;
+  }
+
+  //---------------------------------------------------------------------------
+  //
+  // make_window takes an fbvector, and creates an uninitialized gap (a
+  //  window) at the given position, of the given size. The fbvector must
+  //  have enough capacity.
+  //
+  // Explanation by picture.
+  //
+  //    123456789______
+  //        ^
+  //        make_window here of size 3
+  //
+  //    1234___56789___
+  //
+  // If something goes wrong and the window must be destroyed, use
+  //  undo_window to provide a weak exception guarantee. It destroys
+  //  the right ledge.
+  //
+  //    1234___________
+  //
+  //---------------------------------------------------------------------------
+  //
+  // wrap_frame takes an inverse window and relocates an fbvector around it.
+  //  The fbvector must have at least as many elements as the left ledge.
+  //
+  // Explanation by picture.
+  //
+  //        START
+  //    fbvector:             inverse window:
+  //    123456789______       _____abcde_______
+  //                          [idx][ n ]
+  //
+  //        RESULT
+  //    _______________       12345abcde6789___
+  //
+  //---------------------------------------------------------------------------
+  //
+  // insert_use_fresh_memory returns true iff the fbvector should use a fresh
+  //  block of memory for the insertion. If the fbvector does not have enough
+  //  spare capacity, then it must return true. Otherwise either true or false
+  //  may be returned.
+  //
+  //---------------------------------------------------------------------------
+  //
+  // These three functions, make_window, wrap_frame, and
+  //  insert_use_fresh_memory, can be combined into a uniform interface.
+  // Since that interface involves a lot of case-work, it is built into
+  //  some macros: FOLLY_FBVECTOR_INSERT_(START|TRY|END)
+  // Macros are used in an attempt to let GCC perform better optimizations,
+  //  especially control flow optimization.
+  //
+
+  //---------------------------------------------------------------------------
+  // window
+
+  void make_window(iterator position, size_type n) {
+    assert(isValid(position));
+    assert(size() + n <= capacity());
+    assert(n != 0);
+
+    auto tail = std::distance(position, impl_.e_);
+
+    if (tail <= n) {
+      relocate_move(position + n, position, impl_.e_);
+      relocate_done(position + n, position, impl_.e_);
+      impl_.e_ += n;
+    } else {
+      if (folly::IsRelocatable<T>::value && usingStdAllocator::value) {
+        std::memmove(position + n, position, tail * sizeof(T));
+        impl_.e_ += n;
+      } else {
+        D_uninitialized_move_a(impl_.e_, impl_.e_ - n, impl_.e_);
+        impl_.e_ += n;
+        std::copy_backward(std::make_move_iterator(position),
+                           std::make_move_iterator(impl_.e_ - n), impl_.e_);
+        D_destroy_range_a(position, position + n);
+      }
+    }
+  }
+
+  void undo_window(iterator position, size_type n) noexcept {
+    D_destroy_range_a(position + n, impl_.e_);
+    impl_.e_ = position;
+  }
+
+  //---------------------------------------------------------------------------
+  // frame
+
+  void wrap_frame(T* ledge, size_type idx, size_type n) {
+    assert(size() >= idx);
+    assert(n != 0);
+
+    relocate_move(ledge, impl_.b_, impl_.b_ + idx);
+    try {
+      relocate_move(ledge + idx + n, impl_.b_ + idx, impl_.e_);
+    } catch (...) {
+      relocate_undo(ledge, impl_.b_, impl_.b_ + idx);
+      throw;
+    }
+    relocate_done(ledge, impl_.b_, impl_.b_ + idx);
+    relocate_done(ledge + idx + n, impl_.b_ + idx, impl_.e_);
+  }
+
+  //---------------------------------------------------------------------------
+  // use fresh?
+
+  bool insert_use_fresh(const_iterator cposition, size_type n) {
+    if (cposition == cend()) {
+      if (size() + n <= capacity()) return false;
+      if (reserve_in_place(size() + n)) return false;
+      return true;
+    }
+
+    if (size() + n > capacity()) return true;
+
+    return false;
+  }
+
+  //---------------------------------------------------------------------------
+  // interface
+
+  #define FOLLY_FBVECTOR_INSERT_START(cpos, n)                                \
+    assert(isValid(cpos));                                                    \
+    T* position = const_cast<T*>(cpos);                                       \
+    size_type idx = std::distance(impl_.b_, position);                        \
+    bool fresh = insert_use_fresh(position, n);                               \
+    T* b;                                                                     \
+    size_type newCap = 0;                                                     \
+                                                                              \
+    if (fresh) {                                                              \
+      newCap = computeInsertCapacity(n);                                      \
+      b = M_allocate(newCap);                                                 \
+    } else {                                                                  \
+      make_window(position, n);                                               \
+      b = impl_.b_;                                                           \
+    }                                                                         \
+                                                                              \
+    T* start = b + idx;                                                       \
+                                                                              \
+    try {                                                                     \
+
+    // construct the inserted elements
+
+  #define FOLLY_FBVECTOR_INSERT_TRY(cpos, n)                                  \
+    } catch (...) {                                                           \
+      if (fresh) {                                                            \
+        M_deallocate(b, newCap);                                              \
+      } else {                                                                \
+        undo_window(position, n);                                             \
+      }                                                                       \
+      throw;                                                                  \
+    }                                                                         \
+                                                                              \
+    if (fresh) {                                                              \
+      try {                                                                   \
+        wrap_frame(b, idx, n);                                                \
+      } catch (...) {                                                         \
+
+
+    // delete the inserted elements (exception has been thrown)
+
+  #define FOLLY_FBVECTOR_INSERT_END(cpos, n)                                  \
+        M_deallocate(b, newCap);                                              \
+        throw;                                                                \
+      }                                                                       \
+      if (impl_.b_) M_deallocate(impl_.b_, capacity());                       \
+      impl_.set(b, size() + n, newCap);                                       \
+      return impl_.b_ + idx;                                                  \
+    } else {                                                                  \
+      return position;                                                        \
+    }                                                                         \
+
+  //---------------------------------------------------------------------------
+  // insert functions
+public:
+
+  template <class... Args>
+  iterator emplace(const_iterator cpos, Args&&... args) {
+    FOLLY_FBVECTOR_INSERT_START(cpos, 1)
+      M_construct(start, std::forward<Args>(args)...);
+    FOLLY_FBVECTOR_INSERT_TRY(cpos, 1)
+      M_destroy(start);
+    FOLLY_FBVECTOR_INSERT_END(cpos, 1)
+  }
+
+  iterator insert(const_iterator cpos, const T& value) {
+    if (dataIsInternal(value)) return insert(cpos, T(value));
+
+    FOLLY_FBVECTOR_INSERT_START(cpos, 1)
+      M_construct(start, value);
+    FOLLY_FBVECTOR_INSERT_TRY(cpos, 1)
+      M_destroy(start);
+    FOLLY_FBVECTOR_INSERT_END(cpos, 1)
+  }
+
+  iterator insert(const_iterator cpos, T&& value) {
+    if (dataIsInternal(value)) return insert(cpos, T(std::move(value)));
+
+    FOLLY_FBVECTOR_INSERT_START(cpos, 1)
+      M_construct(start, std::move(value));
+    FOLLY_FBVECTOR_INSERT_TRY(cpos, 1)
+      M_destroy(start);
+    FOLLY_FBVECTOR_INSERT_END(cpos, 1)
+  }
+
+  iterator insert(const_iterator cpos, size_type n, VT value) {
+    if (n == 0) return (iterator)cpos;
+    if (dataIsInternalAndNotVT(value)) return insert(cpos, n, T(value));
+
+    FOLLY_FBVECTOR_INSERT_START(cpos, n)
+      D_uninitialized_fill_n_a(start, n, value);
+    FOLLY_FBVECTOR_INSERT_TRY(cpos, n)
+      D_destroy_range_a(start, start + n);
+    FOLLY_FBVECTOR_INSERT_END(cpos, n)
+  }
+
+  template <class It, class Category = typename
+            std::iterator_traits<It>::iterator_category>
+  iterator insert(const_iterator cpos, It first, It last) {
+    return insert(cpos, first, last, Category());
+  }
+
+  iterator insert(const_iterator cpos, std::initializer_list<T> il) {
+    return insert(cpos, il.begin(), il.end());
+  }
+
+  //---------------------------------------------------------------------------
+  // insert dispatch for iterator types
+private:
+
+  template <class FIt>
+  iterator insert(const_iterator cpos, FIt first, FIt last,
+                  std::forward_iterator_tag) {
+    size_type n = std::distance(first, last);
+    if (n == 0) return (iterator)cpos;
+
+    FOLLY_FBVECTOR_INSERT_START(cpos, n)
+      D_uninitialized_copy_a(start, first, last);
+    FOLLY_FBVECTOR_INSERT_TRY(cpos, n)
+      D_destroy_range_a(start, start + n);
+    FOLLY_FBVECTOR_INSERT_END(cpos, n)
+  }
+
+  template <class IIt>
+  iterator insert(const_iterator cpos, IIt first, IIt last,
+                  std::input_iterator_tag) {
+    T* position = const_cast<T*>(cpos);
+    assert(isValid(position));
+    size_type idx = std::distance(begin(), position);
+
+    fbvector storage(std::make_move_iterator(position),
+                     std::make_move_iterator(end()),
+                     A::select_on_container_copy_construction(impl_));
+    M_destroy_range_e(position);
+    for (; first != last; ++first) emplace_back(*first);
+    insert(cend(), std::make_move_iterator(storage.begin()),
+           std::make_move_iterator(storage.end()));
+    return impl_.b_ + idx;
+  }
+
+  //===========================================================================
+  //---------------------------------------------------------------------------
+  // lexicographical functions (others from boost::totally_ordered superclass)
+public:
+
+  bool operator==(const fbvector& other) const {
+    return size() == other.size() && std::equal(begin(), end(), other.begin());
+  }
+
+  bool operator<(const fbvector& other) const {
+    return std::lexicographical_compare(
+      begin(), end(), other.begin(), other.end());
+  }
+
+  //===========================================================================
+  //---------------------------------------------------------------------------
+  // friends
+private:
+
+  template <class _T, class _A>
+  friend _T* relinquish(fbvector<_T, _A>&);
+
+  template <class _T, class _A>
+  friend void attach(fbvector<_T, _A>&, _T* data, size_t sz, size_t cap);
+
+}; // class fbvector
+
+
+//=============================================================================
+//-----------------------------------------------------------------------------
+// outlined functions (gcc, you finicky compiler you)
+
+template <typename T, typename Allocator>
+template <class... Args>
+void fbvector<T, Allocator>::emplace_back_aux(Args&&... args) {
+  size_type byte_sz = folly::goodMallocSize(
+    computePushBackCapacity() * sizeof(T));
+  if (usingStdAllocator::value
+      && rallocm
+      && ((impl_.z_ - impl_.b_) * sizeof(T) >=
+          folly::jemallocMinInPlaceExpandable)) {
+    // Try to reserve in place.
+    // Ask rallocm to allocate in place at least size()+1 and at most sz space.
+    // rallocm will allocate as much as possible within that range, which
+    //  is the best possible outcome: if sz space is available, take it all,
+    //  otherwise take as much as possible. If nothing is available, then fail.
+    // In this fashion, we never relocate if there is a possibility of
+    //  expanding in place, and we never relocate by less than the desired
+    //  amount unless we cannot expand further. Hence we will not relocate
+    //  sub-optimally twice in a row (modulo the blocking memory being freed).
+    size_type lower = folly::goodMallocSize(sizeof(T) + size() * sizeof(T));
+    size_type upper = byte_sz;
+    size_type extra = upper - lower;
+
+    void* p = impl_.b_;
+    size_t actual;
+
+    if (rallocm(&p, &actual, lower, extra, ALLOCM_NO_MOVE)
+        == ALLOCM_SUCCESS) {
+      impl_.z_ = impl_.b_ + actual / sizeof(T);
+      M_construct(impl_.e_, std::forward<Args>(args)...);
+      ++impl_.e_;
+      return;
+    }
+  }
+
+  // Reallocation failed. Perform a manual relocation.
+  size_type sz = byte_sz / sizeof(T);
+  auto newB = M_allocate(sz);
+  auto newE = newB + size();
+  try {
+    if (folly::IsRelocatable<T>::value && usingStdAllocator::value) {
+      // For linear memory access, relocate before construction.
+      // By the test condition, relocate is noexcept.
+      // Note that there is no cleanup to do if M_construct throws - that's
+      //  one of the beauties of relocation.
+      // Benchmarks for this code have high variance, and seem to be close.
+      relocate_move(newB, impl_.b_, impl_.e_);
+      M_construct(newE, std::forward<Args>(args)...);
+      ++newE;
+    } else {
+      M_construct(newE, std::forward<Args>(args)...);
+      ++newE;
+      try {
+        M_relocate(newB);
+      } catch (...) {
+        M_destroy(newE - 1);
+        throw;
+      }
+    }
+  } catch (...) {
+    M_deallocate(newB, sz);
+    throw;
+  }
+  if (impl_.b_) M_deallocate(impl_.b_, size());
+  impl_.b_ = newB;
+  impl_.e_ = newE;
+  impl_.z_ = newB + sz;
+}
+
+//=============================================================================
+//-----------------------------------------------------------------------------
+// specialized functions
+
+template <class T, class A>
+void swap(fbvector<T, A>& lhs, fbvector<T, A>& rhs) noexcept {
+  lhs.swap(rhs);
+}
+
+//=============================================================================
+//-----------------------------------------------------------------------------
+// other
+
+template <class T, class A>
+void compactResize(fbvector<T, A>* v, size_t sz) {
+  v->resize(sz);
+  v->shrink_to_fit();
+}
+
+// DANGER
+//
+// relinquish and attach are not a members function specifically so that it is
+//  awkward to call them. It is very easy to shoot yourself in the foot with
+//  these functions.
+//
+// If you call relinquish, then it is your responsibility to free the data
+//  and the storage, both of which may have been generated in a non-standard
+//  way through the fbvector's allocator.
+//
+// If you call attach, it is your responsibility to ensure that the fbvector
+//  is fresh (size and capacity both zero), and that the supplied data is
+//  capable of being manipulated by the allocator.
+// It is acceptable to supply a stack pointer IF:
+//  (1) The vector's data does not outlive the stack pointer. This includes
+//      extension of the data's life through a move operation.
+//  (2) The pointer has enough capacity that the vector will never be
+//      relocated.
+//  (3) Insert is not called on the vector; these functions have leeway to
+//      relocate the vector even if there is enough capacity.
+//  (4) A stack pointer is compatible with the fbvector's allocator.
+//
+
+template <class T, class A>
+T* relinquish(fbvector<T, A>& v) {
+  T* ret = v.data();
+  v.impl_.b_ = v.impl_.e_ = v.impl_.z_ = nullptr;
+  return ret;
+}
+
+template <class T, class A>
+void attach(fbvector<T, A>& v, T* data, size_t sz, size_t cap) {
+  assert(v.data() == nullptr);
+  v.impl_.b_ = data;
+  v.impl_.e_ = data + sz;
+  v.impl_.z_ = data + cap;
+}
+
+} // namespace folly
+
+#endif // FOLLY_FBVECTOR_H
--- /dev/null
+++ b/hphp/submodules/folly/folly/File.cpp
@@ -0,0 +1,145 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/File.h"
+
+#include <fcntl.h>
+#include <unistd.h>
+
+#include "folly/Exception.h"
+#include "folly/FileUtil.h"
+#include "folly/Format.h"
+#include "folly/ScopeGuard.h"
+
+#include <system_error>
+
+#include <glog/logging.h>
+
+namespace folly {
+
+File::File()
+  : fd_(-1)
+  , ownsFd_(false)
+{}
+
+File::File(int fd, bool ownsFd)
+  : fd_(fd)
+  , ownsFd_(ownsFd) {
+  CHECK_GE(fd, -1) << "fd must be -1 or non-negative";
+  CHECK(fd != -1 || !ownsFd) << "cannot own -1";
+}
+
+File::File(const char* name, int flags, mode_t mode)
+  : fd_(::open(name, flags, mode))
+  , ownsFd_(false) {
+  if (fd_ == -1) {
+    throwSystemError(folly::format("open(\"{}\", {:#o}, 0{:#o}) failed",
+                                   name, flags, mode).fbstr());
+  }
+  ownsFd_ = true;
+}
+
+File::File(File&& other)
+  : fd_(other.fd_)
+  , ownsFd_(other.ownsFd_) {
+  other.release();
+}
+
+File& File::operator=(File&& other) {
+  closeNoThrow();
+  swap(other);
+  return *this;
+}
+
+File::~File() {
+  closeNoThrow();  // ignore error
+}
+
+/* static */ File File::temporary() {
+  // make a temp file with tmpfile(), dup the fd, then return it in a File.
+  FILE* tmpFile = tmpfile();
+  checkFopenError(tmpFile, "tmpfile() failed");
+  SCOPE_EXIT { fclose(tmpFile); };
+
+  int fd = ::dup(fileno(tmpFile));
+  checkUnixError(fd, "dup() failed");
+
+  return File(fd, true);
+}
+
+int File::release() {
+  int released = fd_;
+  fd_ = -1;
+  ownsFd_ = false;
+  return released;
+}
+
+void File::swap(File& other) {
+  using std::swap;
+  swap(fd_, other.fd_);
+  swap(ownsFd_, other.ownsFd_);
+}
+
+void swap(File& a, File& b) {
+  a.swap(b);
+}
+
+File File::dup() const {
+  if (fd_ != -1) {
+    int fd = ::dup(fd_);
+    checkUnixError(fd, "dup() failed");
+
+    return File(fd, true);
+  }
+
+  return File();
+}
+
+void File::close() {
+  if (!closeNoThrow()) {
+    throwSystemError("close() failed");
+  }
+}
+
+bool File::closeNoThrow() {
+  int r = ownsFd_ ? ::close(fd_) : 0;
+  release();
+  return r == 0;
+}
+
+void File::lock() { doLock(LOCK_EX); }
+bool File::try_lock() { return doTryLock(LOCK_EX); }
+void File::lock_shared() { doLock(LOCK_SH); }
+bool File::try_lock_shared() { return doTryLock(LOCK_SH); }
+
+void File::doLock(int op) {
+  checkUnixError(flockNoInt(fd_, op), "flock() failed (lock)");
+}
+
+bool File::doTryLock(int op) {
+  int r = flockNoInt(fd_, op | LOCK_NB);
+  // flock returns EWOULDBLOCK if already locked
+  if (r == -1 && errno == EWOULDBLOCK) return false;
+  checkUnixError(r, "flock() failed (try_lock)");
+  return true;
+}
+
+void File::unlock() {
+  checkUnixError(flockNoInt(fd_, LOCK_UN), "flock() failed (unlock)");
+}
+void File::unlock_shared() { unlock(); }
+
+}  // namespace folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/File.h
@@ -0,0 +1,132 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_FILE_H_
+#define FOLLY_FILE_H_
+
+#include <sys/stat.h>
+#include <sys/types.h>
+#include <fcntl.h>
+#include <unistd.h>
+
+namespace folly {
+
+/**
+ * A File represents an open file.
+ */
+class File {
+ public:
+  /**
+   * Creates an empty File object, for late initialization.
+   */
+  File();
+
+  /**
+   * Create a File object from an existing file descriptor.
+   * Takes ownership of the file descriptor if ownsFd is true.
+   */
+  explicit File(int fd, bool ownsFd = false);
+
+  /**
+   * Open and create a file object.  Throws on error.
+   */
+  explicit File(const char* name, int flags = O_RDONLY, mode_t mode = 0666);
+
+  ~File();
+
+  /**
+   * Create and return a temporary, owned file (uses tmpfile()).
+   */
+  static File temporary();
+
+  /**
+   * Return the file descriptor, or -1 if the file was closed.
+   */
+  int fd() const { return fd_; }
+
+  /**
+   * Returns 'true' iff the file was successfully opened.
+   */
+  explicit operator bool() const {
+    return fd_ != -1;
+  }
+
+  /**
+   * Duplicate file descriptor and return File that owns it.
+   */
+  File dup() const;
+
+  /**
+   * If we own the file descriptor, close the file and throw on error.
+   * Otherwise, do nothing.
+   */
+  void close();
+
+  /**
+   * Closes the file (if owned).  Returns true on success, false (and sets
+   * errno) on error.
+   */
+  bool closeNoThrow();
+
+  /**
+   * Returns and releases the file descriptor; no longer owned by this File.
+   * Returns -1 if the File object didn't wrap a file.
+   */
+  int release();
+
+  /**
+   * Swap this File with another.
+   */
+  void swap(File& other);
+
+  // movable
+  File(File&&);
+  File& operator=(File&&);
+
+  // FLOCK (INTERPROCESS) LOCKS
+  //
+  // NOTE THAT THESE LOCKS ARE flock() LOCKS.  That is, they may only be used
+  // for inter-process synchronization -- an attempt to acquire a second lock
+  // on the same file descriptor from the same process may succeed.  Attempting
+  // to acquire a second lock on a different file descriptor for the same file
+  // should fail, but some systems might implement flock() using fcntl() locks,
+  // in which case it will succeed.
+  void lock();
+  bool try_lock();
+  void unlock();
+
+  void lock_shared();
+  bool try_lock_shared();
+  void unlock_shared();
+
+ private:
+  void doLock(int op);
+  bool doTryLock(int op);
+
+  // unique
+  File(const File&) = delete;
+  File& operator=(const File&) = delete;
+
+  int fd_;
+  bool ownsFd_;
+};
+
+void swap(File& a, File& b);
+
+
+}  // namespace folly
+
+#endif /* FOLLY_FILE_H_ */
--- /dev/null
+++ b/hphp/submodules/folly/folly/FileUtil.cpp
@@ -0,0 +1,152 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/FileUtil.h"
+
+#include <cerrno>
+#ifdef __APPLE__
+#include <fcntl.h>
+#endif
+#include <sys/file.h>
+#include <sys/socket.h>
+
+#include "folly/detail/FileUtilDetail.h"
+
+namespace folly {
+
+using namespace fileutil_detail;
+
+int openNoInt(const char* name, int flags, mode_t mode) {
+  return wrapNoInt(open, name, flags, mode);
+}
+
+int closeNoInt(int fd) {
+  int r = close(fd);
+  // Ignore EINTR.  On Linux, close() may only return EINTR after the file
+  // descriptor has been closed, so you must not retry close() on EINTR --
+  // in the best case, you'll get EBADF, and in the worst case, you'll end up
+  // closing a different file (one opened from another thread).
+  //
+  // Interestingly enough, the Single Unix Specification says that the state
+  // of the file descriptor is unspecified if close returns EINTR.  In that
+  // case, the safe thing to do is also not to retry close() -- leaking a file
+  // descriptor is probably better than closing the wrong file.
+  if (r == -1 && errno == EINTR) {
+    r = 0;
+  }
+  return r;
+}
+
+int fsyncNoInt(int fd) {
+  return wrapNoInt(fsync, fd);
+}
+
+int dupNoInt(int fd) {
+  return wrapNoInt(dup, fd);
+}
+
+int dup2NoInt(int oldfd, int newfd) {
+  return wrapNoInt(dup2, oldfd, newfd);
+}
+
+int fdatasyncNoInt(int fd) {
+#if defined(__APPLE__)
+  return wrapNoInt(fcntl, fd, F_FULLFSYNC);
+#elif defined(__FreeBSD__)
+  return wrapNoInt(fsync, fd);
+#else
+  return wrapNoInt(fdatasync, fd);
+#endif
+}
+
+int ftruncateNoInt(int fd, off_t len) {
+  return wrapNoInt(ftruncate, fd, len);
+}
+
+int truncateNoInt(const char* path, off_t len) {
+  return wrapNoInt(truncate, path, len);
+}
+
+int flockNoInt(int fd, int operation) {
+  return wrapNoInt(flock, fd, operation);
+}
+
+int shutdownNoInt(int fd, int how) {
+  return wrapNoInt(shutdown, fd, how);
+}
+
+ssize_t readNoInt(int fd, void* buf, size_t count) {
+  return wrapNoInt(read, fd, buf, count);
+}
+
+ssize_t preadNoInt(int fd, void* buf, size_t count, off_t offset) {
+  return wrapNoInt(pread, fd, buf, count, offset);
+}
+
+ssize_t readvNoInt(int fd, const iovec* iov, int count) {
+  return wrapNoInt(writev, fd, iov, count);
+}
+
+ssize_t writeNoInt(int fd, const void* buf, size_t count) {
+  return wrapNoInt(write, fd, buf, count);
+}
+
+ssize_t pwriteNoInt(int fd, const void* buf, size_t count, off_t offset) {
+  return wrapNoInt(pwrite, fd, buf, count, offset);
+}
+
+ssize_t writevNoInt(int fd, const iovec* iov, int count) {
+  return wrapNoInt(writev, fd, iov, count);
+}
+
+ssize_t readFull(int fd, void* buf, size_t count) {
+  return wrapFull(read, fd, buf, count);
+}
+
+ssize_t preadFull(int fd, void* buf, size_t count, off_t offset) {
+  return wrapFull(pread, fd, buf, count, offset);
+}
+
+ssize_t writeFull(int fd, const void* buf, size_t count) {
+  return wrapFull(write, fd, const_cast<void*>(buf), count);
+}
+
+ssize_t pwriteFull(int fd, const void* buf, size_t count, off_t offset) {
+  return wrapFull(pwrite, fd, const_cast<void*>(buf), count, offset);
+}
+
+ssize_t readvFull(int fd, iovec* iov, int count) {
+  return wrapvFull(readv, fd, iov, count);
+}
+
+#if FOLLY_HAVE_PREADV
+ssize_t preadvFull(int fd, iovec* iov, int count, off_t offset) {
+  return wrapvFull(preadv, fd, iov, count, offset);
+}
+#endif
+
+ssize_t writevFull(int fd, iovec* iov, int count) {
+  return wrapvFull(writev, fd, iov, count);
+}
+
+#if FOLLY_HAVE_PWRITEV
+ssize_t pwritevFull(int fd, iovec* iov, int count, off_t offset) {
+  return wrapvFull(pwritev, fd, iov, count, offset);
+}
+#endif
+
+}  // namespaces
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/FileUtil.h
@@ -0,0 +1,171 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_FILEUTIL_H_
+#define FOLLY_FILEUTIL_H_
+
+#include "folly/Conv.h"
+#include "folly/Portability.h"
+#include "folly/ScopeGuard.h"
+
+#include <cassert>
+#include <limits>
+#include <sys/stat.h>
+#include <sys/types.h>
+#include <sys/uio.h>
+#include <fcntl.h>
+#include <unistd.h>
+
+namespace folly {
+
+/**
+ * Convenience wrappers around some commonly used system calls.  The *NoInt
+ * wrappers retry on EINTR.  The *Full wrappers retry on EINTR and also loop
+ * until all data is written.  Note that *Full wrappers weaken the thread
+ * semantics of underlying system calls.
+ */
+int openNoInt(const char* name, int flags, mode_t mode = 0666);
+int closeNoInt(int fd);
+int dupNoInt(int fd);
+int dup2NoInt(int oldfd, int newfd);
+int fsyncNoInt(int fd);
+int fdatasyncNoInt(int fd);
+int ftruncateNoInt(int fd, off_t len);
+int truncateNoInt(const char* path, off_t len);
+int flockNoInt(int fd, int operation);
+int shutdownNoInt(int fd, int how);
+
+ssize_t readNoInt(int fd, void* buf, size_t n);
+ssize_t preadNoInt(int fd, void* buf, size_t n, off_t offset);
+ssize_t readvNoInt(int fd, const iovec* iov, int count);
+
+ssize_t writeNoInt(int fd, const void* buf, size_t n);
+ssize_t pwriteNoInt(int fd, const void* buf, size_t n, off_t offset);
+ssize_t writevNoInt(int fd, const iovec* iov, int count);
+
+/**
+ * Wrapper around read() (and pread()) that, in addition to retrying on
+ * EINTR, will loop until all data is read.
+ *
+ * This wrapper is only useful for blocking file descriptors (for non-blocking
+ * file descriptors, you have to be prepared to deal with incomplete reads
+ * anyway), and only exists because POSIX allows read() to return an incomplete
+ * read if interrupted by a signal (instead of returning -1 and setting errno
+ * to EINTR).
+ *
+ * Note that this wrapper weakens the thread safety of read(): the file pointer
+ * is shared between threads, but the system call is atomic.  If multiple
+ * threads are reading from a file at the same time, you don't know where your
+ * data came from in the file, but you do know that the returned bytes were
+ * contiguous.  You can no longer make this assumption if using readFull().
+ * You should probably use pread() when reading from the same file descriptor
+ * from multiple threads simultaneously, anyway.
+ *
+ * Note that readvFull and preadvFull require iov to be non-const, unlike
+ * readv and preadv.  The contents of iov after these functions return
+ * is unspecified.
+ */
+ssize_t readFull(int fd, void* buf, size_t n);
+ssize_t preadFull(int fd, void* buf, size_t n, off_t offset);
+ssize_t readvFull(int fd, iovec* iov, int count);
+#if FOLLY_HAVE_PREADV
+ssize_t preadvFull(int fd, iovec* iov, int count, off_t offset);
+#endif
+
+/**
+ * Similar to readFull and preadFull above, wrappers around write() and
+ * pwrite() that loop until all data is written.
+ *
+ * Generally, the write() / pwrite() system call may always write fewer bytes
+ * than requested, just like read().  In certain cases (such as when writing to
+ * a pipe), POSIX provides stronger guarantees, but not in the general case.
+ * For example, Linux (even on a 64-bit platform) won't write more than 2GB in
+ * one write() system call.
+ *
+ * Note that writevFull and pwritevFull require iov to be non-const, unlike
+ * writev and pwritev.  The contents of iov after these functions return
+ * is unspecified.
+ */
+ssize_t writeFull(int fd, const void* buf, size_t n);
+ssize_t pwriteFull(int fd, const void* buf, size_t n, off_t offset);
+ssize_t writevFull(int fd, iovec* iov, int count);
+#if FOLLY_HAVE_PWRITEV
+ssize_t pwritevFull(int fd, iovec* iov, int count, off_t offset);
+#endif
+
+/**
+ * Read entire file (if num_bytes is defaulted) or no more than
+ * num_bytes (otherwise) into container *out. The container is assumed
+ * to be contiguous, with element size equal to 1, and offer size(),
+ * reserve(), and random access (e.g. std::vector<char>, std::string,
+ * fbstring).
+ *
+ * Returns: true on success or false on failure. In the latter case
+ * errno will be set appropriately by the failing system primitive.
+ */
+template <class Container>
+bool readFile(const char* file_name, Container& out,
+              size_t num_bytes = std::numeric_limits<size_t>::max()) {
+  static_assert(sizeof(out[0]) == 1,
+                "readFile: only containers with byte-sized elements accepted");
+  assert(file_name);
+
+  const auto fd = open(file_name, O_RDONLY);
+  if (fd == -1) return false;
+
+  size_t soFar = 0; // amount of bytes successfully read
+  SCOPE_EXIT {
+    assert(out.size() >= soFar); // resize better doesn't throw
+    out.resize(soFar);
+    // Ignore errors when closing the file
+    close(fd);
+  };
+
+  // Obtain file size:
+  struct stat buf;
+  if (fstat(fd, &buf) == -1) return false;
+  // Some files (notably under /proc and /sys on Linux) lie about
+  // their size, so treat the size advertised by fstat under advise
+  // but don't rely on it. In particular, if the size is zero, we
+  // should attempt to read stuff. If not zero, we'll attempt to read
+  // one extra byte.
+  constexpr size_t initialAlloc = 1024 * 4;
+  out.resize(
+    std::min(
+      buf.st_size > 0 ? folly::to<size_t>(buf.st_size + 1) : initialAlloc,
+      num_bytes));
+
+  while (soFar < out.size()) {
+    const auto actual = readFull(fd, &out[soFar], out.size() - soFar);
+    if (actual == -1) {
+      return false;
+    }
+    soFar += actual;
+    if (soFar < out.size()) {
+      // File exhausted
+      break;
+    }
+    // Ew, allocate more memory. Use exponential growth to avoid
+    // quadratic behavior. Cap size to num_bytes.
+    out.resize(std::min(out.size() * 3 / 2, num_bytes));
+  }
+
+  return true;
+}
+
+}  // namespaces
+
+#endif /* FOLLY_FILEUTIL_H_ */
--- /dev/null
+++ b/hphp/submodules/folly/folly/Fingerprint.h
@@ -0,0 +1,266 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Compute 64-, 96-, and 128-bit Rabin fingerprints, as described in
+ * Michael O. Rabin (1981)
+ *   Fingerprinting by Random Polynomials
+ *   Center for Research in Computing Technology, Harvard University
+ *   Tech Report TR-CSE-03-01
+ *
+ * The implementation follows the optimization described in
+ * Andrei Z. Broder (1993)
+ *   Some applications of Rabin's fingerprinting method
+ *
+ * extended for fingerprints larger than 64 bits, and modified to use
+ * 64-bit instead of 32-bit integers for computation.
+ *
+ * The precomputed tables are in FingerprintTable.cpp, which is automatically
+ * generated by ComputeFingerprintTable.cpp.
+ *
+ * Benchmarked on 10/13/2009 on a 2.5GHz quad-core Xeon L5420,
+ * - Fingerprint<64>::update64() takes about 12ns
+ * - Fingerprint<96>::update64() takes about 30ns
+ * - Fingerprint<128>::update128() takes about 30ns
+ * (unsurprisingly, Fingerprint<96> and Fingerprint<128> take the
+ * same amount of time, as they both use 128-bit operations; the least
+ * significant 32 bits of Fingerprint<96> will always be 0)
+ *
+ * @author Tudor Bosman (tudorb@facebook.com)
+ */
+
+#ifndef FOLLY_FINGERPRINT_H_
+#define FOLLY_FINGERPRINT_H_
+
+#include <cstdint>
+
+#include "folly/Range.h"
+
+namespace folly {
+
+namespace detail {
+template <int BITS>
+struct FingerprintTable {
+  static const uint64_t poly[1 + (BITS-1)/64];
+  static const uint64_t table[8][256][1 + (BITS-1)/64];
+};
+}  // namespace detail
+
+/**
+ * Compute the Rabin fingerprint.
+ *
+ * TODO(tudorb): Extend this to allow removing values from the computed
+ * fingerprint (so we can fingerprint a sliding window, as in the Rabin-Karp
+ * string matching algorithm)
+ *
+ * update* methods return *this, so you can chain them together:
+ * Fingerprint<96>().update8(x).update(str).update64(val).write(output);
+ */
+template <int BITS>
+class Fingerprint {
+ public:
+  Fingerprint() {
+    // Use a non-zero starting value. We'll use (1 << (BITS-1))
+    fp_[0] = 1UL << 63;
+    for (int i = 1; i < size(); i++)
+      fp_[i] = 0;
+  }
+
+  Fingerprint& update8(uint8_t v) {
+    uint8_t out = shlor8(v);
+    xortab(detail::FingerprintTable<BITS>::table[0][out]);
+    return *this;
+  }
+
+  // update32 and update64 are convenience functions to update the fingerprint
+  // with 4 and 8 bytes at a time.  They are faster than calling update8
+  // in a loop.  They process the bytes in big-endian order.
+  Fingerprint& update32(uint32_t v) {
+    uint32_t out = shlor32(v);
+    for (int i = 0; i < 4; i++) {
+      xortab(detail::FingerprintTable<BITS>::table[i][out&0xff]);
+      out >>= 8;
+    }
+    return *this;
+  }
+
+  Fingerprint& update64(uint64_t v) {
+    uint64_t out = shlor64(v);
+    for (int i = 0; i < 8; i++) {
+      xortab(detail::FingerprintTable<BITS>::table[i][out&0xff]);
+      out >>= 8;
+    }
+    return *this;
+  }
+
+  Fingerprint& update(StringPiece str) {
+    // TODO(tudorb): We could be smart and do update64 or update32 if aligned
+    for (auto c : str) {
+      update8(uint8_t(c));
+    }
+    return *this;
+  }
+
+  /**
+   * Return the number of uint64s needed to hold the fingerprint value.
+   */
+  static int size() {
+    return 1 + (BITS-1)/64;
+  }
+
+  /**
+   * Write the computed fingeprint to an array of size() uint64_t's.
+   * For Fingerprint<64>,  size()==1; we write 64 bits in out[0]
+   * For Fingerprint<96>,  size()==2; we write 64 bits in out[0] and
+   *                                  the most significant 32 bits of out[1]
+   * For Fingerprint<128>, size()==2; we write 64 bits in out[0] and
+   *                                  64 bits in out[1].
+   */
+  void write(uint64_t* out) const {
+    for (int i = 0; i < size(); i++) {
+      out[i] = fp_[i];
+    }
+  }
+
+ private:
+  // XOR the fingerprint with a value from one of the tables.
+  void xortab(const uint64_t* tab) {
+    for (int i = 0; i < size(); i++) {
+      fp_[i] ^= tab[i];
+    }
+  }
+
+  // Helper functions: shift the fingerprint value left by 8/32/64 bits,
+  // return the "out" value (the bits that were shifted out), and add "v"
+  // in the bits on the right.
+  uint8_t  shlor8(uint8_t v);
+  uint32_t shlor32(uint32_t v);
+  uint64_t shlor64(uint64_t v);
+
+  uint64_t fp_[1 + (BITS-1)/64];
+};
+
+// Convenience functions
+
+/**
+ * Return the 64-bit Rabin fingerprint of a string.
+ */
+inline uint64_t fingerprint64(StringPiece str) {
+  uint64_t fp;
+  Fingerprint<64>().update(str).write(&fp);
+  return fp;
+}
+
+/**
+ * Compute the 96-bit Rabin fingerprint of a string.
+ * Return the 64 most significant bits in *msb, and the 32 least significant
+ * bits in *lsb.
+ */
+inline void fingerprint96(StringPiece str,
+                          uint64_t* msb, uint32_t* lsb) {
+  uint64_t fp[2];
+  Fingerprint<96>().update(str).write(fp);
+  *msb = fp[0];
+  *lsb = (uint32_t)(fp[1] >> 32);
+}
+
+/**
+ * Compute the 128-bit Rabin fingerprint of a string.
+ * Return the 64 most significant bits in *msb, and the 64 least significant
+ * bits in *lsb.
+ */
+inline void fingerprint128(StringPiece str,
+                           uint64_t* msb, uint64_t* lsb) {
+  uint64_t fp[2];
+  Fingerprint<128>().update(str).write(fp);
+  *msb = fp[0];
+  *lsb = fp[1];
+}
+
+
+template <>
+inline uint8_t Fingerprint<64>::shlor8(uint8_t v) {
+  uint8_t out = (uint8_t)(fp_[0] >> 56);
+  fp_[0] = (fp_[0] << 8) | ((uint64_t)v);
+  return out;
+}
+
+template <>
+inline uint32_t Fingerprint<64>::shlor32(uint32_t v) {
+  uint32_t out = (uint32_t)(fp_[0] >> 32);
+  fp_[0] = (fp_[0] << 32) | ((uint64_t)v);
+  return out;
+}
+
+template <>
+inline uint64_t Fingerprint<64>::shlor64(uint64_t v) {
+  uint64_t out = fp_[0];
+  fp_[0] = v;
+  return out;
+}
+
+template <>
+inline uint8_t Fingerprint<96>::shlor8(uint8_t v) {
+  uint8_t out = (uint8_t)(fp_[0] >> 56);
+  fp_[0] = (fp_[0] << 8) | (fp_[1] >> 56);
+  fp_[1] = (fp_[1] << 8) | ((uint64_t)v << 32);
+  return out;
+}
+
+template <>
+inline uint32_t Fingerprint<96>::shlor32(uint32_t v) {
+  uint32_t out = (uint32_t)(fp_[0] >> 32);
+  fp_[0] = (fp_[0] << 32) | (fp_[1] >> 32);
+  fp_[1] = ((uint64_t)v << 32);
+  return out;
+}
+
+template <>
+inline uint64_t Fingerprint<96>::shlor64(uint64_t v) {
+  uint64_t out = fp_[0];
+  fp_[0] = fp_[1] | (v >> 32);
+  fp_[1] = v << 32;
+  return out;
+}
+
+template <>
+inline uint8_t Fingerprint<128>::shlor8(uint8_t v) {
+  uint8_t out = (uint8_t)(fp_[0] >> 56);
+  fp_[0] = (fp_[0] << 8) | (fp_[1] >> 56);
+  fp_[1] = (fp_[1] << 8) | ((uint64_t)v);
+  return out;
+}
+
+template <>
+inline uint32_t Fingerprint<128>::shlor32(uint32_t v) {
+  uint32_t out = (uint32_t)(fp_[0] >> 32);
+  fp_[0] = (fp_[0] << 32) | (fp_[1] >> 32);
+  fp_[1] = (fp_[1] << 32) | ((uint64_t)v);
+  return out;
+}
+
+template <>
+inline uint64_t Fingerprint<128>::shlor64(uint64_t v) {
+  uint64_t out = fp_[0];
+  fp_[0] = fp_[1];
+  fp_[1] = v;
+  return out;
+}
+
+}  // namespace folly
+
+#endif /* FOLLY_FINGERPRINT_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/Foreach.h
@@ -0,0 +1,231 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_BASE_FOREACH_H_
+#define FOLLY_BASE_FOREACH_H_
+
+/*
+ * Iterim macros (until we have C++0x range-based for) that simplify
+ * writing loops of the form
+ *
+ * for (Container<data>::iterator i = c.begin(); i != c.end(); ++i) statement
+ *
+ * Just replace the above with:
+ *
+ * FOR_EACH (i, c) statement
+ *
+ * and everything is taken care of.
+ *
+ * The implementation is a bit convoluted to make sure the container is
+ * only evaluated once (however, keep in mind that c.end() is evaluated
+ * at every pass through the loop). To ensure the container is not
+ * evaluated multiple times, the macro defines one do-nothing if
+ * statement to inject the Boolean variable FOR_EACH_state1, and then a
+ * for statement that is executed only once, which defines the variable
+ * FOR_EACH_state2 holding a rvalue reference to the container being
+ * iterated. The workhorse is the last loop, which uses the just defined
+ * rvalue reference FOR_EACH_state2.
+ *
+ * The state variables are nested so they don't interfere; you can use
+ * FOR_EACH multiple times in the same scope, either at the same level or
+ * nested.
+ *
+ * In optimized builds g++ eliminates the extra gymnastics entirely and
+ * generates code 100% identical to the handwritten loop.
+ */
+
+#include <boost/type_traits/remove_cv.hpp>
+
+/*
+ * Shorthand for:
+ *   for (auto i = c.begin(); i != c.end(); ++i)
+ * except that c is only evaluated once.
+ */
+#define FOR_EACH(i, c)                              \
+  if (bool FOR_EACH_state1 = false) {} else         \
+    for (auto && FOR_EACH_state2 = (c);             \
+         !FOR_EACH_state1; FOR_EACH_state1 = true)  \
+      for (auto i = FOR_EACH_state2.begin();        \
+           i != FOR_EACH_state2.end(); ++i)
+
+/*
+ * Similar to FOR_EACH, but iterates the container backwards by
+ * using rbegin() and rend().
+ */
+#define FOR_EACH_R(i, c)                                \
+  if (bool FOR_EACH_R_state1 = false) {} else           \
+    for (auto && FOR_EACH_R_state2 = (c);               \
+         !FOR_EACH_R_state1; FOR_EACH_R_state1 = true)  \
+      for (auto i = FOR_EACH_R_state2.rbegin();         \
+           i != FOR_EACH_R_state2.rend(); ++i)
+
+/*
+ * Similar to FOR_EACH but also allows client to specify a 'count' variable
+ * to track the current iteration in the loop (starting at zero).
+ * Similar to python's enumerate() function.  For example:
+ * string commaSeparatedValues = "VALUES: ";
+ * FOR_EACH_ENUMERATE(ii, value, columns) {   // don't want comma at the end!
+ *   commaSeparatedValues += (ii == 0) ? *value : string(",") + *value;
+ * }
+ */
+#define FOR_EACH_ENUMERATE(count, i, c)                                \
+  if (bool FOR_EACH_state1 = false) {} else                            \
+    for (auto && FOR_EACH_state2 = (c);                                \
+         !FOR_EACH_state1; FOR_EACH_state1 = true)                     \
+      if (size_t FOR_EACH_privateCount = 0) {} else                    \
+        if (const size_t& count = FOR_EACH_privateCount) {} else       \
+          for (auto i = FOR_EACH_state2.begin();                       \
+               i != FOR_EACH_state2.end(); ++FOR_EACH_privateCount, ++i)
+
+/**
+ * Similar to FOR_EACH, but gives the user the key and value for each entry in
+ * the container, instead of just the iterator to the entry. For example:
+ *   map<string, string> testMap;
+ *   FOR_EACH_KV(key, value, testMap) {
+ *      cout << key << " " << value;
+ *   }
+ */
+#define FOR_EACH_KV(k, v, c)                                    \
+  if (unsigned int FOR_EACH_state1 = 0) {} else                 \
+    for (auto && FOR_EACH_state2 = (c);                         \
+         !FOR_EACH_state1; FOR_EACH_state1 = 1)                 \
+      for (auto FOR_EACH_state3 = FOR_EACH_state2.begin();      \
+           FOR_EACH_state3 != FOR_EACH_state2.end();            \
+           FOR_EACH_state1 == 2                                 \
+             ? ((FOR_EACH_state1 = 0), ++FOR_EACH_state3)       \
+             : (FOR_EACH_state3 = FOR_EACH_state2.end()))       \
+        for (auto &k = FOR_EACH_state3->first;                  \
+             !FOR_EACH_state1; ++FOR_EACH_state1)               \
+          for (auto &v = FOR_EACH_state3->second;               \
+               !FOR_EACH_state1; ++FOR_EACH_state1)
+
+namespace folly { namespace detail {
+
+// Boost 1.48 lacks has_less, we emulate a subset of it here.
+template <typename T, typename U>
+class HasLess {
+  struct BiggerThanChar { char unused[2]; };
+  template <typename C, typename D> static char test(decltype(C() < D())*);
+  template <typename, typename> static BiggerThanChar test(...);
+public:
+  enum { value = sizeof(test<T, U>(0)) == 1 };
+};
+
+/**
+ * notThereYet helps the FOR_EACH_RANGE macro by opportunistically
+ * using "<" instead of "!=" whenever available when checking for loop
+ * termination. This makes e.g. examples such as FOR_EACH_RANGE (i,
+ * 10, 5) execute zero iterations instead of looping virtually
+ * forever. At the same time, some iterator types define "!=" but not
+ * "<". The notThereYet function will dispatch differently for those.
+ *
+ * Below is the correct implementation of notThereYet. It is disabled
+ * because of a bug in Boost 1.46: The filesystem::path::iterator
+ * defines operator< (via boost::iterator_facade), but that in turn
+ * uses distance_to which is undefined for that particular
+ * iterator. So HasLess (defined above) identifies
+ * boost::filesystem::path as properly comparable with <, but in fact
+ * attempting to do so will yield a compile-time error.
+ *
+ * The else branch (active) contains a conservative
+ * implementation.
+ */
+
+#if 0
+
+template <class T, class U>
+typename std::enable_if<HasLess<T, U>::value, bool>::type
+notThereYet(T& iter, const U& end) {
+  return iter < end;
+}
+
+template <class T, class U>
+typename std::enable_if<!HasLess<T, U>::value, bool>::type
+notThereYet(T& iter, const U& end) {
+  return iter != end;
+}
+
+#else
+
+template <class T, class U>
+typename std::enable_if<
+  (std::is_arithmetic<T>::value && std::is_arithmetic<U>::value) ||
+  (std::is_pointer<T>::value && std::is_pointer<U>::value),
+  bool>::type
+notThereYet(T& iter, const U& end) {
+  return iter < end;
+}
+
+template <class T, class U>
+typename std::enable_if<
+  !(
+    (std::is_arithmetic<T>::value && std::is_arithmetic<U>::value) ||
+    (std::is_pointer<T>::value && std::is_pointer<U>::value)
+  ),
+  bool>::type
+notThereYet(T& iter, const U& end) {
+  return iter != end;
+}
+
+#endif
+
+
+/**
+ * downTo is similar to notThereYet, but in reverse - it helps the
+ * FOR_EACH_RANGE_R macro.
+ */
+template <class T, class U>
+typename std::enable_if<HasLess<U, T>::value, bool>::type
+downTo(T& iter, const U& begin) {
+  return begin < iter--;
+}
+
+template <class T, class U>
+typename std::enable_if<!HasLess<U, T>::value, bool>::type
+downTo(T& iter, const U& begin) {
+  if (iter == begin) return false;
+  --iter;
+  return true;
+}
+
+} }
+
+/*
+ * Iteration with given limits. end is assumed to be reachable from
+ * begin. end is evaluated every pass through the loop.
+ *
+ * NOTE: The type of the loop variable should be the common type of "begin"
+ *       and "end". e.g. If "begin" is "int" but "end" is "long", we want "i"
+ *       to be "long". This is done by getting the type of (true ? begin : end)
+ */
+#define FOR_EACH_RANGE(i, begin, end)           \
+  for (auto i = (true ? (begin) : (end));       \
+       ::folly::detail::notThereYet(i, (end));  \
+       ++i)
+
+/*
+ * Iteration with given limits. begin is assumed to be reachable from
+ * end by successive decrements. begin is evaluated every pass through
+ * the loop.
+ *
+ * NOTE: The type of the loop variable should be the common type of "begin"
+ *       and "end". e.g. If "begin" is "int" but "end" is "long", we want "i"
+ *       to be "long". This is done by getting the type of (false ? begin : end)
+ */
+#define FOR_EACH_RANGE_R(i, begin, end) \
+  for (auto i = (false ? (begin) : (end)); ::folly::detail::downTo(i, (begin));)
+
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/FormatArg.h
@@ -0,0 +1,270 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_FORMATARG_H_
+#define FOLLY_FORMATARG_H_
+
+#include <stdexcept>
+#include "folly/Conv.h"
+#include "folly/Likely.h"
+#include "folly/Portability.h"
+#include "folly/Range.h"
+
+namespace folly {
+
+class BadFormatArg : public std::invalid_argument {
+ public:
+  explicit BadFormatArg(const std::string& msg)
+    : std::invalid_argument(msg) {}
+};
+
+/**
+ * Parsed format argument.
+ */
+struct FormatArg {
+  /**
+   * Parse a format argument from a string.  Keeps a reference to the
+   * passed-in string -- does not copy the given characters.
+   */
+  explicit FormatArg(StringPiece sp)
+    : fullArgString(sp),
+      fill(kDefaultFill),
+      align(Align::DEFAULT),
+      sign(Sign::DEFAULT),
+      basePrefix(false),
+      thousandsSeparator(false),
+      width(kDefaultWidth),
+      precision(kDefaultPrecision),
+      presentation(kDefaultPresentation),
+      nextKeyMode_(NextKeyMode::NONE) {
+    if (!sp.empty()) {
+      initSlow();
+    }
+  }
+
+  enum class Type {
+    INTEGER,
+    FLOAT,
+    OTHER
+  };
+  /**
+   * Validate the argument for the given type; throws on error.
+   */
+  void validate(Type type) const;
+
+  /**
+   * Throw an exception if the first argument is false.  The exception
+   * message will contain the argument string as well as any passed-in
+   * arguments to enforce, formatted using folly::to<std::string>.
+   */
+  template <typename... Args>
+  void enforce(bool v, Args&&... args) const {
+    if (UNLIKELY(!v)) {
+      error(std::forward<Args>(args)...);
+    }
+  }
+
+  template <typename... Args>
+  std::string errorStr(Args&&... args) const;
+  template <typename... Args>
+  void error(Args&&... args) const FOLLY_NORETURN;
+
+  /**
+   * Full argument string, as passed in to the constructor.
+   */
+  StringPiece fullArgString;
+
+  /**
+   * Fill
+   */
+  static constexpr char kDefaultFill = '\0';
+  char fill;
+
+  /**
+   * Alignment
+   */
+  enum class Align : uint8_t {
+    DEFAULT,
+    LEFT,
+    RIGHT,
+    PAD_AFTER_SIGN,
+    CENTER,
+    INVALID
+  };
+  Align align;
+
+  /**
+   * Sign
+   */
+  enum class Sign : uint8_t {
+    DEFAULT,
+    PLUS_OR_MINUS,
+    MINUS,
+    SPACE_OR_MINUS,
+    INVALID
+  };
+  Sign sign;
+
+  /**
+   * Output base prefix (0 for octal, 0x for hex)
+   */
+  bool basePrefix;
+
+  /**
+   * Output thousands separator (comma)
+   */
+  bool thousandsSeparator;
+
+  /**
+   * Field width
+   */
+  static constexpr int kDefaultWidth = -1;
+  int width;
+
+  /**
+   * Precision
+   */
+  static constexpr int kDefaultPrecision = -1;
+  int precision;
+
+  /**
+   * Presentation
+   */
+  static constexpr char kDefaultPresentation = '\0';
+  char presentation;
+
+  /**
+   * Split a key component from "key", which must be non-empty (an exception
+   * is thrown otherwise).
+   */
+  template <bool emptyOk=false>
+  StringPiece splitKey();
+
+  /**
+   * Is the entire key empty?
+   */
+  bool keyEmpty() const {
+    return nextKeyMode_ == NextKeyMode::NONE && key_.empty();
+  }
+
+  /**
+   * Split an key component from "key", which must be non-empty and a valid
+   * integer (an exception is thrown otherwise).
+   */
+  int splitIntKey();
+
+  void setNextIntKey(int val) {
+    assert(nextKeyMode_ == NextKeyMode::NONE);
+    nextKeyMode_ = NextKeyMode::INT;
+    nextIntKey_ = val;
+  }
+
+  void setNextKey(StringPiece val) {
+    assert(nextKeyMode_ == NextKeyMode::NONE);
+    nextKeyMode_ = NextKeyMode::STRING;
+    nextKey_ = val;
+  }
+
+ private:
+  void initSlow();
+  template <bool emptyOk>
+  StringPiece doSplitKey();
+
+  StringPiece key_;
+  int nextIntKey_;
+  StringPiece nextKey_;
+  enum class NextKeyMode {
+    NONE,
+    INT,
+    STRING,
+  };
+  NextKeyMode nextKeyMode_;
+};
+
+template <typename... Args>
+inline std::string FormatArg::errorStr(Args&&... args) const {
+  return to<std::string>(
+    "invalid format argument {", fullArgString, "}: ",
+    std::forward<Args>(args)...);
+}
+
+template <typename... Args>
+inline void FormatArg::error(Args&&... args) const {
+  throw BadFormatArg(errorStr(std::forward<Args>(args)...));
+}
+
+template <bool emptyOk>
+inline StringPiece FormatArg::splitKey() {
+  enforce(nextKeyMode_ != NextKeyMode::INT, "integer key expected");
+  return doSplitKey<emptyOk>();
+}
+
+template <bool emptyOk>
+inline StringPiece FormatArg::doSplitKey() {
+  if (nextKeyMode_ == NextKeyMode::STRING) {
+    nextKeyMode_ = NextKeyMode::NONE;
+    if (!emptyOk) {  // static
+      enforce(!nextKey_.empty(), "non-empty key required");
+    }
+    return nextKey_;
+  }
+
+  if (key_.empty()) {
+    if (!emptyOk) {  // static
+      error("non-empty key required");
+    }
+    return StringPiece();
+  }
+
+  const char* b = key_.begin();
+  const char* e = key_.end();
+  const char* p;
+  if (e[-1] == ']') {
+    --e;
+    p = static_cast<const char*>(memchr(b, '[', e - b));
+    enforce(p, "unmatched ']'");
+  } else {
+    p = static_cast<const char*>(memchr(b, '.', e - b));
+  }
+  if (p) {
+    key_.assign(p + 1, e);
+  } else {
+    p = e;
+    key_.clear();
+  }
+  if (!emptyOk) {  // static
+    enforce(b != p, "non-empty key required");
+  }
+  return StringPiece(b, p);
+}
+
+inline int FormatArg::splitIntKey() {
+  if (nextKeyMode_ == NextKeyMode::INT) {
+    nextKeyMode_ = NextKeyMode::NONE;
+    return nextIntKey_;
+  }
+  try {
+    return to<int>(doSplitKey<true>());
+  } catch (const std::out_of_range& e) {
+    error("integer key required");
+    return 0;  // unreached
+  }
+}
+
+}  // namespace folly
+
+#endif /* FOLLY_FORMATARG_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/Format.cpp
@@ -0,0 +1,137 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Format.h"
+
+namespace folly {
+namespace detail {
+
+extern const FormatArg::Align formatAlignTable[];
+extern const FormatArg::Sign formatSignTable[];
+
+}  // namespace detail
+
+using namespace folly::detail;
+
+void FormatArg::initSlow() {
+  auto b = fullArgString.begin();
+  auto end = fullArgString.end();
+
+  // Parse key
+  auto p = static_cast<const char*>(memchr(b, ':', end - b));
+  if (!p) {
+    key_ = StringPiece(b, end);
+    return;
+  }
+  key_ = StringPiece(b, p);
+
+  if (*p == ':') {
+    // parse format spec
+    if (++p == end) return;
+
+    // fill/align, or just align
+    Align a;
+    if (p + 1 != end &&
+        (a = formatAlignTable[static_cast<unsigned char>(p[1])]) !=
+        Align::INVALID) {
+      fill = *p;
+      align = a;
+      p += 2;
+      if (p == end) return;
+    } else if ((a = formatAlignTable[static_cast<unsigned char>(*p)]) !=
+               Align::INVALID) {
+      align = a;
+      if (++p == end) return;
+    }
+
+    Sign s;
+    unsigned char uSign = static_cast<unsigned char>(*p);
+    if ((s = formatSignTable[uSign]) != Sign::INVALID) {
+      sign = s;
+      if (++p == end) return;
+    }
+
+    if (*p == '#') {
+      basePrefix = true;
+      if (++p == end) return;
+    }
+
+    if (*p == '0') {
+      enforce(align == Align::DEFAULT, "alignment specified twice");
+      fill = '0';
+      align = Align::PAD_AFTER_SIGN;
+      if (++p == end) return;
+    }
+
+    if (*p >= '0' && *p <= '9') {
+      auto b = p;
+      do {
+        ++p;
+      } while (p != end && *p >= '0' && *p <= '9');
+      width = to<int>(StringPiece(b, p));
+
+      if (p == end) return;
+    }
+
+    if (*p == ',') {
+      thousandsSeparator = true;
+      if (++p == end) return;
+    }
+
+    if (*p == '.') {
+      auto b = ++p;
+      while (p != end && *p >= '0' && *p <= '9') {
+        ++p;
+      }
+      precision = to<int>(StringPiece(b, p));
+
+      if (p == end) return;
+    }
+
+    presentation = *p;
+    if (++p == end) return;
+  }
+
+  error("extra characters in format string");
+}
+
+void FormatArg::validate(Type type) const {
+  enforce(keyEmpty(), "index not allowed");
+  switch (type) {
+  case Type::INTEGER:
+    enforce(precision == kDefaultPrecision,
+            "precision not allowed on integers");
+    break;
+  case Type::FLOAT:
+    enforce(!basePrefix,
+            "base prefix ('#') specifier only allowed on integers");
+    enforce(!thousandsSeparator,
+            "thousands separator (',') only allowed on integers");
+    break;
+  case Type::OTHER:
+    enforce(align != Align::PAD_AFTER_SIGN,
+            "'='alignment only allowed on numbers");
+    enforce(sign == Sign::DEFAULT,
+            "sign specifier only allowed on numbers");
+    enforce(!basePrefix,
+            "base prefix ('#') specifier only allowed on integers");
+    enforce(!thousandsSeparator,
+            "thousands separator (',') only allowed on integers");
+    break;
+  }
+}
+
+}  // namespace folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/Format.h
@@ -0,0 +1,361 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_FORMAT_H_
+#define FOLLY_FORMAT_H_
+
+#include <array>
+#include <cstdio>
+#include <tuple>
+#include <type_traits>
+#include <vector>
+#include <deque>
+#include <map>
+#include <unordered_map>
+
+#include <double-conversion/double-conversion.h>
+
+#include "folly/FBVector.h"
+#include "folly/Conv.h"
+#include "folly/Range.h"
+#include "folly/Traits.h"
+#include "folly/Likely.h"
+#include "folly/String.h"
+#include "folly/small_vector.h"
+#include "folly/FormatArg.h"
+
+// Ignore shadowing warnings within this file, so includers can use -Wshadow.
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wshadow"
+
+namespace folly {
+
+// forward declarations
+template <bool containerMode, class... Args> class Formatter;
+template <class... Args>
+Formatter<false, Args...> format(StringPiece fmt, Args&&... args);
+template <class C>
+Formatter<true, C> vformat(StringPiece fmt, C&& container);
+template <class T, class Enable=void> class FormatValue;
+
+/**
+ * Formatter class.
+ *
+ * Note that this class is tricky, as it keeps *references* to its arguments
+ * (and doesn't copy the passed-in format string).  Thankfully, you can't use
+ * this directly, you have to use format(...) below.
+ */
+
+template <bool containerMode, class... Args>
+class Formatter {
+ public:
+  /*
+   * Change whether or not Formatter should crash or throw exceptions if the
+   * format string is invalid.
+   *
+   * Crashing is desirable for literal format strings that are fixed at compile
+   * time.  Errors in the format string are generally programmer bugs, and
+   * should be caught early on in development.  Crashing helps ensure these
+   * problems are noticed.
+   */
+  void setCrashOnError(bool crash) {
+    crashOnError_ = crash;
+  }
+
+  /**
+   * Append to output.  out(StringPiece sp) may be called (more than once)
+   */
+  template <class Output>
+  void operator()(Output& out) const;
+
+  /**
+   * Append to a string.
+   */
+  template <class Str>
+  typename std::enable_if<IsSomeString<Str>::value>::type
+  appendTo(Str& str) const {
+    auto appender = [&str] (StringPiece s) { str.append(s.data(), s.size()); };
+    (*this)(appender);
+  }
+
+  /**
+   * Conversion to string
+   */
+  std::string str() const {
+    std::string s;
+    appendTo(s);
+    return s;
+  }
+
+  /**
+   * Conversion to fbstring
+   */
+  fbstring fbstr() const {
+    fbstring s;
+    appendTo(s);
+    return s;
+  }
+
+ private:
+  explicit Formatter(StringPiece str, Args&&... args);
+
+  // Not copyable
+  Formatter(const Formatter&) = delete;
+  Formatter& operator=(const Formatter&) = delete;
+
+  // Movable, but the move constructor and assignment operator are private,
+  // for the exclusive use of format() (below).  This way, you can't create
+  // a Formatter object, but can handle references to it (for streaming,
+  // conversion to string, etc) -- which is good, as Formatter objects are
+  // dangerous (they hold references, possibly to temporaries)
+  Formatter(Formatter&&) = default;
+  Formatter& operator=(Formatter&&) = default;
+
+  typedef std::tuple<FormatValue<
+      typename std::decay<Args>::type>...> ValueTuple;
+  static constexpr size_t valueCount = std::tuple_size<ValueTuple>::value;
+
+  void handleFormatStrError() const FOLLY_NORETURN;
+  template <class Output>
+  void appendOutput(Output& out) const;
+
+  template <size_t K, class Callback>
+  typename std::enable_if<K == valueCount>::type
+  doFormatFrom(size_t i, FormatArg& arg, Callback& cb) const {
+    arg.error("argument index out of range, max=", i);
+  }
+
+  template <size_t K, class Callback>
+  typename std::enable_if<(K < valueCount)>::type
+  doFormatFrom(size_t i, FormatArg& arg, Callback& cb) const {
+    if (i == K) {
+      std::get<K>(values_).format(arg, cb);
+    } else {
+      doFormatFrom<K+1>(i, arg, cb);
+    }
+  }
+
+  template <class Callback>
+  void doFormat(size_t i, FormatArg& arg, Callback& cb) const {
+    return doFormatFrom<0>(i, arg, cb);
+  }
+
+  StringPiece str_;
+  ValueTuple values_;
+  bool crashOnError_{true};
+
+  template <class... A>
+  friend Formatter<false, A...> format(StringPiece fmt, A&&... arg);
+  template <class... A>
+  friend Formatter<false, A...> formatChecked(StringPiece fmt, A&&... arg);
+  template <class C>
+  friend Formatter<true, C> vformat(StringPiece fmt, C&& container);
+  template <class C>
+  friend Formatter<true, C> vformatChecked(StringPiece fmt, C&& container);
+};
+
+/**
+ * Formatter objects can be written to streams.
+ */
+template<bool containerMode, class... Args>
+std::ostream& operator<<(std::ostream& out,
+                         const Formatter<containerMode, Args...>& formatter) {
+  auto writer = [&out] (StringPiece sp) { out.write(sp.data(), sp.size()); };
+  formatter(writer);
+  return out;
+}
+
+/**
+ * Formatter objects can be written to stdio FILEs.
+ */
+template<bool containerMode, class... Args>
+void writeTo(FILE* fp, const Formatter<containerMode, Args...>& formatter);
+
+/**
+ * Create a formatter object.
+ *
+ * std::string formatted = format("{} {}", 23, 42).str();
+ * LOG(INFO) << format("{} {}", 23, 42);
+ * writeTo(stdout, format("{} {}", 23, 42));
+ *
+ * Note that format() will crash the program if the format string is invalid.
+ * Normally, the format string is a fixed string literal specified by the
+ * programmer.  Invalid format strings are normally programmer bugs, and should
+ * be caught early on during development.  Crashing helps ensure these bugs are
+ * found.
+ *
+ * Use formatChecked() if you have a dynamic format string (for example, a user
+ * supplied value).  formatChecked() will throw an exception rather than
+ * crashing the program.
+ */
+template <class... Args>
+Formatter<false, Args...> format(StringPiece fmt, Args&&... args) {
+  return Formatter<false, Args...>(
+      fmt, std::forward<Args>(args)...);
+}
+
+/**
+ * Create a formatter object from a dynamic format string.
+ *
+ * This is identical to format(), but throws an exception if the format string
+ * is invalid, rather than aborting the program.  This allows it to be used
+ * with user-specified format strings which are not guaranteed to be well
+ * formed.
+ */
+template <class... Args>
+Formatter<false, Args...> formatChecked(StringPiece fmt, Args&&... args) {
+  Formatter<false, Args...> f(fmt, std::forward<Args>(args)...);
+  f.setCrashOnError(false);
+  return f;
+}
+
+/**
+ * Create a formatter object that takes one argument (of container type)
+ * and uses that container to get argument values from.
+ *
+ * std::map<string, string> map { {"hello", "world"}, {"answer", "42"} };
+ *
+ * The following are equivalent:
+ * format("{0[hello]} {0[answer]}", map);
+ *
+ * vformat("{hello} {answer}", map);
+ *
+ * but the latter is cleaner.
+ */
+template <class Container>
+Formatter<true, Container> vformat(StringPiece fmt, Container&& container) {
+  return Formatter<true, Container>(
+      fmt, std::forward<Container>(container));
+}
+
+/**
+ * Create a formatter object from a dynamic format string.
+ *
+ * This is identical to vformat(), but throws an exception if the format string
+ * is invalid, rather than aborting the program.  This allows it to be used
+ * with user-specified format strings which are not guaranteed to be well
+ * formed.
+ */
+template <class Container>
+Formatter<true, Container> vformatChecked(StringPiece fmt,
+                                          Container&& container) {
+  Formatter<true, Container> f(fmt, std::forward<Container>(container));
+  f.setCrashOnError(false);
+  return f;
+}
+
+/**
+ * Append formatted output to a string.
+ *
+ * std::string foo;
+ * format(&foo, "{} {}", 42, 23);
+ *
+ * Shortcut for toAppend(format(...), &foo);
+ */
+template <class Str, class... Args>
+typename std::enable_if<IsSomeString<Str>::value>::type
+format(Str* out, StringPiece fmt, Args&&... args) {
+  format(fmt, std::forward<Args>(args)...).appendTo(*out);
+}
+
+template <class Str, class... Args>
+typename std::enable_if<IsSomeString<Str>::value>::type
+formatChecked(Str* out, StringPiece fmt, Args&&... args) {
+  formatChecked(fmt, std::forward<Args>(args)...).appendTo(*out);
+}
+
+/**
+ * Append vformatted output to a string.
+ */
+template <class Str, class Container>
+typename std::enable_if<IsSomeString<Str>::value>::type
+vformat(Str* out, StringPiece fmt, Container&& container) {
+  vformat(fmt, std::forward<Container>(container)).appendTo(*out);
+}
+
+template <class Str, class Container>
+typename std::enable_if<IsSomeString<Str>::value>::type
+vformatChecked(Str* out, StringPiece fmt, Container&& container) {
+  vformatChecked(fmt, std::forward<Container>(container)).appendTo(*out);
+}
+
+/**
+ * Utilities for all format value specializations.
+ */
+namespace format_value {
+
+/**
+ * Format a string in "val", obeying appropriate alignment, padding, width,
+ * and precision.  Treats Align::DEFAULT as Align::LEFT, and
+ * Align::PAD_AFTER_SIGN as Align::RIGHT; use formatNumber for
+ * number-specific formatting.
+ */
+template <class FormatCallback>
+void formatString(StringPiece val, FormatArg& arg, FormatCallback& cb);
+
+/**
+ * Format a number in "val"; the first prefixLen characters form the prefix
+ * (sign, "0x" base prefix, etc) which must be left-aligned if the alignment
+ * is Align::PAD_AFTER_SIGN.  Treats Align::DEFAULT as Align::LEFT.  Ignores
+ * arg.precision, as that has a different meaning for numbers (not "maximum
+ * field width")
+ */
+template <class FormatCallback>
+void formatNumber(StringPiece val, int prefixLen, FormatArg& arg,
+                  FormatCallback& cb);
+
+
+/**
+ * Format a Formatter object recursively.  Behaves just like
+ * formatString(fmt.str(), arg, cb); but avoids creating a temporary
+ * string if possible.
+ */
+template <class FormatCallback, bool containerMode, class... Args>
+void formatFormatter(const Formatter<containerMode, Args...>& formatter,
+                     FormatArg& arg,
+                     FormatCallback& cb);
+
+}  // namespace format_value
+
+/*
+ * Specialize folly::FormatValue for your type.
+ *
+ * FormatValue<T> is constructed with a (reference-collapsed) T&&, which is
+ * guaranteed to stay alive until the FormatValue object is destroyed, so you
+ * may keep a reference (or pointer) to it instead of making a copy.
+ *
+ * You must define
+ *   template <class Callback>
+ *   void format(FormatArg& arg, Callback& cb) const;
+ * with the following semantics: format the value using the given argument.
+ *
+ * arg is given by non-const reference for convenience -- it won't be reused,
+ * so feel free to modify it in place if necessary.  (For example, wrap an
+ * existing conversion but change the default, or remove the "key" when
+ * extracting an element from a container)
+ *
+ * Call the callback to append data to the output.  You may call the callback
+ * as many times as you'd like (or not at all, if you want to output an
+ * empty string)
+ */
+
+}  // namespace folly
+
+#include "folly/Format-inl.h"
+
+#pragma GCC diagnostic pop
+
+#endif /* FOLLY_FORMAT_H_ */
--- /dev/null
+++ b/hphp/submodules/folly/folly/Format-inl.h
@@ -0,0 +1,1169 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_FORMAT_H_
+#error This file may only be included from Format.h.
+#endif
+
+#include "folly/Exception.h"
+#include "folly/Traits.h"
+
+namespace folly {
+
+namespace detail {
+
+extern const char formatHexUpper[256][2];
+extern const char formatHexLower[256][2];
+extern const char formatOctal[512][3];
+extern const char formatBinary[256][8];
+
+const size_t kMaxHexLength = 2 * sizeof(uintmax_t);
+const size_t kMaxOctalLength = 3 * sizeof(uintmax_t);
+const size_t kMaxBinaryLength = 8 * sizeof(uintmax_t);
+
+/**
+ * Convert an unsigned to hex, using repr (which maps from each possible
+ * 2-hex-bytes value to the 2-character representation).
+ *
+ * Just like folly::detail::uintToBuffer in Conv.h, writes at the *end* of
+ * the supplied buffer and returns the offset of the beginning of the string
+ * from the start of the buffer.  The formatted string will be in range
+ * [buf+begin, buf+bufLen).
+ */
+template <class Uint>
+size_t uintToHex(char* buffer, size_t bufLen, Uint v,
+                 const char (&repr)[256][2]) {
+  // 'v >>= 7, v >>= 1' is no more than a work around to get rid of shift size
+  // warning when Uint = uint8_t (it's false as v >= 256 implies sizeof(v) > 1).
+  for (; !less_than<unsigned, 256>(v); v >>= 7, v >>= 1) {
+    auto b = v & 0xff;
+    bufLen -= 2;
+    buffer[bufLen] = repr[b][0];
+    buffer[bufLen + 1] = repr[b][1];
+  }
+  buffer[--bufLen] = repr[v][1];
+  if (v >= 16) {
+    buffer[--bufLen] = repr[v][0];
+  }
+  return bufLen;
+}
+
+/**
+ * Convert an unsigned to hex, using lower-case letters for the digits
+ * above 9.  See the comments for uintToHex.
+ */
+template <class Uint>
+inline size_t uintToHexLower(char* buffer, size_t bufLen, Uint v) {
+  return uintToHex(buffer, bufLen, v, formatHexLower);
+}
+
+/**
+ * Convert an unsigned to hex, using upper-case letters for the digits
+ * above 9.  See the comments for uintToHex.
+ */
+template <class Uint>
+inline size_t uintToHexUpper(char* buffer, size_t bufLen, Uint v) {
+  return uintToHex(buffer, bufLen, v, formatHexUpper);
+}
+
+/**
+ * Convert an unsigned to octal.
+ *
+ * Just like folly::detail::uintToBuffer in Conv.h, writes at the *end* of
+ * the supplied buffer and returns the offset of the beginning of the string
+ * from the start of the buffer.  The formatted string will be in range
+ * [buf+begin, buf+bufLen).
+ */
+template <class Uint>
+size_t uintToOctal(char* buffer, size_t bufLen, Uint v) {
+  auto& repr = formatOctal;
+  // 'v >>= 7, v >>= 2' is no more than a work around to get rid of shift size
+  // warning when Uint = uint8_t (it's false as v >= 512 implies sizeof(v) > 1).
+  for (; !less_than<unsigned, 512>(v); v >>= 7, v >>= 2) {
+    auto b = v & 0x1ff;
+    bufLen -= 3;
+    buffer[bufLen] = repr[b][0];
+    buffer[bufLen + 1] = repr[b][1];
+    buffer[bufLen + 2] = repr[b][2];
+  }
+  buffer[--bufLen] = repr[v][2];
+  if (v >= 8) {
+    buffer[--bufLen] = repr[v][1];
+  }
+  if (v >= 64) {
+    buffer[--bufLen] = repr[v][0];
+  }
+  return bufLen;
+}
+
+/**
+ * Convert an unsigned to binary.
+ *
+ * Just like folly::detail::uintToBuffer in Conv.h, writes at the *end* of
+ * the supplied buffer and returns the offset of the beginning of the string
+ * from the start of the buffer.  The formatted string will be in range
+ * [buf+begin, buf+bufLen).
+ */
+template <class Uint>
+size_t uintToBinary(char* buffer, size_t bufLen, Uint v) {
+  auto& repr = formatBinary;
+  if (v == 0) {
+    buffer[--bufLen] = '0';
+    return bufLen;
+  }
+  for (; v; v >>= 7, v >>= 1) {
+    auto b = v & 0xff;
+    bufLen -= 8;
+    memcpy(buffer + bufLen, &(repr[b][0]), 8);
+  }
+  while (buffer[bufLen] == '0') {
+    ++bufLen;
+  }
+  return bufLen;
+}
+
+}  // namespace detail
+
+
+template <bool containerMode, class... Args>
+Formatter<containerMode, Args...>::Formatter(StringPiece str, Args&&... args)
+  : str_(str),
+    values_(FormatValue<typename std::decay<Args>::type>(
+        std::forward<Args>(args))...) {
+  static_assert(!containerMode || sizeof...(Args) == 1,
+                "Exactly one argument required in container mode");
+}
+
+template <bool containerMode, class... Args>
+void Formatter<containerMode, Args...>::handleFormatStrError() const {
+  if (crashOnError_) {
+    LOG(FATAL) << "folly::format: bad format string \"" << str_ << "\": " <<
+      folly::exceptionStr(std::current_exception());
+  }
+  throw;
+}
+
+template <bool containerMode, class... Args>
+template <class Output>
+void Formatter<containerMode, Args...>::operator()(Output& out) const {
+  // Catch BadFormatArg and range_error exceptions, and call
+  // handleFormatStrError().
+  //
+  // These exception types indicate a problem with the format string.  Most
+  // format strings are string literals specified by the programmer.  If they
+  // have a problem, this is usually a programmer bug.  We want to crash to
+  // ensure that these are found early on during development.
+  //
+  // BadFormatArg is thrown by the Format.h code, while range_error is thrown
+  // by Conv.h, which is used in several places in our format string
+  // processing.
+  //
+  // (Note: This behavior is slightly dangerous.  If the Output object throws a
+  // BadFormatArg or a range_error, we will also crash the program, even if it
+  // wasn't an issue with the format string.  This seems highly unlikely
+  // though, and none of our current Output objects can throw these errors.)
+  //
+  // We also throw out_of_range errors if the format string references an
+  // argument that isn't present (or a key that isn't present in one of the
+  // argument containers).  However, at the moment we don't crash on these
+  // errors, as it is likely that the container is dynamic at runtime.
+  try {
+    appendOutput(out);
+  } catch (const BadFormatArg& ex) {
+    handleFormatStrError();
+  } catch (const std::range_error& ex) {
+    handleFormatStrError();
+  }
+}
+
+template <bool containerMode, class... Args>
+template <class Output>
+void Formatter<containerMode, Args...>::appendOutput(Output& out) const {
+  auto p = str_.begin();
+  auto end = str_.end();
+
+  // Copy raw string (without format specifiers) to output;
+  // not as simple as we'd like, as we still need to translate "}}" to "}"
+  // and throw if we see any lone "}"
+  auto outputString = [&out] (StringPiece s) {
+    auto p = s.begin();
+    auto end = s.end();
+    while (p != end) {
+      auto q = static_cast<const char*>(memchr(p, '}', end - p));
+      if (!q) {
+        out(StringPiece(p, end));
+        break;
+      }
+      ++q;
+      out(StringPiece(p, q));
+      p = q;
+
+      if (p == end || *p != '}') {
+        throw BadFormatArg("folly::format: single '}' in format string");
+      }
+      ++p;
+    }
+  };
+
+  int nextArg = 0;
+  bool hasDefaultArgIndex = false;
+  bool hasExplicitArgIndex = false;
+  while (p != end) {
+    auto q = static_cast<const char*>(memchr(p, '{', end - p));
+    if (!q) {
+      outputString(StringPiece(p, end));
+      break;
+    }
+    outputString(StringPiece(p, q));
+    p = q + 1;
+
+    if (p == end) {
+      throw BadFormatArg("folly::format: '}' at end of format string");
+    }
+
+    // "{{" -> "{"
+    if (*p == '{') {
+      out(StringPiece(p, 1));
+      ++p;
+      continue;
+    }
+
+    // Format string
+    q = static_cast<const char*>(memchr(p, '}', end - p));
+    if (q == nullptr) {
+      throw BadFormatArg("folly::format: missing ending '}'");
+    }
+    FormatArg arg(StringPiece(p, q));
+    p = q + 1;
+
+    int argIndex = 0;
+    auto piece = arg.splitKey<true>();  // empty key component is okay
+    if (containerMode) {  // static
+      if (piece.empty()) {
+        arg.setNextIntKey(nextArg++);
+        hasDefaultArgIndex = true;
+      } else {
+        arg.setNextKey(piece);
+        hasExplicitArgIndex = true;
+      }
+    } else {
+      if (piece.empty()) {
+        argIndex = nextArg++;
+        hasDefaultArgIndex = true;
+      } else {
+        try {
+          argIndex = to<int>(piece);
+        } catch (const std::out_of_range& e) {
+          arg.error("argument index must be integer");
+        }
+        arg.enforce(argIndex >= 0, "argument index must be non-negative");
+        hasExplicitArgIndex = true;
+      }
+    }
+
+    if (hasDefaultArgIndex && hasExplicitArgIndex) {
+      throw BadFormatArg(
+          "folly::format: may not have both default and explicit arg indexes");
+    }
+
+    doFormat(argIndex, arg, out);
+  }
+}
+
+template <bool containerMode, class... Args>
+void writeTo(FILE* fp, const Formatter<containerMode, Args...>& formatter) {
+  auto writer = [fp] (StringPiece sp) {
+    ssize_t n = fwrite(sp.data(), 1, sp.size(), fp);
+    if (n < sp.size()) {
+      throwSystemError("Formatter writeTo", "fwrite failed");
+    }
+  };
+  formatter(writer);
+}
+
+namespace format_value {
+
+template <class FormatCallback>
+void formatString(StringPiece val, FormatArg& arg, FormatCallback& cb) {
+  if (arg.precision != FormatArg::kDefaultPrecision &&
+      val.size() > arg.precision) {
+    val.reset(val.data(), arg.precision);
+  }
+
+  constexpr int padBufSize = 128;
+  char padBuf[padBufSize];
+
+  // Output padding, no more than padBufSize at once
+  auto pad = [&padBuf, &cb, padBufSize] (int chars) {
+    while (chars) {
+      int n = std::min(chars, padBufSize);
+      cb(StringPiece(padBuf, n));
+      chars -= n;
+    }
+  };
+
+  int padRemaining = 0;
+  if (arg.width != FormatArg::kDefaultWidth && val.size() < arg.width) {
+    char fill = arg.fill == FormatArg::kDefaultFill ? ' ' : arg.fill;
+    int padChars = arg.width - val.size();
+    memset(padBuf, fill, std::min(padBufSize, padChars));
+
+    switch (arg.align) {
+    case FormatArg::Align::DEFAULT:
+    case FormatArg::Align::LEFT:
+      padRemaining = padChars;
+      break;
+    case FormatArg::Align::CENTER:
+      pad(padChars / 2);
+      padRemaining = padChars - padChars / 2;
+      break;
+    case FormatArg::Align::RIGHT:
+    case FormatArg::Align::PAD_AFTER_SIGN:
+      pad(padChars);
+      break;
+    default:
+      abort();
+      break;
+    }
+  }
+
+  cb(val);
+
+  if (padRemaining) {
+    pad(padRemaining);
+  }
+}
+
+template <class FormatCallback>
+void formatNumber(StringPiece val, int prefixLen, FormatArg& arg,
+                  FormatCallback& cb) {
+  // precision means something different for numbers
+  arg.precision = FormatArg::kDefaultPrecision;
+  if (arg.align == FormatArg::Align::DEFAULT) {
+    arg.align = FormatArg::Align::RIGHT;
+  } else if (prefixLen && arg.align == FormatArg::Align::PAD_AFTER_SIGN) {
+    // Split off the prefix, then do any padding if necessary
+    cb(val.subpiece(0, prefixLen));
+    val.advance(prefixLen);
+    arg.width = std::max(arg.width - prefixLen, 0);
+  }
+  format_value::formatString(val, arg, cb);
+}
+
+template <class FormatCallback, bool containerMode, class... Args>
+void formatFormatter(const Formatter<containerMode, Args...>& formatter,
+                     FormatArg& arg,
+                     FormatCallback& cb) {
+  if (arg.width == FormatArg::kDefaultWidth &&
+      arg.precision == FormatArg::kDefaultPrecision) {
+    // nothing to do
+    formatter(cb);
+  } else if (arg.align != FormatArg::Align::LEFT &&
+             arg.align != FormatArg::Align::DEFAULT) {
+    // We can only avoid creating a temporary string if we align left,
+    // as we'd need to know the size beforehand otherwise
+    format_value::formatString(formatter.fbstr(), arg, cb);
+  } else {
+    auto fn = [&arg, &cb] (StringPiece sp) mutable {
+      int sz = static_cast<int>(sp.size());
+      if (arg.precision != FormatArg::kDefaultPrecision) {
+        sz = std::min(arg.precision, sz);
+        sp.reset(sp.data(), sz);
+        arg.precision -= sz;
+      }
+      if (!sp.empty()) {
+        cb(sp);
+        if (arg.width != FormatArg::kDefaultWidth) {
+          arg.width = std::max(arg.width - sz, 0);
+        }
+      }
+    };
+    formatter(fn);
+    if (arg.width != FormatArg::kDefaultWidth && arg.width != 0) {
+      // Rely on formatString to do appropriate padding
+      format_value::formatString(StringPiece(), arg, cb);
+    }
+  }
+}
+
+}  // namespace format_value
+
+// Definitions for default FormatValue classes
+
+// Integral types (except bool)
+template <class T>
+class FormatValue<
+  T, typename std::enable_if<
+    std::is_integral<T>::value &&
+    !std::is_same<T, bool>::value>::type>
+  {
+ public:
+  explicit FormatValue(T val) : val_(val) { }
+  template <class FormatCallback>
+  void format(FormatArg& arg, FormatCallback& cb) const {
+    arg.validate(FormatArg::Type::INTEGER);
+    doFormat(arg, cb);
+  }
+
+  template <class FormatCallback>
+  void doFormat(FormatArg& arg, FormatCallback& cb) const {
+    char presentation = arg.presentation;
+    if (presentation == FormatArg::kDefaultPresentation) {
+      presentation = std::is_same<T, char>::value ? 'c' : 'd';
+    }
+
+    // Do all work as unsigned, we'll add the prefix ('0' or '0x' if necessary)
+    // and sign ourselves.
+    typedef typename std::make_unsigned<T>::type UT;
+    UT uval;
+    char sign;
+    if (std::is_signed<T>::value) {
+      if (folly::is_negative(val_)) {
+        uval = static_cast<UT>(-val_);
+        sign = '-';
+      } else {
+        uval = static_cast<UT>(val_);
+        switch (arg.sign) {
+        case FormatArg::Sign::PLUS_OR_MINUS:
+          sign = '+';
+          break;
+        case FormatArg::Sign::SPACE_OR_MINUS:
+          sign = ' ';
+          break;
+        default:
+          sign = '\0';
+          break;
+        }
+      }
+    } else {
+      uval = val_;
+      sign = '\0';
+
+      arg.enforce(arg.sign == FormatArg::Sign::DEFAULT,
+                  "sign specifications not allowed for unsigned values");
+    }
+
+    // max of:
+    // #x: 0x prefix + 16 bytes = 18 bytes
+    // #o: 0 prefix + 22 bytes = 23 bytes
+    // #b: 0b prefix + 64 bytes = 65 bytes
+    // ,d: 26 bytes (including thousands separators!)
+    // + nul terminator
+    // + 3 for sign and prefix shenanigans (see below)
+    constexpr size_t valBufSize = 69;
+    char valBuf[valBufSize];
+    char* valBufBegin = nullptr;
+    char* valBufEnd = nullptr;
+
+    // Defer to sprintf
+    auto useSprintf = [&] (const char* format) mutable {
+      valBufBegin = valBuf + 3;  // room for sign and base prefix
+      valBufEnd = valBufBegin + sprintf(valBufBegin, format,
+                                        static_cast<uintmax_t>(uval));
+    };
+
+    int prefixLen = 0;
+
+    switch (presentation) {
+    case 'n':  // TODO(tudorb): locale awareness?
+    case 'd':
+      arg.enforce(!arg.basePrefix,
+                  "base prefix not allowed with '", presentation,
+                  "' specifier");
+      if (arg.thousandsSeparator) {
+        useSprintf("%'ju");
+      } else {
+        // Use uintToBuffer, faster than sprintf
+        valBufBegin = valBuf + 3;
+        valBufEnd = valBufBegin + uint64ToBufferUnsafe(uval, valBufBegin);
+      }
+      break;
+    case 'c':
+      arg.enforce(!arg.basePrefix,
+                  "base prefix not allowed with '", presentation,
+                  "' specifier");
+      arg.enforce(!arg.thousandsSeparator,
+                  "thousands separator (',') not allowed with '",
+                  presentation, "' specifier");
+      valBufBegin = valBuf + 3;
+      *valBufBegin = static_cast<char>(uval);
+      valBufEnd = valBufBegin + 1;
+      break;
+    case 'o':
+    case 'O':
+      arg.enforce(!arg.thousandsSeparator,
+                  "thousands separator (',') not allowed with '",
+                  presentation, "' specifier");
+      valBufEnd = valBuf + valBufSize - 1;
+      valBufBegin = valBuf + detail::uintToOctal(valBuf, valBufSize - 1, uval);
+      if (arg.basePrefix) {
+        *--valBufBegin = '0';
+        prefixLen = 1;
+      }
+      break;
+    case 'x':
+      arg.enforce(!arg.thousandsSeparator,
+                  "thousands separator (',') not allowed with '",
+                  presentation, "' specifier");
+      valBufEnd = valBuf + valBufSize - 1;
+      valBufBegin = valBuf + detail::uintToHexLower(valBuf, valBufSize - 1,
+                                                    uval);
+      if (arg.basePrefix) {
+        *--valBufBegin = 'x';
+        *--valBufBegin = '0';
+        prefixLen = 2;
+      }
+      break;
+    case 'X':
+      arg.enforce(!arg.thousandsSeparator,
+                  "thousands separator (',') not allowed with '",
+                  presentation, "' specifier");
+      valBufEnd = valBuf + valBufSize - 1;
+      valBufBegin = valBuf + detail::uintToHexUpper(valBuf, valBufSize - 1,
+                                                    uval);
+      if (arg.basePrefix) {
+        *--valBufBegin = 'X';
+        *--valBufBegin = '0';
+        prefixLen = 2;
+      }
+      break;
+    case 'b':
+    case 'B':
+      arg.enforce(!arg.thousandsSeparator,
+                  "thousands separator (',') not allowed with '",
+                  presentation, "' specifier");
+      valBufEnd = valBuf + valBufSize - 1;
+      valBufBegin = valBuf + detail::uintToBinary(valBuf, valBufSize - 1,
+                                                  uval);
+      if (arg.basePrefix) {
+        *--valBufBegin = presentation;  // 0b or 0B
+        *--valBufBegin = '0';
+        prefixLen = 2;
+      }
+      break;
+    default:
+      arg.error("invalid specifier '", presentation, "'");
+    }
+
+    if (sign) {
+      *--valBufBegin = sign;
+      ++prefixLen;
+    }
+
+    format_value::formatNumber(StringPiece(valBufBegin, valBufEnd), prefixLen,
+                               arg, cb);
+  }
+
+ private:
+  T val_;
+};
+
+// Bool
+template <>
+class FormatValue<bool> {
+ public:
+  explicit FormatValue(bool val) : val_(val) { }
+
+  template <class FormatCallback>
+  void format(FormatArg& arg, FormatCallback& cb) const {
+    if (arg.presentation == FormatArg::kDefaultPresentation) {
+      arg.validate(FormatArg::Type::OTHER);
+      format_value::formatString(val_ ? "true" : "false", arg, cb);
+    } else {  // number
+      FormatValue<int>(val_).format(arg, cb);
+    }
+  }
+
+ private:
+  bool val_;
+};
+
+// double
+template <>
+class FormatValue<double> {
+ public:
+  explicit FormatValue(double val) : val_(val) { }
+
+  template <class FormatCallback>
+  void format(FormatArg& arg, FormatCallback& cb) const {
+    using ::double_conversion::DoubleToStringConverter;
+    using ::double_conversion::StringBuilder;
+
+    arg.validate(FormatArg::Type::FLOAT);
+
+    if (arg.presentation == FormatArg::kDefaultPresentation) {
+      arg.presentation = 'g';
+    }
+
+    const char* infinitySymbol = isupper(arg.presentation) ? "INF" : "inf";
+    const char* nanSymbol = isupper(arg.presentation) ? "NAN" : "nan";
+    char exponentSymbol = isupper(arg.presentation) ? 'E' : 'e';
+
+    if (arg.precision == FormatArg::kDefaultPrecision) {
+      arg.precision = 6;
+    }
+
+    // 2+: for null terminator and optional sign shenanigans.
+    char buf[2 + std::max({
+        (2 + DoubleToStringConverter::kMaxFixedDigitsBeforePoint +
+         DoubleToStringConverter::kMaxFixedDigitsAfterPoint),
+        (8 + DoubleToStringConverter::kMaxExponentialDigits),
+        (7 + DoubleToStringConverter::kMaxPrecisionDigits)})];
+    StringBuilder builder(buf + 1, sizeof(buf) - 1);
+
+    char plusSign;
+    switch (arg.sign) {
+    case FormatArg::Sign::PLUS_OR_MINUS:
+      plusSign = '+';
+      break;
+    case FormatArg::Sign::SPACE_OR_MINUS:
+      plusSign = ' ';
+      break;
+    default:
+      plusSign = '\0';
+      break;
+    };
+
+    double val = val_;
+    switch (arg.presentation) {
+    case '%':
+      val *= 100;
+    case 'f':
+    case 'F':
+      {
+        if (arg.precision >
+            DoubleToStringConverter::kMaxFixedDigitsAfterPoint) {
+          arg.precision = DoubleToStringConverter::kMaxFixedDigitsAfterPoint;
+        }
+        DoubleToStringConverter conv(
+            DoubleToStringConverter::EMIT_POSITIVE_EXPONENT_SIGN,
+            infinitySymbol,
+            nanSymbol,
+            exponentSymbol,
+            -4, arg.precision,
+            0, 0);
+        arg.enforce(conv.ToFixed(val, arg.precision, &builder),
+                    "fixed double conversion failed");
+      }
+      break;
+    case 'e':
+    case 'E':
+      {
+        if (arg.precision > DoubleToStringConverter::kMaxExponentialDigits) {
+          arg.precision = DoubleToStringConverter::kMaxExponentialDigits;
+        }
+
+        DoubleToStringConverter conv(
+            DoubleToStringConverter::EMIT_POSITIVE_EXPONENT_SIGN,
+            infinitySymbol,
+            nanSymbol,
+            exponentSymbol,
+            -4, arg.precision,
+            0, 0);
+        arg.enforce(conv.ToExponential(val, arg.precision, &builder));
+      }
+      break;
+    case 'n':  // should be locale-aware, but isn't
+    case 'g':
+    case 'G':
+      {
+        if (arg.precision < DoubleToStringConverter::kMinPrecisionDigits) {
+          arg.precision = DoubleToStringConverter::kMinPrecisionDigits;
+        } else if (arg.precision >
+                   DoubleToStringConverter::kMaxPrecisionDigits) {
+          arg.precision = DoubleToStringConverter::kMaxPrecisionDigits;
+        }
+        DoubleToStringConverter conv(
+            DoubleToStringConverter::EMIT_POSITIVE_EXPONENT_SIGN,
+            infinitySymbol,
+            nanSymbol,
+            exponentSymbol,
+            -4, arg.precision,
+            0, 0);
+        arg.enforce(conv.ToShortest(val, &builder));
+      }
+      break;
+    default:
+      arg.error("invalid specifier '", arg.presentation, "'");
+    }
+
+    int len = builder.position();
+    builder.Finalize();
+    DCHECK_GT(len, 0);
+
+    // Add '+' or ' ' sign if needed
+    char* p = buf + 1;
+    // anything that's neither negative nor nan
+    int prefixLen = 0;
+    if (plusSign && (*p != '-' && *p != 'n' && *p != 'N')) {
+      *--p = plusSign;
+      ++len;
+      prefixLen = 1;
+    } else if (*p == '-') {
+      prefixLen = 1;
+    }
+
+    format_value::formatNumber(StringPiece(p, len), prefixLen, arg, cb);
+  }
+
+ private:
+  double val_;
+};
+
+// float (defer to double)
+template <>
+class FormatValue<float> {
+ public:
+  explicit FormatValue(float val) : val_(val) { }
+
+  template <class FormatCallback>
+  void format(FormatArg& arg, FormatCallback& cb) const {
+    FormatValue<double>(val_).format(arg, cb);
+  }
+
+ private:
+  float val_;
+};
+
+// Sring-y types (implicitly convertible to StringPiece, except char*)
+template <class T>
+class FormatValue<
+  T, typename std::enable_if<
+      (!std::is_pointer<T>::value ||
+       !std::is_same<char, typename std::decay<
+          typename std::remove_pointer<T>::type>::type>::value) &&
+      std::is_convertible<T, StringPiece>::value>::type>
+  {
+ public:
+  explicit FormatValue(StringPiece val) : val_(val) { }
+
+  template <class FormatCallback>
+  void format(FormatArg& arg, FormatCallback& cb) const {
+    if (arg.keyEmpty()) {
+      arg.validate(FormatArg::Type::OTHER);
+      arg.enforce(arg.presentation == FormatArg::kDefaultPresentation ||
+                  arg.presentation == 's',
+                  "invalid specifier '", arg.presentation, "'");
+      format_value::formatString(val_, arg, cb);
+    } else {
+      FormatValue<char>(val_.at(arg.splitIntKey())).format(arg, cb);
+    }
+  }
+
+ private:
+  StringPiece val_;
+};
+
+// Null
+template <>
+class FormatValue<std::nullptr_t> {
+ public:
+  explicit FormatValue(std::nullptr_t) { }
+
+  template <class FormatCallback>
+  void format(FormatArg& arg, FormatCallback& cb) const {
+    arg.validate(FormatArg::Type::OTHER);
+    arg.enforce(arg.presentation == FormatArg::kDefaultPresentation,
+                "invalid specifier '", arg.presentation, "'");
+    format_value::formatString("(null)", arg, cb);
+  }
+};
+
+// Partial specialization of FormatValue for char*
+template <class T>
+class FormatValue<
+  T*,
+  typename std::enable_if<
+      std::is_same<char, typename std::decay<T>::type>::value>::type>
+  {
+ public:
+  explicit FormatValue(T* val) : val_(val) { }
+
+  template <class FormatCallback>
+  void format(FormatArg& arg, FormatCallback& cb) const {
+    if (arg.keyEmpty()) {
+      if (!val_) {
+        FormatValue<std::nullptr_t>(nullptr).format(arg, cb);
+      } else {
+        FormatValue<StringPiece>(val_).format(arg, cb);
+      }
+    } else {
+      FormatValue<typename std::decay<T>::type>(
+          val_[arg.splitIntKey()]).format(arg, cb);
+    }
+  }
+
+ private:
+  T* val_;
+};
+
+// Partial specialization of FormatValue for void*
+template <class T>
+class FormatValue<
+  T*,
+  typename std::enable_if<
+      std::is_same<void, typename std::decay<T>::type>::value>::type>
+  {
+ public:
+  explicit FormatValue(T* val) : val_(val) { }
+
+  template <class FormatCallback>
+  void format(FormatArg& arg, FormatCallback& cb) const {
+    if (!val_) {
+      FormatValue<std::nullptr_t>(nullptr).format(arg, cb);
+    } else {
+      // Print as a pointer, in hex.
+      arg.validate(FormatArg::Type::OTHER);
+      arg.enforce(arg.presentation == FormatArg::kDefaultPresentation,
+                  "invalid specifier '", arg.presentation, "'");
+      arg.basePrefix = true;
+      arg.presentation = 'x';
+      if (arg.align == FormatArg::Align::DEFAULT) {
+        arg.align = FormatArg::Align::LEFT;
+      }
+      FormatValue<uintptr_t>(
+          reinterpret_cast<uintptr_t>(val_)).doFormat(arg, cb);
+    }
+  }
+
+ private:
+  T* val_;
+};
+
+template <class T, class = void>
+class TryFormatValue {
+ public:
+  template <class FormatCallback>
+  static void formatOrFail(T& value, FormatArg& arg, FormatCallback& cb) {
+    arg.error("No formatter available for this type");
+  }
+};
+
+template <class T>
+class TryFormatValue<
+  T,
+  typename std::enable_if<
+      0 < sizeof(FormatValue<typename std::decay<T>::type>)>::type>
+  {
+ public:
+  template <class FormatCallback>
+  static void formatOrFail(T& value, FormatArg& arg, FormatCallback& cb) {
+    FormatValue<typename std::decay<T>::type>(value).format(arg, cb);
+  }
+};
+
+// Partial specialization of FormatValue for other pointers
+template <class T>
+class FormatValue<
+  T*,
+  typename std::enable_if<
+      !std::is_same<char, typename std::decay<T>::type>::value &&
+      !std::is_same<void, typename std::decay<T>::type>::value>::type>
+  {
+ public:
+  explicit FormatValue(T* val) : val_(val) { }
+
+  template <class FormatCallback>
+  void format(FormatArg& arg, FormatCallback& cb) const {
+    if (arg.keyEmpty()) {
+      FormatValue<void*>((void*)val_).format(arg, cb);
+    } else {
+      TryFormatValue<T>::formatOrFail(val_[arg.splitIntKey()], arg, cb);
+    }
+  }
+ private:
+  T* val_;
+};
+
+namespace detail {
+
+// Shortcut, so we don't have to use enable_if everywhere
+struct FormatTraitsBase {
+  typedef void enabled;
+};
+
+// Traits that define enabled, value_type, and at() for anything
+// indexable with integral keys: pointers, arrays, vectors, and maps
+// with integral keys
+template <class T, class Enable=void> struct IndexableTraits;
+
+// Base class for sequences (vectors, deques)
+template <class C>
+struct IndexableTraitsSeq : public FormatTraitsBase {
+  typedef C container_type;
+  typedef typename C::value_type value_type;
+  static const value_type& at(const C& c, int idx) {
+    return c.at(idx);
+  }
+};
+
+// Base class for associative types (maps)
+template <class C>
+struct IndexableTraitsAssoc : public FormatTraitsBase {
+  typedef typename C::value_type::second_type value_type;
+  static const value_type& at(const C& c, int idx) {
+    return c.at(static_cast<typename C::key_type>(idx));
+  }
+};
+
+// std::array
+template <class T, size_t N>
+struct IndexableTraits<std::array<T, N>>
+  : public IndexableTraitsSeq<std::array<T, N>> {
+};
+
+// std::vector
+template <class T, class A>
+struct IndexableTraits<std::vector<T, A>>
+  : public IndexableTraitsSeq<std::vector<T, A>> {
+};
+
+// std::deque
+template <class T, class A>
+struct IndexableTraits<std::deque<T, A>>
+  : public IndexableTraitsSeq<std::deque<T, A>> {
+};
+
+// fbvector
+template <class T, class A>
+struct IndexableTraits<fbvector<T, A>>
+  : public IndexableTraitsSeq<fbvector<T, A>> {
+};
+
+// small_vector
+template <class T, size_t M, class A, class B, class C>
+struct IndexableTraits<small_vector<T, M, A, B, C>>
+  : public IndexableTraitsSeq<small_vector<T, M, A, B, C>> {
+};
+
+// std::map with integral keys
+template <class K, class T, class C, class A>
+struct IndexableTraits<
+  std::map<K, T, C, A>,
+  typename std::enable_if<std::is_integral<K>::value>::type>
+  : public IndexableTraitsAssoc<std::map<K, T, C, A>> {
+};
+
+// std::unordered_map with integral keys
+template <class K, class T, class H, class E, class A>
+struct IndexableTraits<
+  std::unordered_map<K, T, H, E, A>,
+  typename std::enable_if<std::is_integral<K>::value>::type>
+  : public IndexableTraitsAssoc<std::unordered_map<K, T, H, E, A>> {
+};
+
+}  // namespace detail
+
+// Partial specialization of FormatValue for integer-indexable containers
+template <class T>
+class FormatValue<
+  T,
+  typename detail::IndexableTraits<T>::enabled> {
+ public:
+  explicit FormatValue(const T& val) : val_(val) { }
+
+  template <class FormatCallback>
+  void format(FormatArg& arg, FormatCallback& cb) const {
+    FormatValue<typename std::decay<
+      typename detail::IndexableTraits<T>::value_type>::type>(
+        detail::IndexableTraits<T>::at(
+            val_, arg.splitIntKey())).format(arg, cb);
+  }
+
+ private:
+  const T& val_;
+};
+
+namespace detail {
+
+// Define enabled, key_type, convert from StringPiece to the key types
+// that we support
+template <class T> struct KeyFromStringPiece;
+
+// std::string
+template <>
+struct KeyFromStringPiece<std::string> : public FormatTraitsBase {
+  typedef std::string key_type;
+  static std::string convert(StringPiece s) {
+    return s.toString();
+  }
+  typedef void enabled;
+};
+
+// fbstring
+template <>
+struct KeyFromStringPiece<fbstring> : public FormatTraitsBase {
+  typedef fbstring key_type;
+  static fbstring convert(StringPiece s) {
+    return s.toFbstring();
+  }
+};
+
+// StringPiece
+template <>
+struct KeyFromStringPiece<StringPiece> : public FormatTraitsBase {
+  typedef StringPiece key_type;
+  static StringPiece convert(StringPiece s) {
+    return s;
+  }
+};
+
+// Base class for associative types keyed by strings
+template <class T> struct KeyableTraitsAssoc : public FormatTraitsBase {
+  typedef typename T::key_type key_type;
+  typedef typename T::value_type::second_type value_type;
+  static const value_type& at(const T& map, StringPiece key) {
+    return map.at(KeyFromStringPiece<key_type>::convert(key));
+  }
+};
+
+// Define enabled, key_type, value_type, at() for supported string-keyed
+// types
+template <class T, class Enabled=void> struct KeyableTraits;
+
+// std::map with string key
+template <class K, class T, class C, class A>
+struct KeyableTraits<
+  std::map<K, T, C, A>,
+  typename KeyFromStringPiece<K>::enabled>
+  : public KeyableTraitsAssoc<std::map<K, T, C, A>> {
+};
+
+// std::unordered_map with string key
+template <class K, class T, class H, class E, class A>
+struct KeyableTraits<
+  std::unordered_map<K, T, H, E, A>,
+  typename KeyFromStringPiece<K>::enabled>
+  : public KeyableTraitsAssoc<std::unordered_map<K, T, H, E, A>> {
+};
+
+}  // namespace detail
+
+// Partial specialization of FormatValue for string-keyed containers
+template <class T>
+class FormatValue<
+  T,
+  typename detail::KeyableTraits<T>::enabled> {
+ public:
+  explicit FormatValue(const T& val) : val_(val) { }
+
+  template <class FormatCallback>
+  void format(FormatArg& arg, FormatCallback& cb) const {
+    FormatValue<typename std::decay<
+      typename detail::KeyableTraits<T>::value_type>::type>(
+        detail::KeyableTraits<T>::at(
+            val_, arg.splitKey())).format(arg, cb);
+  }
+
+ private:
+  const T& val_;
+};
+
+// Partial specialization of FormatValue for pairs
+template <class A, class B>
+class FormatValue<std::pair<A, B>> {
+ public:
+  explicit FormatValue(const std::pair<A, B>& val) : val_(val) { }
+
+  template <class FormatCallback>
+  void format(FormatArg& arg, FormatCallback& cb) const {
+    int key = arg.splitIntKey();
+    switch (key) {
+    case 0:
+      FormatValue<typename std::decay<A>::type>(val_.first).format(arg, cb);
+      break;
+    case 1:
+      FormatValue<typename std::decay<B>::type>(val_.second).format(arg, cb);
+      break;
+    default:
+      arg.error("invalid index for pair");
+    }
+  }
+
+ private:
+  const std::pair<A, B>& val_;
+};
+
+// Partial specialization of FormatValue for tuples
+template <class... Args>
+class FormatValue<std::tuple<Args...>> {
+  typedef std::tuple<Args...> Tuple;
+ public:
+  explicit FormatValue(const Tuple& val) : val_(val) { }
+
+  template <class FormatCallback>
+  void format(FormatArg& arg, FormatCallback& cb) const {
+    int key = arg.splitIntKey();
+    arg.enforce(key >= 0, "tuple index must be non-negative");
+    doFormat(key, arg, cb);
+  }
+
+ private:
+  static constexpr size_t valueCount = std::tuple_size<Tuple>::value;
+
+  template <size_t K, class Callback>
+  typename std::enable_if<K == valueCount>::type
+  doFormatFrom(size_t i, FormatArg& arg, Callback& cb) const {
+    arg.enforce("tuple index out of range, max=", i);
+  }
+
+  template <size_t K, class Callback>
+  typename std::enable_if<(K < valueCount)>::type
+  doFormatFrom(size_t i, FormatArg& arg, Callback& cb) const {
+    if (i == K) {
+      FormatValue<typename std::decay<
+        typename std::tuple_element<K, Tuple>::type>::type>(
+          std::get<K>(val_)).format(arg, cb);
+    } else {
+      doFormatFrom<K+1>(i, arg, cb);
+    }
+  }
+
+  template <class Callback>
+  void doFormat(size_t i, FormatArg& arg, Callback& cb) const {
+    return doFormatFrom<0>(i, arg, cb);
+  }
+
+  const Tuple& val_;
+};
+
+// Partial specialization of FormatValue for nested Formatters
+template <bool containerMode, class... Args>
+class FormatValue<Formatter<containerMode, Args...>, void> {
+  typedef Formatter<containerMode, Args...> FormatterValue;
+ public:
+  explicit FormatValue(const FormatterValue& f) : f_(f) { }
+
+  template <class FormatCallback>
+  void format(FormatArg& arg, FormatCallback& cb) const {
+    format_value::formatFormatter(f_, arg, cb);
+  }
+ private:
+  const FormatterValue& f_;
+};
+
+/**
+ * Formatter objects can be appended to strings, and therefore they're
+ * compatible with folly::toAppend and folly::to.
+ */
+template <class Tgt, bool containerMode, class... Args>
+typename std::enable_if<
+   IsSomeString<Tgt>::value>::type
+toAppend(const Formatter<containerMode, Args...>& value, Tgt * result) {
+  value.appendTo(*result);
+}
+
+}  // namespace folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/gen/Base.h
@@ -0,0 +1,642 @@
+/*
+ * Copyright 2013 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#ifndef FOLLY_GEN_BASE_H
+#define FOLLY_GEN_BASE_H
+
+#include <functional>
+#include <memory>
+#include <type_traits>
+#include <utility>
+#include <algorithm>
+#include <random>
+#include <vector>
+#include <unordered_set>
+
+#include "folly/Range.h"
+#include "folly/Optional.h"
+#include "folly/Conv.h"
+#include "folly/gen/Core.h"
+
+/**
+ * Generator-based Sequence Comprehensions in C++, akin to C#'s LINQ
+ * @author Tom Jackson <tjackson@fb.com>
+ *
+ * This library makes it possible to write declarative comprehensions for
+ * processing sequences of values efficiently in C++. The operators should be
+ * familiar to those with experience in functional programming, and the
+ * performance will be virtually identical to the equivalent, boilerplate C++
+ * implementations.
+ *
+ * Generator objects may be created from either an stl-like container (anything
+ * supporting begin() and end()), from sequences of values, or from another
+ * generator (see below). To create a generator that pulls values from a vector,
+ * for example, one could write:
+ *
+ *   vector<string> names { "Jack", "Jill", "Sara", "Tom" };
+ *   auto gen = from(names);
+ *
+ * Generators are composed by building new generators out of old ones through
+ * the use of operators. These are reminicent of shell pipelines, and afford
+ * similar composition. Lambda functions are used liberally to describe how to
+ * handle individual values:
+ *
+ *   auto lengths = gen
+ *                | mapped([](const fbstring& name) { return name.size(); });
+ *
+ * Generators are lazy; they don't actually perform any work until they need to.
+ * As an example, the 'lengths' generator (above) won't actually invoke the
+ * provided lambda until values are needed:
+ *
+ *   auto lengthVector = lengths | as<std::vector>();
+ *   auto totalLength = lengths | sum;
+ *
+ * 'auto' is useful in here because the actual types of the generators objects
+ * are usually complicated and implementation-sensitive.
+ *
+ * If a simpler type is desired (for returning, as an example), VirtualGen<T>
+ * may be used to wrap the generator in a polymorphic wrapper:
+ *
+ *  VirtualGen<float> powersOfE() {
+ *    return seq(1) | mapped(&expf);
+ *  }
+ *
+ * To learn more about this library, including the use of infinite generators,
+ * see the examples in the comments, or the docs (coming soon).
+*/
+
+namespace folly { namespace gen {
+
+class EmptySequence : public std::exception {
+public:
+  virtual const char* what() const noexcept {
+    return "This operation cannot be called on an empty sequence";
+  }
+};
+
+class Less {
+public:
+  template<class First,
+           class Second>
+  auto operator()(const First& first, const Second& second) const ->
+  decltype(first < second) {
+    return first < second;
+  }
+};
+
+class Greater {
+public:
+  template<class First,
+           class Second>
+  auto operator()(const First& first, const Second& second) const ->
+  decltype(first > second) {
+    return first > second;
+  }
+};
+
+template<int n>
+class Get {
+public:
+  template<class Value>
+  auto operator()(Value&& value) const ->
+  decltype(std::get<n>(std::forward<Value>(value))) {
+    return std::get<n>(std::forward<Value>(value));
+  }
+};
+
+template<class Class,
+         class Result>
+class MemberFunction {
+ public:
+  typedef Result (Class::*MemberPtr)();
+ private:
+  MemberPtr member_;
+ public:
+  explicit MemberFunction(MemberPtr member)
+    : member_(member)
+  {}
+
+  Result operator()(Class&& x) const {
+    return (x.*member_)();
+  }
+
+  Result operator()(Class& x) const {
+    return (x.*member_)();
+  }
+};
+
+template<class Class,
+         class Result>
+class ConstMemberFunction{
+ public:
+  typedef Result (Class::*MemberPtr)() const;
+ private:
+  MemberPtr member_;
+ public:
+  explicit ConstMemberFunction(MemberPtr member)
+    : member_(member)
+  {}
+
+  Result operator()(const Class& x) const {
+    return (x.*member_)();
+  }
+};
+
+template<class Class,
+         class FieldType>
+class Field {
+ public:
+  typedef FieldType (Class::*FieldPtr);
+ private:
+  FieldPtr field_;
+ public:
+  explicit Field(FieldPtr field)
+    : field_(field)
+  {}
+
+  const FieldType& operator()(const Class& x) const {
+    return x.*field_;
+  }
+
+  FieldType& operator()(Class& x) const {
+    return x.*field_;
+  }
+
+  FieldType&& operator()(Class&& x) const {
+    return std::move(x.*field_);
+  }
+};
+
+class Move {
+public:
+  template<class Value>
+  auto operator()(Value&& value) const ->
+  decltype(std::move(std::forward<Value>(value))) {
+    return std::move(std::forward<Value>(value));
+  }
+};
+
+class Identity {
+public:
+  template<class Value>
+  auto operator()(Value&& value) const ->
+  decltype(std::forward<Value>(value)) {
+    return std::forward<Value>(value);
+  }
+};
+
+template <class Dest>
+class Cast {
+ public:
+  template <class Value>
+  Dest operator()(Value&& value) const {
+    return Dest(std::forward<Value>(value));
+  }
+};
+
+template <class Dest>
+class To {
+ public:
+  template <class Value>
+  Dest operator()(Value&& value) const {
+    return ::folly::to<Dest>(std::forward<Value>(value));
+  }
+};
+
+// Specialization to allow String->StringPiece conversion
+template <>
+class To<StringPiece> {
+ public:
+  StringPiece operator()(StringPiece src) const {
+    return src;
+  }
+};
+
+namespace detail {
+
+template<class Self>
+struct FBounded;
+
+/*
+ * Type Traits
+ */
+template<class Container>
+struct ValueTypeOfRange {
+ private:
+  static Container container_;
+ public:
+  typedef decltype(*std::begin(container_))
+    RefType;
+  typedef typename std::decay<decltype(*std::begin(container_))>::type
+    StorageType;
+};
+
+
+/*
+ * Sources
+ */
+template<class Container,
+         class Value = typename ValueTypeOfRange<Container>::RefType>
+class ReferencedSource;
+
+template<class Value,
+         class Container = std::vector<typename std::decay<Value>::type>>
+class CopiedSource;
+
+template<class Value, bool endless = false, bool endInclusive = false>
+class Sequence;
+
+template<class Value, class Source>
+class Yield;
+
+template<class Value>
+class Empty;
+
+
+/*
+ * Operators
+ */
+template<class Predicate>
+class Map;
+
+template<class Predicate>
+class Filter;
+
+template<class Predicate>
+class Until;
+
+class Take;
+
+template<class Rand>
+class Sample;
+
+class Skip;
+
+template<class Selector, class Comparer = Less>
+class Order;
+
+template<class Selector>
+class Distinct;
+
+template<class Expected>
+class TypeAssertion;
+
+class Concat;
+
+class RangeConcat;
+
+class Cycle;
+
+class Batch;
+
+class Dereference;
+
+/*
+ * Sinks
+ */
+template<class Seed,
+         class Fold>
+class FoldLeft;
+
+class First;
+
+class Any;
+
+template<class Predicate>
+class All;
+
+template<class Reducer>
+class Reduce;
+
+class Sum;
+
+template<class Selector,
+         class Comparer>
+class Min;
+
+template<class Container>
+class Collect;
+
+template<template<class, class> class Collection = std::vector,
+         template<class> class Allocator = std::allocator>
+class CollectTemplate;
+
+template<class Collection>
+class Append;
+
+template<class Value>
+struct GeneratorBuilder;
+
+template<class Needle>
+class Contains;
+
+template<class Exception,
+         class ErrorHandler>
+class GuardImpl;
+
+}
+
+/**
+ * Polymorphic wrapper
+ **/
+template<class Value>
+class VirtualGen;
+
+/*
+ * Source Factories
+ */
+template<class Container,
+         class From = detail::ReferencedSource<const Container>>
+From fromConst(const Container& source) {
+  return From(&source);
+}
+
+template<class Container,
+         class From = detail::ReferencedSource<Container>>
+From from(Container& source) {
+  return From(&source);
+}
+
+template<class Container,
+         class Value =
+           typename detail::ValueTypeOfRange<Container>::StorageType,
+         class CopyOf = detail::CopiedSource<Value>>
+CopyOf fromCopy(Container&& source) {
+  return CopyOf(std::forward<Container>(source));
+}
+
+template<class Value,
+         class From = detail::CopiedSource<Value>>
+From from(std::initializer_list<Value> source) {
+  return From(source);
+}
+
+template<class Container,
+         class From = detail::CopiedSource<typename Container::value_type,
+                                           Container>>
+From from(Container&& source) {
+  return From(std::move(source));
+}
+
+template<class Value, class Gen = detail::Sequence<Value, false, false>>
+Gen range(Value begin, Value end) {
+  return Gen(begin, end);
+}
+
+template<class Value,
+         class Gen = detail::Sequence<Value, false, true>>
+Gen seq(Value first, Value last) {
+  return Gen(first, last);
+}
+
+template<class Value,
+         class Gen = detail::Sequence<Value, true>>
+Gen seq(Value begin) {
+  return Gen(begin);
+}
+
+template<class Value,
+         class Source,
+         class Yield = detail::Yield<Value, Source>>
+Yield generator(Source&& source) {
+  return Yield(std::forward<Source>(source));
+}
+
+/*
+ * Create inline generator, used like:
+ *
+ *  auto gen = GENERATOR(int) { yield(1); yield(2); };
+ */
+#define GENERATOR(TYPE)                            \
+  ::folly::gen::detail::GeneratorBuilder<TYPE>() + \
+   [=](const std::function<void(TYPE)>& yield)
+
+/*
+ * empty() - for producing empty sequences.
+ */
+template<class Value>
+detail::Empty<Value> empty() {
+  return {};
+}
+
+/*
+ * Operator Factories
+ */
+template<class Predicate,
+         class Map = detail::Map<Predicate>>
+Map mapped(Predicate pred = Predicate()) {
+  return Map(std::move(pred));
+}
+
+template<class Predicate,
+         class Map = detail::Map<Predicate>>
+Map map(Predicate pred = Predicate()) {
+  return Map(std::move(pred));
+}
+
+/*
+ * member(...) - For extracting a member from each value.
+ *
+ *  vector<string> strings = ...;
+ *  auto sizes = from(strings) | member(&string::size);
+ *
+ * If a member is const overridden (like 'front()'), pass template parameter
+ * 'Const' to select the const version, or 'Mutable' to select the non-const
+ * version:
+ *
+ *  auto heads = from(strings) | member<Const>(&string::front);
+ */
+enum MemberType {
+  Const,
+  Mutable
+};
+
+template<MemberType Constness = Const,
+         class Class,
+         class Return,
+         class Mem = ConstMemberFunction<Class, Return>,
+         class Map = detail::Map<Mem>>
+typename std::enable_if<Constness == Const, Map>::type
+member(Return (Class::*member)() const) {
+  return Map(Mem(member));
+}
+
+template<MemberType Constness = Mutable,
+         class Class,
+         class Return,
+         class Mem = MemberFunction<Class, Return>,
+         class Map = detail::Map<Mem>>
+typename std::enable_if<Constness == Mutable, Map>::type
+member(Return (Class::*member)()) {
+  return Map(Mem(member));
+}
+
+/*
+ * field(...) - For extracting a field from each value.
+ *
+ *  vector<Item> items = ...;
+ *  auto names = from(items) | field(&Item::name);
+ *
+ * Note that if the values of the generator are rvalues, any non-reference
+ * fields will be rvalues as well. As an example, the code below does not copy
+ * any strings, only moves them:
+ *
+ *  auto namesVector = from(items)
+ *                   | move
+ *                   | field(&Item::name)
+ *                   | as<vector>();
+ */
+template<class Class,
+         class FieldType,
+         class Field = Field<Class, FieldType>,
+         class Map = detail::Map<Field>>
+Map field(FieldType Class::*field) {
+  return Map(Field(field));
+}
+
+template<class Predicate,
+         class Filter = detail::Filter<Predicate>>
+Filter filter(Predicate pred = Predicate()) {
+  return Filter(std::move(pred));
+}
+
+template<class Predicate,
+         class All = detail::All<Predicate>>
+All all(Predicate pred = Predicate()) {
+  return All(std::move(pred));
+}
+
+template<class Predicate,
+         class Until = detail::Until<Predicate>>
+Until until(Predicate pred = Predicate()) {
+  return Until(std::move(pred));
+}
+
+template<class Selector,
+         class Comparer = Less,
+         class Order = detail::Order<Selector, Comparer>>
+Order orderBy(Selector selector = Identity(),
+              Comparer comparer = Comparer()) {
+  return Order(std::move(selector),
+               std::move(comparer));
+}
+
+template<class Selector,
+         class Order = detail::Order<Selector, Greater>>
+Order orderByDescending(Selector selector = Identity()) {
+  return Order(std::move(selector));
+}
+
+template<class Selector,
+         class Distinct = detail::Distinct<Selector>>
+Distinct distinctBy(Selector selector = Identity()) {
+  return Distinct(std::move(selector));
+}
+
+template<int n,
+         class Get = detail::Map<Get<n>>>
+Get get() {
+  return Get();
+}
+
+// construct Dest from each value
+template <class Dest,
+          class Cast = detail::Map<Cast<Dest>>>
+Cast eachAs() {
+  return Cast();
+}
+
+// call folly::to on each value
+template <class Dest,
+          class To = detail::Map<To<Dest>>>
+To eachTo() {
+  return To();
+}
+
+template<class Value>
+detail::TypeAssertion<Value> assert_type() {
+  return {};
+}
+
+/*
+ * Sink Factories
+ */
+template<class Seed,
+         class Fold,
+         class FoldLeft = detail::FoldLeft<Seed, Fold>>
+FoldLeft foldl(Seed seed = Seed(),
+               Fold fold = Fold()) {
+  return FoldLeft(std::move(seed),
+                  std::move(fold));
+}
+
+template<class Reducer,
+         class Reduce = detail::Reduce<Reducer>>
+Reduce reduce(Reducer reducer = Reducer()) {
+  return Reduce(std::move(reducer));
+}
+
+template<class Selector = Identity,
+         class Min = detail::Min<Selector, Less>>
+Min minBy(Selector selector = Selector()) {
+  return Min(std::move(selector));
+}
+
+template<class Selector,
+         class MaxBy = detail::Min<Selector, Greater>>
+MaxBy maxBy(Selector selector = Selector()) {
+  return MaxBy(std::move(selector));
+}
+
+template<class Collection,
+         class Collect = detail::Collect<Collection>>
+Collect as() {
+  return Collect();
+}
+
+template<template<class, class> class Container = std::vector,
+         template<class> class Allocator = std::allocator,
+         class Collect = detail::CollectTemplate<Container, Allocator>>
+Collect as() {
+  return Collect();
+}
+
+template<class Collection,
+         class Append = detail::Append<Collection>>
+Append appendTo(Collection& collection) {
+  return Append(&collection);
+}
+
+template<class Needle,
+         class Contains = detail::Contains<typename std::decay<Needle>::type>>
+Contains contains(Needle&& needle) {
+  return Contains(std::forward<Needle>(needle));
+}
+
+template<class Exception,
+         class ErrorHandler,
+         class GuardImpl =
+           detail::GuardImpl<
+             Exception,
+             typename std::decay<ErrorHandler>::type>>
+GuardImpl guard(ErrorHandler&& handler) {
+  return GuardImpl(std::forward<ErrorHandler>(handler));
+}
+
+}} // folly::gen
+
+#include "folly/gen/Base-inl.h"
+
+#endif // FOLLY_GEN_BASE_H
--- /dev/null
+++ b/hphp/submodules/folly/folly/gen/Base-inl.h
@@ -0,0 +1,1822 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_GEN_BASE_H
+#error This file may only be included from folly/gen/Base.h
+#endif
+
+// Ignore shadowing warnings within this file, so includers can use -Wshadow.
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wshadow"
+
+namespace folly { namespace gen {
+
+/**
+ * ArgumentReference - For determining ideal argument type to receive a value.
+ */
+template <class T>
+struct ArgumentReference
+    : public std::conditional<
+          std::is_reference<T>::value,
+          T, // T& -> T&, T&& -> T&&, const T& -> const T&
+          typename std::conditional<std::is_const<T>::value,
+                                    T&, // const int -> const int&
+                                    T&& // int -> int&&
+                                    >::type> {};
+
+namespace detail {
+
+/*
+ * ReferencedSource - Generate values from an STL-like container using
+ * iterators from .begin() until .end(). Value type defaults to the type of
+ * *container->begin(). For std::vector<int>, this would be int&. Note that the
+ * value here is a reference, so the values in the vector will be passed by
+ * reference to downstream operators.
+ *
+ * This type is primarily used through the 'from' helper method, like:
+ *
+ *   string& longestName = from(names)
+ *                       | maxBy([](string& s) { return s.size() });
+ */
+template<class Container,
+         class Value>
+class ReferencedSource :
+    public GenImpl<Value, ReferencedSource<Container, Value>> {
+  Container* container_;
+public:
+  explicit ReferencedSource(Container* container)
+    : container_(container) {}
+
+  template<class Body>
+  void foreach(Body&& body) const {
+    for (auto& value : *container_) {
+      body(std::forward<Value>(value));
+    }
+  }
+
+  template<class Handler>
+  bool apply(Handler&& handler) const {
+    for (auto& value : *container_) {
+      if (!handler(std::forward<Value>(value))) {
+        return false;
+      }
+    }
+    return true;
+  }
+};
+
+/**
+ * CopiedSource - For producing values from eagerly from a sequence of values
+ * whose storage is owned by this class. Useful for preparing a generator for
+ * use after a source collection will no longer be available, or for when the
+ * values are specified literally with an initializer list.
+ *
+ * This type is primarily used through the 'fromCopy' function, like:
+ *
+ *   auto sourceCopy = fromCopy(makeAVector());
+ *   auto sum = sourceCopy | sum;
+ *   auto max = sourceCopy | max;
+ *
+ * Though it is also used for the initializer_list specialization of from().
+ */
+template<class StorageType,
+         class Container>
+class CopiedSource :
+  public GenImpl<const StorageType&,
+                 CopiedSource<StorageType, Container>> {
+  static_assert(
+    !std::is_reference<StorageType>::value, "StorageType must be decayed");
+ public:
+  // Generator objects are often copied during normal construction as they are
+  // encapsulated by downstream generators. It would be bad if this caused
+  // a copy of the entire container each time, and since we're only exposing a
+  // const reference to the value, it's safe to share it between multiple
+  // generators.
+  static_assert(
+    !std::is_reference<Container>::value,
+    "Can't copy into a reference");
+  std::shared_ptr<const Container> copy_;
+public:
+  typedef Container ContainerType;
+
+  template<class SourceContainer>
+  explicit CopiedSource(const SourceContainer& container)
+    : copy_(new Container(begin(container), end(container))) {}
+
+  explicit CopiedSource(Container&& container) :
+    copy_(new Container(std::move(container))) {}
+
+  // To enable re-use of cached results.
+  CopiedSource(const CopiedSource<StorageType, Container>& source)
+    : copy_(source.copy_) {}
+
+  template<class Body>
+  void foreach(Body&& body) const {
+    for (const auto& value : *copy_) {
+      body(value);
+    }
+  }
+
+  template<class Handler>
+  bool apply(Handler&& handler) const {
+    // The collection may be reused by others, we can't allow it to be changed.
+    for (const auto& value : *copy_) {
+      if (!handler(value)) {
+        return false;
+      }
+    }
+    return true;
+  }
+};
+
+/**
+ * Sequence - For generating values from beginning value, incremented along the
+ * way with the ++ and += operators. Iteration may continue indefinitely by
+ * setting the 'endless' template parameter to true. If set to false, iteration
+ * will stop when value reaches 'end', either inclusively or exclusively,
+ * depending on the template parameter 'endInclusive'. Value type specified
+ * explicitly.
+ *
+ * This type is primarily used through the 'seq' and 'range' function, like:
+ *
+ *   int total = seq(1, 10) | sum;
+ *   auto indexes = range(0, 10);
+ */
+template<class Value,
+         bool endless,
+         bool endInclusive>
+class Sequence : public GenImpl<const Value&,
+                                Sequence<Value, endless, endInclusive>> {
+  static_assert(!std::is_reference<Value>::value &&
+                !std::is_const<Value>::value, "Value mustn't be const or ref.");
+  Value bounds_[endless ? 1 : 2];
+public:
+  explicit Sequence(Value begin)
+      : bounds_{std::move(begin)} {
+    static_assert(endless, "Must supply 'end'");
+  }
+
+  Sequence(Value begin,
+           Value end)
+    : bounds_{std::move(begin), std::move(end)} {}
+
+  template<class Handler>
+  bool apply(Handler&& handler) const {
+    Value value = bounds_[0];
+    for (;endless || value < bounds_[1]; ++value) {
+      const Value& arg = value;
+      if (!handler(arg)) {
+        return false;
+      }
+    }
+    if (endInclusive && value == bounds_[1]) {
+      const Value& arg = value;
+      if (!handler(arg)) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  template<class Body>
+  void foreach(Body&& body) const {
+    Value value = bounds_[0];
+    for (;endless || value < bounds_[1]; ++value) {
+      const Value& arg = value;
+      body(arg);
+    }
+    if (endInclusive && value == bounds_[1]) {
+      const Value& arg = value;
+      body(arg);
+    }
+  }
+
+  static constexpr bool infinite = endless;
+};
+
+/**
+ * GenratorBuilder - Helper for GENERTATOR macro.
+ **/
+template<class Value>
+struct GeneratorBuilder {
+  template<class Source,
+           class Yield = detail::Yield<Value, Source>>
+  Yield operator+(Source&& source) {
+    return Yield(std::forward<Source>(source));
+  }
+};
+
+/**
+ * Yield - For producing values from a user-defined generator by way of a
+ * 'yield' function.
+ **/
+template<class Value, class Source>
+class Yield : public GenImpl<Value, Yield<Value, Source>> {
+  Source source_;
+ public:
+  explicit Yield(Source source)
+    : source_(std::move(source)) {
+  }
+
+  template<class Handler>
+  bool apply(Handler&& handler) const {
+    struct Break {};
+    auto body = [&](Value value) {
+      if (!handler(std::forward<Value>(value))) {
+        throw Break();
+      }
+    };
+    try {
+      source_(body);
+      return true;
+    } catch (Break&) {
+      return false;
+    }
+  }
+
+  template<class Body>
+  void foreach(Body&& body) const {
+    source_(std::forward<Body>(body));
+  }
+};
+
+template<class Value>
+class Empty : public GenImpl<Value, Empty<Value>> {
+ public:
+  template<class Handler>
+  bool apply(Handler&&) const { return true; }
+};
+
+/*
+ * Operators
+ */
+
+/**
+ * Map - For producing a sequence of values by passing each value from a source
+ * collection through a predicate.
+ *
+ * This type is usually used through the 'map' or 'mapped' helper function:
+ *
+ *   auto squares = seq(1, 10) | map(square) | asVector;
+ */
+template<class Predicate>
+class Map : public Operator<Map<Predicate>> {
+  Predicate pred_;
+ public:
+  Map() {}
+
+  explicit Map(Predicate pred)
+    : pred_(std::move(pred))
+  { }
+
+  template<class Value,
+           class Source,
+           class Result = typename ArgumentReference<
+                            typename std::result_of<Predicate(Value)>::type
+                          >::type>
+  class Generator :
+      public GenImpl<Result, Generator<Value, Source, Result>> {
+    Source source_;
+    Predicate pred_;
+  public:
+    explicit Generator(Source source, const Predicate& pred)
+      : source_(std::move(source)), pred_(pred) {}
+
+    template<class Body>
+    void foreach(Body&& body) const {
+      source_.foreach([&](Value value) {
+        body(pred_(std::forward<Value>(value)));
+      });
+    }
+
+    template<class Handler>
+    bool apply(Handler&& handler) const {
+      return source_.apply([&](Value value) {
+        return handler(pred_(std::forward<Value>(value)));
+      });
+    }
+
+    static constexpr bool infinite = Source::infinite;
+  };
+
+  template<class Source,
+           class Value,
+           class Gen = Generator<Value, Source>>
+  Gen compose(GenImpl<Value, Source>&& source) const {
+    return Gen(std::move(source.self()), pred_);
+  }
+
+  template<class Source,
+           class Value,
+           class Gen = Generator<Value, Source>>
+  Gen compose(const GenImpl<Value, Source>& source) const {
+    return Gen(source.self(), pred_);
+  }
+};
+
+
+/**
+ * Filter - For filtering values from a source sequence by a predicate.
+ *
+ * This type is usually used through the 'filter' helper function, like:
+ *
+ *   auto nonEmpty = from(strings)
+ *                 | filter([](const string& str) -> bool {
+ *                     return !str.empty();
+ *                   });
+ */
+template<class Predicate>
+class Filter : public Operator<Filter<Predicate>> {
+  Predicate pred_;
+ public:
+  Filter() {}
+  explicit Filter(Predicate pred)
+    : pred_(std::move(pred))
+  { }
+
+  template<class Value,
+           class Source>
+  class Generator : public GenImpl<Value, Generator<Value, Source>> {
+    Source source_;
+    Predicate pred_;
+  public:
+    explicit Generator(Source source, const Predicate& pred)
+      : source_(std::move(source)), pred_(pred) {}
+
+    template<class Body>
+    void foreach(Body&& body) const {
+      source_.foreach([&](Value value) {
+        if (pred_(std::forward<Value>(value))) {
+          body(std::forward<Value>(value));
+        }
+      });
+    }
+
+    template<class Handler>
+    bool apply(Handler&& handler) const {
+      return source_.apply([&](Value value) -> bool {
+        if (pred_(std::forward<Value>(value))) {
+          return handler(std::forward<Value>(value));
+        }
+        return true;
+      });
+    }
+
+    static constexpr bool infinite = Source::infinite;
+  };
+
+  template<class Source,
+           class Value,
+           class Gen = Generator<Value, Source>>
+  Gen compose(GenImpl<Value, Source>&& source) const {
+    return Gen(std::move(source.self()), pred_);
+  }
+
+  template<class Source,
+           class Value,
+           class Gen = Generator<Value, Source>>
+  Gen compose(const GenImpl<Value, Source>& source) const {
+    return Gen(source.self(), pred_);
+  }
+};
+
+/**
+ * Until - For producing values from a source until a predicate is satisfied.
+ *
+ * This type is usually used through the 'until' helper function, like:
+ *
+ *   auto best = from(sortedItems)
+ *             | until([](Item& item) { return item.score > 100; })
+ *             | asVector;
+ */
+template<class Predicate>
+class Until : public Operator<Until<Predicate>> {
+  Predicate pred_;
+ public:
+  Until() {}
+  explicit Until(Predicate pred)
+    : pred_(std::move(pred))
+  {}
+
+  template<class Value,
+           class Source>
+  class Generator : public GenImpl<Value, Generator<Value, Source>> {
+    Source source_;
+    Predicate pred_;
+   public:
+    explicit Generator(Source source, const Predicate& pred)
+      : source_(std::move(source)), pred_(pred) {}
+
+    template<class Handler>
+    bool apply(Handler&& handler) const {
+      bool cancelled = false;
+      source_.apply([&](Value value) -> bool {
+        if (pred_(value)) { // un-forwarded to disable move
+          return false;
+        }
+        if (!handler(std::forward<Value>(value))) {
+          cancelled = true;
+          return false;
+        }
+        return true;
+      });
+      return !cancelled;
+    }
+  };
+
+  template<class Source,
+           class Value,
+           class Gen = Generator<Value, Source>>
+  Gen compose(GenImpl<Value, Source>&& source) const {
+    return Gen(std::move(source.self()), pred_);
+  }
+
+  template<class Source,
+           class Value,
+           class Gen = Generator<Value, Source>>
+  Gen compose(const GenImpl<Value, Source>& source) const {
+    return Gen(source.self(), pred_);
+  }
+
+  // Theoretically an 'until' might stop an infinite
+  static constexpr bool infinite = false;
+};
+
+/**
+ * Take - For producing up to N values from a source.
+ *
+ * This type is usually used through the 'take' helper function, like:
+ *
+ *   auto best = from(docs)
+ *             | orderByDescending(scoreDoc)
+ *             | take(10);
+ */
+class Take : public Operator<Take> {
+  size_t count_;
+ public:
+  explicit Take(size_t count)
+    : count_(count) {}
+
+  template<class Value,
+           class Source>
+  class Generator :
+      public GenImpl<Value, Generator<Value, Source>> {
+    Source source_;
+    size_t count_;
+  public:
+    explicit Generator(Source source, size_t count)
+      : source_(std::move(source)) , count_(count) {}
+
+    template<class Handler>
+    bool apply(Handler&& handler) const {
+      if (count_ == 0) { return false; }
+      size_t n = count_;
+      bool cancelled = false;
+      source_.apply([&](Value value) -> bool {
+        if (!handler(std::forward<Value>(value))) {
+          cancelled = true;
+          return false;
+        }
+        return --n;
+      });
+      return !cancelled;
+    }
+  };
+
+  template<class Source,
+           class Value,
+           class Gen = Generator<Value, Source>>
+  Gen compose(GenImpl<Value, Source>&& source) const {
+    return Gen(std::move(source.self()), count_);
+  }
+
+  template<class Source,
+           class Value,
+           class Gen = Generator<Value, Source>>
+  Gen compose(const GenImpl<Value, Source>& source) const {
+    return Gen(source.self(), count_);
+  }
+};
+
+/**
+ * Sample - For taking a random sample of N elements from a sequence
+ * (without replacement).
+ */
+template<class Random>
+class Sample : public Operator<Sample<Random>> {
+  size_t count_;
+  Random rng_;
+ public:
+  explicit Sample(size_t count, Random rng)
+    : count_(count), rng_(std::move(rng)) {}
+
+  template<class Value,
+           class Source,
+           class Rand,
+           class StorageType = typename std::decay<Value>::type>
+  class Generator :
+          public GenImpl<StorageType&&,
+                         Generator<Value, Source, Rand, StorageType>> {
+    static_assert(!Source::infinite, "Cannot sample infinite source!");
+    // It's too easy to bite ourselves if random generator is only 16-bit
+    static_assert(Random::max() >= std::numeric_limits<int32_t>::max() - 1,
+                  "Random number generator must support big values");
+    Source source_;
+    size_t count_;
+    mutable Rand rng_;
+  public:
+    explicit Generator(Source source, size_t count, Random rng)
+      : source_(std::move(source)) , count_(count), rng_(std::move(rng)) {}
+
+    template<class Handler>
+    bool apply(Handler&& handler) const {
+      if (count_ == 0) { return false; }
+      std::vector<StorageType> v;
+      v.reserve(count_);
+      // use reservoir sampling to give each source value an equal chance
+      // of appearing in our output.
+      size_t n = 1;
+      source_.foreach([&](Value value) -> void {
+          if (v.size() < count_) {
+            v.push_back(std::forward<Value>(value));
+          } else {
+            // alternatively, we could create a std::uniform_int_distribution
+            // instead of using modulus, but benchmarks show this has
+            // substantial overhead.
+            size_t index = rng_() % n;
+            if (index < v.size()) {
+              v[index] = std::forward<Value>(value);
+            }
+          }
+          ++n;
+        });
+
+      // output is unsorted!
+      for (auto& val: v) {
+        if (!handler(std::move(val))) {
+          return false;
+        }
+      }
+      return true;
+    }
+  };
+
+  template<class Source,
+           class Value,
+           class Gen = Generator<Value, Source, Random>>
+  Gen compose(GenImpl<Value, Source>&& source) const {
+    return Gen(std::move(source.self()), count_, rng_);
+  }
+
+  template<class Source,
+           class Value,
+           class Gen = Generator<Value, Source, Random>>
+  Gen compose(const GenImpl<Value, Source>& source) const {
+    return Gen(source.self(), count_, rng_);
+  }
+};
+
+/**
+ * Skip - For skipping N items from the beginning of a source generator.
+ *
+ * This type is usually used through the 'skip' helper function, like:
+ *
+ *   auto page = from(results)
+ *             | skip(pageSize * startPage)
+ *             | take(10);
+ */
+class Skip : public Operator<Skip> {
+  size_t count_;
+ public:
+  explicit Skip(size_t count)
+    : count_(count) {}
+
+  template<class Value,
+           class Source>
+  class Generator :
+      public GenImpl<Value, Generator<Value, Source>> {
+    Source source_;
+    size_t count_;
+   public:
+    explicit Generator(Source source, size_t count)
+      : source_(std::move(source)) , count_(count) {}
+
+    template<class Body>
+    void foreach(Body&& body) const {
+      if (count_ == 0) {
+        source_.foreach(body);
+        return;
+      }
+      size_t n = 0;
+      source_.foreach([&](Value value) {
+          if (n < count_) {
+            ++n;
+          } else {
+            body(std::forward<Value>(value));
+          }
+        });
+    }
+
+    template<class Handler>
+    bool apply(Handler&& handler) const {
+      if (count_ == 0) {
+        return source_.apply(std::forward<Handler>(handler));
+      }
+      size_t n = 0;
+      return source_.apply([&](Value value) -> bool {
+          if (n < count_) {
+            ++n;
+            return true;
+          }
+          return handler(std::forward<Value>(value));
+        });
+    }
+
+    static constexpr bool infinite = Source::infinite;
+  };
+
+  template<class Source,
+           class Value,
+           class Gen = Generator<Value, Source>>
+  Gen compose(GenImpl<Value, Source>&& source) const {
+    return Gen(std::move(source.self()), count_);
+  }
+
+  template<class Source,
+           class Value,
+           class Gen = Generator<Value, Source>>
+  Gen compose(const GenImpl<Value, Source>& source) const {
+    return Gen(source.self(), count_);
+  }
+};
+
+/**
+ * Order - For ordering a sequence of values from a source by key.
+ * The key is extracted by the given selector functor, and this key is then
+ * compared using the specified comparator.
+ *
+ * This type is usually used through the 'order' helper function, like:
+ *
+ *   auto closest = from(places)
+ *                | orderBy([](Place& p) {
+ *                    return -distance(p.location, here);
+ *                  })
+ *                | take(10);
+ */
+template<class Selector, class Comparer>
+class Order : public Operator<Order<Selector, Comparer>> {
+  Selector selector_;
+  Comparer comparer_;
+ public:
+  Order() {}
+
+  explicit Order(Selector selector)
+    : selector_(std::move(selector))
+  {}
+
+  Order(Selector selector,
+        Comparer comparer)
+    : selector_(std::move(selector))
+    , comparer_(std::move(comparer))
+  {}
+
+  template<class Value,
+           class Source,
+           class StorageType = typename std::decay<Value>::type,
+           class Result = typename std::result_of<Selector(Value)>::type>
+  class Generator :
+    public GenImpl<StorageType&&,
+                   Generator<Value, Source, StorageType, Result>> {
+    static_assert(!Source::infinite, "Cannot sort infinite source!");
+    Source source_;
+    Selector selector_;
+    Comparer comparer_;
+
+    typedef std::vector<StorageType> VectorType;
+
+    VectorType asVector() const {
+      auto comparer = [&](const StorageType& a, const StorageType& b) {
+        return comparer_(selector_(a), selector_(b));
+      };
+      auto vals = source_ | as<VectorType>();
+      std::sort(vals.begin(), vals.end(), comparer);
+      return std::move(vals);
+    }
+   public:
+    Generator(Source source,
+              Selector selector,
+              Comparer comparer)
+      : source_(std::move(source)),
+        selector_(std::move(selector)),
+        comparer_(std::move(comparer)) {}
+
+    VectorType operator|(const Collect<VectorType>&) const {
+      return asVector();
+    }
+
+    VectorType operator|(const CollectTemplate<std::vector>&) const {
+      return asVector();
+    }
+
+    template<class Body>
+    void foreach(Body&& body) const {
+      for (auto& value : asVector()) {
+        body(std::move(value));
+      }
+    }
+
+    template<class Handler>
+    bool apply(Handler&& handler) const {
+      auto comparer = [&](const StorageType& a, const StorageType& b) {
+        // swapped for minHeap
+        return comparer_(selector_(b), selector_(a));
+      };
+      auto heap = source_ | as<VectorType>();
+      std::make_heap(heap.begin(), heap.end(), comparer);
+      while (!heap.empty()) {
+        std::pop_heap(heap.begin(), heap.end(), comparer);
+        if (!handler(std::move(heap.back()))) {
+          return false;
+        }
+        heap.pop_back();
+      }
+      return true;
+    }
+  };
+
+  template<class Source,
+           class Value,
+           class Gen = Generator<Value, Source>>
+  Gen compose(GenImpl<Value, Source>&& source) const {
+    return Gen(std::move(source.self()), selector_, comparer_);
+  }
+
+  template<class Source,
+           class Value,
+           class Gen = Generator<Value, Source>>
+  Gen compose(const GenImpl<Value, Source>& source) const {
+    return Gen(source.self(), selector_, comparer_);
+  }
+};
+
+/*
+ * TypeAssertion - For verifying the exact type of the value produced by a
+ * generator. Useful for testing and debugging, and acts as a no-op at runtime.
+ * Pass-through at runtime. Used through the 'assert_type<>()' factory method
+ * like so:
+ *
+ *   auto c =  from(vector) | assert_type<int&>() | sum;
+ *
+ */
+template<class Expected>
+class TypeAssertion : public Operator<TypeAssertion<Expected>> {
+ public:
+  template<class Source, class Value>
+  const Source& compose(const GenImpl<Value, Source>& source) const {
+    static_assert(std::is_same<Expected, Value>::value,
+                  "assert_type() check failed");
+    return source.self();
+  }
+
+  template<class Source, class Value>
+  Source&& compose(GenImpl<Value, Source>&& source) const {
+    static_assert(std::is_same<Expected, Value>::value,
+                  "assert_type() check failed");
+    return std::move(source.self());
+  }
+};
+
+/**
+ * Distinct - For filtering duplicates out of a sequence. A selector may be
+ * provided to generate a key to uniquify for each value.
+ *
+ * This type is usually used through the 'distinct' helper function, like:
+ *
+ *   auto closest = from(results)
+ *                | distinctBy([](Item& i) {
+ *                    return i.target;
+ *                  })
+ *                | take(10);
+ */
+template<class Selector>
+class Distinct : public Operator<Distinct<Selector>> {
+  Selector selector_;
+ public:
+  Distinct() {}
+
+  explicit Distinct(Selector selector)
+    : selector_(std::move(selector))
+  {}
+
+  template<class Value,
+           class Source>
+  class Generator : public GenImpl<Value, Generator<Value, Source>> {
+    Source source_;
+    Selector selector_;
+
+    typedef typename std::decay<Value>::type StorageType;
+
+    // selector_ cannot be passed an rvalue or it would end up passing the husk
+    // of a value to the downstream operators.
+    typedef const StorageType& ParamType;
+
+    typedef typename std::result_of<Selector(ParamType)>::type KeyType;
+    typedef typename std::decay<KeyType>::type KeyStorageType;
+
+   public:
+    Generator(Source source,
+              Selector selector)
+      : source_(std::move(source)),
+        selector_(std::move(selector)) {}
+
+    template<class Body>
+    void foreach(Body&& body) const {
+      std::unordered_set<KeyStorageType> keysSeen;
+      source_.foreach([&](Value value) {
+        if (keysSeen.insert(selector_(ParamType(value))).second) {
+          body(std::forward<Value>(value));
+        }
+      });
+    }
+
+    template<class Handler>
+    bool apply(Handler&& handler) const {
+      std::unordered_set<KeyStorageType> keysSeen;
+      return source_.apply([&](Value value) -> bool {
+        if (keysSeen.insert(selector_(ParamType(value))).second) {
+          return handler(std::forward<Value>(value));
+        }
+        return true;
+      });
+    }
+  };
+
+  template<class Source,
+           class Value,
+           class Gen = Generator<Value, Source>>
+  Gen compose(GenImpl<Value, Source>&& source) const {
+    return Gen(std::move(source.self()), selector_);
+  }
+
+  template<class Source,
+           class Value,
+           class Gen = Generator<Value, Source>>
+  Gen compose(const GenImpl<Value, Source>& source) const {
+    return Gen(source.self(), selector_);
+  }
+};
+
+/**
+ * Batch - For producing fixed-size batches of each value from a source.
+ *
+ * This type is usually used through the 'batch' helper function:
+ *
+ *   auto batchSums
+ *     = seq(1, 10)
+ *     | batch(3)
+ *     | map([](const std::vector<int>& batch) {
+ *         return from(batch) | sum;
+ *       })
+ *     | as<vector>();
+ */
+class Batch : public Operator<Batch> {
+  size_t batchSize_;
+ public:
+  explicit Batch(size_t batchSize)
+    : batchSize_(batchSize) {
+    if (batchSize_ == 0) {
+      throw std::invalid_argument("Batch size must be non-zero!");
+    }
+  }
+
+  template<class Value,
+           class Source,
+           class StorageType = typename std::decay<Value>::type,
+           class VectorType = std::vector<StorageType>>
+  class Generator :
+      public GenImpl<VectorType&,
+                     Generator<Value, Source, StorageType, VectorType>> {
+    Source source_;
+    size_t batchSize_;
+  public:
+    explicit Generator(Source source, size_t batchSize)
+      : source_(std::move(source))
+      , batchSize_(batchSize) {}
+
+    template<class Handler>
+    bool apply(Handler&& handler) const {
+      VectorType batch_;
+      batch_.reserve(batchSize_);
+      bool shouldContinue = source_.apply([&](Value value) -> bool {
+          batch_.push_back(std::forward<Value>(value));
+          if (batch_.size() == batchSize_) {
+            bool needMore = handler(batch_);
+            batch_.clear();
+            return needMore;
+          }
+          // Always need more if the handler is not called.
+          return true;
+        });
+      // Flush everything, if and only if `handler` hasn't returned false.
+      if (shouldContinue && !batch_.empty()) {
+        shouldContinue = handler(batch_);
+        batch_.clear();
+      }
+      return shouldContinue;
+    }
+
+    static constexpr bool infinite = Source::infinite;
+  };
+
+  template<class Source,
+           class Value,
+           class Gen = Generator<Value, Source>>
+  Gen compose(GenImpl<Value, Source>&& source) const {
+    return Gen(std::move(source.self()), batchSize_);
+  }
+
+  template<class Source,
+           class Value,
+           class Gen = Generator<Value, Source>>
+  Gen compose(const GenImpl<Value, Source>& source) const {
+    return Gen(source.self(), batchSize_);
+  }
+};
+/*
+ * Sinks
+ */
+
+/**
+ * FoldLeft - Left-associative functional fold. For producing an aggregate value
+ * from a seed and a folder function. Useful for custom aggregators on a
+ * sequence.
+ *
+ * This type is primarily used through the 'foldl' helper method, like:
+ *
+ *   double movingAverage = from(values)
+ *                        | foldl(0.0, [](double avg, double sample) {
+ *                            return sample * 0.1 + avg * 0.9;
+ *                          });
+ */
+template<class Seed,
+         class Fold>
+class FoldLeft : public Operator<FoldLeft<Seed, Fold>> {
+  Seed seed_;
+  Fold fold_;
+ public:
+  FoldLeft() {}
+  FoldLeft(Seed seed,
+           Fold fold)
+    : seed_(std::move(seed))
+    , fold_(std::move(fold))
+  {}
+
+  template<class Source,
+           class Value>
+  Seed compose(const GenImpl<Value, Source>& source) const {
+    static_assert(!Source::infinite, "Cannot foldl infinite source");
+    Seed accum = seed_;
+    source | [&](Value v) {
+      accum = fold_(std::move(accum), std::forward<Value>(v));
+    };
+    return accum;
+  }
+};
+
+/**
+ * First - For finding the first value in a sequence.
+ *
+ * This type is primarily used through the 'first' static value, like:
+ *
+ *   int firstThreeDigitPrime = seq(100) | filter(isPrime) | first;
+ */
+class First : public Operator<First> {
+ public:
+  First() { }
+
+  template<class Source,
+           class Value,
+           class StorageType = typename std::decay<Value>::type>
+  StorageType compose(const GenImpl<Value, Source>& source) const {
+    Optional<StorageType> accum;
+    source | [&](Value v) -> bool {
+      accum = std::forward<Value>(v);
+      return false;
+    };
+    if (!accum.hasValue()) {
+      throw EmptySequence();
+    }
+    return std::move(accum.value());
+  }
+};
+
+
+/**
+ * Any - For determining whether any values in a sequence satisfy a predicate.
+ *
+ * This type is primarily used through the 'any' static value, like:
+ *
+ *   bool any20xPrimes = seq(200, 210) | filter(isPrime) | any;
+ *
+ * Note that it may also be used like so:
+ *
+ *   bool any20xPrimes = seq(200, 210) | any(isPrime);
+ *
+ */
+class Any : public Operator<Any> {
+ public:
+  Any() { }
+
+  template<class Source,
+           class Value>
+  bool compose(const GenImpl<Value, Source>& source) const {
+    bool any = false;
+    source | [&](Value v) -> bool {
+      any = true;
+      return false;
+    };
+    return any;
+  }
+
+  /**
+   * Convenience function for use like:
+   *
+   *  bool found = gen | any([](int i) { return i * i > 100; });
+   */
+  template<class Predicate,
+           class Filter = Filter<Predicate>,
+           class Composed = Composed<Filter, Any>>
+  Composed operator()(Predicate pred) const {
+    return Composed(Filter(std::move(pred)), Any());
+  }
+};
+
+/**
+ * All - For determining whether all values in a sequence satisfy a predicate.
+ *
+ * This type is primarily used through the 'any' static value, like:
+ *
+ *   bool valid = from(input) | all(validate);
+ *
+ * Note: Passing an empty sequence through 'all()' will always return true.
+ */
+template<class Predicate>
+class All : public Operator<All<Predicate>> {
+  Predicate pred_;
+ public:
+  All() {}
+  explicit All(Predicate pred)
+    : pred_(std::move(pred))
+  { }
+
+  template<class Source,
+           class Value>
+  bool compose(const GenImpl<Value, Source>& source) const {
+    static_assert(!Source::infinite, "Cannot call 'all' on infinite source");
+    bool all = true;
+    source | [&](Value v) -> bool {
+      if (!pred_(std::forward<Value>(v))) {
+        all = false;
+        return false;
+      }
+      return true;
+    };
+    return all;
+  }
+};
+
+/**
+ * Reduce - Functional reduce, for recursively combining values from a source
+ * using a reducer function until there is only one item left. Useful for
+ * combining values when an empty sequence doesn't make sense.
+ *
+ * This type is primarily used through the 'reduce' helper method, like:
+ *
+ *   sring longest = from(names)
+ *                 | reduce([](string&& best, string& current) {
+ *                     return best.size() >= current.size() ? best : current;
+ *                   });
+ */
+template<class Reducer>
+class Reduce : public Operator<Reduce<Reducer>> {
+  Reducer reducer_;
+ public:
+  Reduce() {}
+  explicit Reduce(Reducer reducer)
+    : reducer_(std::move(reducer))
+  {}
+
+  template<class Source,
+           class Value,
+           class StorageType = typename std::decay<Value>::type>
+  StorageType compose(const GenImpl<Value, Source>& source) const {
+    Optional<StorageType> accum;
+    source | [&](Value v) {
+      if (accum.hasValue()) {
+        accum = reducer_(std::move(accum.value()), std::forward<Value>(v));
+      } else {
+        accum = std::forward<Value>(v);
+      }
+    };
+    if (!accum.hasValue()) {
+      throw EmptySequence();
+    }
+    return accum.value();
+  }
+};
+
+/**
+ * Count - for simply counting the items in a collection.
+ *
+ * This type is usually used through its singleton, 'count':
+ *
+ *   auto shortPrimes = seq(1, 100) | filter(isPrime) | count;
+ */
+class Count : public Operator<Count> {
+ public:
+  Count() { }
+
+  template<class Source,
+           class Value>
+  size_t compose(const GenImpl<Value, Source>& source) const {
+    static_assert(!Source::infinite, "Cannot count infinite source");
+    return foldl(size_t(0),
+                 [](size_t accum, Value v) {
+                   return accum + 1;
+                 }).compose(source);
+  }
+};
+
+/**
+ * Sum - For simply summing up all the values from a source.
+ *
+ * This type is usually used through its singleton, 'sum':
+ *
+ *   auto gaussSum = seq(1, 100) | sum;
+ */
+class Sum : public Operator<Sum> {
+ public:
+  Sum() : Operator<Sum>() {}
+
+  template<class Source,
+           class Value,
+           class StorageType = typename std::decay<Value>::type>
+  StorageType compose(const GenImpl<Value, Source>& source) const {
+    static_assert(!Source::infinite, "Cannot sum infinite source");
+    return foldl(StorageType(0),
+                 [](StorageType&& accum, Value v) {
+                   return std::move(accum) + std::forward<Value>(v);
+                 }).compose(source);
+  }
+};
+
+/**
+ * Contains - For testing whether a value matching the given value is contained
+ * in a sequence.
+ *
+ * This type should be used through the 'contains' helper method, like:
+ *
+ *   bool contained = seq(1, 10) | map(square) | contains(49);
+ */
+template<class Needle>
+class Contains : public Operator<Contains<Needle>> {
+  Needle needle_;
+ public:
+  explicit Contains(Needle needle)
+    : needle_(std::move(needle))
+  {}
+
+  template<class Source,
+           class Value,
+           class StorageType = typename std::decay<Value>::type>
+  bool compose(const GenImpl<Value, Source>& source) const {
+    static_assert(!Source::infinite,
+                  "Calling contains on an infinite source might cause "
+                  "an infinite loop.");
+    return !(source | [this](Value value) {
+        return !(needle_ == std::forward<Value>(value));
+      });
+  }
+};
+
+/**
+ * Min - For a value which minimizes a key, where the key is determined by a
+ * given selector, and compared by given comparer.
+ *
+ * This type is usually used through the singletone 'min' or through the helper
+ * functions 'minBy' and 'maxBy'.
+ *
+ *   auto oldest = from(people)
+ *               | minBy([](Person& p) {
+ *                   return p.dateOfBirth;
+ *                 });
+ */
+template<class Selector,
+         class Comparer>
+class Min : public Operator<Min<Selector, Comparer>> {
+  Selector selector_;
+  Comparer comparer_;
+ public:
+  Min() {}
+
+  explicit Min(Selector selector)
+    : selector_(std::move(selector))
+  {}
+
+  Min(Selector selector,
+        Comparer comparer)
+    : selector_(std::move(selector))
+    , comparer_(std::move(comparer))
+  {}
+
+  template<class Value,
+           class Source,
+           class StorageType = typename std::decay<Value>::type,
+           class Key = typename std::decay<
+               typename std::result_of<Selector(Value)>::type
+             >::type>
+  StorageType compose(const GenImpl<Value, Source>& source) const {
+    Optional<StorageType> min;
+    Optional<Key> minKey;
+    source | [&](Value v) {
+      Key key = selector_(std::forward<Value>(v));
+      if (!minKey.hasValue() || comparer_(key, minKey.value())) {
+        minKey = key;
+        min = std::forward<Value>(v);
+      }
+    };
+    if (!min.hasValue()) {
+      throw EmptySequence();
+    }
+    return min.value();
+  }
+};
+
+/**
+ * Append - For collecting values from a source into a given output container
+ * by appending.
+ *
+ * This type is usually used through the helper function 'appendTo', like:
+ *
+ *   vector<int64_t> ids;
+ *   from(results) | map([](Person& p) { return p.id })
+ *                 | appendTo(ids);
+ */
+template<class Collection>
+class Append : public Operator<Append<Collection>> {
+  Collection* collection_;
+ public:
+  explicit Append(Collection* collection)
+    : collection_(collection)
+  {}
+
+  template<class Value,
+           class Source>
+  Collection& compose(const GenImpl<Value, Source>& source) const {
+    source | [&](Value v) {
+      collection_->insert(collection_->end(), std::forward<Value>(v));
+    };
+    return *collection_;
+  }
+};
+
+/**
+ * Collect - For collecting values from a source in a collection of the desired
+ * type.
+ *
+ * This type is usually used through the helper function 'as', like:
+ *
+ *   std::string upper = from(stringPiece)
+ *                     | map(&toupper)
+ *                     | as<std::string>();
+ */
+template<class Collection>
+class Collect : public Operator<Collect<Collection>> {
+ public:
+  Collect() { }
+
+  template<class Value,
+           class Source,
+           class StorageType = typename std::decay<Value>::type>
+  Collection compose(const GenImpl<Value, Source>& source) const {
+    Collection collection;
+    source | [&](Value v) {
+      collection.insert(collection.end(), std::forward<Value>(v));
+    };
+    return collection;
+  }
+};
+
+
+/**
+ * CollectTemplate - For collecting values from a source in a collection
+ * constructed using the specified template type. Given the type of values
+ * produced by the given generator, the collection type will be:
+ *   Container<Value, Allocator<Value>>
+ *
+ * The allocator defaults to std::allocator, so this may be used for the STL
+ * containers by simply using operators like 'as<set>', 'as<deque>',
+ * 'as<vector>'. 'as', here is the helper method which is the usual means of
+ * consturcting this operator.
+ *
+ * Example:
+ *
+ *   set<string> uniqueNames = from(names) | as<set>();
+ */
+template<template<class, class> class Container,
+         template<class> class Allocator>
+class CollectTemplate : public Operator<CollectTemplate<Container, Allocator>> {
+ public:
+  CollectTemplate() { }
+
+  template<class Value,
+           class Source,
+           class StorageType = typename std::decay<Value>::type,
+           class Collection = Container<StorageType, Allocator<StorageType>>>
+  Collection compose(const GenImpl<Value, Source>& source) const {
+    Collection collection;
+    source | [&](Value v) {
+      collection.insert(collection.end(), std::forward<Value>(v));
+    };
+    return collection;
+  }
+};
+
+/**
+ * Concat - For flattening generators of generators.
+ *
+ * This type is usually used through the 'concat' static value, like:
+ *
+ *   auto edges =
+ *       from(nodes)
+ *     | map([](Node& x) {
+ *           return from(x.neighbors)
+ *                | map([&](Node& y) {
+ *                    return Edge(x, y);
+ *                  });
+ *         })
+ *     | concat
+ *     | as<std::set>();
+ */
+class Concat : public Operator<Concat> {
+ public:
+  Concat() { }
+
+  template<class Inner,
+           class Source,
+           class InnerValue = typename std::decay<Inner>::type::ValueType>
+  class Generator :
+      public GenImpl<InnerValue, Generator<Inner, Source, InnerValue>> {
+    Source source_;
+   public:
+    explicit Generator(Source source)
+      : source_(std::move(source)) {}
+
+    template<class Handler>
+    bool apply(Handler&& handler) const {
+      return source_.apply([&](Inner inner) -> bool {
+          return inner.apply(std::forward<Handler>(handler));
+        });
+    }
+
+    template<class Body>
+    void foreach(Body&& body) const {
+      source_.foreach([&](Inner inner) {
+          inner.foreach(std::forward<Body>(body));
+        });
+    }
+
+    static constexpr bool infinite = Source::infinite;
+  };
+
+  template<class Value,
+           class Source,
+           class Gen = Generator<Value, Source>>
+  Gen compose(GenImpl<Value, Source>&& source) const {
+    return Gen(std::move(source.self()));
+  }
+
+  template<class Value,
+           class Source,
+           class Gen = Generator<Value, Source>>
+  Gen compose(const GenImpl<Value, Source>& source) const {
+    return Gen(source.self());
+  }
+};
+
+/**
+ * RangeConcat - For flattening generators of iterables.
+ *
+ * This type is usually used through the 'rconcat' static value, like:
+ *
+ *   map<int, vector<int>> adjacency;
+ *   auto sinks =
+ *       from(adjacency)
+ *     | get<1>()
+ *     | rconcat()
+ *     | as<std::set>();
+ */
+class RangeConcat : public Operator<RangeConcat> {
+ public:
+  RangeConcat() { }
+
+  template<class Range,
+           class Source,
+           class InnerValue = typename ValueTypeOfRange<Range>::RefType>
+  class Generator
+    : public GenImpl<InnerValue, Generator<Range, Source, InnerValue>> {
+    Source source_;
+   public:
+    explicit Generator(Source source)
+      : source_(std::move(source)) {}
+
+    template<class Body>
+    void foreach(Body&& body) const {
+      source_.foreach([&](Range range) {
+          for (auto& value : range) {
+            body(value);
+          }
+        });
+    }
+
+    template<class Handler>
+    bool apply(Handler&& handler) const {
+      return source_.apply([&](Range range) -> bool {
+          for (auto& value : range) {
+            if (!handler(value)) {
+              return false;
+            }
+          }
+          return true;
+        });
+    }
+  };
+
+  template<class Value,
+           class Source,
+           class Gen = Generator<Value, Source>>
+  Gen compose(GenImpl<Value, Source>&& source) const {
+    return Gen(std::move(source.self()));
+  }
+
+  template<class Value,
+           class Source,
+           class Gen = Generator<Value, Source>>
+  Gen compose(const GenImpl<Value, Source>& source) const {
+    return Gen(source.self());
+  }
+};
+
+
+/**
+ * GuardImpl - For handling exceptions from downstream computation. Requires the
+ * type of exception to catch, and handler function to invoke in the event of
+ * the exception. Note that the handler may:
+ *   1) return true to continue processing the sequence
+ *   2) return false to end the sequence immediately
+ *   3) throw, to pass the exception to the next catch
+ * The handler must match the signature 'bool(Exception&, Value)'.
+ *
+ * This type is used through the `guard` helper, like so:
+ *
+ *  auto indexes
+ *    = byLine(STDIN_FILENO)
+ *    | guard<std::runtime_error>([](std::runtime_error& e,
+ *                                   StringPiece sp) {
+ *        LOG(ERROR) << sp << ": " << e.str();
+ *        return true; // continue processing subsequent lines
+ *      })
+ *    | eachTo<int>()
+ *    | as<vector>();
+ *
+ *  TODO(tjackson): Rename this back to Guard.
+ **/
+template<class Exception,
+         class ErrorHandler>
+class GuardImpl : public Operator<GuardImpl<Exception, ErrorHandler>> {
+  ErrorHandler handler_;
+ public:
+  GuardImpl(ErrorHandler handler)
+    : handler_(std::move(handler)) {}
+
+  template<class Value,
+           class Source>
+  class Generator : public GenImpl<Value, Generator<Value, Source>> {
+    Source source_;
+    ErrorHandler handler_;
+  public:
+    explicit Generator(Source source,
+                       ErrorHandler handler)
+      : source_(std::move(source)),
+        handler_(std::move(handler)) {}
+
+    template<class Handler>
+    bool apply(Handler&& handler) const {
+      return source_.apply([&](Value value) -> bool {
+        try {
+          handler(std::forward<Value>(value));
+          return true;
+        } catch (Exception& e) {
+          return handler_(e, std::forward<Value>(value));
+        }
+      });
+    }
+
+    static constexpr bool infinite = Source::infinite;
+  };
+
+  template<class Value,
+           class Source,
+           class Gen = Generator<Value, Source>>
+  Gen compose(GenImpl<Value, Source>&& source) const {
+    return Gen(std::move(source.self()), handler_);
+  }
+
+  template<class Value,
+           class Source,
+           class Gen = Generator<Value, Source>>
+  Gen compose(const GenImpl<Value, Source>& source) const {
+    return Gen(source.self(), handler_);
+  }
+};
+
+/**
+ * Cycle - For repeating a sequence forever.
+ *
+ * This type is usually used through the 'cycle' static value, like:
+ *
+ *   auto tests
+ *     = from(samples)
+ *     | cycle
+ *     | take(100);
+ */
+class Cycle : public Operator<Cycle> {
+  off_t limit_; // -1 for infinite
+ public:
+  Cycle()
+    : limit_(-1) { }
+
+  explicit Cycle(off_t limit)
+    : limit_(limit) { }
+
+  template<class Value,
+           class Source>
+  class Generator : public GenImpl<Value, Generator<Value, Source>> {
+    Source source_;
+    off_t limit_; // -1 for infinite
+  public:
+    explicit Generator(Source source, off_t limit)
+      : source_(std::move(source))
+      , limit_(limit) {}
+
+    template<class Handler>
+    bool apply(Handler&& handler) const {
+      bool cont;
+      auto handler2 = [&](Value value) {
+        cont = handler(std::forward<Value>(value));
+        return cont;
+      };
+      for (off_t count = 0; count != limit_; ++count) {
+        cont = false;
+        source_.apply(handler2);
+        if (!cont) {
+          return false;
+        }
+      }
+      return true;
+    }
+
+    // not actually infinite, since an empty generator will end the cycles.
+    static constexpr bool infinite = Source::infinite;
+  };
+
+  template<class Source,
+           class Value,
+           class Gen = Generator<Value, Source>>
+  Gen compose(GenImpl<Value, Source>&& source) const {
+    return Gen(std::move(source.self()), limit_);
+  }
+
+  template<class Source,
+           class Value,
+           class Gen = Generator<Value, Source>>
+  Gen compose(const GenImpl<Value, Source>& source) const {
+    return Gen(source.self(), limit_);
+  }
+
+  /**
+   * Convenience function for use like:
+   *
+   *  auto tripled = gen | cycle(3);
+   */
+  Cycle operator()(off_t limit) const {
+    return Cycle(limit);
+  }
+};
+
+/**
+ * Dereference - For dereferencing a sequence of pointers while filtering out
+ * null pointers.
+ *
+ * This type is usually used through the 'dereference' static value, like:
+ *
+ *   auto refs = from(ptrs) | dereference;
+ */
+class Dereference : public Operator<Dereference> {
+ public:
+  Dereference() {}
+
+  template<class Value,
+           class Source,
+           class Result = decltype(*std::declval<Value>())>
+  class Generator : public GenImpl<Result, Generator<Value, Source, Result>> {
+    Source source_;
+  public:
+    explicit Generator(Source source)
+      : source_(std::move(source)) {}
+
+    template<class Body>
+    void foreach(Body&& body) const {
+      source_.foreach([&](Value value) {
+        if (value) {
+          return body(*value);
+        }
+      });
+    }
+
+    template<class Handler>
+    bool apply(Handler&& handler) const {
+      return source_.apply([&](Value value) -> bool {
+        if (value) {
+          return handler(*value);
+        }
+        return true;
+      });
+    }
+
+    // not actually infinite, since an empty generator will end the cycles.
+    static constexpr bool infinite = Source::infinite;
+  };
+
+  template<class Source,
+           class Value,
+           class Gen = Generator<Value, Source>>
+  Gen compose(GenImpl<Value, Source>&& source) const {
+    return Gen(std::move(source.self()));
+  }
+
+  template<class Source,
+           class Value,
+           class Gen = Generator<Value, Source>>
+  Gen compose(const GenImpl<Value, Source>& source) const {
+    return Gen(source.self());
+  }
+};
+
+} //::detail
+
+/**
+ * VirtualGen<T> - For wrapping template types in simple polymorphic wrapper.
+ **/
+template<class Value>
+class VirtualGen : public GenImpl<Value, VirtualGen<Value>> {
+  class WrapperBase {
+   public:
+    virtual ~WrapperBase() {}
+    virtual bool apply(const std::function<bool(Value)>& handler) const = 0;
+    virtual void foreach(const std::function<void(Value)>& body) const = 0;
+    virtual std::unique_ptr<const WrapperBase> clone() const = 0;
+  };
+
+  template<class Wrapped>
+  class WrapperImpl : public WrapperBase {
+    Wrapped wrapped_;
+   public:
+    explicit WrapperImpl(Wrapped wrapped)
+     : wrapped_(std::move(wrapped)) {
+    }
+
+    virtual bool apply(const std::function<bool(Value)>& handler) const {
+      return wrapped_.apply(handler);
+    }
+
+    virtual void foreach(const std::function<void(Value)>& body) const {
+      wrapped_.foreach(body);
+    }
+
+    virtual std::unique_ptr<const WrapperBase> clone() const {
+      return std::unique_ptr<const WrapperBase>(new WrapperImpl(wrapped_));
+    }
+  };
+
+  std::unique_ptr<const WrapperBase> wrapper_;
+
+ public:
+  template<class Self>
+  /* implicit */ VirtualGen(Self source)
+   : wrapper_(new WrapperImpl<Self>(std::move(source)))
+  { }
+
+  VirtualGen(VirtualGen&& source)
+   : wrapper_(std::move(source.wrapper_))
+  { }
+
+  VirtualGen(const VirtualGen& source)
+   : wrapper_(source.wrapper_->clone())
+  { }
+
+  VirtualGen& operator=(const VirtualGen& source) {
+    wrapper_.reset(source.wrapper_->clone());
+    return *this;
+  }
+
+  VirtualGen& operator=(VirtualGen&& source) {
+    wrapper_= std::move(source.wrapper_);
+    return *this;
+  }
+
+  bool apply(const std::function<bool(Value)>& handler) const {
+    return wrapper_->apply(handler);
+  }
+
+  void foreach(const std::function<void(Value)>& body) const {
+    wrapper_->foreach(body);
+  }
+};
+
+/**
+ * non-template operators, statically defined to avoid the need for anything but
+ * the header.
+ */
+static const detail::Sum sum;
+
+static const detail::Count count;
+
+static const detail::First first;
+
+/**
+ * Use directly for detecting any values, or as a function to detect values
+ * which pass a predicate:
+ *
+ *  auto nonempty = g | any;
+ *  auto evens = g | any(even);
+ */
+static const detail::Any any;
+
+static const detail::Min<Identity, Less> min;
+
+static const detail::Min<Identity, Greater> max;
+
+static const detail::Order<Identity> order;
+
+static const detail::Distinct<Identity> distinct;
+
+static const detail::Map<Move> move;
+
+static const detail::Concat concat;
+
+static const detail::RangeConcat rconcat;
+
+/**
+ * Use directly for infinite sequences, or as a function to limit cycle count.
+ *
+ *  auto forever = g | cycle;
+ *  auto thrice = g | cycle(3);
+ */
+static const detail::Cycle cycle;
+
+static const detail::Dereference dereference;
+
+inline detail::Take take(size_t count) {
+  return detail::Take(count);
+}
+
+template<class Random = std::default_random_engine>
+inline detail::Sample<Random> sample(size_t count, Random rng = Random()) {
+  return detail::Sample<Random>(count, std::move(rng));
+}
+
+inline detail::Skip skip(size_t count) {
+  return detail::Skip(count);
+}
+
+inline detail::Batch batch(size_t batchSize) {
+  return detail::Batch(batchSize);
+}
+
+}} //folly::gen
+
+#pragma GCC diagnostic pop
--- /dev/null
+++ b/hphp/submodules/folly/folly/gen/Combine.h
@@ -0,0 +1,45 @@
+/*
+ * Copyright 2013 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#ifndef FOLLY_GEN_COMBINE_H
+#define FOLLY_GEN_COMBINE_H
+
+#include "folly/gen/Base.h"
+
+namespace folly {
+namespace gen {
+namespace detail {
+
+template<class Container>
+class Interleave;
+
+template<class Container>
+class Zip;
+
+}  // namespace detail
+
+template<class Source2,
+         class Source2Decayed = typename std::decay<Source2>::type,
+         class Interleave = detail::Interleave<Source2Decayed>>
+Interleave interleave(Source2&& source2) {
+  return Interleave(std::forward<Source2>(source2));
+}
+
+}  // namespace gen
+}  // namespace folly
+
+#include "folly/gen/Combine-inl.h"
+
+#endif // FOLLY_GEN_COMBINE_H
--- /dev/null
+++ b/hphp/submodules/folly/folly/gen/Combine-inl.h
@@ -0,0 +1,217 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_GEN_COMBINE_H
+#error This file may only be included from folly/gen/Combine.h
+#endif
+
+#include <iterator>
+#include <system_error>
+#include <tuple>
+#include <type_traits>
+
+namespace folly {
+namespace gen {
+namespace detail {
+
+/**
+ * Interleave
+ *
+ * Alternate values from a sequence with values from a sequence container.
+ * Stops once we run out of values from either source.
+ */
+template<class Container>
+class Interleave : public Operator<Interleave<Container>> {
+  // see comment about copies in CopiedSource
+  const std::shared_ptr<const Container> container_;
+ public:
+  explicit Interleave(Container container)
+    : container_(new Container(std::move(container))) {}
+
+  template<class Value,
+           class Source>
+  class Generator : public GenImpl<Value, Generator<Value, Source>> {
+    Source source_;
+    const std::shared_ptr<const Container> container_;
+    typedef const typename Container::value_type& ConstRefType;
+
+    static_assert(std::is_same<const Value&, ConstRefType>::value,
+                  "Only matching types may be interleaved");
+  public:
+    explicit Generator(Source source,
+                       const std::shared_ptr<const Container> container)
+      : source_(std::move(source)),
+        container_(container) { }
+
+    template<class Handler>
+    bool apply(Handler&& handler) const {
+      auto iter = container_->begin();
+      return source_.apply([&](const Value& value) -> bool {
+            if (iter == container_->end()) {
+              return false;
+            }
+            if (!handler(value)) {
+              return false;
+            }
+            if (!handler(*iter)) {
+              return false;
+            }
+            iter++;
+            return true;
+        });
+    }
+  };
+
+  template<class Value2,
+           class Source,
+           class Gen = Generator<Value2,Source>>
+  Gen compose(GenImpl<Value2, Source>&& source) const {
+    return Gen(std::move(source.self()), container_);
+  }
+
+  template<class Value2,
+           class Source,
+           class Gen = Generator<Value2,Source>>
+  Gen compose(const GenImpl<Value2, Source>& source) const {
+    return Gen(source.self(), container_);
+  }
+};
+
+/**
+ * Zip
+ *
+ * Combine inputs from Source with values from a sequence container by merging
+ * them into a tuple.
+ *
+ */
+template<class Container>
+class Zip : public Operator<Zip<Container>> {
+  // see comment about copies in CopiedSource
+  const std::shared_ptr<const Container> container_;
+ public:
+  explicit Zip(Container container)
+    : container_(new Container(std::move(container))) {}
+
+  template<class Value1,
+           class Source,
+           class Value2 = decltype(*std::begin(*container_)),
+           class Result = std::tuple<typename std::decay<Value1>::type,
+                                     typename std::decay<Value2>::type>>
+  class Generator : public GenImpl<Result,
+                                   Generator<Value1,Source,Value2,Result>> {
+    Source source_;
+    const std::shared_ptr<const Container> container_;
+  public:
+    explicit Generator(Source source,
+                       const std::shared_ptr<const Container> container)
+      : source_(std::move(source)),
+        container_(container) { }
+
+    template<class Handler>
+    bool apply(Handler&& handler) const {
+      auto iter = container_->begin();
+      return (source_.apply([&](Value1 value) -> bool {
+            if (iter == container_->end()) {
+              return false;
+            }
+            if (!handler(std::make_tuple(std::forward<Value1>(value), *iter))) {
+              return false;
+            }
+            ++iter;
+            return true;
+          }));
+    }
+  };
+
+  template<class Source,
+           class Value,
+           class Gen = Generator<Value, Source>>
+  Gen compose(GenImpl<Value, Source>&& source) const {
+    return Gen(std::move(source.self()), container_);
+  }
+
+  template<class Source,
+           class Value,
+           class Gen = Generator<Value, Source>>
+  Gen compose(const GenImpl<Value, Source>& source) const {
+    return Gen(source.self(), container_);
+  }
+};
+
+template<class... Types1,
+         class... Types2>
+auto add_to_tuple(std::tuple<Types1...> t1, std::tuple<Types2...> t2) ->
+std::tuple<Types1..., Types2...> {
+  return std::tuple_cat(std::move(t1), std::move(t2));
+}
+
+template<class... Types1,
+         class Type2>
+auto add_to_tuple(std::tuple<Types1...> t1, Type2&& t2) ->
+decltype(std::tuple_cat(std::move(t1),
+                        std::make_tuple(std::forward<Type2>(t2)))) {
+  return std::tuple_cat(std::move(t1),
+                        std::make_tuple(std::forward<Type2>(t2)));
+}
+
+template<class Type1,
+         class... Types2>
+auto add_to_tuple(Type1&& t1, std::tuple<Types2...> t2) ->
+decltype(std::tuple_cat(std::make_tuple(std::forward<Type1>(t1)),
+                        std::move(t2))) {
+  return std::tuple_cat(std::make_tuple(std::forward<Type1>(t1)),
+                        std::move(t2));
+}
+
+template<class Type1,
+         class Type2>
+auto add_to_tuple(Type1&& t1, Type2&& t2) ->
+decltype(std::make_tuple(std::forward<Type1>(t1),
+                         std::forward<Type2>(t2))) {
+  return std::make_tuple(std::forward<Type1>(t1),
+                         std::forward<Type2>(t2));
+}
+
+// Merges a 2-tuple into a single tuple (get<0> could already be a tuple)
+class MergeTuples {
+ public:
+  template<class Tuple>
+  auto operator()(Tuple&& value) const ->
+  decltype(add_to_tuple(std::get<0>(std::forward<Tuple>(value)),
+                        std::get<1>(std::forward<Tuple>(value)))) {
+    static_assert(std::tuple_size<
+                    typename std::remove_reference<Tuple>::type
+                    >::value == 2,
+                  "Can only merge tuples of size 2");
+    return add_to_tuple(std::get<0>(std::forward<Tuple>(value)),
+                        std::get<1>(std::forward<Tuple>(value)));
+  }
+};
+
+}  // namespace detail
+
+static const detail::Map<detail::MergeTuples> tuple_flatten;
+
+// TODO(mcurtiss): support zip() for N>1 operands. Because of variadic problems,
+// this might not be easily possible until gcc4.8 is available.
+template<class Source,
+         class Zip = detail::Zip<typename std::decay<Source>::type>>
+Zip zip(Source&& source) {
+  return Zip(std::forward<Source>(source));
+}
+
+}  // namespace gen
+}  // namespace folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/gen/Core.h
@@ -0,0 +1,45 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_GEN_CORE_H
+#define FOLLY_GEN_CORE_H
+
+namespace folly { namespace gen {
+
+template<class Value, class Self>
+class GenImpl;
+
+template<class Self>
+class Operator;
+
+namespace detail {
+
+template<class Self>
+struct FBounded;
+
+template<class First, class Second>
+class Composed;
+
+template<class Value, class First, class Second>
+class Chain;
+
+} // detail
+
+}} // folly::gen
+
+#include "folly/gen/Core-inl.h"
+
+#endif // FOLLY_GEN_CORE_H
--- /dev/null
+++ b/hphp/submodules/folly/folly/gen/Core-inl.h
@@ -0,0 +1,364 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_GEN_CORE_H
+#error This file may only be included from folly/gen/Core.h
+#endif
+
+// Ignore shadowing warnings within this file, so includers can use -Wshadow.
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wshadow"
+
+namespace folly { namespace gen {
+
+/**
+ * IsCompatibleSignature - Trait type for testing whether a given Functor
+ * matches an expected signature.
+ *
+ * Usage:
+ *   IsCompatibleSignature<FunctorType, bool(int, float)>::value
+ */
+template<class Candidate, class Expected>
+class IsCompatibleSignature {
+  static constexpr bool value = false;
+};
+
+template<class Candidate,
+         class ExpectedReturn,
+         class... ArgTypes>
+class IsCompatibleSignature<Candidate, ExpectedReturn(ArgTypes...)> {
+  template<class F,
+           class ActualReturn =
+             decltype(std::declval<F>()(std::declval<ArgTypes>()...)),
+           bool good = std::is_same<ExpectedReturn, ActualReturn>::value>
+  static constexpr bool testArgs(int* p) {
+    return good;
+  }
+
+  template<class F>
+  static constexpr bool testArgs(...) {
+    return false;
+  }
+public:
+  static constexpr bool value = testArgs<Candidate>(nullptr);
+};
+
+/**
+ * FBounded - Helper type for the curiously recurring template pattern, used
+ * heavily here to enable inlining and obviate virtual functions
+ */
+template<class Self>
+struct FBounded {
+  const Self& self() const {
+    return *static_cast<const Self*>(this);
+  }
+
+  Self& self() {
+    return *static_cast<Self*>(this);
+  }
+};
+
+/**
+ * Operator - Core abstraction of an operation which may be applied to a
+ * generator. All operators implement a method compose(), which takes a
+ * generator and produces an output generator.
+ */
+template<class Self>
+class Operator : public FBounded<Self> {
+ public:
+  /**
+   * compose() - Must be implemented by child class to compose a new Generator
+   * out of a given generator. This function left intentionally unimplemented.
+   */
+  template<class Source,
+           class Value,
+           class ResultGen = void>
+  ResultGen compose(const GenImpl<Value, Source>& source) const;
+
+ protected:
+  Operator() = default;
+  Operator(const Operator&) = default;
+  Operator(Operator&&) = default;
+};
+
+/**
+ * operator|() - For composing two operators without binding it to a
+ * particular generator.
+ */
+template<class Left,
+         class Right,
+         class Composed = detail::Composed<Left, Right>>
+Composed operator|(const Operator<Left>& left,
+                   const Operator<Right>& right) {
+  return Composed(left.self(), right.self());
+}
+
+template<class Left,
+         class Right,
+         class Composed = detail::Composed<Left, Right>>
+Composed operator|(const Operator<Left>& left,
+                   Operator<Right>&& right) {
+  return Composed(left.self(), std::move(right.self()));
+}
+
+template<class Left,
+         class Right,
+         class Composed = detail::Composed<Left, Right>>
+Composed operator|(Operator<Left>&& left,
+                   const Operator<Right>& right) {
+  return Composed(std::move(left.self()), right.self());
+}
+
+template<class Left,
+         class Right,
+         class Composed = detail::Composed<Left, Right>>
+Composed operator|(Operator<Left>&& left,
+                   Operator<Right>&& right) {
+  return Composed(std::move(left.self()), std::move(right.self()));
+}
+
+/**
+ * GenImpl - Core abstraction of a generator, an object which produces values by
+ * passing them to a given handler lambda. All generator implementations must
+ * implement apply(). foreach() may also be implemented to special case the
+ * condition where the entire sequence is consumed.
+ */
+template<class Value,
+         class Self>
+class GenImpl : public FBounded<Self> {
+ protected:
+  // To prevent slicing
+  GenImpl() = default;
+  GenImpl(const GenImpl&) = default;
+  GenImpl(GenImpl&&) = default;
+
+ public:
+  typedef Value ValueType;
+  typedef typename std::decay<Value>::type StorageType;
+
+  /**
+   * apply() - Send all values produced by this generator to given handler until
+   * the handler returns false. Returns false if and only if the handler passed
+   * in returns false. Note: It should return true even if it completes (without
+   * the handler returning false), as 'Chain' uses the return value of apply to
+   * determine if it should process the second object in its chain.
+   */
+  template<class Handler>
+  bool apply(Handler&& handler) const;
+
+  /**
+   * foreach() - Send all values produced by this generator to given lambda.
+   */
+  template<class Body>
+  void foreach(Body&& body) const {
+    this->self().apply([&](Value value) -> bool {
+        static_assert(!infinite, "Cannot call foreach on infinite GenImpl");
+        body(std::forward<Value>(value));
+        return true;
+      });
+  }
+
+  // Child classes should override if the sequence generated is *definitely*
+  // infinite. 'infinite' may be false_type for some infinite sequences
+  // (due the the Halting Problem).
+  static constexpr bool infinite = false;
+};
+
+template<class LeftValue,
+         class Left,
+         class RightValue,
+         class Right,
+         class Chain = detail::Chain<LeftValue, Left, Right>>
+Chain operator+(const GenImpl<LeftValue, Left>& left,
+                const GenImpl<RightValue, Right>& right) {
+  static_assert(
+    std::is_same<LeftValue, RightValue>::value,
+    "Generators may ony be combined if Values are the exact same type.");
+  return Chain(left.self(), right.self());
+}
+
+template<class LeftValue,
+         class Left,
+         class RightValue,
+         class Right,
+         class Chain = detail::Chain<LeftValue, Left, Right>>
+Chain operator+(const GenImpl<LeftValue, Left>& left,
+                GenImpl<RightValue, Right>&& right) {
+  static_assert(
+    std::is_same<LeftValue, RightValue>::value,
+    "Generators may ony be combined if Values are the exact same type.");
+  return Chain(left.self(), std::move(right.self()));
+}
+
+template<class LeftValue,
+         class Left,
+         class RightValue,
+         class Right,
+         class Chain = detail::Chain<LeftValue, Left, Right>>
+Chain operator+(GenImpl<LeftValue, Left>&& left,
+                const GenImpl<RightValue, Right>& right) {
+  static_assert(
+    std::is_same<LeftValue, RightValue>::value,
+    "Generators may ony be combined if Values are the exact same type.");
+  return Chain(std::move(left.self()), right.self());
+}
+
+template<class LeftValue,
+         class Left,
+         class RightValue,
+         class Right,
+         class Chain = detail::Chain<LeftValue, Left, Right>>
+Chain operator+(GenImpl<LeftValue, Left>&& left,
+                GenImpl<RightValue, Right>&& right) {
+  static_assert(
+    std::is_same<LeftValue, RightValue>::value,
+    "Generators may ony be combined if Values are the exact same type.");
+  return Chain(std::move(left.self()), std::move(right.self()));
+}
+
+/**
+ * operator|() which enables foreach-like usage:
+ *   gen | [](Value v) -> void {...};
+ */
+template<class Value,
+         class Gen,
+         class Handler>
+typename std::enable_if<
+  IsCompatibleSignature<Handler, void(Value)>::value>::type
+operator|(const GenImpl<Value, Gen>& gen, Handler&& handler) {
+  static_assert(!Gen::infinite,
+                "Cannot pull all values from an infinite sequence.");
+  gen.self().foreach(std::forward<Handler>(handler));
+}
+
+/**
+ * operator|() which enables foreach-like usage with 'break' support:
+ *   gen | [](Value v) -> bool { return shouldContinue(); };
+ */
+template<class Value,
+         class Gen,
+         class Handler>
+typename std::enable_if<
+  IsCompatibleSignature<Handler, bool(Value)>::value, bool>::type
+operator|(const GenImpl<Value, Gen>& gen, Handler&& handler) {
+  return gen.self().apply(std::forward<Handler>(handler));
+}
+
+/**
+ * operator|() for composing generators with operators, similar to boosts' range
+ * adaptors:
+ *   gen | map(square) | sum
+ */
+template<class Value,
+         class Gen,
+         class Op>
+auto operator|(const GenImpl<Value, Gen>& gen, const Operator<Op>& op) ->
+decltype(op.self().compose(gen.self())) {
+  return op.self().compose(gen.self());
+}
+
+template<class Value,
+         class Gen,
+         class Op>
+auto operator|(GenImpl<Value, Gen>&& gen, const Operator<Op>& op) ->
+decltype(op.self().compose(std::move(gen.self()))) {
+  return op.self().compose(std::move(gen.self()));
+}
+
+namespace detail {
+
+/**
+ * Composed - For building up a pipeline of operations to perform, absent any
+ * particular source generator. Useful for building up custom pipelines.
+ *
+ * This type is usually used by just piping two operators together:
+ *
+ * auto valuesOf = filter([](Optional<int>& o) { return o.hasValue(); })
+ *               | map([](Optional<int>& o) -> int& { return o.value(); });
+ *
+ *  auto valuesIncluded = from(optionals) | valuesOf | as<vector>();
+ */
+template<class First,
+         class Second>
+class Composed : public Operator<Composed<First, Second>> {
+  First first_;
+  Second second_;
+ public:
+  Composed() {}
+
+  Composed(First first, Second second)
+    : first_(std::move(first))
+    , second_(std::move(second)) {}
+
+  template<class Source,
+           class Value,
+           class FirstRet = decltype(std::declval<First>()
+                                     .compose(std::declval<Source>())),
+           class SecondRet = decltype(std::declval<Second>()
+                                      .compose(std::declval<FirstRet>()))>
+  SecondRet compose(const GenImpl<Value, Source>& source) const {
+    return second_.compose(first_.compose(source.self()));
+  }
+
+  template<class Source,
+           class Value,
+           class FirstRet = decltype(std::declval<First>()
+                                     .compose(std::declval<Source>())),
+           class SecondRet = decltype(std::declval<Second>()
+                                      .compose(std::declval<FirstRet>()))>
+  SecondRet compose(GenImpl<Value, Source>&& source) const {
+    return second_.compose(first_.compose(std::move(source.self())));
+  }
+};
+
+/**
+ * Chain - For concatenating the values produced by two Generators.
+ *
+ * This type is primarily used through using '+' to combine generators, like:
+ *
+ *   auto nums = seq(1, 10) + seq(20, 30);
+ *   int total = nums | sum;
+ */
+template<class Value, class First, class Second>
+class Chain : public GenImpl<Value,
+                             Chain<Value, First, Second>> {
+  First first_;
+  Second second_;
+public:
+  explicit Chain(First first, Second second)
+      : first_(std::move(first))
+      , second_(std::move(second)) {}
+
+  template<class Handler>
+  bool apply(Handler&& handler) const {
+    return first_.apply(std::forward<Handler>(handler))
+        && second_.apply(std::forward<Handler>(handler));
+  }
+
+  template<class Body>
+  void foreach(Body&& body) const {
+    first_.foreach(std::forward<Body>(body));
+    second_.foreach(std::forward<Body>(body));
+  }
+
+  static constexpr bool infinite = First::infinite || Second::infinite;
+};
+
+} // detail
+
+}} // folly::gen
+
+#pragma GCC diagnostic pop
--- /dev/null
+++ b/hphp/submodules/folly/folly/gen/File.h
@@ -0,0 +1,72 @@
+/*
+ * Copyright 2013 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_GEN_FILE_H
+#define FOLLY_GEN_FILE_H
+
+#include "folly/File.h"
+#include "folly/gen/Base.h"
+#include "folly/io/IOBuf.h"
+
+namespace folly {
+namespace gen {
+
+namespace detail {
+class FileReader;
+class FileWriter;
+}  // namespace detail
+
+/**
+ * Generator that reads from a file with a buffer of the given size.
+ * Reads must be buffered (the generator interface expects the generator
+ * to hold each value).
+ */
+template <class S = detail::FileReader>
+S fromFile(File file, size_t bufferSize=4096) {
+  return S(std::move(file), IOBuf::create(bufferSize));
+}
+
+/**
+ * Generator that reads from a file using a given buffer.
+ */
+template <class S = detail::FileReader>
+S fromFile(File file, std::unique_ptr<IOBuf> buffer) {
+  return S(std::move(file), std::move(buffer));
+}
+
+/**
+ * Sink that writes to a file with a buffer of the given size.
+ * If bufferSize is 0, writes will be unbuffered.
+ */
+template <class S = detail::FileWriter>
+S toFile(File file, size_t bufferSize=4096) {
+  return S(std::move(file), bufferSize ? nullptr : IOBuf::create(bufferSize));
+}
+
+/**
+ * Sink that writes to a file using a given buffer.
+ * If the buffer is nullptr, writes will be unbuffered.
+ */
+template <class S = detail::FileWriter>
+S toFile(File file, std::unique_ptr<IOBuf> buffer) {
+  return S(std::move(file), std::move(buffer));
+}
+
+}}  // !folly::gen
+
+#include "folly/gen/File-inl.h"
+
+#endif // FOLLY_GEN_FILE_H
--- /dev/null
+++ b/hphp/submodules/folly/folly/gen/File-inl.h
@@ -0,0 +1,145 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_GEN_FILE_H
+#error This file may only be included from folly/gen/File.h
+#endif
+
+#include <system_error>
+
+#include "folly/gen/String.h"
+
+namespace folly {
+namespace gen {
+namespace detail {
+
+class FileReader : public GenImpl<ByteRange, FileReader> {
+ public:
+  FileReader(File file, std::unique_ptr<IOBuf> buffer)
+    : file_(std::move(file)),
+      buffer_(std::move(buffer)) {
+    buffer_->clear();
+  }
+
+  template <class Body>
+  bool apply(Body&& body) const {
+    for (;;) {
+      ssize_t n;
+      do {
+        n = ::read(file_.fd(), buffer_->writableTail(), buffer_->capacity());
+      } while (n == -1 && errno == EINTR);
+      if (n == -1) {
+        throw std::system_error(errno, std::system_category(), "read failed");
+      }
+      if (n == 0) {
+        return true;
+      }
+      if (!body(ByteRange(buffer_->tail(), n))) {
+        return false;
+      }
+    }
+  }
+
+  // Technically, there could be infinite files (e.g. /dev/random), but people
+  // who open those can do so at their own risk.
+  static constexpr bool infinite = false;
+
+ private:
+  File file_;
+  std::unique_ptr<IOBuf> buffer_;
+};
+
+class FileWriter : public Operator<FileWriter> {
+ public:
+  FileWriter(File file, std::unique_ptr<IOBuf> buffer)
+    : file_(std::move(file)),
+      buffer_(std::move(buffer)) {
+    if (buffer_) {
+      buffer_->clear();
+    }
+  }
+
+  template <class Source, class Value>
+  void compose(const GenImpl<Value, Source>& source) const {
+    auto fn = [&](ByteRange v) {
+      if (!this->buffer_ || v.size() >= this->buffer_->capacity()) {
+        this->flushBuffer();
+        this->write(v);
+      } else {
+        if (v.size() > this->buffer_->tailroom()) {
+          this->flushBuffer();
+        }
+        memcpy(this->buffer_->writableTail(), v.data(), v.size());
+        this->buffer_->append(v.size());
+      }
+    };
+
+    // Iterate
+    source.foreach(std::move(fn));
+
+    flushBuffer();
+    file_.close();
+  }
+
+ private:
+  void write(ByteRange v) const {
+    ssize_t n;
+    while (!v.empty()) {
+      do {
+        n = ::write(file_.fd(), v.data(), v.size());
+      } while (n == -1 && errno == EINTR);
+      if (n == -1) {
+        throw std::system_error(errno, std::system_category(),
+                                "write() failed");
+      }
+      v.advance(n);
+    }
+  }
+
+  void flushBuffer() const {
+    if (buffer_ && buffer_->length() != 0) {
+      write(ByteRange(buffer_->data(), buffer_->length()));
+      buffer_->clear();
+    }
+  }
+
+  mutable File file_;
+  std::unique_ptr<IOBuf> buffer_;
+};
+
+}  // !detail
+
+/**
+ * Generator which reads lines from a file.
+ * Note: This produces StringPieces which reference temporary strings which are
+ * only valid during iteration.
+ */
+inline auto byLine(File file, char delim = '\n')
+    -> decltype(fromFile(std::move(file))
+                | eachAs<StringPiece>()
+                | resplit(delim)) {
+  return fromFile(std::move(file))
+       | eachAs<StringPiece>()
+       | resplit(delim);
+}
+
+inline auto byLine(int fd, char delim = '\n')
+  -> decltype(byLine(File(fd), delim)) { return byLine(File(fd), delim); }
+
+inline auto byLine(const char* f, char delim = '\n')
+  -> decltype(byLine(File(f), delim)) { return byLine(File(f), delim); }
+
+}}  // !folly::gen
--- /dev/null
+++ b/hphp/submodules/folly/folly/gen/String.h
@@ -0,0 +1,155 @@
+/*
+ * Copyright 2013 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_GEN_STRING_H
+#define FOLLY_GEN_STRING_H
+
+#include "folly/Range.h"
+#include "folly/gen/Base.h"
+
+namespace folly {
+namespace gen {
+
+namespace detail {
+class StringResplitter;
+class SplitStringSource;
+
+template<class Delimiter, class Output>
+class Unsplit;
+
+template<class Delimiter, class OutputBuffer>
+class UnsplitBuffer;
+
+template<class TargetContainer,
+         class Delimiter,
+         class... Targets>
+class SplitTo;
+
+}  // namespace detail
+
+/**
+ * Split the output from a generator into StringPiece "lines" delimited by
+ * the given delimiter.  Delimters are NOT included in the output.
+ *
+ * resplit() behaves as if the input strings were concatenated into one long
+ * string and then split.
+ */
+// make this a template so we don't require StringResplitter to be complete
+// until use
+template <class S=detail::StringResplitter>
+S resplit(char delimiter) {
+  return S(delimiter);
+}
+
+template <class S=detail::SplitStringSource>
+S split(const StringPiece& source, char delimiter) {
+  return S(source, delimiter);
+}
+
+/*
+ * Joins a sequence of tokens into a string, with the chosen delimiter.
+ *
+ * E.G.
+ *   fbstring result = split("a,b,c", ",") | unsplit(",");
+ *   assert(result == "a,b,c");
+ *
+ *   std::string result = split("a,b,c", ",") | unsplit<std::string>(" ");
+ *   assert(result == "a b c");
+ */
+
+
+// NOTE: The template arguments are reversed to allow the user to cleanly
+// specify the output type while still inferring the type of the delimiter.
+template<class Output = folly::fbstring,
+         class Delimiter,
+         class Unsplit = detail::Unsplit<Delimiter, Output>>
+Unsplit unsplit(const Delimiter& delimiter) {
+  return Unsplit(delimiter);
+}
+
+template<class Output = folly::fbstring,
+         class Unsplit = detail::Unsplit<fbstring, Output>>
+Unsplit unsplit(const char* delimiter) {
+  return Unsplit(delimiter);
+}
+
+/*
+ * Joins a sequence of tokens into a string, appending them to the output
+ * buffer.  If the output buffer is empty, an initial delimiter will not be
+ * inserted at the start.
+ *
+ * E.G.
+ *   std::string buffer;
+ *   split("a,b,c", ",") | unsplit(",", &buffer);
+ *   assert(buffer == "a,b,c");
+ *
+ *   std::string anotherBuffer("initial");
+ *   split("a,b,c", ",") | unsplit(",", &anotherbuffer);
+ *   assert(anotherBuffer == "initial,a,b,c");
+ */
+template<class Delimiter,
+         class OutputBuffer,
+         class UnsplitBuffer = detail::UnsplitBuffer<Delimiter, OutputBuffer>>
+UnsplitBuffer unsplit(Delimiter delimiter, OutputBuffer* outputBuffer) {
+  return UnsplitBuffer(delimiter, outputBuffer);
+}
+
+template<class OutputBuffer,
+         class UnsplitBuffer = detail::UnsplitBuffer<fbstring, OutputBuffer>>
+UnsplitBuffer unsplit(const char* delimiter, OutputBuffer* outputBuffer) {
+  return UnsplitBuffer(delimiter, outputBuffer);
+}
+
+
+template<class... Targets>
+detail::Map<detail::SplitTo<std::tuple<Targets...>, char, Targets...>>
+eachToTuple(char delim) {
+  return detail::Map<
+    detail::SplitTo<std::tuple<Targets...>, char, Targets...>>(
+    detail::SplitTo<std::tuple<Targets...>, char, Targets...>(delim));
+}
+
+template<class... Targets>
+detail::Map<detail::SplitTo<std::tuple<Targets...>, fbstring, Targets...>>
+eachToTuple(StringPiece delim) {
+  return detail::Map<
+    detail::SplitTo<std::tuple<Targets...>, fbstring, Targets...>>(
+    detail::SplitTo<std::tuple<Targets...>, fbstring, Targets...>(delim));
+}
+
+template<class First, class Second>
+detail::Map<detail::SplitTo<std::pair<First, Second>, char, First, Second>>
+eachToPair(char delim) {
+  return detail::Map<
+    detail::SplitTo<std::pair<First, Second>, char, First, Second>>(
+    detail::SplitTo<std::pair<First, Second>, char, First, Second>(delim));
+}
+
+template<class First, class Second>
+detail::Map<detail::SplitTo<std::pair<First, Second>, fbstring, First, Second>>
+eachToPair(StringPiece delim) {
+  return detail::Map<
+    detail::SplitTo<std::pair<First, Second>, fbstring, First, Second>>(
+    detail::SplitTo<std::pair<First, Second>, fbstring, First, Second>(
+      to<fbstring>(delim)));
+}
+
+}  // namespace gen
+}  // namespace folly
+
+#include "folly/gen/String-inl.h"
+
+#endif // FOLLY_GEN_STRING_H
--- /dev/null
+++ b/hphp/submodules/folly/folly/gen/String-inl.h
@@ -0,0 +1,271 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_GEN_STRING_H
+#error This file may only be included from folly/gen/String.h
+#endif
+
+#include "folly/Conv.h"
+#include "folly/String.h"
+#include "folly/io/IOBuf.h"
+
+namespace folly {
+namespace gen {
+namespace detail {
+
+inline bool splitPrefix(StringPiece& in, StringPiece& prefix, char delimiter) {
+  auto p = static_cast<const char*>(memchr(in.data(), delimiter, in.size()));
+  if (p) {
+    prefix.assign(in.data(), p);
+    in.assign(p + 1, in.end());
+    return true;
+  }
+  prefix.clear();
+  return false;
+}
+
+inline const char* ch(const unsigned char* p) {
+  return reinterpret_cast<const char*>(p);
+}
+
+class StringResplitter : public Operator<StringResplitter> {
+  char delimiter_;
+ public:
+  explicit StringResplitter(char delimiter) : delimiter_(delimiter) { }
+
+  template <class Source>
+  class Generator : public GenImpl<StringPiece, Generator<Source>> {
+    Source source_;
+    char delimiter_;
+   public:
+    Generator(Source source, char delimiter)
+      : source_(std::move(source)), delimiter_(delimiter) { }
+
+    template <class Body>
+    bool apply(Body&& body) const {
+      std::unique_ptr<IOBuf> buffer;
+
+      auto fn = [&](StringPiece in) -> bool {
+        StringPiece prefix;
+        bool found = splitPrefix(in, prefix, this->delimiter_);
+        if (found && buffer && buffer->length() != 0) {
+          // Append to end of buffer, return line
+          if (!prefix.empty()) {
+            buffer->reserve(0, prefix.size());
+            memcpy(buffer->writableTail(), prefix.data(), prefix.size());
+            buffer->append(prefix.size());
+          }
+          if (!body(StringPiece(ch(buffer->data()), buffer->length()))) {
+            return false;
+          }
+          buffer->clear();
+          found = splitPrefix(in, prefix, this->delimiter_);
+        }
+        // Buffer is empty, return lines directly from input (no buffer)
+        while (found) {
+          if (!body(prefix)) {
+            return false;
+          }
+          found = splitPrefix(in, prefix, this->delimiter_);
+        }
+        if (!in.empty()) {
+          // Incomplete line left, append to buffer
+          if (!buffer) {
+            // Arbitrarily assume that we have half a line and get enough
+            // room for twice that.
+            constexpr size_t kDefaultLineSize = 256;
+            buffer = IOBuf::create(std::max(kDefaultLineSize, 2 * in.size()));
+          }
+          buffer->reserve(0, in.size());
+          memcpy(buffer->writableTail(), in.data(), in.size());
+          buffer->append(in.size());
+        }
+        return true;
+      };
+
+      // Iterate
+      if (!source_.apply(std::move(fn))) {
+        return false;
+      }
+
+      // Incomplete last line
+      if (buffer && buffer->length() != 0) {
+        if (!body(StringPiece(ch(buffer->data()), buffer->length()))) {
+          return false;
+        }
+      }
+      return true;
+    }
+
+    static constexpr bool infinite = Source::infinite;
+  };
+
+  template<class Source,
+           class Value,
+           class Gen = Generator<Source>>
+  Gen compose(GenImpl<Value, Source>&& source) const {
+    return Gen(std::move(source.self()), delimiter_);
+  }
+
+  template<class Source,
+           class Value,
+           class Gen = Generator<Source>>
+  Gen compose(const GenImpl<Value, Source>& source) const {
+    return Gen(source.self(), delimiter_);
+  }
+};
+
+class SplitStringSource : public GenImpl<StringPiece, SplitStringSource> {
+  StringPiece source_;
+  char delimiter_;
+ public:
+  SplitStringSource(const StringPiece& source,
+                    char delimiter)
+    : source_(source)
+    , delimiter_(delimiter) { }
+
+  template <class Body>
+  bool apply(Body&& body) const {
+    StringPiece rest(source_);
+    StringPiece prefix;
+    while (splitPrefix(rest, prefix, this->delimiter_)) {
+      if (!body(prefix)) {
+        return false;
+      }
+    }
+    if (!rest.empty()) {
+      if (!body(rest)) {
+        return false;
+      }
+    }
+    return true;
+  }
+};
+
+/**
+ * Unsplit - For joining tokens from a generator into a string.  This is
+ * the inverse of `split` above.
+ *
+ * This type is primarily used through the 'unsplit' function.
+ */
+template<class Delimiter,
+         class Output>
+class Unsplit : public Operator<Unsplit<Delimiter, Output>> {
+  Delimiter delimiter_;
+ public:
+  Unsplit(const Delimiter& delimiter)
+    : delimiter_(delimiter) {
+  }
+
+  template<class Source,
+           class Value>
+  Output compose(const GenImpl<Value, Source>& source) const {
+    Output outputBuffer;
+    UnsplitBuffer<Delimiter, Output> unsplitter(delimiter_, &outputBuffer);
+    unsplitter.compose(source);
+    return outputBuffer;
+  }
+};
+
+/**
+ * UnsplitBuffer - For joining tokens from a generator into a string,
+ * and inserting them into a custom buffer.
+ *
+ * This type is primarily used through the 'unsplit' function.
+ */
+template<class Delimiter,
+         class OutputBuffer>
+class UnsplitBuffer : public Operator<UnsplitBuffer<Delimiter, OutputBuffer>> {
+  Delimiter delimiter_;
+  OutputBuffer* outputBuffer_;
+ public:
+  UnsplitBuffer(const Delimiter& delimiter, OutputBuffer* outputBuffer)
+    : delimiter_(delimiter)
+    , outputBuffer_(outputBuffer) {
+    CHECK(outputBuffer);
+  }
+
+  template<class Source,
+           class Value>
+  void compose(const GenImpl<Value, Source>& source) const {
+    // If the output buffer is empty, we skip inserting the delimiter for the
+    // first element.
+    bool skipDelim = outputBuffer_->empty();
+    source | [&](Value v) {
+      if (skipDelim) {
+        skipDelim = false;
+        toAppend(std::forward<Value>(v), outputBuffer_);
+      } else {
+        toAppend(delimiter_, std::forward<Value>(v), outputBuffer_);
+      }
+    };
+  }
+};
+
+
+/**
+ * Hack for static for-like constructs
+ */
+template<class Target, class=void>
+inline Target passthrough(Target target) { return target; }
+
+#pragma GCC diagnostic push
+#ifdef __clang__
+// Clang isn't happy with eatField() hack below.
+#pragma GCC diagnostic ignored "-Wreturn-stack-address"
+#endif  // __clang__
+
+/**
+ * ParseToTuple - For splitting a record and immediatlely converting it to a
+ * target tuple type. Primary used through the 'eachToTuple' helper, like so:
+ *
+ *  auto config
+ *    = split("1:a 2:b", ' ')
+ *    | eachToTuple<int, string>()
+ *    | as<vector<tuple<int, string>>>();
+ *
+ */
+template<class TargetContainer,
+         class Delimiter,
+         class... Targets>
+class SplitTo {
+  Delimiter delimiter_;
+ public:
+  explicit SplitTo(Delimiter delimiter)
+    : delimiter_(delimiter) {}
+
+  TargetContainer operator()(StringPiece line) const {
+    int i = 0;
+    StringPiece fields[sizeof...(Targets)];
+    // HACK(tjackson): Used for referencing fields[] corresponding to variadic
+    // template parameters.
+    auto eatField = [&]() -> StringPiece& { return fields[i++]; };
+    if (!split(delimiter_,
+               line,
+               detail::passthrough<StringPiece&, Targets>(eatField())...)) {
+      throw std::runtime_error("field count mismatch");
+    }
+    i = 0;
+    return TargetContainer(To<Targets>()(eatField())...);
+  }
+};
+
+#pragma GCC diagnostic pop
+
+}  // namespace detail
+
+}  // namespace gen
+}  // namespace folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/gen/test/BaseBenchmark.cpp
@@ -0,0 +1,344 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#include <atomic>
+#include <glog/logging.h>
+
+#include "folly/Benchmark.h"
+#include "folly/gen/Base.h"
+
+using namespace folly::gen;
+using folly::fbstring;
+using std::pair;
+using std::set;
+using std::vector;
+using std::tuple;
+
+static std::atomic<int> testSize(1000);
+static vector<int> testVector =
+    seq(1, testSize.load())
+  | mapped([](int) { return rand(); })
+  | as<vector>();
+
+static vector<vector<int>> testVectorVector =
+    seq(1, 100)
+  | map([](int i) {
+      return seq(1, i) | as<vector>();
+    })
+  | as<vector>();
+static vector<fbstring> strings =
+    from(testVector)
+  | eachTo<fbstring>()
+  | as<vector>();
+
+auto square = [](int x) { return x * x; };
+auto add = [](int a, int b) { return a + b; };
+auto multiply = [](int a, int b) { return a * b; };
+
+BENCHMARK(Sum_Basic_NoGen, iters) {
+  int limit = testSize.load();
+  int s = 0;
+  while (iters--) {
+    for (int i = 0; i < limit; ++i) {
+      s += i;
+    }
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_RELATIVE(Sum_Basic_Gen, iters) {
+  int limit = testSize.load();
+  int s = 0;
+  while (iters--) {
+    s += range(0, limit) | sum;
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_DRAW_LINE()
+
+BENCHMARK(Sum_Vector_NoGen, iters) {
+  int s = 0;
+  while (iters--) {
+    for (auto& i : testVector) {
+      s += i;
+    }
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_RELATIVE(Sum_Vector_Gen, iters) {
+  int s = 0;
+  while (iters--) {
+    s += from(testVector) | sum;
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_DRAW_LINE()
+
+BENCHMARK(Member, iters) {
+  int s = 0;
+  while(iters--) {
+    s += from(strings)
+       | member(&fbstring::size)
+       | sum;
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_RELATIVE(MapMember, iters) {
+  int s = 0;
+  while(iters--) {
+    s += from(strings)
+       | map([](const fbstring& x) { return x.size(); })
+       | sum;
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_DRAW_LINE()
+
+BENCHMARK(Count_Vector_NoGen, iters) {
+  int s = 0;
+  while (iters--) {
+    for (auto& i : testVector) {
+      if (i * 2 < rand()) {
+        ++s;
+      }
+    }
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_RELATIVE(Count_Vector_Gen, iters) {
+  int s = 0;
+  while (iters--) {
+    s += from(testVector)
+       | filter([](int i) {
+                  return i * 2 < rand();
+                })
+       | count;
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_DRAW_LINE()
+
+BENCHMARK(Fib_Sum_NoGen, iters) {
+  int s = 0;
+  while (iters--) {
+    auto fib = [](int limit) -> vector<int> {
+      vector<int> ret;
+      int a = 0;
+      int b = 1;
+      for (int i = 0; i * 2 < limit; ++i) {
+        ret.push_back(a += b);
+        ret.push_back(b += a);
+      }
+      return ret;
+    };
+    for (auto& v : fib(testSize.load())) {
+      s += v;
+    }
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_RELATIVE(Fib_Sum_Gen, iters) {
+  int s = 0;
+  while (iters--) {
+    auto fib = GENERATOR(int) {
+      int a = 0;
+      int b = 1;
+      for (;;) {
+        yield(a += b);
+        yield(b += a);
+      }
+    };
+    s += fib | take(testSize.load()) | sum;
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+struct FibYielder {
+  template<class Yield>
+  void operator()(Yield&& yield) const {
+    int a = 0;
+    int b = 1;
+    for (;;) {
+      yield(a += b);
+      yield(b += a);
+    }
+  }
+};
+
+BENCHMARK_RELATIVE(Fib_Sum_Gen_Static, iters) {
+  int s = 0;
+  while (iters--) {
+    auto fib = generator<int>(FibYielder());
+    s += fib | take(testSize.load()) | sum;
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_DRAW_LINE()
+
+BENCHMARK(VirtualGen_0Virtual, iters) {
+  int s = 0;
+  while (iters--) {
+    auto numbers = seq(1, 10000);
+    auto squares = numbers | map(square);
+    auto quads = squares | map(square);
+    s += quads | sum;
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_RELATIVE(VirtualGen_1Virtual, iters) {
+  int s = 0;
+  while (iters--) {
+    VirtualGen<int> numbers = seq(1, 10000);
+    auto squares = numbers | map(square);
+    auto quads = squares | map(square);
+    s += quads | sum;
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_RELATIVE(VirtualGen_2Virtual, iters) {
+  int s = 0;
+  while (iters--) {
+    VirtualGen<int> numbers = seq(1, 10000);
+    VirtualGen<int> squares = numbers | map(square);
+    auto quads = squares | map(square);
+    s += quads | sum;
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_RELATIVE(VirtualGen_3Virtual, iters) {
+  int s = 0;
+  while (iters--) {
+    VirtualGen<int> numbers = seq(1, 10000);
+    VirtualGen<int> squares = numbers | map(square);
+    VirtualGen<int> quads = squares | map(square);
+    s += quads | sum;
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_DRAW_LINE()
+
+BENCHMARK(Concat_NoGen, iters) {
+  int s = 0;
+  while (iters--) {
+    for (auto& v : testVectorVector) {
+      for (auto& i : v) {
+        s += i;
+      }
+    }
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_RELATIVE(Concat_Gen, iters) {
+  int s = 0;
+  while (iters--) {
+    s += from(testVectorVector) | rconcat | sum;
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_DRAW_LINE()
+
+BENCHMARK(Composed_NoGen, iters) {
+  int s = 0;
+  while (iters--) {
+    for (auto& i : testVector) {
+      s += i * i;
+    }
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_RELATIVE(Composed_Gen, iters) {
+  int s = 0;
+  auto sumSq = map(square) | sum;
+  while (iters--) {
+    s += from(testVector) | sumSq;
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_RELATIVE(Composed_GenRegular, iters) {
+  int s = 0;
+  while (iters--) {
+    s += from(testVector) | map(square) | sum;
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_DRAW_LINE()
+
+BENCHMARK(Sample, iters) {
+  size_t s = 0;
+  while (iters--) {
+    auto sampler = seq(1, 10 * 1000 * 1000) | sample(1000);
+    s += (sampler | sum);
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+// Results from an Intel(R) Xeon(R) CPU E5-2660 0 @ 2.20GHz
+// ============================================================================
+// folly/gen/test/BaseBenchmark.cpp                relative  time/iter  iters/s
+// ============================================================================
+// Sum_Basic_NoGen                                            372.39ns    2.69M
+// Sum_Basic_Gen                                    195.96%   190.03ns    5.26M
+// ----------------------------------------------------------------------------
+// Sum_Vector_NoGen                                           200.41ns    4.99M
+// Sum_Vector_Gen                                    77.14%   259.81ns    3.85M
+// ----------------------------------------------------------------------------
+// Member                                                       4.56us  219.42K
+// MapMember                                        400.47%     1.14us  878.73K
+// ----------------------------------------------------------------------------
+// Count_Vector_NoGen                                          13.96us   71.64K
+// Count_Vector_Gen                                  86.05%    16.22us   61.65K
+// ----------------------------------------------------------------------------
+// Fib_Sum_NoGen                                                2.21us  452.63K
+// Fib_Sum_Gen                                       23.94%     9.23us  108.36K
+// Fib_Sum_Gen_Static                                48.77%     4.53us  220.73K
+// ----------------------------------------------------------------------------
+// VirtualGen_0Virtual                                          9.60us  104.13K
+// VirtualGen_1Virtual                               28.00%    34.30us   29.15K
+// VirtualGen_2Virtual                               22.62%    42.46us   23.55K
+// VirtualGen_3Virtual                               16.96%    56.64us   17.66K
+// ----------------------------------------------------------------------------
+// Concat_NoGen                                                 2.20us  453.66K
+// Concat_Gen                                       109.49%     2.01us  496.70K
+// ----------------------------------------------------------------------------
+// Composed_NoGen                                             545.32ns    1.83M
+// Composed_Gen                                      87.94%   620.07ns    1.61M
+// Composed_GenRegular                               88.13%   618.74ns    1.62M
+// ----------------------------------------------------------------------------
+// Sample                                                     176.48ms     5.67
+// ============================================================================
+
+int main(int argc, char *argv[]) {
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  folly::runBenchmarks();
+  return 0;
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/gen/test/BaseTest.cpp
@@ -0,0 +1,998 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+#include <iosfwd>
+#include <random>
+#include <set>
+#include <vector>
+
+#include "folly/FBVector.h"
+#include "folly/MapUtil.h"
+#include "folly/Memory.h"
+#include "folly/dynamic.h"
+#include "folly/gen/Base.h"
+#include "folly/experimental/TestUtil.h"
+
+using namespace folly::gen;
+using namespace folly;
+using std::make_tuple;
+using std::ostream;
+using std::pair;
+using std::set;
+using std::string;
+using std::tuple;
+using std::unique_ptr;
+using std::vector;
+
+#define EXPECT_SAME(A, B) \
+  static_assert(std::is_same<A, B>::value, "Mismatched: " #A ", " #B)
+EXPECT_SAME(int&&, typename ArgumentReference<int>::type);
+EXPECT_SAME(int&, typename ArgumentReference<int&>::type);
+EXPECT_SAME(const int&, typename ArgumentReference<const int&>::type);
+EXPECT_SAME(const int&, typename ArgumentReference<const int>::type);
+
+template<typename T>
+ostream& operator<<(ostream& os, const set<T>& values) {
+  return os << from(values);
+}
+
+template<typename T>
+ostream& operator<<(ostream& os, const vector<T>& values) {
+  os << "[";
+  for (auto& value : values) {
+    if (&value != &values.front()) {
+      os << " ";
+    }
+    os << value;
+  }
+  return os << "]";
+}
+
+auto square = [](int x) { return x * x; };
+auto add = [](int a, int b) { return a + b; };
+auto multiply = [](int a, int b) { return a * b; };
+
+auto product = foldl(1, multiply);
+
+template<typename A, typename B>
+ostream& operator<<(ostream& os, const pair<A, B>& pair) {
+  return os << "(" << pair.first << ", " << pair.second << ")";
+}
+
+TEST(Gen, Count) {
+  auto gen = seq(1, 10);
+  EXPECT_EQ(10, gen | count);
+  EXPECT_EQ(5, gen | take(5) | count);
+}
+
+TEST(Gen, Sum) {
+  auto gen = seq(1, 10);
+  EXPECT_EQ((1 + 10) * 10 / 2, gen | sum);
+  EXPECT_EQ((1 + 5) * 5 / 2, gen | take(5) | sum);
+}
+
+TEST(Gen, Foreach) {
+  auto gen = seq(1, 4);
+  int accum = 0;
+  gen | [&](int x) { accum += x; };
+  EXPECT_EQ(10, accum);
+  int accum2 = 0;
+  gen | take(3) | [&](int x) { accum2 += x; };
+  EXPECT_EQ(6, accum2);
+}
+
+TEST(Gen, Map) {
+  auto expected = vector<int>{4, 9, 16};
+  auto gen = from({2, 3, 4}) | map(square);
+  EXPECT_EQ((vector<int>{4, 9, 16}), gen | as<vector>());
+  EXPECT_EQ((vector<int>{4, 9}), gen | take(2) | as<vector>());
+}
+
+TEST(Gen, Member) {
+  struct Counter {
+    Counter(int start = 0)
+      : c(start)
+    {}
+
+    int count() const { return c; }
+    int incr() { return ++c; }
+
+    int& ref() { return c; }
+    const int& ref() const { return c; }
+   private:
+    int c;
+  };
+  auto counters = seq(1, 10) | eachAs<Counter>() | as<vector>();
+  EXPECT_EQ(10 * (1 + 10) / 2,
+            from(counters)
+          | member(&Counter::count)
+          | sum);
+  EXPECT_EQ(10 * (2 + 11) / 2,
+            from(counters)
+          | member(&Counter::incr)
+          | sum);
+  EXPECT_EQ(10 * (2 + 11) / 2,
+            from(counters)
+          | member(&Counter::count)
+          | sum);
+
+  // type-verifications
+  auto m = empty<Counter&>();
+  auto c = empty<const Counter&>();
+  m | member(&Counter::incr) | assert_type<int&&>();
+  m | member(&Counter::count) | assert_type<int&&>();
+  m | member(&Counter::count) | assert_type<int&&>();
+  m | member<Const>(&Counter::ref) | assert_type<const int&>();
+  m | member<Mutable>(&Counter::ref) | assert_type<int&>();
+  c | member<Const>(&Counter::ref) | assert_type<const int&>();
+}
+
+TEST(Gen, Field) {
+  struct X {
+    X() : a(2), b(3), c(4), d(b) {}
+
+    const int a;
+    int b;
+    mutable int c;
+    int& d; // can't access this with a field pointer.
+  };
+
+  std::vector<X> xs(1);
+  EXPECT_EQ(2, from(xs)
+             | field(&X::a)
+             | first);
+  EXPECT_EQ(3, from(xs)
+             | field(&X::b)
+             | first);
+  EXPECT_EQ(4, from(xs)
+             | field(&X::c)
+             | first);
+  // type-verification
+  empty<X&>() | field(&X::a) | assert_type<const int&>();
+  empty<X&>() | field(&X::b) | assert_type<int&>();
+  empty<X&>() | field(&X::c) | assert_type<int&>();
+  empty<X&&>() | field(&X::a) | assert_type<const int&&>();
+  empty<X&&>() | field(&X::b) | assert_type<int&&>();
+  empty<X&&>() | field(&X::c) | assert_type<int&&>();
+  // references don't imply ownership so they're not moved
+  empty<const X&>() | field(&X::a) | assert_type<const int&>();
+  empty<const X&>() | field(&X::b) | assert_type<const int&>();
+  // 'mutable' has no effect on field pointers, by C++ spec
+  empty<const X&>() | field(&X::c) | assert_type<const int&>();
+
+  // can't form pointer-to-reference field: empty<X&>() | field(&X::d)
+}
+
+TEST(Gen, Seq) {
+  // cover the fenceposts of the loop unrolling
+  for (int n = 1; n < 100; ++n) {
+    EXPECT_EQ(n, seq(1, n) | count);
+    EXPECT_EQ(n + 1, seq(1) | take(n + 1) | count);
+  }
+}
+
+TEST(Gen, Range) {
+  // cover the fenceposts of the loop unrolling
+  for (int n = 1; n < 100; ++n) {
+    EXPECT_EQ(gen::range(0, n) | count, n);
+  }
+}
+
+TEST(Gen, FromIterators) {
+  vector<int> source {2, 3, 5, 7, 11};
+  auto gen = from(folly::range(source.begin() + 1, source.end() - 1));
+  EXPECT_EQ(3 * 5 * 7, gen | product);
+}
+
+TEST(Gen, FromMap) {
+  auto source = seq(0, 10)
+              | map([](int i) { return std::make_pair(i, i * i); })
+              | as<std::map<int, int>>();
+  auto gen = fromConst(source)
+           | map([&](const std::pair<const int, int>& p) {
+               return p.second - p.first;
+             });
+  EXPECT_EQ(330, gen | sum);
+}
+
+TEST(Gen, Filter) {
+  const auto expected = vector<int>{1, 2, 4, 5, 7, 8};
+  auto actual =
+      seq(1, 9)
+    | filter([](int x) { return x % 3; })
+    | as<vector<int>>();
+  EXPECT_EQ(expected, actual);
+}
+
+TEST(Gen, Contains) {
+  {
+    auto gen =
+        seq(1, 9)
+      | map(square);
+    EXPECT_TRUE(gen | contains(49));
+    EXPECT_FALSE(gen | contains(50));
+  }
+  {
+    auto gen =
+        seq(1) // infinite, to prove laziness
+      | map(square)
+      | eachTo<std::string>();
+
+    // std::string gen, const char* needle
+    EXPECT_TRUE(gen | take(9999) | contains("49"));
+  }
+}
+
+TEST(Gen, Take) {
+  {
+    auto expected = vector<int>{1, 4, 9, 16};
+    auto actual =
+      seq(1, 1000)
+      | mapped([](int x) { return x * x; })
+      | take(4)
+      | as<vector<int>>();
+    EXPECT_EQ(expected, actual);
+  }
+  {
+    auto expected = vector<int>{ 0, 1, 4, 5, 8 };
+    auto actual
+      = ((seq(0) | take(2)) +
+         (seq(4) | take(2)) +
+         (seq(8) | take(2)))
+      | take(5)
+      | as<vector>();
+    EXPECT_EQ(expected, actual);
+  }
+  {
+    auto expected = vector<int>{ 0, 1, 4, 5, 8 };
+    auto actual
+      = seq(0)
+      | mapped([](int i) {
+          return seq(i * 4) | take(2);
+        })
+      | concat
+      | take(5)
+      | as<vector>();
+    EXPECT_EQ(expected, actual);
+  }
+}
+
+TEST(Gen, Sample) {
+  std::mt19937 rnd(42);
+
+  auto sampler =
+      seq(1, 100)
+    | sample(50, rnd);
+  std::unordered_map<int,int> hits;
+  const int kNumIters = 80;
+  for (int i = 0; i < kNumIters; i++) {
+    auto vec = sampler | as<vector<int>>();
+    EXPECT_EQ(vec.size(), 50);
+    auto uniq = fromConst(vec) | as<set<int>>();
+    EXPECT_EQ(uniq.size(), vec.size());  // sampling without replacement
+    for (auto v: vec) {
+      ++hits[v];
+    }
+  }
+
+  // In 80 separate samples of our range, we should have seen every value
+  // at least once and no value all 80 times. (The odds of either of those
+  // events is 1/2^80).
+  EXPECT_EQ(hits.size(), 100);
+  for (auto hit: hits) {
+    EXPECT_GT(hit.second, 0);
+    EXPECT_LT(hit.second, kNumIters);
+  }
+
+  auto small =
+      seq(1, 5)
+    | sample(10);
+  EXPECT_EQ((small | sum), 15);
+  EXPECT_EQ((small | take(3) | count), 3);
+}
+
+TEST(Gen, Skip) {
+  auto gen =
+      seq(1, 1000)
+    | mapped([](int x) { return x * x; })
+    | skip(4)
+    | take(4);
+  EXPECT_EQ((vector<int>{25, 36, 49, 64}), gen | as<vector>());
+}
+
+TEST(Gen, Until) {
+  {
+    auto expected = vector<int>{1, 4, 9, 16};
+    auto actual
+      = seq(1, 1000)
+      | mapped([](int x) { return x * x; })
+      | until([](int x) { return x > 20; })
+      | as<vector<int>>();
+    EXPECT_EQ(expected, actual);
+  }
+  {
+    auto expected = vector<int>{ 0, 1, 4, 5, 8 };
+    auto actual
+      = ((seq(0) | until([](int i) { return i > 1; })) +
+         (seq(4) | until([](int i) { return i > 5; })) +
+         (seq(8) | until([](int i) { return i > 9; })))
+      | until([](int i) { return i > 8; })
+      | as<vector<int>>();
+    EXPECT_EQ(expected, actual);
+  }
+  /*
+  {
+    auto expected = vector<int>{ 0, 1, 5, 6, 10 };
+    auto actual
+      = seq(0)
+      | mapped([](int i) {
+          return seq(i * 5) | until([=](int j) { return j > i * 5 + 1; });
+        })
+      | concat
+      | until([](int i) { return i > 10; })
+      | as<vector<int>>();
+    EXPECT_EQ(expected, actual);
+  }
+  */
+}
+
+TEST(Gen, Composed) {
+  // Operator, Operator
+  auto valuesOf =
+      filter([](Optional<int>& o) { return o.hasValue(); })
+    | map([](Optional<int>& o) -> int& { return o.value(); });
+  std::vector<Optional<int>> opts {
+    none, 4, none, 6, none
+  };
+  EXPECT_EQ(4 * 4 + 6 * 6, from(opts) | valuesOf | map(square) | sum);
+  // Operator, Sink
+  auto sumOpt = valuesOf | sum;
+  EXPECT_EQ(10, from(opts) | sumOpt);
+}
+
+TEST(Gen, Chain) {
+  std::vector<int> nums {2, 3, 5, 7};
+  std::map<int, int> mappings { { 3, 9}, {5, 25} };
+  auto gen = from(nums) + (from(mappings) | get<1>());
+  EXPECT_EQ(51, gen | sum);
+  EXPECT_EQ(5, gen | take(2) | sum);
+  EXPECT_EQ(26, gen | take(5) | sum);
+}
+
+TEST(Gen, Concat) {
+  std::vector<std::vector<int>> nums {{2, 3}, {5, 7}};
+  auto gen = from(nums) | rconcat;
+  EXPECT_EQ(17, gen | sum);
+  EXPECT_EQ(10, gen | take(3) | sum);
+}
+
+TEST(Gen, ConcatGen) {
+  auto gen = seq(1, 10)
+           | map([](int i) { return seq(1, i); })
+           | concat;
+  EXPECT_EQ(220, gen | sum);
+  EXPECT_EQ(10, gen | take(6) | sum);
+}
+
+TEST(Gen, ConcatAlt) {
+  std::vector<std::vector<int>> nums {{2, 3}, {5, 7}};
+  auto actual = from(nums)
+              | map([](std::vector<int>& v) { return from(v); })
+              | concat
+              | sum;
+  auto expected = 17;
+  EXPECT_EQ(expected, actual);
+}
+
+TEST(Gen, Order) {
+  auto expected = vector<int>{0, 3, 5, 6, 7, 8, 9};
+  auto actual =
+      from({8, 6, 7, 5, 3, 0, 9})
+    | order
+    | as<vector>();
+  EXPECT_EQ(expected, actual);
+}
+
+TEST(Gen, OrderMoved) {
+  auto expected = vector<int>{0, 9, 25, 36, 49, 64, 81};
+  auto actual =
+      from({8, 6, 7, 5, 3, 0, 9})
+    | move
+    | order
+    | map(square)
+    | as<vector>();
+  EXPECT_EQ(expected, actual);
+}
+
+TEST(Gen, OrderTake) {
+  auto expected = vector<int>{9, 8, 7};
+  auto actual =
+      from({8, 6, 7, 5, 3, 0, 9})
+    | orderByDescending(square)
+    | take(3)
+    | as<vector>();
+  EXPECT_EQ(expected, actual);
+}
+
+TEST(Gen, Distinct) {
+  auto expected = vector<int>{3, 1, 2};
+  auto actual =
+      from({3, 1, 3, 2, 1, 2, 3})
+    | distinct
+    | as<vector>();
+  EXPECT_EQ(expected, actual);
+}
+
+TEST(Gen, DistinctBy) {   //  0  1  4  9  6  5  6  9  4  1  0
+  auto expected = vector<int>{0, 1, 2, 3, 4, 5};
+  auto actual =
+      seq(0, 100)
+    | distinctBy([](int i) { return i * i % 10; })
+    | as<vector>();
+  EXPECT_EQ(expected, actual);
+}
+
+TEST(Gen, DistinctMove) {   //  0  1  4  9  6  5  6  9  4  1  0
+  auto expected = vector<int>{0, 1, 2, 3, 4, 5};
+  auto actual =
+      seq(0, 100)
+    | mapped([](int i) { return std::unique_ptr<int>(new int(i)); })
+      // see comment below about selector parameters for Distinct
+    | distinctBy([](const std::unique_ptr<int>& pi) { return *pi * *pi % 10; })
+    | mapped([](std::unique_ptr<int> pi) { return *pi; })
+    | as<vector>();
+
+  // NOTE(tjackson): the following line intentionally doesn't work:
+  //  | distinctBy([](std::unique_ptr<int> pi) { return *pi * *pi % 10; })
+  // This is because distinctBy because the selector intentionally requires a
+  // const reference.  If it required a move-reference, the value might get
+  // gutted by the selector before said value could be passed to downstream
+  // operators.
+  EXPECT_EQ(expected, actual);
+}
+
+TEST(Gen, MinBy) {
+  EXPECT_EQ(7, seq(1, 10)
+             | minBy([](int i) -> double {
+                 double d = i - 6.8;
+                 return d * d;
+               }));
+}
+
+TEST(Gen, MaxBy) {
+  auto gen = from({"three", "eleven", "four"});
+
+  EXPECT_EQ("eleven", gen | maxBy(&strlen));
+}
+
+TEST(Gen, Append) {
+  string expected = "facebook";
+  string actual = "face";
+  from(StringPiece("book")) | appendTo(actual);
+  EXPECT_EQ(expected, actual);
+}
+
+TEST(Gen, FromRValue) {
+  {
+    // AFAICT The C++ Standard does not specify what happens to the rvalue
+    // reference of a std::vector when it is used as the 'other' for an rvalue
+    // constructor.  Use fbvector because we're sure its size will be zero in
+    // this case.
+    fbvector<int> v({1,2,3,4});
+    auto q1 = from(v);
+    EXPECT_EQ(v.size(), 4);  // ensure that the lvalue version was called!
+    auto expected = 1 * 2 * 3 * 4;
+    EXPECT_EQ(expected, q1 | product);
+
+    auto q2 = from(std::move(v));
+    EXPECT_EQ(v.size(), 0);  // ensure that rvalue version was called
+    EXPECT_EQ(expected, q2 | product);
+  }
+  {
+    auto expected = 7;
+    auto q = from([] {return vector<int>({3,7,5}); }());
+    EXPECT_EQ(expected, q | max);
+  }
+  {
+    for (auto size: {5, 1024, 16384, 1<<20}) {
+      auto q1 = from(vector<int>(size, 2));
+      auto q2 = from(vector<int>(size, 3));
+      // If the rvalue specialization is broken/gone, then the compiler will
+      // (disgustingly!) just store a *reference* to the temporary object,
+      // which is bad.  Try to catch this by allocating two temporary vectors
+      // of the same size, so that they'll probably use the same underlying
+      // buffer if q1's vector is destructed before q2's vector is constructed.
+      EXPECT_EQ(size * 2 + size * 3, (q1 | sum) + (q2 | sum));
+    }
+  }
+  {
+    auto q = from(set<int>{1,2,3,2,1});
+    EXPECT_EQ(q | sum, 6);
+  }
+}
+
+TEST(Gen, OrderBy) {
+  auto expected = vector<int>{5, 6, 4, 7, 3, 8, 2, 9, 1, 10};
+  auto actual =
+      seq(1, 10)
+    | orderBy([](int x) { return (5.1 - x) * (5.1 - x); })
+    | as<vector>();
+  EXPECT_EQ(expected, actual);
+}
+
+TEST(Gen, Foldl) {
+  int expected = 2 * 3 * 4 * 5;
+  auto actual =
+      seq(2, 5)
+    | foldl(1, multiply);
+  EXPECT_EQ(expected, actual);
+}
+
+TEST(Gen, Reduce) {
+  int expected = 2 + 3 + 4 + 5;
+  auto actual = seq(2, 5) | reduce(add);
+  EXPECT_EQ(expected, actual);
+}
+
+TEST(Gen, ReduceBad) {
+  auto gen = seq(1) | take(0);
+  try {
+    EXPECT_TRUE(true);
+    gen | reduce(add);
+    EXPECT_TRUE(false);
+  } catch (...) {
+  }
+}
+
+TEST(Gen, Moves) {
+  std::vector<unique_ptr<int>> ptrs;
+  ptrs.emplace_back(new int(1));
+  EXPECT_NE(ptrs.front().get(), nullptr);
+  auto ptrs2 = from(ptrs) | move | as<vector>();
+  EXPECT_EQ(ptrs.front().get(), nullptr);
+  EXPECT_EQ(**ptrs2.data(), 1);
+}
+
+TEST(Gen, First) {
+  auto gen =
+      seq(0)
+    | filter([](int x) { return x > 3; });
+  EXPECT_EQ(4, gen | first);
+}
+
+TEST(Gen, FromCopy) {
+  vector<int> v {3, 5};
+  auto src = from(v);
+  auto copy = fromCopy(v);
+  EXPECT_EQ(8, src | sum);
+  EXPECT_EQ(8, copy | sum);
+  v[1] = 7;
+  EXPECT_EQ(10, src | sum);
+  EXPECT_EQ(8, copy | sum);
+}
+
+TEST(Gen, Get) {
+  std::map<int, int> pairs {
+    {1, 1},
+    {2, 4},
+    {3, 9},
+    {4, 16},
+  };
+  auto pairSrc = from(pairs);
+  auto keys = pairSrc | get<0>();
+  auto values = pairSrc | get<1>();
+  EXPECT_EQ(10, keys | sum);
+  EXPECT_EQ(30, values | sum);
+  EXPECT_EQ(30, keys | map(square) | sum);
+  pairs[5] = 25;
+  EXPECT_EQ(15, keys | sum);
+  EXPECT_EQ(55, values | sum);
+
+  vector<tuple<int, int, int>> tuples {
+    make_tuple(1, 1, 1),
+    make_tuple(2, 4, 8),
+    make_tuple(3, 9, 27),
+  };
+  EXPECT_EQ(36, from(tuples) | get<2>() | sum);
+}
+
+TEST(Gen, Any) {
+  EXPECT_TRUE(seq(0) | any);
+  EXPECT_TRUE(seq(0, 1) | any);
+  EXPECT_TRUE(seq(0, 10) | any([](int i) { return i == 7; }));
+  EXPECT_FALSE(seq(0, 10) | any([](int i) { return i == 11; }));
+
+  EXPECT_TRUE(from({1}) | any);
+  EXPECT_FALSE(gen::range(0, 0) | any);
+  EXPECT_FALSE(from({1}) | take(0) | any);
+}
+
+TEST(Gen, All) {
+  EXPECT_TRUE(seq(0, 10) | all([](int i) { return i < 11; }));
+  EXPECT_FALSE(seq(0, 10) | all([](int i) { return i < 5; }));
+  EXPECT_FALSE(seq(0) | take(9999) | all([](int i) { return i < 10; }));
+
+  // empty lists satisfies all
+  EXPECT_TRUE(seq(0) | take(0) | all([](int i) { return i < 50; }));
+  EXPECT_TRUE(seq(0) | take(0) | all([](int i) { return i > 50; }));
+}
+
+TEST(Gen, Yielders) {
+  auto gen = GENERATOR(int) {
+    for (int i = 1; i <= 5; ++i) {
+      yield(i);
+    }
+    yield(7);
+    for (int i = 3; ; ++i) {
+      yield(i * i);
+    }
+  };
+  vector<int> expected {
+    1, 2, 3, 4, 5, 7, 9, 16, 25
+  };
+  EXPECT_EQ(expected, gen | take(9) | as<vector>());
+}
+
+TEST(Gen, NestedYield) {
+  auto nums = GENERATOR(int) {
+    for (int i = 1; ; ++i) {
+      yield(i);
+    }
+  };
+  auto gen = GENERATOR(int) {
+    nums | take(10) | yield;
+    seq(1, 5) | [&](int i) {
+      yield(i);
+    };
+  };
+  EXPECT_EQ(70, gen | sum);
+}
+
+TEST(Gen, MapYielders) {
+  auto gen = seq(1, 5)
+           | map([](int n) {
+               return GENERATOR(int) {
+                 int i;
+                 for (i = 1; i < n; ++i)
+                   yield(i);
+                 for (; i >= 1; --i)
+                   yield(i);
+               };
+             })
+           | concat;
+  vector<int> expected {
+                1,
+             1, 2, 1,
+          1, 2, 3, 2, 1,
+       1, 2, 3, 4, 3, 2, 1,
+    1, 2, 3, 4, 5, 4, 3, 2, 1,
+  };
+  EXPECT_EQ(expected, gen | as<vector>());
+}
+
+TEST(Gen, VirtualGen) {
+  VirtualGen<int> v(seq(1, 10));
+  EXPECT_EQ(55, v | sum);
+  v = v | map(square);
+  EXPECT_EQ(385, v | sum);
+  v = v | take(5);
+  EXPECT_EQ(55, v | sum);
+  EXPECT_EQ(30, v | take(4) | sum);
+}
+
+
+TEST(Gen, CustomType) {
+  struct Foo{
+    int y;
+  };
+  auto gen = from({Foo{2}, Foo{3}})
+           | map([](const Foo& f) { return f.y; });
+  EXPECT_EQ(5, gen | sum);
+}
+
+TEST(Gen, NoNeedlessCopies) {
+  auto gen = seq(1, 5)
+           | map([](int x) { return unique_ptr<int>(new int(x)); })
+           | map([](unique_ptr<int> p) { return p; })
+           | map([](unique_ptr<int>&& p) { return std::move(p); })
+           | map([](const unique_ptr<int>& p) { return *p; });
+  EXPECT_EQ(15, gen | sum);
+  EXPECT_EQ(6, gen | take(3) | sum);
+}
+
+namespace {
+
+class TestIntSeq : public GenImpl<int, TestIntSeq> {
+ public:
+  TestIntSeq() { }
+
+  template <class Body>
+  bool apply(Body&& body) const {
+    for (int i = 1; i < 6; ++i) {
+      if (!body(i)) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  TestIntSeq(TestIntSeq&&) = default;
+  TestIntSeq& operator=(TestIntSeq&&) = default;
+  TestIntSeq(const TestIntSeq&) = delete;
+  TestIntSeq& operator=(const TestIntSeq&) = delete;
+};
+
+}  // namespace
+
+TEST(Gen, NoGeneratorCopies) {
+  EXPECT_EQ(15, TestIntSeq() | sum);
+  auto x = TestIntSeq() | take(3);
+  EXPECT_EQ(6, std::move(x) | sum);
+}
+
+TEST(Gen, FromArray) {
+  int source[] = {2, 3, 5, 7};
+  auto gen = from(source);
+  EXPECT_EQ(2 * 3 * 5 * 7, gen | product);
+}
+
+TEST(Gen, FromStdArray) {
+  std::array<int,4> source {{2, 3, 5, 7}};
+  auto gen = from(source);
+  EXPECT_EQ(2 * 3 * 5 * 7, gen | product);
+}
+
+TEST(Gen, StringConcat) {
+  auto gen = seq(1, 10)
+           | eachTo<string>()
+           | rconcat;
+  EXPECT_EQ("12345678910", gen | as<string>());
+}
+
+struct CopyCounter {
+  static int alive;
+  int copies;
+  int moves;
+
+  CopyCounter() : copies(0), moves(0) {
+    ++alive;
+  }
+
+  CopyCounter(CopyCounter&& source) {
+    *this = std::move(source);
+    ++alive;
+  }
+
+  CopyCounter(const CopyCounter& source) {
+    *this = source;
+    ++alive;
+  }
+
+  ~CopyCounter() {
+    --alive;
+  }
+
+  CopyCounter& operator=(const CopyCounter& source) {
+    this->copies = source.copies + 1;
+    this->moves = source.moves;
+    return *this;
+  }
+
+  CopyCounter& operator=(CopyCounter&& source) {
+    this->copies = source.copies;
+    this->moves = source.moves + 1;
+    return *this;
+  }
+};
+
+int CopyCounter::alive = 0;
+
+TEST(Gen, CopyCount) {
+  vector<CopyCounter> originals;
+  originals.emplace_back();
+  EXPECT_EQ(1, originals.size());
+  EXPECT_EQ(0, originals.back().copies);
+
+  vector<CopyCounter> copies = from(originals) | as<vector>();
+  EXPECT_EQ(1, copies.back().copies);
+  EXPECT_EQ(0, copies.back().moves);
+
+  vector<CopyCounter> moves = from(originals) | move | as<vector>();
+  EXPECT_EQ(0, moves.back().copies);
+  EXPECT_EQ(1, moves.back().moves);
+}
+
+// test dynamics with various layers of nested arrays.
+TEST(Gen, Dynamic) {
+  dynamic array1 = {1, 2};
+  EXPECT_EQ(dynamic(3), from(array1) | sum);
+  dynamic array2 = {{1}, {1, 2}};
+  EXPECT_EQ(dynamic(4), from(array2) | rconcat | sum);
+  dynamic array3 = {{{1}}, {{1}, {1, 2}}};
+  EXPECT_EQ(dynamic(5), from(array3) | rconcat | rconcat | sum);
+}
+
+TEST(Gen, DynamicObject) {
+  const dynamic obj = dynamic::object(1, 2)(3, 4);
+  EXPECT_EQ(dynamic(4), from(obj.keys()) | sum);
+  EXPECT_EQ(dynamic(6), from(obj.values()) | sum);
+  EXPECT_EQ(dynamic(4), from(obj.items()) | get<0>() | sum);
+  EXPECT_EQ(dynamic(6), from(obj.items()) | get<1>() | sum);
+}
+
+TEST(Gen, Collect) {
+  auto s = from({7, 6, 5, 4, 3}) | as<set<int>>();
+  EXPECT_EQ(s.size(), 5);
+}
+
+
+TEST(Gen, Cycle) {
+  {
+    auto s = from({1, 2});
+    EXPECT_EQ((vector<int> { 1, 2, 1, 2, 1 }),
+              s | cycle | take(5) | as<vector>());
+  }
+  {
+    auto s = from({1, 2});
+    EXPECT_EQ((vector<int> { 1, 2, 1, 2 }),
+              s | cycle(2) | as<vector>());
+  }
+  {
+    auto s = from({1, 2, 3});
+    EXPECT_EQ((vector<int> { 1, 2, 1, 2, 1 }),
+              s | take(2) | cycle | take(5) | as<vector>());
+  }
+  {
+    auto s = empty<int>();
+    EXPECT_EQ((vector<int> { }),
+              s | cycle | take(4) | as<vector>());
+  }
+  {
+    int count = 3;
+    int* pcount = &count;
+    auto countdown = GENERATOR(int) {
+      ASSERT_GE(*pcount, 0)
+        << "Cycle should have stopped when it didnt' get values!";
+      for (int i = 1; i <= *pcount; ++i) {
+        yield(i);
+      }
+      --*pcount;
+    };
+    auto s = countdown;
+    EXPECT_EQ((vector<int> { 1, 2, 3, 1, 2, 1}),
+              s | cycle | as<vector>());
+  }
+}
+
+TEST(Gen, Dereference) {
+  {
+    const int x = 4, y = 2;
+    auto s = from<const int*>({&x, nullptr, &y});
+    EXPECT_EQ(6, s | dereference | sum);
+  }
+  {
+    vector<int> a { 1, 2 };
+    vector<int> b { 3, 4 };
+    vector<vector<int>*> pv { &a, nullptr, &b };
+    from(pv)
+      | dereference
+      | [&](vector<int>& v) {
+          v.push_back(5);
+        };
+    EXPECT_EQ(3, a.size());
+    EXPECT_EQ(3, b.size());
+    EXPECT_EQ(5, a.back());
+    EXPECT_EQ(5, b.back());
+  }
+  {
+    vector<std::map<int, int>> maps {
+      {
+        { 2, 31 },
+        { 3, 41 },
+      },
+      {
+        { 3, 52 },
+        { 4, 62 },
+      },
+      {
+        { 4, 73 },
+        { 5, 83 },
+      },
+    };
+    EXPECT_EQ(
+      93,
+      from(maps)
+      | map([](std::map<int, int>& m) {
+          return get_ptr(m, 3);
+        })
+      | dereference
+      | sum);
+  }
+  {
+    vector<unique_ptr<int>> ups;
+    ups.emplace_back(new int(3));
+    ups.emplace_back();
+    ups.emplace_back(new int(7));
+    EXPECT_EQ(10, from(ups) | dereference | sum);
+    EXPECT_EQ(10, from(ups) | move | dereference | sum);
+  }
+}
+
+TEST(Gen, Guard) {
+  using std::runtime_error;
+  EXPECT_THROW(from({"1", "a", "3"})
+               | eachTo<int>()
+               | sum,
+               runtime_error);
+  EXPECT_EQ(4,
+            from({"1", "a", "3"})
+            | guard<runtime_error>([](runtime_error&, const char*) {
+                return true; // continue
+              })
+            | eachTo<int>()
+            | sum);
+  EXPECT_EQ(1,
+            from({"1", "a", "3"})
+            | guard<runtime_error>([](runtime_error&, const char*) {
+                return false; // break
+              })
+            | eachTo<int>()
+            | sum);
+  EXPECT_THROW(from({"1", "a", "3"})
+                | guard<runtime_error>([](runtime_error&, const char* v) {
+                    if (v[0] == 'a') {
+                      throw;
+                    }
+                    return true;
+                  })
+               | eachTo<int>()
+               | sum,
+               runtime_error);
+}
+
+TEST(Gen, Batch) {
+  EXPECT_EQ((vector<vector<int>> { {1} }),
+            seq(1, 1) | batch(5) | as<vector>());
+  EXPECT_EQ((vector<vector<int>> { {1, 2, 3}, {4, 5, 6}, {7, 8, 9}, {10, 11} }),
+            seq(1, 11) | batch(3) | as<vector>());
+  EXPECT_THROW(seq(1, 1) | batch(0) | as<vector>(),
+               std::invalid_argument);
+}
+
+TEST(Gen, BatchMove) {
+  auto expected = vector<vector<int>>{ {0, 1}, {2, 3}, {4} };
+  auto actual =
+      seq(0, 4)
+    | mapped([](int i) { return std::unique_ptr<int>(new int(i)); })
+    | batch(2)
+    | mapped([](std::vector<std::unique_ptr<int>>& pVector) {
+        std::vector<int> iVector;
+        for (const auto& p : pVector) {
+          iVector.push_back(*p);
+        };
+        return iVector;
+      })
+    | as<vector>();
+  EXPECT_EQ(expected, actual);
+}
+
+int main(int argc, char *argv[]) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  return RUN_ALL_TESTS();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/gen/test/CombineTest.cpp
@@ -0,0 +1,167 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#include <gtest/gtest.h>
+#include <string>
+#include <vector>
+#include <tuple>
+
+#include "folly/Range.h"
+#include "folly/FBVector.h"
+#include "folly/experimental/TestUtil.h"
+#include "folly/gen/Base.h"
+#include "folly/gen/Combine.h"
+
+using namespace folly::gen;
+using namespace folly;
+using std::string;
+using std::vector;
+using std::tuple;
+
+auto even = [](int i) -> bool { return i % 2 == 0; };
+auto odd = [](int i) -> bool { return i % 2 == 1; };
+
+TEST(CombineGen, Interleave) {
+  { // large (infinite) base, small container
+    auto base = seq(1) | filter(odd);
+    auto toInterleave = seq(1, 6) | filter(even);
+    auto interleaved = base | interleave(toInterleave | as<vector>());
+    EXPECT_EQ(interleaved | as<vector>(), vector<int>({1, 2, 3, 4, 5, 6}));
+  }
+  { // small base, large container
+    auto base = seq(1) | filter(odd) | take(3);
+    auto toInterleave = seq(1) | filter(even) | take(50);
+    auto interleaved = base | interleave(toInterleave | as<vector>());
+    EXPECT_EQ(interleaved | as<vector>(),
+              vector<int>({1, 2, 3, 4, 5, 6}));
+  }
+}
+
+TEST(CombineGen, Zip) {
+  auto base0 = seq(1);
+  // We rely on std::move(fbvector) emptying the source vector
+  auto zippee = fbvector<string>{"one", "two", "three"};
+  {
+    auto combined = base0
+      | zip(zippee)
+      | as<vector>();
+    ASSERT_EQ(combined.size(), 3);
+    EXPECT_EQ(std::get<0>(combined[0]), 1);
+    EXPECT_EQ(std::get<1>(combined[0]), "one");
+    EXPECT_EQ(std::get<0>(combined[1]), 2);
+    EXPECT_EQ(std::get<1>(combined[1]), "two");
+    EXPECT_EQ(std::get<0>(combined[2]), 3);
+    EXPECT_EQ(std::get<1>(combined[2]), "three");
+    ASSERT_FALSE(zippee.empty());
+    EXPECT_FALSE(zippee.front().empty());  // shouldn't have been move'd
+  }
+
+  { // same as top, but using std::move.
+    auto combined = base0
+      | zip(std::move(zippee))
+      | as<vector>();
+    ASSERT_EQ(combined.size(), 3);
+    EXPECT_EQ(std::get<0>(combined[0]), 1);
+    EXPECT_TRUE(zippee.empty());
+  }
+
+  { // same as top, but base is truncated
+    auto baseFinite = seq(1) | take(1);
+    auto combined = baseFinite
+      | zip(vector<string>{"one", "two", "three"})
+      | as<vector>();
+    ASSERT_EQ(combined.size(), 1);
+    EXPECT_EQ(std::get<0>(combined[0]), 1);
+    EXPECT_EQ(std::get<1>(combined[0]), "one");
+  }
+}
+
+TEST(CombineGen, TupleFlatten) {
+  vector<tuple<int,string>> intStringTupleVec{
+    tuple<int,string>{1, "1"},
+    tuple<int,string>{2, "2"},
+    tuple<int,string>{3, "3"},
+  };
+
+  vector<tuple<char>> charTupleVec{
+    tuple<char>{'A'},
+    tuple<char>{'B'},
+    tuple<char>{'C'},
+    tuple<char>{'D'},
+  };
+
+  vector<double> doubleVec{
+    1.0,
+    4.0,
+    9.0,
+    16.0,
+    25.0,
+  };
+
+  auto zipped1 = from(intStringTupleVec)
+    | zip(charTupleVec)
+    | assert_type<tuple<tuple<int, string>, tuple<char>>>()
+    | as<vector>();
+  EXPECT_EQ(std::get<0>(zipped1[0]), std::make_tuple(1, "1"));
+  EXPECT_EQ(std::get<1>(zipped1[0]), std::make_tuple('A'));
+
+  auto zipped2 = from(zipped1)
+    | tuple_flatten
+    | assert_type<tuple<int, string, char>&&>()
+    | as<vector>();
+  ASSERT_EQ(zipped2.size(), 3);
+  EXPECT_EQ(zipped2[0], std::make_tuple(1, "1", 'A'));
+
+  auto zipped3 = from(charTupleVec)
+    | zip(intStringTupleVec)
+    | tuple_flatten
+    | assert_type<tuple<char, int, string>&&>()
+    | as<vector>();
+  ASSERT_EQ(zipped3.size(), 3);
+  EXPECT_EQ(zipped3[0], std::make_tuple('A', 1, "1"));
+
+  auto zipped4 = from(intStringTupleVec)
+    | zip(doubleVec)
+    | tuple_flatten
+    | assert_type<tuple<int, string, double>&&>()
+    | as<vector>();
+  ASSERT_EQ(zipped4.size(), 3);
+  EXPECT_EQ(zipped4[0], std::make_tuple(1, "1", 1.0));
+
+  auto zipped5 = from(doubleVec)
+    | zip(doubleVec)
+    | assert_type<tuple<double, double>>()
+    | tuple_flatten  // essentially a no-op
+    | assert_type<tuple<double, double>&&>()
+    | as<vector>();
+  ASSERT_EQ(zipped5.size(), 5);
+  EXPECT_EQ(zipped5[0], std::make_tuple(1.0, 1.0));
+
+  auto zipped6 = from(intStringTupleVec)
+    | zip(charTupleVec)
+    | tuple_flatten
+    | zip(doubleVec)
+    | tuple_flatten
+    | assert_type<tuple<int, string, char, double>&&>()
+    | as<vector>();
+  ASSERT_EQ(zipped6.size(), 3);
+  EXPECT_EQ(zipped6[0], std::make_tuple(1, "1", 'A', 1.0));
+}
+
+int main(int argc, char *argv[]) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  return RUN_ALL_TESTS();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/gen/test/FileBenchmark.cpp
@@ -0,0 +1,70 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#include <thread>
+#include <glog/logging.h>
+
+#include "folly/Benchmark.h"
+#include "folly/File.h"
+#include "folly/gen/Base.h"
+#include "folly/gen/File.h"
+
+using namespace folly::gen;
+
+BENCHMARK(ByLine_Pipes, iters) {
+  std::thread thread;
+  int rfd;
+  int wfd;
+  BENCHMARK_SUSPEND {
+    int p[2];
+    CHECK_ERR(::pipe(p));
+    rfd = p[0];
+    wfd = p[1];
+    thread = std::thread([wfd, iters] {
+      char x = 'x';
+      PCHECK(::write(wfd, &x, 1) == 1);  // signal startup
+      FILE* f = fdopen(wfd, "w");
+      PCHECK(f);
+      for (int i = 1; i <= iters; ++i) {
+        fprintf(f, "%d\n", i);
+      }
+      fclose(f);
+    });
+    char buf;
+    PCHECK(::read(rfd, &buf, 1) == 1);  // wait for startup
+  }
+
+  auto s = byLine(folly::File(rfd)) | eachTo<int64_t>() | sum;
+  folly::doNotOptimizeAway(s);
+
+  BENCHMARK_SUSPEND {
+    ::close(rfd);
+    CHECK_EQ(s, int64_t(iters) * (iters + 1) / 2);
+    thread.join();
+  }
+}
+
+// Results from an Intel(R) Xeon(R) CPU E5-2660 0 @ 2.20GHz
+// ============================================================================
+// folly/gen/test/FileBenchmark.cpp                relative  time/iter  iters/s
+// ============================================================================
+// ByLine_Pipes                                               148.63ns    6.73M
+// ============================================================================
+
+int main(int argc, char *argv[]) {
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  folly::runBenchmarks();
+  return 0;
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/gen/test/FileTest.cpp
@@ -0,0 +1,80 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#include <gtest/gtest.h>
+#include <string>
+#include <vector>
+
+#include "folly/File.h"
+#include "folly/Range.h"
+#include "folly/experimental/TestUtil.h"
+#include "folly/gen/Base.h"
+#include "folly/gen/File.h"
+
+using namespace folly::gen;
+using namespace folly;
+using std::string;
+using std::vector;
+
+TEST(FileGen, ByLine) {
+  auto collect = eachTo<std::string>() | as<vector>();
+  test::TemporaryFile file("ByLine");
+  static const std::string lines(
+      "Hello world\n"
+      "This is the second line\n"
+      "\n"
+      "\n"
+      "a few empty lines above\n"
+      "incomplete last line");
+  EXPECT_EQ(lines.size(), write(file.fd(), lines.data(), lines.size()));
+
+  auto expected = from({lines}) | resplit('\n') | collect;
+  auto found = byLine(file.path().c_str()) | collect;
+
+  EXPECT_TRUE(expected == found);
+}
+
+class FileGenBufferedTest : public ::testing::TestWithParam<int> { };
+
+TEST_P(FileGenBufferedTest, FileWriter) {
+  size_t bufferSize = GetParam();
+  test::TemporaryFile file("FileWriter");
+
+  static const std::string lines(
+      "Hello world\n"
+      "This is the second line\n"
+      "\n"
+      "\n"
+      "a few empty lines above\n");
+
+  auto src = from({lines, lines, lines, lines, lines, lines, lines, lines});
+  auto collect = eachTo<std::string>() | as<vector>();
+  auto expected = src | resplit('\n') | collect;
+
+  src | eachAs<StringPiece>() | toFile(File(file.fd()), bufferSize);
+  auto found = byLine(file.path().c_str()) | collect;
+
+  EXPECT_TRUE(expected == found);
+}
+
+INSTANTIATE_TEST_CASE_P(
+    DifferentBufferSizes,
+    FileGenBufferedTest,
+    ::testing::Values(0, 1, 2, 4, 8, 64, 4096));
+int main(int argc, char *argv[]) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  return RUN_ALL_TESTS();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/gen/test/StringBenchmark.cpp
@@ -0,0 +1,329 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#include <atomic>
+#include <glog/logging.h>
+
+#include "folly/Benchmark.h"
+#include "folly/String.h"
+#include "folly/gen/Base.h"
+#include "folly/gen/String.h"
+
+using namespace folly;
+using namespace folly::gen;
+using std::pair;
+using std::set;
+using std::vector;
+using std::tuple;
+
+namespace {
+
+static std::atomic<int> testSize(1000);
+static vector<fbstring> testStrVector
+  = seq(1, testSize.load())
+  | eachTo<fbstring>()
+  | as<vector>();
+
+const char* const kLine = "The quick brown fox jumped over the lazy dog.\n";
+const size_t kLineCount = 10000;
+std::string bigLines;
+const size_t kSmallLineSize = 17;
+std::vector<std::string> smallLines;
+
+void initStringResplitterBenchmark() {
+  bigLines.reserve(kLineCount * strlen(kLine));
+  for (size_t i = 0; i < kLineCount; ++i) {
+    bigLines += kLine;
+  }
+  size_t remaining = bigLines.size();
+  size_t pos = 0;
+  while (remaining) {
+    size_t n = std::min(kSmallLineSize, remaining);
+    smallLines.push_back(bigLines.substr(pos, n));
+    pos += n;
+    remaining -= n;
+  }
+}
+
+size_t len(folly::StringPiece s) { return s.size(); }
+
+}  // namespace
+
+BENCHMARK(StringResplitter_Big, iters) {
+  size_t s = 0;
+  while (iters--) {
+    s += from({bigLines}) | resplit('\n') | map(&len) | sum;
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_RELATIVE(StringResplitter_Small, iters) {
+  size_t s = 0;
+  while (iters--) {
+    s += from(smallLines) | resplit('\n') | map(&len) | sum;
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_DRAW_LINE()
+
+BENCHMARK(StringSplit_Old, iters) {
+  size_t s = 0;
+  std::string line(kLine);
+  while (iters--) {
+    std::vector<StringPiece> parts;
+    split(' ', line, parts);
+    s += parts.size();
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+
+BENCHMARK_RELATIVE(StringSplit_Gen_Vector, iters) {
+  size_t s = 0;
+  StringPiece line(kLine);
+  while (iters--) {
+    s += (split(line, ' ') | as<vector>()).size();
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_DRAW_LINE()
+
+BENCHMARK(StringSplit_Old_ReuseVector, iters) {
+  size_t s = 0;
+  std::string line(kLine);
+  std::vector<StringPiece> parts;
+  while (iters--) {
+    parts.clear();
+    split(' ', line, parts);
+    s += parts.size();
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_RELATIVE(StringSplit_Gen_ReuseVector, iters) {
+  size_t s = 0;
+  StringPiece line(kLine);
+  std::vector<StringPiece> parts;
+  while (iters--) {
+    parts.clear();
+    split(line, ' ') | appendTo(parts);
+    s += parts.size();
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_RELATIVE(StringSplit_Gen, iters) {
+  size_t s = 0;
+  StringPiece line(kLine);
+  while (iters--) {
+    s += split(line, ' ') | count;
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_RELATIVE(StringSplit_Gen_Take, iters) {
+  size_t s = 0;
+  StringPiece line(kLine);
+  while (iters--) {
+    s += split(line, ' ') | take(10) | count;
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_DRAW_LINE()
+
+BENCHMARK(StringUnsplit_Old, iters) {
+  size_t s = 0;
+  while (iters--) {
+    fbstring joined;
+    join(',', testStrVector, joined);
+    s += joined.size();
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_RELATIVE(StringUnsplit_Old_ReusedBuffer, iters) {
+  size_t s = 0;
+  fbstring joined;
+  while (iters--) {
+    joined.clear();
+    join(',', testStrVector, joined);
+    s += joined.size();
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_RELATIVE(StringUnsplit_Gen, iters) {
+  size_t s = 0;
+  StringPiece line(kLine);
+  while (iters--) {
+    fbstring joined = from(testStrVector) | unsplit(',');
+    s += joined.size();
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_RELATIVE(StringUnsplit_Gen_ReusedBuffer, iters) {
+  size_t s = 0;
+  fbstring buffer;
+  while (iters--) {
+    buffer.clear();
+    from(testStrVector) | unsplit(',', &buffer);
+    s += buffer.size();
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_DRAW_LINE()
+
+void StringUnsplit_Gen(size_t iters, size_t joinSize) {
+  std::vector<fbstring> v;
+  BENCHMARK_SUSPEND {
+    FOR_EACH_RANGE(i, 0, joinSize) {
+      v.push_back(to<fbstring>(rand()));
+    }
+  }
+  size_t s = 0;
+  fbstring buffer;
+  while (iters--) {
+    buffer.clear();
+    from(v) | unsplit(',', &buffer);
+    s += buffer.size();
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_PARAM(StringUnsplit_Gen, 1000)
+BENCHMARK_RELATIVE_PARAM(StringUnsplit_Gen, 2000)
+BENCHMARK_RELATIVE_PARAM(StringUnsplit_Gen, 4000)
+BENCHMARK_RELATIVE_PARAM(StringUnsplit_Gen, 8000)
+
+BENCHMARK_DRAW_LINE()
+
+fbstring records
+= seq<size_t>(1, 1000)
+  | mapped([](size_t i) {
+      return folly::to<fbstring>(i, ' ', i * i, ' ', i * i * i);
+    })
+  | unsplit('\n');
+
+BENCHMARK(Records_EachToTuple, iters) {
+  size_t s = 0;
+  for (size_t i = 0; i < iters; i += 1000) {
+    s += split(records, '\n')
+       | eachToTuple<int, size_t, StringPiece>(' ')
+       | get<1>()
+       | sum;
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_RELATIVE(Records_VectorStringPieceReused, iters) {
+  size_t s = 0;
+  std::vector<StringPiece> fields;
+  for (size_t i = 0; i < iters; i += 1000) {
+    s += split(records, '\n')
+       | mapped([&](StringPiece line) {
+           fields.clear();
+           folly::split(' ', line, fields);
+           CHECK(fields.size() == 3);
+           return std::make_tuple(
+             folly::to<int>(fields[0]),
+             folly::to<size_t>(fields[1]),
+             StringPiece(fields[2]));
+         })
+       | get<1>()
+       | sum;
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_RELATIVE(Records_VectorStringPiece, iters) {
+  size_t s = 0;
+  for (size_t i = 0; i < iters; i += 1000) {
+    s += split(records, '\n')
+       | mapped([](StringPiece line) {
+           std::vector<StringPiece> fields;
+           folly::split(' ', line, fields);
+           CHECK(fields.size() == 3);
+           return std::make_tuple(
+             folly::to<int>(fields[0]),
+             folly::to<size_t>(fields[1]),
+             StringPiece(fields[2]));
+         })
+       | get<1>()
+       | sum;
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+BENCHMARK_RELATIVE(Records_VectorString, iters) {
+  size_t s = 0;
+  for (size_t i = 0; i < iters; i += 1000) {
+    s += split(records, '\n')
+       | mapped([](StringPiece line) {
+           std::vector<std::string> fields;
+           folly::split(' ', line, fields);
+           CHECK(fields.size() == 3);
+           return std::make_tuple(
+             folly::to<int>(fields[0]),
+             folly::to<size_t>(fields[1]),
+             StringPiece(fields[2]));
+         })
+       | get<1>()
+       | sum;
+  }
+  folly::doNotOptimizeAway(s);
+}
+
+// Results from an Intel(R) Xeon(R) CPU E5-2660 0 @ 2.20GHz
+// ============================================================================
+// folly/gen/test/StringBenchmark.cpp              relative  time/iter  iters/s
+// ============================================================================
+// StringResplitter_Big                                       108.58us    9.21K
+// StringResplitter_Small                            10.60%     1.02ms   976.48
+// ----------------------------------------------------------------------------
+// StringSplit_Old                                            357.82ns    2.79M
+// StringSplit_Gen_Vector                           105.10%   340.46ns    2.94M
+// ----------------------------------------------------------------------------
+// StringSplit_Old_ReuseVector                                 96.45ns   10.37M
+// StringSplit_Gen_ReuseVector                      124.01%    77.78ns   12.86M
+// StringSplit_Gen                                  140.10%    68.85ns   14.52M
+// StringSplit_Gen_Take                             122.97%    78.44ns   12.75M
+// ----------------------------------------------------------------------------
+// StringUnsplit_Old                                           42.99us   23.26K
+// StringUnsplit_Old_ReusedBuffer                   100.48%    42.79us   23.37K
+// StringUnsplit_Gen                                 96.37%    44.61us   22.42K
+// StringUnsplit_Gen_ReusedBuffer                   116.96%    36.76us   27.20K
+// ----------------------------------------------------------------------------
+// StringUnsplit_Gen(1000)                                     44.71us   22.37K
+// StringUnsplit_Gen(2000)                           49.28%    90.72us   11.02K
+// StringUnsplit_Gen(4000)                           24.05%   185.91us    5.38K
+// StringUnsplit_Gen(8000)                           12.23%   365.42us    2.74K
+// ----------------------------------------------------------------------------
+// Records_EachToTuple                                        101.43us    9.86K
+// Records_VectorStringPieceReused                   93.72%   108.22us    9.24K
+// Records_VectorStringPiece                         37.14%   273.11us    3.66K
+// Records_VectorString                              16.70%   607.47us    1.65K
+// ============================================================================
+
+int main(int argc, char *argv[]) {
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  initStringResplitterBenchmark();
+  runBenchmarks();
+  return 0;
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/gen/test/StringTest.cpp
@@ -0,0 +1,286 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+#include <iosfwd>
+#include <map>
+#include <vector>
+
+#include "folly/gen/String.h"
+
+using namespace folly::gen;
+using namespace folly;
+using std::make_tuple;
+using std::ostream;
+using std::pair;
+using std::string;
+using std::tuple;
+using std::unique_ptr;
+using std::vector;
+
+TEST(StringGen, EmptySplit) {
+  auto collect = eachTo<std::string>() | as<vector>();
+  {
+    auto pieces = split("", ',') | collect;
+    EXPECT_EQ(0, pieces.size());
+  }
+
+  // The last delimiter is eaten, just like std::getline
+  {
+    auto pieces = split(",", ',') | collect;
+    EXPECT_EQ(1, pieces.size());
+    EXPECT_EQ("", pieces[0]);
+  }
+
+  {
+    auto pieces = split(",,", ',') | collect;
+    EXPECT_EQ(2, pieces.size());
+    EXPECT_EQ("", pieces[0]);
+    EXPECT_EQ("", pieces[1]);
+  }
+
+  {
+    auto pieces = split(",,", ',') | take(1) | collect;
+    EXPECT_EQ(1, pieces.size());
+    EXPECT_EQ("", pieces[0]);
+  }
+}
+
+TEST(StringGen, Split) {
+  auto collect = eachTo<std::string>() | as<vector>();
+  {
+    auto pieces = split("hello,, world, goodbye, meow", ',') | collect;
+    EXPECT_EQ(5, pieces.size());
+    EXPECT_EQ("hello", pieces[0]);
+    EXPECT_EQ("", pieces[1]);
+    EXPECT_EQ(" world", pieces[2]);
+    EXPECT_EQ(" goodbye", pieces[3]);
+    EXPECT_EQ(" meow", pieces[4]);
+  }
+
+  {
+    auto pieces = split("hello,, world, goodbye, meow", ',')
+                | take(3) | collect;
+    EXPECT_EQ(3, pieces.size());
+    EXPECT_EQ("hello", pieces[0]);
+    EXPECT_EQ("", pieces[1]);
+    EXPECT_EQ(" world", pieces[2]);
+  }
+
+  {
+    auto pieces = split("hello,, world, goodbye, meow", ',')
+                | take(5) | collect;
+    EXPECT_EQ(5, pieces.size());
+    EXPECT_EQ("hello", pieces[0]);
+    EXPECT_EQ("", pieces[1]);
+    EXPECT_EQ(" world", pieces[2]);
+  }
+}
+
+TEST(StringGen, EmptyResplit) {
+  auto collect = eachTo<std::string>() | as<vector>();
+  {
+    auto pieces = from({""}) | resplit(',') | collect;
+    EXPECT_EQ(0, pieces.size());
+  }
+
+  // The last delimiter is eaten, just like std::getline
+  {
+    auto pieces = from({","}) | resplit(',') | collect;
+    EXPECT_EQ(1, pieces.size());
+    EXPECT_EQ("", pieces[0]);
+  }
+
+  {
+    auto pieces = from({",,"}) | resplit(',') | collect;
+    EXPECT_EQ(2, pieces.size());
+    EXPECT_EQ("", pieces[0]);
+    EXPECT_EQ("", pieces[1]);
+  }
+}
+
+TEST(StringGen, EachToTuple) {
+  {
+    auto lines = "2:1.414:yo 3:1.732:hi";
+    auto actual
+      = split(lines, ' ')
+      | eachToTuple<int, double, std::string>(':')
+      | as<vector>();
+    vector<tuple<int, double, std::string>> expected {
+      make_tuple(2, 1.414, "yo"),
+      make_tuple(3, 1.732, "hi"),
+    };
+    EXPECT_EQ(expected, actual);
+  }
+  {
+    auto lines = "2 3";
+    auto actual
+      = split(lines, ' ')
+      | eachToTuple<int>(',')
+      | as<vector>();
+    vector<tuple<int>> expected {
+      make_tuple(2),
+      make_tuple(3),
+    };
+    EXPECT_EQ(expected, actual);
+  }
+  {
+    // StringPiece target
+    auto lines = "1:cat 2:dog";
+    auto actual
+      = split(lines, ' ')
+      | eachToTuple<int, StringPiece>(':')
+      | as<vector>();
+    vector<tuple<int, StringPiece>> expected {
+      make_tuple(1, "cat"),
+      make_tuple(2, "dog"),
+    };
+    EXPECT_EQ(expected, actual);
+  }
+  {
+    // Empty field
+    auto lines = "2:tjackson:4 3::5";
+    auto actual
+      = split(lines, ' ')
+      | eachToTuple<int, fbstring, int>(':')
+      | as<vector>();
+    vector<tuple<int, fbstring, int>> expected {
+      make_tuple(2, "tjackson", 4),
+      make_tuple(3, "", 5),
+    };
+    EXPECT_EQ(expected, actual);
+  }
+  {
+    // Excess fields
+    auto lines = "1:2 3:4:5";
+    EXPECT_THROW((split(lines, ' ')
+                    | eachToTuple<int, int>(':')
+                    | as<vector>()),
+                 std::runtime_error);
+  }
+  {
+    // Missing fields
+    auto lines = "1:2:3 4:5";
+    EXPECT_THROW((split(lines, ' ')
+                    | eachToTuple<int, int, int>(':')
+                    | as<vector>()),
+                 std::runtime_error);
+  }
+}
+
+TEST(StringGen, EachToPair) {
+  {
+    // char delimiters
+    auto lines = "2:1.414 3:1.732";
+    auto actual
+      = split(lines, ' ')
+      | eachToPair<int, double>(':')
+      | as<std::map<int, double>>();
+    std::map<int, double> expected {
+      { 3, 1.732 },
+      { 2, 1.414 },
+    };
+    EXPECT_EQ(expected, actual);
+  }
+  {
+    // string delimiters
+    auto lines = "ab=>cd ef=>gh";
+    auto actual
+      = split(lines, ' ')
+      | eachToPair<string, string>("=>")
+      | as<std::map<string, string>>();
+    std::map<string, string> expected {
+      { "ab", "cd" },
+      { "ef", "gh" },
+    };
+    EXPECT_EQ(expected, actual);
+  }
+}
+
+TEST(StringGen, Resplit) {
+  auto collect = eachTo<std::string>() | as<vector>();
+  {
+    auto pieces = from({"hello,, world, goodbye, meow"}) |
+      resplit(',') | collect;
+    EXPECT_EQ(5, pieces.size());
+    EXPECT_EQ("hello", pieces[0]);
+    EXPECT_EQ("", pieces[1]);
+    EXPECT_EQ(" world", pieces[2]);
+    EXPECT_EQ(" goodbye", pieces[3]);
+    EXPECT_EQ(" meow", pieces[4]);
+  }
+  {
+    auto pieces = from({"hel", "lo,", ", world", ", goodbye, m", "eow"}) |
+      resplit(',') | collect;
+    EXPECT_EQ(5, pieces.size());
+    EXPECT_EQ("hello", pieces[0]);
+    EXPECT_EQ("", pieces[1]);
+    EXPECT_EQ(" world", pieces[2]);
+    EXPECT_EQ(" goodbye", pieces[3]);
+    EXPECT_EQ(" meow", pieces[4]);
+  }
+}
+
+template<typename F>
+void runUnsplitSuite(F fn) {
+  fn("hello, world");
+  fn("hello,world,goodbye");
+  fn(" ");
+  fn("");
+  fn(", ");
+  fn(", a, b,c");
+}
+
+TEST(StringGen, Unsplit) {
+
+  auto basicFn = [](StringPiece s) {
+    EXPECT_EQ(split(s, ',') | unsplit(','), s);
+  };
+
+  auto existingBuffer = [](StringPiece s) {
+    folly::fbstring buffer("asdf");
+    split(s, ',') | unsplit(',', &buffer);
+    auto expected = folly::to<folly::fbstring>(
+        "asdf", s.empty() ? "" : ",", s);
+    EXPECT_EQ(expected, buffer);
+  };
+
+  auto emptyBuffer = [](StringPiece s) {
+    std::string buffer;
+    split(s, ',') | unsplit(',', &buffer);
+    EXPECT_EQ(s, buffer);
+  };
+
+  auto stringDelim = [](StringPiece s) {
+    EXPECT_EQ(s, split(s, ',') | unsplit(","));
+    std::string buffer;
+    split(s, ',') | unsplit(",", &buffer);
+    EXPECT_EQ(buffer, s);
+  };
+
+  runUnsplitSuite(basicFn);
+  runUnsplitSuite(existingBuffer);
+  runUnsplitSuite(emptyBuffer);
+  runUnsplitSuite(stringDelim);
+  EXPECT_EQ("1, 2, 3", seq(1, 3) | unsplit(", "));
+}
+
+int main(int argc, char *argv[]) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  return RUN_ALL_TESTS();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/GroupVarint.cpp
@@ -0,0 +1,33 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/GroupVarint.h"
+
+#if HAVE_GROUP_VARINT
+namespace folly {
+
+const uint32_t GroupVarint32::kMask[] = {
+  0xff, 0xffff, 0xffffff, 0xffffffff
+};
+
+const uint64_t GroupVarint64::kMask[] = {
+  0xff, 0xffff, 0xffffff, 0xffffffff,
+  0xffffffffffULL, 0xffffffffffffULL, 0xffffffffffffffULL,
+  0xffffffffffffffffULL
+};
+
+}  // namespace folly
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/GroupVarint.h
@@ -0,0 +1,612 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_GROUPVARINT_H_
+#define FOLLY_GROUPVARINT_H_
+
+#ifndef __GNUC__
+#error GroupVarint.h requires GCC
+#endif
+
+#if defined(__x86_64__) || defined(__i386__)
+#define HAVE_GROUP_VARINT 1
+
+#include <cstdint>
+#include <limits>
+#include "folly/detail/GroupVarintDetail.h"
+#include "folly/Bits.h"
+#include "folly/Range.h"
+#include <glog/logging.h>
+
+#ifdef __SSSE3__
+#include <x86intrin.h>
+namespace folly {
+namespace detail {
+extern const __m128i groupVarintSSEMasks[];
+}  // namespace detail
+}  // namespace folly
+#endif
+
+namespace folly {
+namespace detail {
+extern const uint8_t groupVarintLengths[];
+}  // namespace detail
+}  // namespace folly
+
+namespace folly {
+
+template <typename T>
+class GroupVarint;
+
+/**
+ * GroupVarint encoding for 32-bit values.
+ *
+ * Encodes 4 32-bit integers at once, each using 1-4 bytes depending on size.
+ * There is one byte of overhead.  (The first byte contains the lengths of
+ * the four integers encoded as two bits each; 00=1 byte .. 11=4 bytes)
+ *
+ * This implementation assumes little-endian and does unaligned 32-bit
+ * accesses, so it's basically not portable outside of the x86[_64] world.
+ */
+template <>
+class GroupVarint<uint32_t> : public detail::GroupVarintBase<uint32_t> {
+ public:
+
+  /**
+   * Return the number of bytes used to encode these four values.
+   */
+  static size_t size(uint32_t a, uint32_t b, uint32_t c, uint32_t d) {
+    return kHeaderSize + kGroupSize + key(a) + key(b) + key(c) + key(d);
+  }
+
+  /**
+   * Return the number of bytes used to encode four uint32_t values stored
+   * at consecutive positions in an array.
+   */
+  static size_t size(const uint32_t* p) {
+    return size(p[0], p[1], p[2], p[3]);
+  }
+
+  /**
+   * Return the number of bytes used to encode count (<= 4) values.
+   * If you clip a buffer after these many bytes, you can still decode
+   * the first "count" values correctly (if the remaining size() -
+   * partialSize() bytes are filled with garbage).
+   */
+  static size_t partialSize(const type* p, size_t count) {
+    DCHECK_LE(count, kGroupSize);
+    size_t s = kHeaderSize + count;
+    for (; count; --count, ++p) {
+      s += key(*p);
+    }
+    return s;
+  }
+
+  /**
+   * Return the number of values from *p that are valid from an encoded
+   * buffer of size bytes.
+   */
+  static size_t partialCount(const char* p, size_t size) {
+    char v = *p;
+    size_t s = kHeaderSize;
+    s += 1 + b0key(v);
+    if (s > size) return 0;
+    s += 1 + b1key(v);
+    if (s > size) return 1;
+    s += 1 + b2key(v);
+    if (s > size) return 2;
+    s += 1 + b3key(v);
+    if (s > size) return 3;
+    return 4;
+  }
+
+  /**
+   * Given a pointer to the beginning of an GroupVarint32-encoded block,
+   * return the number of bytes used by the encoding.
+   */
+  static size_t encodedSize(const char* p) {
+    return (kHeaderSize + kGroupSize +
+            b0key(*p) + b1key(*p) + b2key(*p) + b3key(*p));
+  }
+
+  /**
+   * Encode four uint32_t values into the buffer pointed-to by p, and return
+   * the next position in the buffer (that is, one character past the last
+   * encoded byte).  p needs to have at least size()+4 bytes available.
+   */
+  static char* encode(char* p, uint32_t a, uint32_t b, uint32_t c, uint32_t d) {
+    uint8_t b0key = key(a);
+    uint8_t b1key = key(b);
+    uint8_t b2key = key(c);
+    uint8_t b3key = key(d);
+    *p++ = (b3key << 6) | (b2key << 4) | (b1key << 2) | b0key;
+    storeUnaligned(p, a);
+    p += b0key+1;
+    storeUnaligned(p, b);
+    p += b1key+1;
+    storeUnaligned(p, c);
+    p += b2key+1;
+    storeUnaligned(p, d);
+    p += b3key+1;
+    return p;
+  }
+
+  /**
+   * Encode four uint32_t values from the array pointed-to by src into the
+   * buffer pointed-to by p, similar to encode(p,a,b,c,d) above.
+   */
+  static char* encode(char* p, const uint32_t* src) {
+    return encode(p, src[0], src[1], src[2], src[3]);
+  }
+
+  /**
+   * Decode four uint32_t values from a buffer, and return the next position
+   * in the buffer (that is, one character past the last encoded byte).
+   * The buffer needs to have at least 3 extra bytes available (they
+   * may be read but ignored).
+   */
+  static const char* decode_simple(const char* p, uint32_t* a, uint32_t* b,
+                                   uint32_t* c, uint32_t* d) {
+    size_t k = loadUnaligned<uint8_t>(p);
+    const char* end = p + detail::groupVarintLengths[k];
+    ++p;
+    size_t k0 = b0key(k);
+    *a = loadUnaligned<uint32_t>(p) & kMask[k0];
+    p += k0+1;
+    size_t k1 = b1key(k);
+    *b = loadUnaligned<uint32_t>(p) & kMask[k1];
+    p += k1+1;
+    size_t k2 = b2key(k);
+    *c = loadUnaligned<uint32_t>(p) & kMask[k2];
+    p += k2+1;
+    size_t k3 = b3key(k);
+    *d = loadUnaligned<uint32_t>(p) & kMask[k3];
+    p += k3+1;
+    return end;
+  }
+
+  /**
+   * Decode four uint32_t values from a buffer and store them in the array
+   * pointed-to by dest, similar to decode(p,a,b,c,d) above.
+   */
+  static const char* decode_simple(const char* p, uint32_t* dest) {
+    return decode_simple(p, dest, dest+1, dest+2, dest+3);
+  }
+
+#ifdef __SSSE3__
+  static const char* decode(const char* p, uint32_t* dest) {
+    uint8_t key = p[0];
+    __m128i val = _mm_loadu_si128((const __m128i*)(p+1));
+    __m128i mask = detail::groupVarintSSEMasks[key];
+    __m128i r = _mm_shuffle_epi8(val, mask);
+    _mm_storeu_si128((__m128i*)dest, r);
+    return p + detail::groupVarintLengths[key];
+  }
+
+  static const char* decode(const char* p, uint32_t* a, uint32_t* b,
+                            uint32_t* c, uint32_t* d) {
+    uint8_t key = p[0];
+    __m128i val = _mm_loadu_si128((const __m128i*)(p+1));
+    __m128i mask = detail::groupVarintSSEMasks[key];
+    __m128i r = _mm_shuffle_epi8(val, mask);
+
+    // Extracting 32 bits at a time out of an XMM register is a SSE4 feature
+#ifdef __SSE4__
+    *a = _mm_extract_epi32(r, 0);
+    *b = _mm_extract_epi32(r, 1);
+    *c = _mm_extract_epi32(r, 2);
+    *d = _mm_extract_epi32(r, 3);
+#else  /* !__SSE4__ */
+    *a = _mm_extract_epi16(r, 0) + (_mm_extract_epi16(r, 1) << 16);
+    *b = _mm_extract_epi16(r, 2) + (_mm_extract_epi16(r, 3) << 16);
+    *c = _mm_extract_epi16(r, 4) + (_mm_extract_epi16(r, 5) << 16);
+    *d = _mm_extract_epi16(r, 6) + (_mm_extract_epi16(r, 7) << 16);
+#endif  /* __SSE4__ */
+
+    return p + detail::groupVarintLengths[key];
+  }
+
+#else  /* !__SSSE3__ */
+  static const char* decode(const char* p, uint32_t* a, uint32_t* b,
+                            uint32_t* c, uint32_t* d) {
+    return decode_simple(p, a, b, c, d);
+  }
+
+  static const char* decode(const char* p, uint32_t* dest) {
+    return decode_simple(p, dest);
+  }
+#endif  /* __SSSE3__ */
+
+ private:
+  static uint8_t key(uint32_t x) {
+    // __builtin_clz is undefined for the x==0 case
+    return 3 - (__builtin_clz(x|1) / 8);
+  }
+  static size_t b0key(size_t x) { return x & 3; }
+  static size_t b1key(size_t x) { return (x >> 2) & 3; }
+  static size_t b2key(size_t x) { return (x >> 4) & 3; }
+  static size_t b3key(size_t x) { return (x >> 6) & 3; }
+
+  static const uint32_t kMask[];
+};
+
+
+/**
+ * GroupVarint encoding for 64-bit values.
+ *
+ * Encodes 5 64-bit integers at once, each using 1-8 bytes depending on size.
+ * There are two bytes of overhead.  (The first two bytes contain the lengths
+ * of the five integers encoded as three bits each; 000=1 byte .. 111 = 8 bytes)
+ *
+ * This implementation assumes little-endian and does unaligned 64-bit
+ * accesses, so it's basically not portable outside of the x86[_64] world.
+ */
+template <>
+class GroupVarint<uint64_t> : public detail::GroupVarintBase<uint64_t> {
+ public:
+  /**
+   * Return the number of bytes used to encode these five values.
+   */
+  static size_t size(uint64_t a, uint64_t b, uint64_t c, uint64_t d,
+                     uint64_t e) {
+    return (kHeaderSize + kGroupSize +
+            key(a) + key(b) + key(c) + key(d) + key(e));
+  }
+
+  /**
+   * Return the number of bytes used to encode five uint64_t values stored
+   * at consecutive positions in an array.
+   */
+  static size_t size(const uint64_t* p) {
+    return size(p[0], p[1], p[2], p[3], p[4]);
+  }
+
+  /**
+   * Return the number of bytes used to encode count (<= 4) values.
+   * If you clip a buffer after these many bytes, you can still decode
+   * the first "count" values correctly (if the remaining size() -
+   * partialSize() bytes are filled with garbage).
+   */
+  static size_t partialSize(const type* p, size_t count) {
+    DCHECK_LE(count, kGroupSize);
+    size_t s = kHeaderSize + count;
+    for (; count; --count, ++p) {
+      s += key(*p);
+    }
+    return s;
+  }
+
+  /**
+   * Return the number of values from *p that are valid from an encoded
+   * buffer of size bytes.
+   */
+  static size_t partialCount(const char* p, size_t size) {
+    uint16_t v = loadUnaligned<uint16_t>(p);
+    size_t s = kHeaderSize;
+    s += 1 + b0key(v);
+    if (s > size) return 0;
+    s += 1 + b1key(v);
+    if (s > size) return 1;
+    s += 1 + b2key(v);
+    if (s > size) return 2;
+    s += 1 + b3key(v);
+    if (s > size) return 3;
+    s += 1 + b4key(v);
+    if (s > size) return 4;
+    return 5;
+  }
+
+  /**
+   * Given a pointer to the beginning of an GroupVarint64-encoded block,
+   * return the number of bytes used by the encoding.
+   */
+  static size_t encodedSize(const char* p) {
+    uint16_t n = loadUnaligned<uint16_t>(p);
+    return (kHeaderSize + kGroupSize +
+            b0key(n) + b1key(n) + b2key(n) + b3key(n) + b4key(n));
+  }
+
+  /**
+   * Encode five uint64_t values into the buffer pointed-to by p, and return
+   * the next position in the buffer (that is, one character past the last
+   * encoded byte).  p needs to have at least size()+8 bytes available.
+   */
+  static char* encode(char* p, uint64_t a, uint64_t b, uint64_t c,
+                      uint64_t d, uint64_t e) {
+    uint8_t b0key = key(a);
+    uint8_t b1key = key(b);
+    uint8_t b2key = key(c);
+    uint8_t b3key = key(d);
+    uint8_t b4key = key(e);
+    storeUnaligned<uint16_t>(
+        p,
+        (b4key << 12) | (b3key << 9) | (b2key << 6) | (b1key << 3) | b0key);
+    p += 2;
+    storeUnaligned(p, a);
+    p += b0key+1;
+    storeUnaligned(p, b);
+    p += b1key+1;
+    storeUnaligned(p, c);
+    p += b2key+1;
+    storeUnaligned(p, d);
+    p += b3key+1;
+    storeUnaligned(p, e);
+    p += b4key+1;
+    return p;
+  }
+
+  /**
+   * Encode five uint64_t values from the array pointed-to by src into the
+   * buffer pointed-to by p, similar to encode(p,a,b,c,d,e) above.
+   */
+  static char* encode(char* p, const uint64_t* src) {
+    return encode(p, src[0], src[1], src[2], src[3], src[4]);
+  }
+
+  /**
+   * Decode five uint64_t values from a buffer, and return the next position
+   * in the buffer (that is, one character past the last encoded byte).
+   * The buffer needs to have at least 7 bytes available (they may be read
+   * but ignored).
+   */
+  static const char* decode(const char* p, uint64_t* a, uint64_t* b,
+                            uint64_t* c, uint64_t* d, uint64_t* e) {
+    uint16_t k = loadUnaligned<uint16_t>(p);
+    p += 2;
+    uint8_t k0 = b0key(k);
+    *a = loadUnaligned<uint64_t>(p) & kMask[k0];
+    p += k0+1;
+    uint8_t k1 = b1key(k);
+    *b = loadUnaligned<uint64_t>(p) & kMask[k1];
+    p += k1+1;
+    uint8_t k2 = b2key(k);
+    *c = loadUnaligned<uint64_t>(p) & kMask[k2];
+    p += k2+1;
+    uint8_t k3 = b3key(k);
+    *d = loadUnaligned<uint64_t>(p) & kMask[k3];
+    p += k3+1;
+    uint8_t k4 = b4key(k);
+    *e = loadUnaligned<uint64_t>(p) & kMask[k4];
+    p += k4+1;
+    return p;
+  }
+
+  /**
+   * Decode five uint64_t values from a buffer and store them in the array
+   * pointed-to by dest, similar to decode(p,a,b,c,d,e) above.
+   */
+  static const char* decode(const char* p, uint64_t* dest) {
+    return decode(p, dest, dest+1, dest+2, dest+3, dest+4);
+  }
+
+ private:
+  enum { kHeaderBytes = 2 };
+
+  static uint8_t key(uint64_t x) {
+    // __builtin_clzll is undefined for the x==0 case
+    return 7 - (__builtin_clzll(x|1) / 8);
+  }
+
+  static uint8_t b0key(uint16_t x) { return x & 7; }
+  static uint8_t b1key(uint16_t x) { return (x >> 3) & 7; }
+  static uint8_t b2key(uint16_t x) { return (x >> 6) & 7; }
+  static uint8_t b3key(uint16_t x) { return (x >> 9) & 7; }
+  static uint8_t b4key(uint16_t x) { return (x >> 12) & 7; }
+
+  static const uint64_t kMask[];
+};
+
+typedef GroupVarint<uint32_t> GroupVarint32;
+typedef GroupVarint<uint64_t> GroupVarint64;
+
+/**
+ * Simplify use of GroupVarint* for the case where data is available one
+ * entry at a time (instead of one group at a time).  Handles buffering
+ * and an incomplete last chunk.
+ *
+ * Output is a function object that accepts character ranges:
+ * out(StringPiece) appends the given character range to the output.
+ */
+template <class T, class Output>
+class GroupVarintEncoder {
+ public:
+  typedef GroupVarint<T> Base;
+  typedef T type;
+
+  explicit GroupVarintEncoder(Output out)
+    : out_(out),
+      count_(0) {
+  }
+
+  ~GroupVarintEncoder() {
+    finish();
+  }
+
+  /**
+   * Add a value to the encoder.
+   */
+  void add(type val) {
+    buf_[count_++] = val;
+    if (count_ == Base::kGroupSize) {
+      char* p = Base::encode(tmp_, buf_);
+      out_(StringPiece(tmp_, p));
+      count_ = 0;
+    }
+  }
+
+  /**
+   * Finish encoding, flushing any buffered values if necessary.
+   * After finish(), the encoder is immediately ready to encode more data
+   * to the same output.
+   */
+  void finish() {
+    if (count_) {
+      // This is not strictly necessary, but it makes testing easy;
+      // uninitialized bytes are guaranteed to be recorded as taking one byte
+      // (not more).
+      for (size_t i = count_; i < Base::kGroupSize; i++) {
+        buf_[i] = 0;
+      }
+      Base::encode(tmp_, buf_);
+      out_(StringPiece(tmp_, Base::partialSize(buf_, count_)));
+      count_ = 0;
+    }
+  }
+
+  /**
+   * Return the appender that was used.
+   */
+  Output& output() {
+    return out_;
+  }
+  const Output& output() const {
+    return out_;
+  }
+
+  /**
+   * Reset the encoder, disregarding any state (except what was already
+   * flushed to the output, of course).
+   */
+  void clear() {
+    count_ = 0;
+  }
+
+ private:
+  Output out_;
+  char tmp_[Base::kMaxSize];
+  type buf_[Base::kGroupSize];
+  size_t count_;
+};
+
+/**
+ * Simplify use of GroupVarint* for the case where the last group in the
+ * input may be incomplete (but the exact size of the input is known).
+ * Allows for extracting values one at a time.
+ */
+template <typename T>
+class GroupVarintDecoder {
+ public:
+  typedef GroupVarint<T> Base;
+  typedef T type;
+
+  GroupVarintDecoder() { }
+
+  explicit GroupVarintDecoder(StringPiece data,
+                              size_t maxCount = (size_t)-1)
+    : rrest_(data.end()),
+      p_(data.data()),
+      end_(data.end()),
+      limit_(end_),
+      pos_(0),
+      count_(0),
+      remaining_(maxCount) {
+  }
+
+  void reset(StringPiece data, size_t maxCount = (size_t)-1) {
+    rrest_ = data.end();
+    p_ = data.data();
+    end_ = data.end();
+    limit_ = end_;
+    pos_ = 0;
+    count_ = 0;
+    remaining_ = maxCount;
+  }
+
+  /**
+   * Read and return the next value.
+   */
+  bool next(type* val) {
+    if (pos_ == count_) {
+      // refill
+      size_t rem = end_ - p_;
+      if (rem == 0 || remaining_ == 0) {
+        return false;
+      }
+      // next() attempts to read one full group at a time, and so we must have
+      // at least enough bytes readable after its end to handle the case if the
+      // last group is full.
+      //
+      // The best way to ensure this is to ensure that data has at least
+      // Base::kMaxSize - 1 bytes readable *after* the end, otherwise we'll copy
+      // into a temporary buffer.
+      if (limit_ - p_ < Base::kMaxSize) {
+        memcpy(tmp_, p_, rem);
+        p_ = tmp_;
+        end_ = p_ + rem;
+        limit_ = tmp_ + sizeof(tmp_);
+      }
+      pos_ = 0;
+      const char* n = Base::decode(p_, buf_);
+      if (n <= end_) {
+        // Full group could be decoded
+        if (remaining_ >= Base::kGroupSize) {
+          remaining_ -= Base::kGroupSize;
+          count_ = Base::kGroupSize;
+          p_ = n;
+        } else {
+          count_ = remaining_;
+          remaining_ = 0;
+          p_ += Base::partialSize(buf_, count_);
+        }
+      } else {
+        // Can't decode a full group
+        count_ = Base::partialCount(p_, end_ - p_);
+        if (remaining_ >= count_) {
+          remaining_ -= count_;
+          p_ = end_;
+        } else {
+          count_ = remaining_;
+          remaining_ = 0;
+          p_ += Base::partialSize(buf_, count_);
+        }
+        if (count_ == 0) {
+          return false;
+        }
+      }
+    }
+    *val = buf_[pos_++];
+    return true;
+  }
+
+  StringPiece rest() const {
+    // This is only valid after next() returned false
+    CHECK(pos_ == count_ && (p_ == end_ || remaining_ == 0));
+    // p_ may point to the internal buffer (tmp_), but we want
+    // to return subpiece of the original data
+    size_t size = end_ - p_;
+    return StringPiece(rrest_ - size, rrest_);
+  }
+
+ private:
+  const char* rrest_;
+  const char* p_;
+  const char* end_;
+  const char* limit_;
+  char tmp_[2 * Base::kMaxSize];
+  type buf_[Base::kGroupSize];
+  size_t pos_;
+  size_t count_;
+  size_t remaining_;
+};
+
+typedef GroupVarintDecoder<uint32_t> GroupVarint32Decoder;
+typedef GroupVarintDecoder<uint64_t> GroupVarint64Decoder;
+
+}  // namespace folly
+
+#endif /* defined(__x86_64__) || defined(__i386__) */
+#endif /* FOLLY_GROUPVARINT_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/Hash.h
@@ -0,0 +1,399 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_BASE_HASH_H_
+#define FOLLY_BASE_HASH_H_
+
+#include <cstring>
+#include <stdint.h>
+#include <string>
+#include <utility>
+#include <tuple>
+
+#include "folly/SpookyHashV1.h"
+#include "folly/SpookyHashV2.h"
+
+/*
+ * Various hashing functions.
+ */
+
+namespace folly { namespace hash {
+
+// This is a general-purpose way to create a single hash from multiple
+// hashable objects. hash_combine_generic takes a class Hasher implementing
+// hash<T>; hash_combine uses a default hasher StdHasher that uses std::hash.
+// hash_combine_generic hashes each argument and combines those hashes in
+// an order-dependent way to yield a new hash.
+
+
+// This is the Hash128to64 function from Google's cityhash (available
+// under the MIT License).  We use it to reduce multiple 64 bit hashes
+// into a single hash.
+inline size_t hash_128_to_64(const size_t upper, const size_t lower) {
+  // Murmur-inspired hashing.
+  const size_t kMul = 0x9ddfea08eb382d69ULL;
+  size_t a = (lower ^ upper) * kMul;
+  a ^= (a >> 47);
+  size_t b = (upper ^ a) * kMul;
+  b ^= (b >> 47);
+  b *= kMul;
+  return b;
+}
+
+// Never used, but gcc demands it.
+template <class Hasher>
+inline size_t hash_combine_generic() {
+  return 0;
+}
+
+template <class Hasher, typename T, typename... Ts>
+size_t hash_combine_generic(const T& t, const Ts&... ts) {
+  size_t seed = Hasher::hash(t);
+  if (sizeof...(ts) == 0) {
+    return seed;
+  }
+  size_t remainder = hash_combine_generic<Hasher>(ts...);
+  return hash_128_to_64(seed, remainder);
+}
+
+// Simply uses std::hash to hash.  Note that std::hash is not guaranteed
+// to be a very good hash function; provided std::hash doesn't collide on
+// the individual inputs, you are fine, but that won't be true for, say,
+// strings or pairs
+class StdHasher {
+ public:
+  template <typename T>
+  static size_t hash(const T& t) {
+    return std::hash<T>()(t);
+  }
+};
+
+template <typename T, typename... Ts>
+size_t hash_combine(const T& t, const Ts&... ts) {
+  return hash_combine_generic<StdHasher>(t, ts...);
+}
+
+//////////////////////////////////////////////////////////////////////
+
+/*
+ * Thomas Wang 64 bit mix hash function
+ */
+
+inline uint64_t twang_mix64(uint64_t key) {
+  key = (~key) + (key << 21);  // key *= (1 << 21) - 1; key -= 1;
+  key = key ^ (key >> 24);
+  key = key + (key << 3) + (key << 8);  // key *= 1 + (1 << 3) + (1 << 8)
+  key = key ^ (key >> 14);
+  key = key + (key << 2) + (key << 4);  // key *= 1 + (1 << 2) + (1 << 4)
+  key = key ^ (key >> 28);
+  key = key + (key << 31);  // key *= 1 + (1 << 31)
+  return key;
+}
+
+/*
+ * Inverse of twang_mix64
+ *
+ * Note that twang_unmix64 is significantly slower than twang_mix64.
+ */
+
+inline uint64_t twang_unmix64(uint64_t key) {
+  // See the comments in jenkins_rev_unmix32 for an explanation as to how this
+  // was generated
+  key *= 4611686016279904257U;
+  key ^= (key >> 28) ^ (key >> 56);
+  key *= 14933078535860113213U;
+  key ^= (key >> 14) ^ (key >> 28) ^ (key >> 42) ^ (key >> 56);
+  key *= 15244667743933553977U;
+  key ^= (key >> 24) ^ (key >> 48);
+  key = (key + 1) * 9223367638806167551U;
+  return key;
+}
+
+/*
+ * Thomas Wang downscaling hash function
+ */
+
+inline uint32_t twang_32from64(uint64_t key) {
+  key = (~key) + (key << 18);
+  key = key ^ (key >> 31);
+  key = key * 21;
+  key = key ^ (key >> 11);
+  key = key + (key << 6);
+  key = key ^ (key >> 22);
+  return (uint32_t) key;
+}
+
+/*
+ * Robert Jenkins' reversible 32 bit mix hash function
+ */
+
+inline uint32_t jenkins_rev_mix32(uint32_t key) {
+  key += (key << 12);  // key *= (1 + (1 << 12))
+  key ^= (key >> 22);
+  key += (key << 4);   // key *= (1 + (1 << 4))
+  key ^= (key >> 9);
+  key += (key << 10);  // key *= (1 + (1 << 10))
+  key ^= (key >> 2);
+  // key *= (1 + (1 << 7)) * (1 + (1 << 12))
+  key += (key << 7);
+  key += (key << 12);
+  return key;
+}
+
+/*
+ * Inverse of jenkins_rev_mix32
+ *
+ * Note that jenkinks_rev_unmix32 is significantly slower than
+ * jenkins_rev_mix32.
+ */
+
+inline uint32_t jenkins_rev_unmix32(uint32_t key) {
+  // These are the modular multiplicative inverses (in Z_2^32) of the
+  // multiplication factors in jenkins_rev_mix32, in reverse order.  They were
+  // computed using the Extended Euclidean algorithm, see
+  // http://en.wikipedia.org/wiki/Modular_multiplicative_inverse
+  key *= 2364026753U;
+
+  // The inverse of a ^= (a >> n) is
+  // b = a
+  // for (int i = n; i < 32; i += n) {
+  //   b ^= (a >> i);
+  // }
+  key ^=
+    (key >> 2) ^ (key >> 4) ^ (key >> 6) ^ (key >> 8) ^
+    (key >> 10) ^ (key >> 12) ^ (key >> 14) ^ (key >> 16) ^
+    (key >> 18) ^ (key >> 20) ^ (key >> 22) ^ (key >> 24) ^
+    (key >> 26) ^ (key >> 28) ^ (key >> 30);
+  key *= 3222273025U;
+  key ^= (key >> 9) ^ (key >> 18) ^ (key >> 27);
+  key *= 4042322161U;
+  key ^= (key >> 22);
+  key *= 16773121U;
+  return key;
+}
+
+/*
+ * Fowler / Noll / Vo (FNV) Hash
+ *     http://www.isthe.com/chongo/tech/comp/fnv/
+ */
+
+const uint32_t FNV_32_HASH_START = 2166136261UL;
+const uint64_t FNV_64_HASH_START = 14695981039346656037ULL;
+
+inline uint32_t fnv32(const char* s,
+                      uint32_t hash = FNV_32_HASH_START) {
+  for (; *s; ++s) {
+    hash += (hash << 1) + (hash << 4) + (hash << 7) +
+            (hash << 8) + (hash << 24);
+    hash ^= *s;
+  }
+  return hash;
+}
+
+inline uint32_t fnv32_buf(const void* buf,
+                          int n,
+                          uint32_t hash = FNV_32_HASH_START) {
+  const char* char_buf = reinterpret_cast<const char*>(buf);
+
+  for (int i = 0; i < n; ++i) {
+    hash += (hash << 1) + (hash << 4) + (hash << 7) +
+            (hash << 8) + (hash << 24);
+    hash ^= char_buf[i];
+  }
+
+  return hash;
+}
+
+inline uint32_t fnv32(const std::string& str,
+                      uint32_t hash = FNV_32_HASH_START) {
+  return fnv32_buf(str.data(), str.size(), hash);
+}
+
+inline uint64_t fnv64(const char* s,
+                      uint64_t hash = FNV_64_HASH_START) {
+  for (; *s; ++s) {
+    hash += (hash << 1) + (hash << 4) + (hash << 5) + (hash << 7) +
+      (hash << 8) + (hash << 40);
+    hash ^= *s;
+  }
+  return hash;
+}
+
+inline uint64_t fnv64_buf(const void* buf,
+                          int n,
+                          uint64_t hash = FNV_64_HASH_START) {
+  const char* char_buf = reinterpret_cast<const char*>(buf);
+
+  for (int i = 0; i < n; ++i) {
+    hash += (hash << 1) + (hash << 4) + (hash << 5) + (hash << 7) +
+      (hash << 8) + (hash << 40);
+    hash ^= char_buf[i];
+  }
+  return hash;
+}
+
+inline uint64_t fnv64(const std::string& str,
+                      uint64_t hash = FNV_64_HASH_START) {
+  return fnv64_buf(str.data(), str.size(), hash);
+}
+
+/*
+ * Paul Hsieh: http://www.azillionmonkeys.com/qed/hash.html
+ */
+
+#define get16bits(d) (*((const uint16_t*) (d)))
+
+inline uint32_t hsieh_hash32_buf(const void* buf, int len) {
+  const char* s = reinterpret_cast<const char*>(buf);
+  uint32_t hash = len;
+  uint32_t tmp;
+  int rem;
+
+  if (len <= 0 || buf == 0) {
+    return 0;
+  }
+
+  rem = len & 3;
+  len >>= 2;
+
+  /* Main loop */
+  for (;len > 0; len--) {
+    hash  += get16bits (s);
+    tmp    = (get16bits (s+2) << 11) ^ hash;
+    hash   = (hash << 16) ^ tmp;
+    s  += 2*sizeof (uint16_t);
+    hash  += hash >> 11;
+  }
+
+  /* Handle end cases */
+  switch (rem) {
+  case 3:
+    hash += get16bits(s);
+    hash ^= hash << 16;
+    hash ^= s[sizeof (uint16_t)] << 18;
+    hash += hash >> 11;
+    break;
+  case 2:
+    hash += get16bits(s);
+    hash ^= hash << 11;
+    hash += hash >> 17;
+    break;
+  case 1:
+    hash += *s;
+    hash ^= hash << 10;
+    hash += hash >> 1;
+  }
+
+  /* Force "avalanching" of final 127 bits */
+  hash ^= hash << 3;
+  hash += hash >> 5;
+  hash ^= hash << 4;
+  hash += hash >> 17;
+  hash ^= hash << 25;
+  hash += hash >> 6;
+
+  return hash;
+};
+
+#undef get16bits
+
+inline uint32_t hsieh_hash32(const char* s) {
+  return hsieh_hash32_buf(s, std::strlen(s));
+}
+
+inline uint32_t hsieh_hash32_str(const std::string& str) {
+  return hsieh_hash32_buf(str.data(), str.size());
+}
+
+//////////////////////////////////////////////////////////////////////
+
+} // namespace hash
+
+template<class Key>
+struct hasher;
+
+template<> struct hasher<int32_t> {
+  size_t operator()(int32_t key) const {
+    return hash::jenkins_rev_mix32(uint32_t(key));
+  }
+};
+
+template<> struct hasher<uint32_t> {
+  size_t operator()(uint32_t key) const {
+    return hash::jenkins_rev_mix32(key);
+  }
+};
+
+template<> struct hasher<int64_t> {
+  size_t operator()(int64_t key) const {
+    return hash::twang_mix64(uint64_t(key));
+  }
+};
+
+template<> struct hasher<uint64_t> {
+  size_t operator()(uint64_t key) const {
+    return hash::twang_mix64(key);
+  }
+};
+
+// recursion
+template <size_t index, typename... Ts>
+struct TupleHasher {
+  size_t operator()(std::tuple<Ts...> const& key) const {
+    return hash::hash_combine(
+      TupleHasher<index - 1, Ts...>()(key),
+      std::get<index>(key));
+  }
+};
+
+// base
+template <typename... Ts>
+struct TupleHasher<0, Ts...> {
+  size_t operator()(std::tuple<Ts...> const& key) const {
+    // we could do std::hash here directly, but hash_combine hides all the
+    // ugly templating implicitly
+    return hash::hash_combine(std::get<0>(key));
+  }
+};
+
+} // namespace folly
+
+// Custom hash functions.
+namespace std {
+  // Hash function for pairs. Requires default hash functions for both
+  // items in the pair.
+  template <typename T1, typename T2>
+  class hash<std::pair<T1, T2> > {
+  public:
+    size_t operator()(const std::pair<T1, T2>& x) const {
+      return folly::hash::hash_combine(x.first, x.second);
+    }
+  };
+
+  // Hash function for tuples. Requires default hash functions for all types.
+  template <typename... Ts>
+  struct hash<std::tuple<Ts...>> {
+    size_t operator()(std::tuple<Ts...> const& key) const {
+      folly::TupleHasher<
+        std::tuple_size<std::tuple<Ts...>>::value - 1, // start index
+        Ts...> hasher;
+
+      return hasher(key);
+    }
+  };
+} // namespace std
+
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/IndexedMemPool.h
@@ -0,0 +1,427 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_INDEXEDMEMPOOL_H
+#define FOLLY_INDEXEDMEMPOOL_H
+
+#include <stdint.h>
+#include <assert.h>
+#include <unistd.h>
+#include <sys/mman.h>
+#include <boost/noncopyable.hpp>
+#include <folly/AtomicStruct.h>
+#include <folly/detail/CacheLocality.h>
+
+namespace folly {
+
+namespace detail {
+template <typename Pool>
+struct IndexedMemPoolRecycler;
+}
+
+/// Instances of IndexedMemPool dynamically allocate and then pool
+/// their element type (T), returning 4-byte integer indices that can be
+/// passed to the pool's operator[] method to access or obtain pointers
+/// to the actual elements.  Once they are constructed, elements are
+/// never destroyed.  These two features are useful for lock-free
+/// algorithms.  The indexing behavior makes it easy to build tagged
+/// pointer-like-things, since a large number of elements can be managed
+/// using fewer bits than a full pointer.  The pooling behavior makes
+/// it safe to read from T-s even after they have been recycled, since
+/// it is guaranteed that the memory won't have been returned to the OS
+/// and unmapped (the algorithm must still use a mechanism to validate
+/// that the read was correct, but it doesn't have to worry about page
+/// faults), and if the elements use internal sequence numbers it can be
+/// guaranteed that there won't be an ABA match due to the element being
+/// overwritten with a different type that has the same bit pattern.
+///
+/// IMPORTANT: Space for extra elements is allocated to account for those
+/// that are inaccessible because they are in other local lists, so the
+/// actual number of items that can be allocated ranges from capacity to
+/// capacity + (NumLocalLists_-1)*LocalListLimit_.  This is important if
+/// you are trying to maximize the capacity of the pool while constraining
+/// the bit size of the resulting pointers, because the pointers will
+/// actually range up to the boosted capacity.  See maxIndexForCapacity
+/// and capacityForMaxIndex.
+///
+/// To avoid contention, NumLocalLists_ free lists of limited (less than
+/// or equal to LocalListLimit_) size are maintained, and each thread
+/// retrieves and returns entries from its associated local list.  If the
+/// local list becomes too large then elements are placed in bulk in a
+/// global free list.  This allows items to be efficiently recirculated
+/// from consumers to producers.  AccessSpreader is used to access the
+/// local lists, so there is no performance advantage to having more
+/// local lists than L1 caches.
+///
+/// The pool mmap-s the entire necessary address space when the pool is
+/// constructed, but delays element construction.  This means that only
+/// elements that are actually returned to the caller get paged into the
+/// process's resident set (RSS).
+template <typename T,
+          int NumLocalLists_ = 32,
+          int LocalListLimit_ = 200,
+          template<typename> class Atom = std::atomic>
+struct IndexedMemPool : boost::noncopyable {
+  typedef T value_type;
+
+  typedef std::unique_ptr<T, detail::IndexedMemPoolRecycler<IndexedMemPool>>
+      UniquePtr;
+
+  static_assert(LocalListLimit_ <= 255, "LocalListLimit must fit in 8 bits");
+  enum {
+    NumLocalLists = NumLocalLists_,
+    LocalListLimit = LocalListLimit_
+  };
+
+
+  // these are public because clients may need to reason about the number
+  // of bits required to hold indices from a pool, given its capacity
+
+  static constexpr uint32_t maxIndexForCapacity(uint32_t capacity) {
+    // index of uint32_t(-1) == UINT32_MAX is reserved for isAllocated tracking
+    return std::min(uint64_t(capacity) + (NumLocalLists - 1) * LocalListLimit,
+                    uint64_t(uint32_t(-1) - 1));
+  }
+
+  static constexpr uint32_t capacityForMaxIndex(uint32_t maxIndex) {
+    return maxIndex - (NumLocalLists - 1) * LocalListLimit;
+  }
+
+
+  /// Constructs a pool that can allocate at least _capacity_ elements,
+  /// even if all the local lists are full
+  explicit IndexedMemPool(uint32_t capacity)
+    : actualCapacity_(maxIndexForCapacity(capacity))
+    , size_(0)
+    , globalHead_(TaggedPtr{})
+  {
+    const size_t needed = sizeof(Slot) * (actualCapacity_ + 1);
+    long pagesize = sysconf(_SC_PAGESIZE);
+    mmapLength_ = ((needed - 1) & ~(pagesize - 1)) + pagesize;
+    assert(needed <= mmapLength_ && mmapLength_ < needed + pagesize);
+    assert((mmapLength_ % pagesize) == 0);
+
+    slots_ = static_cast<Slot*>(mmap(nullptr, mmapLength_,
+                                     PROT_READ | PROT_WRITE,
+                                     MAP_PRIVATE | MAP_ANONYMOUS, -1, 0));
+    if (slots_ == nullptr) {
+      assert(errno == ENOMEM);
+      throw std::bad_alloc();
+    }
+  }
+
+  /// Destroys all of the contained elements
+  ~IndexedMemPool() {
+    for (size_t i = size_; i > 0; --i) {
+      slots_[i].~Slot();
+    }
+    munmap(slots_, mmapLength_);
+  }
+
+  /// Returns a lower bound on the number of elements that may be
+  /// simultaneously allocated and not yet recycled.  Because of the
+  /// local lists it is possible that more elements than this are returned
+  /// successfully
+  size_t capacity() {
+    return capacityForMaxIndex(actualCapacity_);
+  }
+
+  /// Grants ownership of (*this)[retval], or returns 0 if no elements
+  /// are available
+  uint32_t allocIndex() {
+    return localPop(localHead());
+  }
+
+  /// If an element is available, returns a std::unique_ptr to it that will
+  /// recycle the element to the pool when it is reclaimed, otherwise returns
+  /// a null (falsy) std::unique_ptr
+  UniquePtr allocElem() {
+    auto idx = allocIndex();
+    auto ptr = idx == 0 ? nullptr : &slot(idx).elem;
+    return UniquePtr(ptr, typename UniquePtr::deleter_type(this));
+  }
+
+  /// Gives up ownership previously granted by alloc()
+  void recycleIndex(uint32_t idx) {
+    assert(isAllocated(idx));
+    localPush(localHead(), idx);
+  }
+
+  /// Provides access to the pooled element referenced by idx
+  T& operator[](uint32_t idx) {
+    return slot(idx).elem;
+  }
+
+  /// Provides access to the pooled element referenced by idx
+  const T& operator[](uint32_t idx) const {
+    return slot(idx).elem;
+  }
+
+  /// If elem == &pool[idx], then pool.locateElem(elem) == idx.  Also,
+  /// pool.locateElem(nullptr) == 0
+  uint32_t locateElem(const T* elem) const {
+    if (!elem) {
+      return 0;
+    }
+
+    static_assert(std::is_standard_layout<Slot>::value, "offsetof needs POD");
+
+    auto slot = reinterpret_cast<const Slot*>(
+        reinterpret_cast<const char*>(elem) - offsetof(Slot, elem));
+    auto rv = slot - slots_;
+
+    // this assert also tests that rv is in range
+    assert(elem == &(*this)[rv]);
+    return rv;
+  }
+
+  /// Returns true iff idx has been alloc()ed and not recycleIndex()ed
+  bool isAllocated(uint32_t idx) const {
+    return slot(idx).localNext == uint32_t(-1);
+  }
+
+
+ private:
+  ///////////// types
+
+  struct Slot {
+    T elem;
+    uint32_t localNext;
+    uint32_t globalNext;
+
+    Slot() : localNext{}, globalNext{} {}
+  };
+
+  struct TaggedPtr {
+    uint32_t idx;
+
+    // size is bottom 8 bits, tag in top 24.  g++'s code generation for
+    // bitfields seems to depend on the phase of the moon, plus we can
+    // do better because we can rely on other checks to avoid masking
+    uint32_t tagAndSize;
+
+    enum : uint32_t {
+        SizeBits = 8,
+        SizeMask = (1U << SizeBits) - 1,
+        TagIncr = 1U << SizeBits,
+    };
+
+    uint32_t size() const {
+      return tagAndSize & SizeMask;
+    }
+
+    TaggedPtr withSize(uint32_t repl) const {
+      assert(repl <= LocalListLimit);
+      return TaggedPtr{ idx, (tagAndSize & ~SizeMask) | repl };
+    }
+
+    TaggedPtr withSizeIncr() const {
+      assert(size() < LocalListLimit);
+      return TaggedPtr{ idx, tagAndSize + 1 };
+    }
+
+    TaggedPtr withSizeDecr() const {
+      assert(size() > 0);
+      return TaggedPtr{ idx, tagAndSize - 1 };
+    }
+
+    TaggedPtr withIdx(uint32_t repl) const {
+      return TaggedPtr{ repl, tagAndSize + TagIncr };
+    }
+
+    TaggedPtr withEmpty() const {
+      return withIdx(0).withSize(0);
+    }
+  };
+
+  struct FOLLY_ALIGN_TO_AVOID_FALSE_SHARING LocalList {
+    AtomicStruct<TaggedPtr,Atom> head;
+
+    LocalList() : head(TaggedPtr{}) {}
+  };
+
+  ////////// fields
+
+  /// the actual number of slots that we will allocate, to guarantee
+  /// that we will satisfy the capacity requested at construction time.
+  /// They will be numbered 1..actualCapacity_ (note the 1-based counting),
+  /// and occupy slots_[1..actualCapacity_].
+  size_t actualCapacity_;
+
+  /// the number of bytes allocated from mmap, which is a multiple of
+  /// the page size of the machine
+  size_t mmapLength_;
+
+  /// this records the number of slots that have actually been constructed.
+  /// To allow use of atomic ++ instead of CAS, we let this overflow.
+  /// The actual number of constructed elements is min(actualCapacity_,
+  /// size_)
+  std::atomic<uint32_t> size_;
+
+  /// raw storage, only 1..min(size_,actualCapacity_) (inclusive) are
+  /// actually constructed.  Note that slots_[0] is not constructed or used
+  Slot* FOLLY_ALIGN_TO_AVOID_FALSE_SHARING slots_;
+
+  /// use AccessSpreader to find your list.  We use stripes instead of
+  /// thread-local to avoid the need to grow or shrink on thread start
+  /// or join.   These are heads of lists chained with localNext
+  LocalList local_[NumLocalLists];
+
+  /// this is the head of a list of node chained by globalNext, that are
+  /// themselves each the head of a list chained by localNext
+  AtomicStruct<TaggedPtr,Atom> FOLLY_ALIGN_TO_AVOID_FALSE_SHARING globalHead_;
+
+  ///////////// private methods
+
+  size_t slotIndex(uint32_t idx) const {
+    assert(0 < idx &&
+           idx <= actualCapacity_ &&
+           idx <= size_.load(std::memory_order_acquire));
+    return idx;
+  }
+
+  Slot& slot(uint32_t idx) {
+    return slots_[slotIndex(idx)];
+  }
+
+  const Slot& slot(uint32_t idx) const {
+    return slots_[slotIndex(idx)];
+  }
+
+  // localHead references a full list chained by localNext.  s should
+  // reference slot(localHead), it is passed as a micro-optimization
+  void globalPush(Slot& s, uint32_t localHead) {
+    while (true) {
+      TaggedPtr gh = globalHead_.load(std::memory_order_acquire);
+      s.globalNext = gh.idx;
+      if (globalHead_.compare_exchange_strong(gh, gh.withIdx(localHead))) {
+        // success
+        return;
+      }
+    }
+  }
+
+  // idx references a single node
+  void localPush(AtomicStruct<TaggedPtr,Atom>& head, uint32_t idx) {
+    Slot& s = slot(idx);
+    TaggedPtr h = head.load(std::memory_order_acquire);
+    while (true) {
+      s.localNext = h.idx;
+
+      if (h.size() == LocalListLimit) {
+        // push will overflow local list, steal it instead
+        if (head.compare_exchange_strong(h, h.withEmpty())) {
+          // steal was successful, put everything in the global list
+          globalPush(s, idx);
+          return;
+        }
+      } else {
+        // local list has space
+        if (head.compare_exchange_strong(h, h.withIdx(idx).withSizeIncr())) {
+          // success
+          return;
+        }
+      }
+      // h was updated by failing CAS
+    }
+  }
+
+  // returns 0 if empty
+  uint32_t globalPop() {
+    while (true) {
+      TaggedPtr gh = globalHead_.load(std::memory_order_acquire);
+      if (gh.idx == 0 || globalHead_.compare_exchange_strong(
+                  gh, gh.withIdx(slot(gh.idx).globalNext))) {
+        // global list is empty, or pop was successful
+        return gh.idx;
+      }
+    }
+  }
+
+  // returns 0 if allocation failed
+  uint32_t localPop(AtomicStruct<TaggedPtr,Atom>& head) {
+    while (true) {
+      TaggedPtr h = head.load(std::memory_order_acquire);
+      if (h.idx != 0) {
+        // local list is non-empty, try to pop
+        Slot& s = slot(h.idx);
+        if (head.compare_exchange_strong(
+                    h, h.withIdx(s.localNext).withSizeDecr())) {
+          // success
+          s.localNext = uint32_t(-1);
+          return h.idx;
+        }
+        continue;
+      }
+
+      uint32_t idx = globalPop();
+      if (idx == 0) {
+        // global list is empty, allocate and construct new slot
+        if (size_.load(std::memory_order_relaxed) >= actualCapacity_ ||
+            (idx = ++size_) > actualCapacity_) {
+          // allocation failed
+          return 0;
+        }
+        // construct it
+        new (&slot(idx)) T();
+        slot(idx).localNext = uint32_t(-1);
+        return idx;
+      }
+
+      Slot& s = slot(idx);
+      if (head.compare_exchange_strong(
+                  h, h.withIdx(s.localNext).withSize(LocalListLimit))) {
+        // global list moved to local list, keep head for us
+        s.localNext = uint32_t(-1);
+        return idx;
+      }
+      // local bulk push failed, return idx to the global list and try again
+      globalPush(s, idx);
+    }
+  }
+
+  AtomicStruct<TaggedPtr,Atom>& localHead() {
+    auto stripe = detail::AccessSpreader<Atom>::current(NumLocalLists);
+    return local_[stripe].head;
+  }
+};
+
+namespace detail {
+
+/// This is a stateful Deleter functor, which allows std::unique_ptr
+/// to track elements allocated from an IndexedMemPool by tracking the
+/// associated pool.  See IndexedMemPool::allocElem.
+template <typename Pool>
+struct IndexedMemPoolRecycler {
+  Pool* pool;
+
+  explicit IndexedMemPoolRecycler(Pool* pool) : pool(pool) {}
+
+  IndexedMemPoolRecycler(const IndexedMemPoolRecycler<Pool>& rhs)
+      = default;
+  IndexedMemPoolRecycler& operator= (const IndexedMemPoolRecycler<Pool>& rhs)
+      = default;
+
+  void operator()(typename Pool::value_type* elem) const {
+    pool->recycleIndex(pool->locateElem(elem));
+  }
+};
+
+}
+
+} // namespace folly
+
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/IntrusiveList.h
@@ -0,0 +1,135 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_INTRUSIVELIST_H_
+#define FOLLY_INTRUSIVELIST_H_
+
+/*
+ * This file contains convenience typedefs that make boost::intrusive::list
+ * easier to use.
+ */
+
+#include <boost/intrusive/list.hpp>
+
+namespace folly {
+
+/**
+ * An auto-unlink intrusive list hook.
+ */
+typedef boost::intrusive::list_member_hook<
+      boost::intrusive::link_mode<boost::intrusive::auto_unlink> >
+        IntrusiveListHook;
+
+/**
+ * An intrusive list.
+ *
+ * An IntrusiveList always uses an auto-unlink hook.
+ * Beware that IntrusiveList::size() is an O(n) operation, since it has to walk
+ * the entire list.
+ *
+ * Example usage:
+ *
+ *   class Foo {
+ *     // Note that the listHook member variable needs to be visible
+ *     // to the code that defines the IntrusiveList instantiation.
+ *     // The list hook can be made public, or you can make the other class a
+ *     // friend.
+ *     IntrusiveListHook listHook;
+ *   };
+ *
+ *   typedef IntrusiveList<Foo, &Foo::listHook> FooList;
+ *
+ *   Foo *foo = new Foo();
+ *   FooList myList;
+ *   myList.push_back(*foo);
+ *
+ * Note that each IntrusiveListHook can only be part of a single list at any
+ * given time.  If you need the same object to be stored in two lists at once,
+ * you need to use two different IntrusiveListHook member variables.
+ *
+ * The elements stored in the list must contain an IntrusiveListHook member
+ * variable.
+ *
+ * TODO: This should really be a template alias.  However, gcc doesn't support
+ * template aliases yet.  A subclass is a reasonable workaround for now.  This
+ * subclass only supports the default constructor, but we could add other
+ * constructors if necessary.
+ */
+template<typename T, IntrusiveListHook T::* PtrToMember>
+class IntrusiveList : public boost::intrusive::list<
+    T,
+    boost::intrusive::member_hook<T, IntrusiveListHook, PtrToMember>,
+    boost::intrusive::constant_time_size<false> > {
+};
+
+/**
+ * A safe-link intrusive list hook.
+ */
+typedef boost::intrusive::list_member_hook<
+      boost::intrusive::link_mode<boost::intrusive::safe_link> >
+        SafeIntrusiveListHook;
+
+/**
+ * An intrusive list with const-time size() method.
+ *
+ * A CountedIntrusiveList always uses a safe-link hook.
+ * CountedIntrusiveList::size() is an O(1) operation. Users of this type
+ * of lists need to remove a member from a list by calling one of the
+ * methods on the list (e.g., erase(), pop_front(), etc.), rather than
+ * calling unlink on the member's list hook. Given references to a
+ * list and a member, a constant-time removal operation can be
+ * accomplished by list.erase(list.iterator_to(member)). Also, when a
+ * member is destroyed, it is NOT automatically removed from the list.
+ *
+ * Example usage:
+ *
+ *   class Foo {
+ *     // Note that the listHook member variable needs to be visible
+ *     // to the code that defines the CountedIntrusiveList instantiation.
+ *     // The list hook can be made public, or you can make the other class a
+ *     // friend.
+ *     SafeIntrusiveListHook listHook;
+ *   };
+ *
+ *   typedef CountedIntrusiveList<Foo, &Foo::listHook> FooList;
+ *
+ *   Foo *foo = new Foo();
+ *   FooList myList;
+ *   myList.push_back(*foo);
+ *   myList.pop_front();
+ *
+ * Note that each SafeIntrusiveListHook can only be part of a single list at any
+ * given time.  If you need the same object to be stored in two lists at once,
+ * you need to use two different SafeIntrusiveListHook member variables.
+ *
+ * The elements stored in the list must contain an SafeIntrusiveListHook member
+ * variable.
+ *
+ * TODO: This should really be a template alias.  However, gcc doesn't support
+ * template aliases yet.  A subclass is a reasonable workaround for now.  This
+ * subclass only supports the default constructor, but we could add other
+ * constructors if necessary.
+ */
+template<typename T, SafeIntrusiveListHook T::* PtrToMember>
+class CountedIntrusiveList : public boost::intrusive::list<
+    T,
+    boost::intrusive::member_hook<T, SafeIntrusiveListHook, PtrToMember>,
+    boost::intrusive::constant_time_size<true> > {
+};
+
+} // folly
+
+#endif // FOLLY_INTRUSIVELIST_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/async/AsyncTimeout.cpp
@@ -0,0 +1,156 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements. See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership. The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License. You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied. See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+#include "folly/io/async/AsyncTimeout.h"
+#include "folly/io/async/EventBase.h"
+#include "folly/io/async/EventUtil.h"
+#include "folly/io/async/Request.h"
+
+#include <assert.h>
+#include <glog/logging.h>
+
+namespace folly {
+
+AsyncTimeout::AsyncTimeout(TimeoutManager* timeoutManager)
+    : timeoutManager_(timeoutManager) {
+
+  event_set(&event_, -1, EV_TIMEOUT, &AsyncTimeout::libeventCallback, this);
+  event_.ev_base = nullptr;
+  timeoutManager_->attachTimeoutManager(
+      this,
+      TimeoutManager::InternalEnum::NORMAL);
+  RequestContext::getStaticContext();
+}
+
+AsyncTimeout::AsyncTimeout(EventBase* eventBase)
+    : timeoutManager_(eventBase) {
+
+  event_set(&event_, -1, EV_TIMEOUT, &AsyncTimeout::libeventCallback, this);
+  event_.ev_base = nullptr;
+  timeoutManager_->attachTimeoutManager(
+      this,
+      TimeoutManager::InternalEnum::NORMAL);
+  RequestContext::getStaticContext();
+}
+
+AsyncTimeout::AsyncTimeout(TimeoutManager* timeoutManager,
+                             InternalEnum internal)
+    : timeoutManager_(timeoutManager) {
+
+  event_set(&event_, -1, EV_TIMEOUT, &AsyncTimeout::libeventCallback, this);
+  event_.ev_base = nullptr;
+  timeoutManager_->attachTimeoutManager(this, internal);
+  RequestContext::getStaticContext();
+}
+
+AsyncTimeout::AsyncTimeout(EventBase* eventBase, InternalEnum internal)
+    : timeoutManager_(eventBase) {
+
+  event_set(&event_, -1, EV_TIMEOUT, &AsyncTimeout::libeventCallback, this);
+  event_.ev_base = nullptr;
+  timeoutManager_->attachTimeoutManager(this, internal);
+  RequestContext::getStaticContext();
+}
+
+AsyncTimeout::AsyncTimeout(): timeoutManager_(nullptr) {
+  event_set(&event_, -1, EV_TIMEOUT, &AsyncTimeout::libeventCallback, this);
+  event_.ev_base = nullptr;
+  RequestContext::getStaticContext();
+}
+
+AsyncTimeout::~AsyncTimeout() {
+  cancelTimeout();
+}
+
+bool AsyncTimeout::scheduleTimeout(std::chrono::milliseconds timeout) {
+  assert(timeoutManager_ != nullptr);
+  context_ = RequestContext::saveContext();
+  return timeoutManager_->scheduleTimeout(this, timeout);
+}
+
+bool AsyncTimeout::scheduleTimeout(uint32_t milliseconds) {
+  return scheduleTimeout(std::chrono::milliseconds(milliseconds));
+}
+
+void AsyncTimeout::cancelTimeout() {
+  if (isScheduled()) {
+    timeoutManager_->cancelTimeout(this);
+  }
+}
+
+bool AsyncTimeout::isScheduled() const {
+  return EventUtil::isEventRegistered(&event_);
+}
+
+void AsyncTimeout::attachTimeoutManager(
+    TimeoutManager* timeoutManager,
+    InternalEnum internal) {
+  // This also implies no timeout is scheduled.
+  assert(timeoutManager_ == nullptr);
+  assert(timeoutManager->isInTimeoutManagerThread());
+  timeoutManager_ = timeoutManager;
+
+  timeoutManager_->attachTimeoutManager(this, internal);
+}
+
+void AsyncTimeout::attachEventBase(
+    EventBase* eventBase,
+    InternalEnum internal) {
+  attachTimeoutManager(eventBase, internal);
+}
+
+void AsyncTimeout::detachTimeoutManager() {
+  // Only allow the event base to be changed if the timeout is not
+  // currently installed.
+  if (isScheduled()) {
+    // Programmer bug.  Abort the program.
+    LOG(ERROR) << "detachEventBase() called on scheduled timeout; aborting";
+    abort();
+    return;
+  }
+
+  if (timeoutManager_) {
+    timeoutManager_->detachTimeoutManager(this);
+    timeoutManager_ = nullptr;
+  }
+}
+
+void AsyncTimeout::detachEventBase() {
+  detachTimeoutManager();
+}
+
+void AsyncTimeout::libeventCallback(int fd, short events, void* arg) {
+  AsyncTimeout* timeout = reinterpret_cast<AsyncTimeout*>(arg);
+  assert(fd == -1);
+  assert(events == EV_TIMEOUT);
+
+  // double check that ev_flags gets reset when the timeout is not running
+  assert((timeout->event_.ev_flags & ~EVLIST_INTERNAL) == EVLIST_INIT);
+
+  // this can't possibly fire if timeout->eventBase_ is nullptr
+  (void) timeout->timeoutManager_->bumpHandlingTime();
+
+  auto old_ctx =
+    RequestContext::setContext(timeout->context_);
+
+  timeout->timeoutExpired();
+
+  RequestContext::setContext(old_ctx);
+}
+
+} // folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/async/AsyncTimeout.h
@@ -0,0 +1,164 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements. See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership. The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License. You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied. See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+#pragma once
+
+#include "folly/io/async/TimeoutManager.h"
+
+#include <boost/noncopyable.hpp>
+#include <event.h>
+#include <memory>
+
+namespace folly {
+
+class EventBase;
+class RequestContext;
+class TimeoutManager;
+
+/**
+ * AsyncTimeout is used to asynchronously wait for a timeout to occur.
+ */
+class AsyncTimeout : private boost::noncopyable {
+ public:
+  typedef TimeoutManager::InternalEnum InternalEnum;
+
+  /**
+   * Create a new AsyncTimeout object, driven by the specified TimeoutManager.
+   */
+  explicit AsyncTimeout(TimeoutManager* timeoutManager);
+  explicit AsyncTimeout(EventBase* eventBase);
+
+  /**
+   * Create a new internal AsyncTimeout object.
+   *
+   * Internal timeouts are like regular timeouts, but will not stop the
+   * TimeoutManager loop from exiting if the only remaining events are internal
+   * timeouts.
+   *
+   * This is useful for implementing fallback timeouts to abort the
+   * TimeoutManager loop if the other events have not been processed within a
+   * specified time period: if the event loop takes too long the timeout will
+   * fire and can stop the event loop.  However, if all other events complete,
+   * the event loop will exit even though the internal timeout is still
+   * installed.
+   */
+  AsyncTimeout(TimeoutManager* timeoutManager, InternalEnum internal);
+  AsyncTimeout(EventBase* eventBase, InternalEnum internal);
+
+  /**
+   * Create a new AsyncTimeout object, not yet assigned to a TimeoutManager.
+   *
+   * attachEventBase() must be called prior to scheduling the timeout.
+   */
+  AsyncTimeout();
+
+  /**
+   * AsyncTimeout destructor.
+   *
+   * The timeout will be automatically cancelled if it is running.
+   */
+  virtual ~AsyncTimeout();
+
+  /**
+   * timeoutExpired() is invoked when the timeout period has expired.
+   */
+  virtual void timeoutExpired() noexcept = 0;
+
+  /**
+   * Schedule the timeout to fire in the specified number of milliseconds.
+   *
+   * After the specified number of milliseconds has elapsed, timeoutExpired()
+   * will be invoked by the TimeoutManager's main loop.
+   *
+   * If the timeout is already running, it will be rescheduled with the
+   * new timeout value.
+   *
+   * @param milliseconds  The timeout duration, in milliseconds.
+   *
+   * @return Returns true if the timeout was successfully scheduled,
+   *         and false if an error occurred.  After an error, the timeout is
+   *         always unscheduled, even if scheduleTimeout() was just
+   *         rescheduling an existing timeout.
+   */
+  bool scheduleTimeout(uint32_t milliseconds);
+  bool scheduleTimeout(std::chrono::milliseconds timeout);
+
+  /**
+   * Cancel the timeout, if it is running.
+   */
+  void cancelTimeout();
+
+  /**
+   * Returns true if the timeout is currently scheduled.
+   */
+  bool isScheduled() const;
+
+  /**
+   * Attach the timeout to a TimeoutManager.
+   *
+   * This may only be called if the timeout is not currently attached to a
+   * TimeoutManager (either by using the default constructor, or by calling
+   * detachTimeoutManager()).
+   *
+   * This method must be invoked in the TimeoutManager's thread.
+   *
+   * The internal parameter specifies if this timeout should be treated as an
+   * internal event.  TimeoutManager::loop() will return when there are no more
+   * non-internal events remaining.
+   */
+  void attachTimeoutManager(TimeoutManager* timeoutManager,
+                            InternalEnum internal = InternalEnum::NORMAL);
+  void attachEventBase(EventBase* eventBase,
+                       InternalEnum internal = InternalEnum::NORMAL);
+
+  /**
+   * Detach the timeout from its TimeoutManager.
+   *
+   * This may only be called when the timeout is not running.
+   * Once detached, the timeout may not be scheduled again until it is
+   * re-attached to a EventBase by calling attachEventBase().
+   *
+   * This method must be called from the current TimeoutManager's thread.
+   */
+  void detachTimeoutManager();
+  void detachEventBase();
+
+  /**
+   * Returns the internal handle to the event
+   */
+  struct event* getEvent() {
+    return &event_;
+  }
+
+ private:
+  static void libeventCallback(int fd, short events, void* arg);
+
+  struct event event_;
+
+  /*
+   * Store a pointer to the TimeoutManager.  We only use this
+   * for some assert() statements, to make sure that AsyncTimeout is always
+   * used from the correct thread.
+   */
+  TimeoutManager* timeoutManager_;
+
+  // Save the request context for when the timeout fires.
+  std::shared_ptr<RequestContext> context_;
+};
+
+} // folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/async/EventBase.cpp
@@ -0,0 +1,660 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements. See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership. The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License. You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied. See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+#ifndef __STDC_FORMAT_MACROS
+#define __STDC_FORMAT_MACROS
+#endif
+
+#include "folly/io/async/EventBase.h"
+
+#include "folly/io/async/NotificationQueue.h"
+
+#include <boost/static_assert.hpp>
+#include <fcntl.h>
+#include <pthread.h>
+#include <unistd.h>
+
+namespace {
+
+using folly::Cob;
+using folly::EventBase;
+
+class Tr1FunctionLoopCallback : public EventBase::LoopCallback {
+ public:
+  explicit Tr1FunctionLoopCallback(const Cob& function)
+    : function_(function) {}
+
+  virtual void runLoopCallback() noexcept {
+    function_();
+    delete this;
+  }
+
+ private:
+  Cob function_;
+};
+
+}
+
+namespace folly {
+
+const int kNoFD = -1;
+
+/*
+ * EventBase::FunctionRunner
+ */
+
+class EventBase::FunctionRunner
+    : public NotificationQueue<std::pair<void (*)(void*), void*>>::Consumer {
+ public:
+  void messageAvailable(std::pair<void (*)(void*), void*>&& msg) {
+
+    // In libevent2, internal events do not break the loop.
+    // Most users would expect loop(), followed by runInEventBaseThread(),
+    // to break the loop and check if it should exit or not.
+    // To have similar bejaviour to libevent1.4, tell the loop to break here.
+    // Note that loop() may still continue to loop, but it will also check the
+    // stop_ flag as well as runInLoop callbacks, etc.
+    event_base_loopbreak(getEventBase()->evb_);
+
+    if (msg.first == nullptr && msg.second == nullptr) {
+      // terminateLoopSoon() sends a null message just to
+      // wake up the loop.  We can ignore these messages.
+      return;
+    }
+
+    // If function is nullptr, just log and move on
+    if (!msg.first) {
+      LOG(ERROR) << "nullptr callback registered to be run in "
+                 << "event base thread";
+      return;
+    }
+
+    // The function should never throw an exception, because we have no
+    // way of knowing what sort of error handling to perform.
+    //
+    // If it does throw, log a message and abort the program.
+    try {
+      msg.first(msg.second);
+    } catch (const std::exception& ex) {
+      LOG(ERROR) << "runInEventBaseThread() function threw a "
+                 << typeid(ex).name() << " exception: " << ex.what();
+      abort();
+    } catch (...) {
+      LOG(ERROR) << "runInEventBaseThread() function threw an exception";
+      abort();
+    }
+  }
+};
+
+/*
+ * EventBase::CobTimeout methods
+ */
+
+void EventBase::CobTimeout::timeoutExpired() noexcept {
+  // For now, we just swallow any exceptions that the callback threw.
+  try {
+    cob_();
+  } catch (const std::exception& ex) {
+    LOG(ERROR) << "EventBase::runAfterDelay() callback threw "
+               << typeid(ex).name() << " exception: " << ex.what();
+  } catch (...) {
+    LOG(ERROR) << "EventBase::runAfterDelay() callback threw non-exception "
+               << "type";
+  }
+
+  // The CobTimeout object was allocated on the heap by runAfterDelay(),
+  // so delete it now that the it has fired.
+  delete this;
+}
+
+/*
+ * EventBase methods
+ */
+
+EventBase::EventBase()
+  : runOnceCallbacks_(nullptr)
+  , stop_(false)
+  , loopThread_(0)
+  , evb_(static_cast<event_base*>(event_init()))
+  , queue_(nullptr)
+  , fnRunner_(nullptr)
+  , maxLatency_(0)
+  , avgLoopTime_(2000000)
+  , maxLatencyLoopTime_(avgLoopTime_)
+  , nextLoopCnt_(-40)       // Early wrap-around so bugs will manifest soon
+  , latestLoopCnt_(nextLoopCnt_)
+  , startWork_(0)
+  , observer_(nullptr)
+  , observerSampleCount_(0) {
+  VLOG(5) << "EventBase(): Created.";
+  initNotificationQueue();
+  RequestContext::getStaticContext();
+}
+
+// takes ownership of the event_base
+EventBase::EventBase(event_base* evb)
+  : runOnceCallbacks_(nullptr)
+  , stop_(false)
+  , loopThread_(0)
+  , evb_(evb)
+  , queue_(nullptr)
+  , fnRunner_(nullptr)
+  , maxLatency_(0)
+  , avgLoopTime_(2000000)
+  , maxLatencyLoopTime_(avgLoopTime_)
+  , nextLoopCnt_(-40)       // Early wrap-around so bugs will manifest soon
+  , latestLoopCnt_(nextLoopCnt_)
+  , startWork_(0)
+  , observer_(nullptr)
+  , observerSampleCount_(0) {
+  initNotificationQueue();
+  RequestContext::getStaticContext();
+}
+
+EventBase::~EventBase() {
+  // Delete any unfired CobTimeout objects, so that we don't leak memory
+  // (Note that we don't fire them.  The caller is responsible for cleaning up
+  // its own data structures if it destroys the EventBase with unfired events
+  // remaining.)
+  while (!pendingCobTimeouts_.empty()) {
+    CobTimeout* timeout = &pendingCobTimeouts_.front();
+    delete timeout;
+  }
+
+  (void) runLoopCallbacks(false);
+
+  // Stop consumer before deleting NotificationQueue
+  fnRunner_->stopConsuming();
+  event_base_free(evb_);
+  VLOG(5) << "EventBase(): Destroyed.";
+}
+
+int EventBase::getNotificationQueueSize() const {
+  return queue_->size();
+}
+
+// Set smoothing coefficient for loop load average; input is # of milliseconds
+// for exp(-1) decay.
+void EventBase::setLoadAvgMsec(uint32_t ms) {
+  uint64_t us = 1000 * ms;
+  if (ms > 0) {
+    maxLatencyLoopTime_.setTimeInterval(us);
+    avgLoopTime_.setTimeInterval(us);
+  } else {
+    LOG(ERROR) << "non-positive arg to setLoadAvgMsec()";
+  }
+}
+
+void EventBase::resetLoadAvg(double value) {
+  avgLoopTime_.reset(value);
+  maxLatencyLoopTime_.reset(value);
+}
+
+static int64_t getTimeDelta(int64_t *prev) {
+  int64_t now = std::chrono::duration_cast<std::chrono::milliseconds>(
+    std::chrono::steady_clock::now().time_since_epoch()).count();
+  int64_t delta = now - *prev;
+  *prev = now;
+  return delta;
+}
+
+void EventBase::waitUntilRunning() {
+  while (!isRunning()) {
+    sched_yield();
+  }
+}
+
+// enters the event_base loop -- will only exit when forced to
+bool EventBase::loop() {
+  VLOG(5) << "EventBase(): Starting loop.";
+  int res = 0;
+  bool ranLoopCallbacks;
+  int nonBlocking;
+
+  loopThread_.store(pthread_self(), std::memory_order_release);
+
+#if (__GLIBC__ >= 2) && (__GLIBC_MINOR__ >= 12)
+  if (!name_.empty()) {
+    pthread_setname_np(pthread_self(), name_.c_str());
+  }
+#endif
+
+  int64_t prev = std::chrono::duration_cast<std::chrono::milliseconds>(
+    std::chrono::steady_clock::now().time_since_epoch()).count();
+  int64_t idleStart = std::chrono::duration_cast<std::chrono::microseconds>(
+    std::chrono::steady_clock::now().time_since_epoch()).count();
+
+  // TODO: Read stop_ atomically with an acquire barrier.
+  while (!stop_) {
+    ++nextLoopCnt_;
+
+    // nobody can add loop callbacks from within this thread if
+    // we don't have to handle anything to start with...
+    nonBlocking = (loopCallbacks_.empty() ? 0 : EVLOOP_NONBLOCK);
+    res = event_base_loop(evb_, EVLOOP_ONCE | nonBlocking);
+    ranLoopCallbacks = runLoopCallbacks();
+
+    int64_t busy = std::chrono::duration_cast<std::chrono::microseconds>(
+      std::chrono::steady_clock::now().time_since_epoch()).count() - startWork_;
+    int64_t idle = startWork_ - idleStart;
+
+    avgLoopTime_.addSample(idle, busy);
+    maxLatencyLoopTime_.addSample(idle, busy);
+
+    if (observer_) {
+      if (observerSampleCount_++ == observer_->getSampleRate()) {
+        observerSampleCount_ = 0;
+        observer_->loopSample(busy, idle);
+      }
+    }
+
+    VLOG(11) << "EventBase " << this         << " did not timeout "
+     " loop time guess: "    << busy + idle  <<
+     " idle time: "          << idle         <<
+     " busy time: "          << busy         <<
+     " avgLoopTime: "        << avgLoopTime_.get() <<
+     " maxLatencyLoopTime: " << maxLatencyLoopTime_.get() <<
+     " maxLatency_: "        << maxLatency_ <<
+     " nothingHandledYet(): "<< nothingHandledYet();
+
+    // see if our average loop time has exceeded our limit
+    if ((maxLatency_ > 0) &&
+        (maxLatencyLoopTime_.get() > double(maxLatency_))) {
+      maxLatencyCob_();
+      // back off temporarily -- don't keep spamming maxLatencyCob_
+      // if we're only a bit over the limit
+      maxLatencyLoopTime_.dampen(0.9);
+    }
+
+    // Our loop run did real work; reset the idle timer
+    idleStart = std::chrono::duration_cast<std::chrono::microseconds>(
+      std::chrono::steady_clock::now().time_since_epoch()).count();
+
+    // If the event loop indicate that there were no more events, and
+    // we also didn't have any loop callbacks to run, there is nothing left to
+    // do.
+    if (res != 0 && !ranLoopCallbacks) {
+      // Since Notification Queue is marked 'internal' some events may not have
+      // run.  Run them manually if so, and continue looping.
+      //
+      if (getNotificationQueueSize() > 0) {
+        fnRunner_->handlerReady(0);
+      } else {
+        break;
+      }
+    }
+
+    VLOG(5) << "EventBase " << this << " loop time: " << getTimeDelta(&prev);
+  }
+  // Reset stop_ so loop() can be called again
+  stop_ = false;
+
+  if (res < 0) {
+    LOG(ERROR) << "EventBase: -- error in event loop, res = " << res;
+    return false;
+  } else if (res == 1) {
+    VLOG(5) << "EventBase: ran out of events (exiting loop)!";
+  } else if (res > 1) {
+    LOG(ERROR) << "EventBase: unknown event loop result = " << res;
+    return false;
+  }
+
+  loopThread_.store(0, std::memory_order_release);
+
+  VLOG(5) << "EventBase(): Done with loop.";
+  return true;
+}
+
+void EventBase::loopForever() {
+  // Update the notification queue event to treat it as a normal (non-internal)
+  // event.  The notification queue event always remains installed, and the main
+  // loop won't exit with it installed.
+  fnRunner_->stopConsuming();
+  fnRunner_->startConsuming(this, queue_.get());
+
+  bool ret = loop();
+
+  // Restore the notification queue internal flag
+  fnRunner_->stopConsuming();
+  fnRunner_->startConsumingInternal(this, queue_.get());
+
+  if (!ret) {
+    folly::throwSystemError("error in EventBase::loopForever()");
+  }
+}
+
+bool EventBase::bumpHandlingTime() {
+  VLOG(11) << "EventBase " << this << " " << __PRETTY_FUNCTION__ <<
+    " (loop) latest " << latestLoopCnt_ << " next " << nextLoopCnt_;
+  if(nothingHandledYet()) {
+    latestLoopCnt_ = nextLoopCnt_;
+    // set the time
+    startWork_ = std::chrono::duration_cast<std::chrono::microseconds>(
+      std::chrono::steady_clock::now().time_since_epoch()).count();
+
+    VLOG(11) << "EventBase " << this << " " << __PRETTY_FUNCTION__ <<
+      " (loop) startWork_ " << startWork_;
+    return true;
+  }
+  return false;
+}
+
+void EventBase::terminateLoopSoon() {
+  VLOG(5) << "EventBase(): Received terminateLoopSoon() command.";
+
+  if (!isRunning()) {
+    return;
+  }
+
+  // Set stop to true, so the event loop will know to exit.
+  // TODO: We should really use an atomic operation here with a release
+  // barrier.
+  stop_ = true;
+
+  // Call event_base_loopbreak() so that libevent will exit the next time
+  // around the loop.
+  event_base_loopbreak(evb_);
+
+  // If terminateLoopSoon() is called from another thread,
+  // the EventBase thread might be stuck waiting for events.
+  // In this case, it won't wake up and notice that stop_ is set until it
+  // receives another event.  Send an empty frame to the notification queue
+  // so that the event loop will wake up even if there are no other events.
+  //
+  // We don't care about the return value of trySendFrame().  If it fails
+  // this likely means the EventBase already has lots of events waiting
+  // anyway.
+  try {
+    queue_->putMessage(std::make_pair(nullptr, nullptr));
+  } catch (...) {
+    // We don't care if putMessage() fails.  This likely means
+    // the EventBase already has lots of events waiting anyway.
+  }
+}
+
+void EventBase::runInLoop(LoopCallback* callback, bool thisIteration) {
+  DCHECK(isInEventBaseThread());
+  callback->cancelLoopCallback();
+  callback->context_ = RequestContext::saveContext();
+  if (runOnceCallbacks_ != nullptr && thisIteration) {
+    runOnceCallbacks_->push_back(*callback);
+  } else {
+    loopCallbacks_.push_back(*callback);
+  }
+}
+
+void EventBase::runInLoop(const Cob& cob, bool thisIteration) {
+  DCHECK(isInEventBaseThread());
+  Tr1FunctionLoopCallback* wrapper = new Tr1FunctionLoopCallback(cob);
+  wrapper->context_ = RequestContext::saveContext();
+  if (runOnceCallbacks_ != nullptr && thisIteration) {
+    runOnceCallbacks_->push_back(*wrapper);
+  } else {
+    loopCallbacks_.push_back(*wrapper);
+  }
+}
+
+bool EventBase::runInEventBaseThread(void (*fn)(void*), void* arg) {
+  // Send the message.
+  // It will be received by the FunctionRunner in the EventBase's thread.
+
+  // We try not to schedule nullptr callbacks
+  if (!fn) {
+    LOG(ERROR) << "EventBase " << this
+               << ": Scheduling nullptr callbacks is not allowed";
+    return false;
+  }
+
+  // Short-circuit if we are already in our event base
+  if (inRunningEventBaseThread()) {
+    runInLoop(new RunInLoopCallback(fn, arg));
+    return true;
+
+  }
+
+  try {
+    queue_->putMessage(std::make_pair(fn, arg));
+  } catch (const std::exception& ex) {
+    LOG(ERROR) << "EventBase " << this << ": failed to schedule function "
+               << fn << "for EventBase thread: " << ex.what();
+    return false;
+  }
+
+  return true;
+}
+
+bool EventBase::runInEventBaseThread(const Cob& fn) {
+  // Short-circuit if we are already in our event base
+  if (inRunningEventBaseThread()) {
+    runInLoop(fn);
+    return true;
+  }
+
+  Cob* fnCopy;
+  // Allocate a copy of the function so we can pass it to the other thread
+  // The other thread will delete this copy once the function has been run
+  try {
+    fnCopy = new Cob(fn);
+  } catch (const std::bad_alloc& ex) {
+    LOG(ERROR) << "failed to allocate tr::function copy "
+               << "for runInEventBaseThread()";
+    return false;
+  }
+
+  if (!runInEventBaseThread(&EventBase::runTr1FunctionPtr, fnCopy)) {
+    delete fnCopy;
+    return false;
+  }
+
+  return true;
+}
+
+bool EventBase::runAfterDelay(const Cob& cob,
+                               int milliseconds,
+                               TimeoutManager::InternalEnum in) {
+  CobTimeout* timeout = new CobTimeout(this, cob, in);
+  if (!timeout->scheduleTimeout(milliseconds)) {
+    delete timeout;
+    return false;
+  }
+
+  pendingCobTimeouts_.push_back(*timeout);
+  return true;
+}
+
+bool EventBase::runLoopCallbacks(bool setContext) {
+  if (!loopCallbacks_.empty()) {
+    bumpHandlingTime();
+    // Swap the loopCallbacks_ list with a temporary list on our stack.
+    // This way we will only run callbacks scheduled at the time
+    // runLoopCallbacks() was invoked.
+    //
+    // If any of these callbacks in turn call runInLoop() to schedule more
+    // callbacks, those new callbacks won't be run until the next iteration
+    // around the event loop.  This prevents runInLoop() callbacks from being
+    // able to start file descriptor and timeout based events.
+    LoopCallbackList currentCallbacks;
+    currentCallbacks.swap(loopCallbacks_);
+    runOnceCallbacks_ = &currentCallbacks;
+
+    while (!currentCallbacks.empty()) {
+      LoopCallback* callback = &currentCallbacks.front();
+      currentCallbacks.pop_front();
+      if (setContext) {
+        RequestContext::setContext(callback->context_);
+      }
+      callback->runLoopCallback();
+    }
+
+    runOnceCallbacks_ = nullptr;
+    return true;
+  }
+  return false;
+}
+
+void EventBase::initNotificationQueue() {
+  // Infinite size queue
+  queue_.reset(new NotificationQueue<std::pair<void (*)(void*), void*>>());
+
+  // We allocate fnRunner_ separately, rather than declaring it directly
+  // as a member of EventBase solely so that we don't need to include
+  // NotificationQueue.h from EventBase.h
+  fnRunner_.reset(new FunctionRunner());
+
+  // Mark this as an internal event, so event_base_loop() will return if
+  // there are no other events besides this one installed.
+  //
+  // Most callers don't care about the internal notification queue used by
+  // EventBase.  The queue is always installed, so if we did count the queue as
+  // an active event, loop() would never exit with no more events to process.
+  // Users can use loopForever() if they do care about the notification queue.
+  // (This is useful for EventBase threads that do nothing but process
+  // runInEventBaseThread() notifications.)
+  fnRunner_->startConsumingInternal(this, queue_.get());
+}
+
+void EventBase::SmoothLoopTime::setTimeInterval(uint64_t timeInterval) {
+  expCoeff_ = -1.0/timeInterval;
+  VLOG(11) << "expCoeff_ " << expCoeff_ << " " << __PRETTY_FUNCTION__;
+}
+
+void EventBase::SmoothLoopTime::reset(double value) {
+  value_ = value;
+}
+
+void EventBase::SmoothLoopTime::addSample(int64_t idle, int64_t busy) {
+    /*
+     * Position at which the busy sample is considered to be taken.
+     * (Allows to quickly skew our average without editing much code)
+     */
+    enum BusySamplePosition {
+      RIGHT = 0,  // busy sample placed at the end of the iteration
+      CENTER = 1, // busy sample placed at the middle point of the iteration
+      LEFT = 2,   // busy sample placed at the beginning of the iteration
+    };
+
+  VLOG(11) << "idle " << idle << " oldBusyLeftover_ " << oldBusyLeftover_ <<
+              " idle + oldBusyLeftover_ " << idle + oldBusyLeftover_ <<
+              " busy " << busy << " " << __PRETTY_FUNCTION__;
+  idle += oldBusyLeftover_ + busy;
+  oldBusyLeftover_ = (busy * BusySamplePosition::CENTER) / 2;
+  idle -= oldBusyLeftover_;
+
+  double coeff = exp(idle * expCoeff_);
+  value_ *= coeff;
+  value_ += (1.0 - coeff) * busy;
+}
+
+bool EventBase::nothingHandledYet() {
+  VLOG(11) << "latest " << latestLoopCnt_ << " next " << nextLoopCnt_;
+  return (nextLoopCnt_ != latestLoopCnt_);
+}
+
+/* static */
+void EventBase::runTr1FunctionPtr(Cob* fn) {
+  // The function should never throw an exception, because we have no
+  // way of knowing what sort of error handling to perform.
+  //
+  // If it does throw, log a message and abort the program.
+  try {
+    (*fn)();
+  } catch (const std::exception &ex) {
+    LOG(ERROR) << "runInEventBaseThread() std::function threw a "
+               << typeid(ex).name() << " exception: " << ex.what();
+    abort();
+  } catch (...) {
+    LOG(ERROR) << "runInEventBaseThread() std::function threw an exception";
+    abort();
+  }
+
+  // The function object was allocated by runInEventBaseThread().
+  // Delete it once it has been run.
+  delete fn;
+}
+
+EventBase::RunInLoopCallback::RunInLoopCallback(void (*fn)(void*), void* arg)
+    : fn_(fn)
+    , arg_(arg) {}
+
+void EventBase::RunInLoopCallback::runLoopCallback() noexcept {
+  fn_(arg_);
+  delete this;
+}
+
+void EventBase::attachTimeoutManager(AsyncTimeout* obj,
+                                      InternalEnum internal) {
+
+  struct event* ev = obj->getEvent();
+  assert(ev->ev_base == nullptr);
+
+  event_base_set(getLibeventBase(), ev);
+  if (internal == AsyncTimeout::InternalEnum::INTERNAL) {
+    // Set the EVLIST_INTERNAL flag
+    ev->ev_flags |= EVLIST_INTERNAL;
+  }
+}
+
+void EventBase::detachTimeoutManager(AsyncTimeout* obj) {
+  cancelTimeout(obj);
+  struct event* ev = obj->getEvent();
+  ev->ev_base = nullptr;
+}
+
+bool EventBase::scheduleTimeout(AsyncTimeout* obj,
+                                 std::chrono::milliseconds timeout) {
+  assert(isInEventBaseThread());
+  // Set up the timeval and add the event
+  struct timeval tv;
+  tv.tv_sec = timeout.count() / 1000LL;
+  tv.tv_usec = (timeout.count() % 1000LL) * 1000LL;
+
+  struct event* ev = obj->getEvent();
+  if (event_add(ev, &tv) < 0) {
+    LOG(ERROR) << "EventBase: failed to schedule timeout: " << strerror(errno);
+    return false;
+  }
+
+  return true;
+}
+
+void EventBase::cancelTimeout(AsyncTimeout* obj) {
+  assert(isInEventBaseThread());
+  struct event* ev = obj->getEvent();
+  if (EventUtil::isEventRegistered(ev)) {
+    event_del(ev);
+  }
+}
+
+void EventBase::setName(const std::string& name) {
+  assert(isInEventBaseThread());
+  name_ = name;
+#if (__GLIBC__ >= 2) && (__GLIBC_MINOR__ >= 12)
+  if (isRunning()) {
+    pthread_setname_np(loopThread_.load(std::memory_order_relaxed),
+                       name_.c_str());
+  }
+#endif
+}
+
+const std::string& EventBase::getName() {
+  assert(isInEventBaseThread());
+  return name_;
+}
+
+} // folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/async/EventBase.h
@@ -0,0 +1,548 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements. See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership. The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License. You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied. See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+#pragma once
+
+#include <glog/logging.h>
+#include "folly/io/async/AsyncTimeout.h"
+#include "folly/io/async/TimeoutManager.h"
+#include <memory>
+#include <stack>
+#include <list>
+#include <queue>
+#include <cstdlib>
+#include <set>
+#include <utility>
+#include <boost/intrusive/list.hpp>
+#include <boost/utility.hpp>
+#include <functional>
+#include <event.h>  // libevent
+#include <errno.h>
+#include <math.h>
+#include <atomic>
+
+namespace folly {
+
+typedef std::function<void()> Cob;
+template <typename MessageT>
+class NotificationQueue;
+
+class EventBaseObserver {
+ public:
+  virtual ~EventBaseObserver() {}
+
+  virtual uint32_t getSampleRate() const = 0;
+
+  virtual void loopSample(
+    int64_t busyTime, int64_t idleTime) = 0;
+};
+
+/**
+ * This class is a wrapper for all asynchronous I/O processing functionality
+ *
+ * EventBase provides a main loop that notifies EventHandler callback objects
+ * when I/O is ready on a file descriptor, and notifies AsyncTimeout objects
+ * when a specified timeout has expired.  More complex, higher-level callback
+ * mechanisms can then be built on top of EventHandler and AsyncTimeout.
+ *
+ * A EventBase object can only drive an event loop for a single thread.  To
+ * take advantage of multiple CPU cores, most asynchronous I/O servers have one
+ * thread per CPU, and use a separate EventBase for each thread.
+ *
+ * In general, most EventBase methods may only be called from the thread
+ * running the EventBase's loop.  There are a few exceptions to this rule, for
+ * methods that are explicitly intended to allow communication with a
+ * EventBase from other threads.  When it is safe to call a method from
+ * another thread it is explicitly listed in the method comments.
+ */
+class EventBase : private boost::noncopyable, public TimeoutManager {
+ public:
+  /**
+   * A callback interface to use with runInLoop()
+   *
+   * Derive from this class if you need to delay some code execution until the
+   * next iteration of the event loop.  This allows you to schedule code to be
+   * invoked from the top-level of the loop, after your immediate callers have
+   * returned.
+   *
+   * If a LoopCallback object is destroyed while it is scheduled to be run in
+   * the next loop iteration, it will automatically be cancelled.
+   */
+  class LoopCallback {
+   public:
+    virtual ~LoopCallback() {}
+
+    virtual void runLoopCallback() noexcept = 0;
+    void cancelLoopCallback() {
+      hook_.unlink();
+    }
+
+    bool isLoopCallbackScheduled() const {
+      return hook_.is_linked();
+    }
+
+   private:
+    typedef boost::intrusive::list_member_hook<
+      boost::intrusive::link_mode<boost::intrusive::auto_unlink> > ListHook;
+
+    ListHook hook_;
+
+    typedef boost::intrusive::list<
+      LoopCallback,
+      boost::intrusive::member_hook<LoopCallback, ListHook,
+                                    &LoopCallback::hook_>,
+      boost::intrusive::constant_time_size<false> > List;
+
+    // EventBase needs access to LoopCallbackList (and therefore to hook_)
+    friend class EventBase;
+    std::shared_ptr<RequestContext> context_;
+  };
+
+  /**
+   * Create a new EventBase object.
+   */
+  EventBase();
+
+  /**
+   * Create a new EventBase object that will use the specified libevent
+   * event_base object to drive the event loop.
+   *
+   * The EventBase will take ownership of this event_base, and will call
+   * event_base_free(evb) when the EventBase is destroyed.
+   */
+  explicit EventBase(event_base* evb);
+  ~EventBase();
+
+  /**
+   * Runs the event loop.
+   *
+   * loop() will loop waiting for I/O or timeouts and invoking EventHandler
+   * and AsyncTimeout callbacks as their events become ready.  loop() will
+   * only return when there are no more events remaining to process, or after
+   * terminateLoopSoon() has been called.
+   *
+   * loop() may be called again to restart event processing after a previous
+   * call to loop() or loopForever() has returned.
+   *
+   * Returns true if the loop completed normally (if it processed all
+   * outstanding requests, or if terminateLoopSoon() was called).  If an error
+   * occurs waiting for events, false will be returned.
+   */
+  bool loop();
+
+  /**
+   * Runs the event loop.
+   *
+   * loopForever() behaves like loop(), except that it keeps running even if
+   * when there are no more user-supplied EventHandlers or AsyncTimeouts
+   * registered.  It will only return after terminateLoopSoon() has been
+   * called.
+   *
+   * This is useful for callers that want to wait for other threads to call
+   * runInEventBaseThread(), even when there are no other scheduled events.
+   *
+   * loopForever() may be called again to restart event processing after a
+   * previous call to loop() or loopForever() has returned.
+   *
+   * Throws a std::system_error if an error occurs.
+   */
+  void loopForever();
+
+  /**
+   * Causes the event loop to exit soon.
+   *
+   * This will cause an existing call to loop() or loopForever() to stop event
+   * processing and return, even if there are still events remaining to be
+   * processed.
+   *
+   * It is safe to call terminateLoopSoon() from another thread to cause loop()
+   * to wake up and return in the EventBase loop thread.  terminateLoopSoon()
+   * may also be called from the loop thread itself (for example, a
+   * EventHandler or AsyncTimeout callback may call terminateLoopSoon() to
+   * cause the loop to exit after the callback returns.)
+   *
+   * Note that the caller is responsible for ensuring that cleanup of all event
+   * callbacks occurs properly.  Since terminateLoopSoon() causes the loop to
+   * exit even when there are pending events present, there may be remaining
+   * callbacks present waiting to be invoked.  If the loop is later restarted
+   * pending events will continue to be processed normally, however if the
+   * EventBase is destroyed after calling terminateLoopSoon() it is the
+   * caller's responsibility to ensure that cleanup happens properly even if
+   * some outstanding events are never processed.
+   */
+  void terminateLoopSoon();
+
+  /**
+   * Adds the given callback to a queue of things run after the current pass
+   * through the event loop completes.  Note that if this callback calls
+   * runInLoop() the new callback won't be called until the main event loop
+   * has gone through a cycle.
+   *
+   * This method may only be called from the EventBase's thread.  This
+   * essentially allows an event handler to schedule an additional callback to
+   * be invoked after it returns.
+   *
+   * Use runInEventBaseThread() to schedule functions from another thread.
+   *
+   * The thisIteration parameter makes this callback run in this loop
+   * iteration, instead of the next one, even if called from a
+   * runInLoop callback (normal io callbacks that call runInLoop will
+   * always run in this iteration).  This was originally added to
+   * support detachEventBase, as a user callback may have called
+   * terminateLoopSoon(), but we want to make sure we detach.  Also,
+   * detachEventBase almost always must be called from the base event
+   * loop to ensure the stack is unwound, since most users of
+   * EventBase are not thread safe.
+   *
+   * Ideally we would not need thisIteration, and instead just use
+   * runInLoop with loop() (instead of terminateLoopSoon).
+   */
+  void runInLoop(LoopCallback* callback, bool thisIteration = false);
+
+  /**
+   * Convenience function to call runInLoop() with a std::function.
+   *
+   * This creates a LoopCallback object to wrap the std::function, and invoke
+   * the std::function when the loop callback fires.  This is slightly more
+   * expensive than defining your own LoopCallback, but more convenient in
+   * areas that aren't performance sensitive where you just want to use
+   * std::bind.  (std::bind is fairly slow on even by itself.)
+   *
+   * This method may only be called from the EventBase's thread.  This
+   * essentially allows an event handler to schedule an additional callback to
+   * be invoked after it returns.
+   *
+   * Use runInEventBaseThread() to schedule functions from another thread.
+   */
+  void runInLoop(const Cob& c, bool thisIteration = false);
+
+  /**
+   * Run the specified function in the EventBase's thread.
+   *
+   * This method is thread-safe, and may be called from another thread.
+   *
+   * If runInEventBaseThread() is called when the EventBase loop is not
+   * running, the function call will be delayed until the next time the loop is
+   * started.
+   *
+   * If runInEventBaseThread() returns true the function has successfully been
+   * scheduled to run in the loop thread.  However, if the loop is terminated
+   * (and never later restarted) before it has a chance to run the requested
+   * function, the function may never be run at all.  The caller is responsible
+   * for handling this situation correctly if they may terminate the loop with
+   * outstanding runInEventBaseThread() calls pending.
+   *
+   * If two calls to runInEventBaseThread() are made from the same thread, the
+   * functions will always be run in the order that they were scheduled.
+   * Ordering between functions scheduled from separate threads is not
+   * guaranteed.
+   *
+   * @param fn  The function to run.  The function must not throw any
+   *     exceptions.
+   * @param arg An argument to pass to the function.
+   *
+   * @return Returns true if the function was successfully scheduled, or false
+   *         if there was an error scheduling the function.
+   */
+  template<typename T>
+  bool runInEventBaseThread(void (*fn)(T*), T* arg) {
+    return runInEventBaseThread(reinterpret_cast<void (*)(void*)>(fn),
+                                reinterpret_cast<void*>(arg));
+  }
+
+  bool runInEventBaseThread(void (*fn)(void*), void* arg);
+
+  /**
+   * Run the specified function in the EventBase's thread
+   *
+   * This version of runInEventBaseThread() takes a std::function object.
+   * Note that this is less efficient than the version that takes a plain
+   * function pointer and void* argument, as it has to allocate memory to copy
+   * the std::function object.
+   *
+   * If the EventBase loop is terminated before it has a chance to run this
+   * function, the allocated memory will be leaked.  The caller is responsible
+   * for ensuring that the EventBase loop is not terminated before this
+   * function can run.
+   *
+   * The function must not throw any exceptions.
+   */
+  bool runInEventBaseThread(const Cob& fn);
+
+  /**
+   * Runs the given Cob at some time after the specified number of
+   * milliseconds.  (No guarantees exactly when.)
+   *
+   * @return  true iff the cob was successfully registered.
+   */
+  bool runAfterDelay(
+      const Cob& c,
+      int milliseconds,
+      TimeoutManager::InternalEnum = TimeoutManager::InternalEnum::NORMAL);
+
+  /**
+   * Set the maximum desired latency in us and provide a callback which will be
+   * called when that latency is exceeded.
+   */
+  void setMaxLatency(int64_t maxLatency, const Cob& maxLatencyCob) {
+    maxLatency_ = maxLatency;
+    maxLatencyCob_ = maxLatencyCob;
+  }
+
+  /**
+   * Set smoothing coefficient for loop load average; # of milliseconds
+   * for exp(-1) (1/2.71828...) decay.
+   */
+  void setLoadAvgMsec(uint32_t ms);
+
+  /**
+   * reset the load average to a desired value
+   */
+  void resetLoadAvg(double value = 0.0);
+
+  /**
+   * Get the average loop time in microseconds (an exponentially-smoothed ave)
+   */
+  double getAvgLoopTime() const {
+    return avgLoopTime_.get();
+  }
+
+  /**
+    * check if the event base loop is running.
+   */
+  bool isRunning() const {
+    return loopThread_.load(std::memory_order_relaxed) != 0;
+  }
+
+  /**
+   * wait until the event loop starts (after starting the event loop thread).
+   */
+  void waitUntilRunning();
+
+  int getNotificationQueueSize() const;
+
+  /**
+   * Verify that current thread is the EventBase thread, if the EventBase is
+   * running.
+   */
+  bool isInEventBaseThread() const {
+    auto tid = loopThread_.load(std::memory_order_relaxed);
+    return tid == 0 || pthread_equal(tid, pthread_self());
+  }
+
+  bool inRunningEventBaseThread() const {
+    return pthread_equal(
+      loopThread_.load(std::memory_order_relaxed), pthread_self());
+  }
+
+  // --------- interface to underlying libevent base ------------
+  // Avoid using these functions if possible.  These functions are not
+  // guaranteed to always be present if we ever provide alternative EventBase
+  // implementations that do not use libevent internally.
+  event_base* getLibeventBase() const { return evb_; }
+  static const char* getLibeventVersion() { return event_get_version(); }
+  static const char* getLibeventMethod() { return event_get_method(); }
+
+  /**
+   * only EventHandler/AsyncTimeout subclasses and ourselves should
+   * ever call this.
+   *
+   * This is used to mark the beginning of a new loop cycle by the
+   * first handler fired within that cycle.
+   *
+   */
+  bool bumpHandlingTime();
+
+  class SmoothLoopTime {
+   public:
+    explicit SmoothLoopTime(uint64_t timeInterval)
+      : expCoeff_(-1.0/timeInterval)
+      , value_(0.0)
+      , oldBusyLeftover_(0) {
+      VLOG(11) << "expCoeff_ " << expCoeff_ << " " << __PRETTY_FUNCTION__;
+    }
+
+    void setTimeInterval(uint64_t timeInterval);
+    void reset(double value = 0.0);
+
+    void addSample(int64_t idle, int64_t busy);
+
+    double get() const {
+      return value_;
+    }
+
+    void dampen(double factor) {
+      value_ *= factor;
+    }
+
+   private:
+    double  expCoeff_;
+    double  value_;
+    int64_t oldBusyLeftover_;
+  };
+
+  void setObserver(
+    const std::shared_ptr<EventBaseObserver>& observer) {
+    observer_ = observer;
+  }
+
+  const std::shared_ptr<EventBaseObserver>& getObserver() {
+    return observer_;
+  }
+
+  /**
+   * Set the name of the thread that runs this event base.
+   */
+  void setName(const std::string& name);
+
+  /**
+   * Returns the name of the thread that runs this event base.
+   */
+  const std::string& getName();
+
+ private:
+
+  // TimeoutManager
+  void attachTimeoutManager(AsyncTimeout* obj,
+                            TimeoutManager::InternalEnum internal);
+
+  void detachTimeoutManager(AsyncTimeout* obj);
+
+  bool scheduleTimeout(AsyncTimeout* obj, std::chrono::milliseconds timeout);
+
+  void cancelTimeout(AsyncTimeout* obj);
+
+  bool isInTimeoutManagerThread() {
+    return isInEventBaseThread();
+  }
+
+  // Helper class used to short circuit runInEventBaseThread
+  class RunInLoopCallback : public LoopCallback {
+   public:
+    RunInLoopCallback(void (*fn)(void*), void* arg);
+    void runLoopCallback() noexcept;
+
+   private:
+    void (*fn_)(void*);
+    void* arg_;
+  };
+
+  /*
+   * Helper function that tells us whether we have already handled
+   * some event/timeout/callback in this loop iteration.
+   */
+  bool nothingHandledYet();
+
+  // --------- libevent callbacks (not for client use) ------------
+
+  static void runTr1FunctionPtr(std::function<void()>* fn);
+
+  // small object used as a callback arg with enough info to execute the
+  // appropriate client-provided Cob
+  class CobTimeout : public AsyncTimeout {
+   public:
+    CobTimeout(EventBase* b, const Cob& c, TimeoutManager::InternalEnum in)
+        : AsyncTimeout(b, in), cob_(c) {}
+
+    virtual void timeoutExpired() noexcept;
+
+   private:
+    Cob cob_;
+
+   public:
+    typedef boost::intrusive::list_member_hook<
+      boost::intrusive::link_mode<boost::intrusive::auto_unlink> > ListHook;
+
+    ListHook hook;
+
+    typedef boost::intrusive::list<
+      CobTimeout,
+      boost::intrusive::member_hook<CobTimeout, ListHook, &CobTimeout::hook>,
+      boost::intrusive::constant_time_size<false> > List;
+  };
+
+  typedef LoopCallback::List LoopCallbackList;
+  class FunctionRunner;
+
+  // executes any callbacks queued by runInLoop(); returns false if none found
+  bool runLoopCallbacks(bool setContext = true);
+
+  void initNotificationQueue();
+
+  CobTimeout::List pendingCobTimeouts_;
+
+  LoopCallbackList loopCallbacks_;
+
+  // This will be null most of the time, but point to currentCallbacks
+  // if we are in the middle of running loop callbacks, such that
+  // runInLoop(..., true) will always run in the current loop
+  // iteration.
+  LoopCallbackList* runOnceCallbacks_;
+
+  // stop_ is set by terminateLoopSoon() and is used by the main loop
+  // to determine if it should exit
+  bool stop_;
+
+  // The ID of the thread running the main loop.
+  // 0 if loop is not running.
+  // Note: POSIX doesn't guarantee that 0 is an invalid pthread_t (or
+  // even that atomic<pthread_t> is valid), but that's how it is
+  // everywhere (at least on Linux, FreeBSD, and OSX).
+  std::atomic<pthread_t> loopThread_;
+
+  // pointer to underlying event_base class doing the heavy lifting
+  event_base* evb_;
+
+  // A notification queue for runInEventBaseThread() to use
+  // to send function requests to the EventBase thread.
+  std::unique_ptr<NotificationQueue<std::pair<void (*)(void*), void*>>> queue_;
+  std::unique_ptr<FunctionRunner> fnRunner_;
+
+  // limit for latency in microseconds (0 disables)
+  int64_t maxLatency_;
+
+  // exponentially-smoothed average loop time for latency-limiting
+  SmoothLoopTime avgLoopTime_;
+
+  // smoothed loop time used to invoke latency callbacks; differs from
+  // avgLoopTime_ in that it's scaled down after triggering a callback
+  // to reduce spamminess
+  SmoothLoopTime maxLatencyLoopTime_;
+
+  // callback called when latency limit is exceeded
+  Cob maxLatencyCob_;
+
+  // we'll wait this long before running deferred callbacks if the event
+  // loop is idle.
+  static const int kDEFAULT_IDLE_WAIT_USEC = 20000; // 20ms
+
+  // Wrap-around loop counter to detect beginning of each loop
+  uint64_t nextLoopCnt_;
+  uint64_t latestLoopCnt_;
+  uint64_t startWork_;
+
+  // Observer to export counters
+  std::shared_ptr<EventBaseObserver> observer_;
+  uint32_t observerSampleCount_;
+
+  // Name of the thread running this EventBase
+  std::string name_;
+};
+
+} // folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/async/EventFDWrapper.h
@@ -0,0 +1,65 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements. See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership. The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License. You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied. See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+/**
+ * Work around the lack of <sys/eventfd.h> on glibc 2.5.1 which we still
+ * need to support, sigh.
+ */
+
+#pragma once
+
+#include <features.h>
+
+// <sys/eventfd.h> doesn't exist on older glibc versions
+#if (defined(__GLIBC__) && __GLIBC_PREREQ(2, 9))
+#include <sys/eventfd.h>
+#else /* !(defined(__GLIBC__) && __GLIBC_PREREQ(2, 9)) */
+
+#include <sys/syscall.h>
+#include <unistd.h>
+#include <fcntl.h>
+
+// Use existing __NR_eventfd2 if already defined
+// Values from the Linux kernel source:
+// arch/x86/include/asm/unistd_{32,64}.h
+#ifndef __NR_eventfd2
+#if defined(__x86_64__)
+#define __NR_eventfd2  290
+#elif defined(__i386__)
+#define __NR_eventfd2  328
+#else
+#error "Can't define __NR_eventfd2 for your architecture."
+#endif
+#endif
+
+enum
+  {
+    EFD_SEMAPHORE = 1,
+#define EFD_SEMAPHORE EFD_SEMAPHORE
+    EFD_CLOEXEC = 02000000,
+#define EFD_CLOEXEC EFD_CLOEXEC
+    EFD_NONBLOCK = 04000
+#define EFD_NONBLOCK EFD_NONBLOCK
+  };
+
+// http://www.kernel.org/doc/man-pages/online/pages/man2/eventfd.2.html
+// Use the eventfd2 system call, as in glibc 2.9+
+// (requires kernel 2.6.30+)
+#define eventfd(initval, flags) syscall(__NR_eventfd2,(initval),(flags))
+
+#endif /* !(defined(__GLIBC__) && __GLIBC_PREREQ(2, 9)) */
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/async/EventHandler.cpp
@@ -0,0 +1,168 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements. See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership. The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License. You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied. See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+#include "folly/io/async/EventHandler.h"
+#include "folly/io/async/EventBase.h"
+
+#include <assert.h>
+
+namespace folly {
+
+EventHandler::EventHandler(EventBase* eventBase, int fd) {
+  event_set(&event_, fd, 0, &EventHandler::libeventCallback, this);
+  if (eventBase != nullptr) {
+    setEventBase(eventBase);
+  } else {
+    // Callers must set the EventBase and fd before using this timeout.
+    // Set event_->ev_base to nullptr to ensure that this happens.
+    // (otherwise libevent will initialize it to the "default" event_base)
+    event_.ev_base = nullptr;
+    eventBase_ = nullptr;
+  }
+}
+
+EventHandler::~EventHandler() {
+  unregisterHandler();
+}
+
+bool EventHandler::registerImpl(uint16_t events, bool internal) {
+  assert(event_.ev_base != nullptr);
+
+  // We have to unregister the event before we can change the event flags
+  if (isHandlerRegistered()) {
+    // If the new events are the same are the same as the already registered
+    // flags, we don't have to do anything.  Just return.
+    if (events == event_.ev_events &&
+        static_cast<bool>(event_.ev_flags & EVLIST_INTERNAL) == internal) {
+      return true;
+    }
+
+    event_del(&event_);
+  }
+
+  // Update the event flags
+  // Unfortunately, event_set() resets the event_base, so we have to remember
+  // it before hand, then pass it back into event_base_set() afterwards
+  struct event_base* evb = event_.ev_base;
+  event_set(&event_, event_.ev_fd, events,
+            &EventHandler::libeventCallback, this);
+  event_base_set(evb, &event_);
+
+  // Set EVLIST_INTERNAL if this is an internal event
+  if (internal) {
+    event_.ev_flags |= EVLIST_INTERNAL;
+  }
+
+  // Add the event.
+  //
+  // Although libevent allows events to wait on both I/O and a timeout,
+  // we intentionally don't allow an EventHandler to also use a timeout.
+  // Callers must maintain a separate AsyncTimeout object if they want a
+  // timeout.
+  //
+  // Otherwise, it is difficult to handle persistent events properly.  (The I/O
+  // event and timeout may both fire together the same time around the event
+  // loop.  Normally we would want to inform the caller of the I/O event first,
+  // then the timeout.  However, it is difficult to do this properly since the
+  // I/O callback could delete the EventHandler.)  Additionally, if a caller
+  // uses the same struct event for both I/O and timeout, and they just want to
+  // reschedule the timeout, libevent currently makes an epoll_ctl() call even
+  // if the I/O event flags haven't changed.  Using a separate event struct is
+  // therefore slightly more efficient in this case (although it does take up
+  // more space).
+  if (event_add(&event_, nullptr) < 0) {
+    LOG(ERROR) << "EventBase: failed to register event handler for fd "
+               << event_.ev_fd << ": " << strerror(errno);
+    // Call event_del() to make sure the event is completely uninstalled
+    event_del(&event_);
+    return false;
+  }
+
+  return true;
+}
+
+void EventHandler::unregisterHandler() {
+  if (isHandlerRegistered()) {
+    event_del(&event_);
+  }
+}
+
+void EventHandler::attachEventBase(EventBase* eventBase) {
+  // attachEventBase() may only be called on detached handlers
+  assert(event_.ev_base == nullptr);
+  assert(!isHandlerRegistered());
+  // This must be invoked from the EventBase's thread
+  assert(eventBase->isInEventBaseThread());
+
+  setEventBase(eventBase);
+}
+
+void EventHandler::detachEventBase() {
+  ensureNotRegistered(__func__);
+  event_.ev_base = nullptr;
+}
+
+void EventHandler::changeHandlerFD(int fd) {
+  ensureNotRegistered(__func__);
+  // event_set() resets event_base.ev_base, so manually restore it afterwards
+  struct event_base* evb = event_.ev_base;
+  event_set(&event_, fd, 0, &EventHandler::libeventCallback, this);
+  event_.ev_base = evb; // don't use event_base_set(), since evb may be nullptr
+}
+
+void EventHandler::initHandler(EventBase* eventBase, int fd) {
+  ensureNotRegistered(__func__);
+  event_set(&event_, fd, 0, &EventHandler::libeventCallback, this);
+  setEventBase(eventBase);
+}
+
+void EventHandler::ensureNotRegistered(const char* fn) {
+  // Neither the EventBase nor file descriptor may be changed while the
+  // handler is registered.  Treat it as a programmer bug and abort the program
+  // if this requirement is violated.
+  if (isHandlerRegistered()) {
+    LOG(ERROR) << fn << " called on registered handler; aborting";
+    abort();
+  }
+}
+
+void EventHandler::libeventCallback(int fd, short events, void* arg) {
+  EventHandler* handler = reinterpret_cast<EventHandler*>(arg);
+  assert(fd == handler->event_.ev_fd);
+
+  // this can't possibly fire if handler->eventBase_ is nullptr
+  (void) handler->eventBase_->bumpHandlingTime();
+
+  handler->handlerReady(events);
+}
+
+void EventHandler::setEventBase(EventBase* eventBase) {
+  event_base_set(eventBase->getLibeventBase(), &event_);
+  eventBase_ = eventBase;
+}
+
+bool EventHandler::isPending() {
+  if (event_.ev_flags & EVLIST_ACTIVE) {
+    if (event_.ev_res & EV_READ) {
+      return true;
+    }
+  }
+  return false;
+}
+
+} // folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/async/EventHandler.h
@@ -0,0 +1,184 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements. See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership. The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License. You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied. See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+#pragma once
+
+#include <glog/logging.h>
+#include "folly/io/async/EventUtil.h"
+#include <boost/noncopyable.hpp>
+#include <stddef.h>
+
+namespace folly {
+
+class EventBase;
+
+/**
+ * The EventHandler class is used to asynchronously wait for events on a file
+ * descriptor.
+ *
+ * Users that wish to wait on I/O events should derive from EventHandler and
+ * implement the handlerReady() method.
+ */
+class EventHandler : private boost::noncopyable {
+ public:
+  enum EventFlags {
+    NONE = 0,
+    READ = EV_READ,
+    WRITE = EV_WRITE,
+    READ_WRITE = (READ | WRITE),
+    PERSIST = EV_PERSIST
+  };
+
+  /**
+   * Create a new EventHandler object.
+   *
+   * @param eventBase  The EventBase to use to drive this event handler.
+   *                   This may be nullptr, in which case the EventBase must be
+   *                   set separately using initHandler() or attachEventBase()
+   *                   before the handler can be registered.
+   * @param fd         The file descriptor that this EventHandler will
+   *                   monitor.  This may be -1, in which case the file
+   *                   descriptor must be set separately using initHandler() or
+   *                   changeHandlerFD() before the handler can be registered.
+   */
+  explicit EventHandler(EventBase* eventBase = nullptr, int fd = -1);
+
+  /**
+   * EventHandler destructor.
+   *
+   * The event will be automatically unregistered if it is still registered.
+   */
+  virtual ~EventHandler();
+
+  /**
+   * handlerReady() is invoked when the handler is ready.
+   *
+   * @param events  A bitset indicating the events that are ready.
+   */
+  virtual void handlerReady(uint16_t events) noexcept = 0;
+
+  /**
+   * Register the handler.
+   *
+   * If the handler is already registered, the registration will be updated
+   * to wait on the new set of events.
+   *
+   * @param events   A bitset specifying the events to monitor.
+   *                 If the PERSIST bit is set, the handler will remain
+   *                 registered even after handlerReady() is called.
+   *
+   * @return Returns true if the handler was successfully registered,
+   *         or false if an error occurred.  After an error, the handler is
+   *         always unregistered, even if it was already registered prior to
+   *         this call to registerHandler().
+   */
+  bool registerHandler(uint16_t events) {
+    return registerImpl(events, false);
+  }
+
+  /**
+   * Unregister the handler, if it is registered.
+   */
+  void unregisterHandler();
+
+  /**
+   * Returns true if the handler is currently registered.
+   */
+  bool isHandlerRegistered() const {
+    return EventUtil::isEventRegistered(&event_);
+  }
+
+  /**
+   * Attach the handler to a EventBase.
+   *
+   * This may only be called if the handler is not currently attached to a
+   * EventBase (either by using the default constructor, or by calling
+   * detachEventBase()).
+   *
+   * This method must be invoked in the EventBase's thread.
+   */
+  void attachEventBase(EventBase* eventBase);
+
+  /**
+   * Detach the handler from its EventBase.
+   *
+   * This may only be called when the handler is not currently registered.
+   * Once detached, the handler may not be registered again until it is
+   * re-attached to a EventBase by calling attachEventBase().
+   *
+   * This method must be called from the current EventBase's thread.
+   */
+  void detachEventBase();
+
+  /**
+   * Change the file descriptor that this handler is associated with.
+   *
+   * This may only be called when the handler is not currently registered.
+   */
+  void changeHandlerFD(int fd);
+
+  /**
+   * Attach the handler to a EventBase, and change the file descriptor.
+   *
+   * This method may only be called if the handler is not currently attached to
+   * a EventBase.  This is primarily intended to be used to initialize
+   * EventHandler objects created using the default constructor.
+   */
+  void initHandler(EventBase* eventBase, int fd);
+
+  /**
+   * Return the set of events that we're currently registered for.
+   */
+  uint16_t getRegisteredEvents() const {
+    return (isHandlerRegistered()) ?
+      event_.ev_events : 0;
+  }
+
+  /**
+   * Register the handler as an internal event.
+   *
+   * This event will not count as an active event for determining if the
+   * EventBase loop has more events to process.  The EventBase loop runs
+   * only as long as there are active EventHandlers, however "internal" event
+   * handlers are not counted.  Therefore this event handler will not prevent
+   * EventBase loop from exiting with no more work to do if there are no other
+   * non-internal event handlers registered.
+   *
+   * This is intended to be used only in very rare cases by the internal
+   * EventBase code.  This API is not guaranteed to remain stable or portable
+   * in the future.
+   */
+  bool registerInternalHandler(uint16_t events) {
+    return registerImpl(events, true);
+  }
+
+  bool isPending();
+
+ private:
+  bool registerImpl(uint16_t events, bool internal);
+  void ensureNotRegistered(const char* fn);
+
+  void setEventBase(EventBase* eventBase);
+
+  static void libeventCallback(int fd, short events, void* arg);
+
+  struct event event_;
+  EventBase* eventBase_;
+};
+
+} // folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/async/EventUtil.h
@@ -0,0 +1,40 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements. See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership. The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License. You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied. See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+#pragma once
+
+#include <event.h>  // libevent
+
+namespace folly {
+
+/**
+ * low-level libevent utility functions
+ */
+class EventUtil {
+ public:
+  static bool isEventRegistered(const struct event* ev) {
+    // If any of these flags are set, the event is registered.
+    enum {
+      EVLIST_REGISTERED = (EVLIST_INSERTED | EVLIST_ACTIVE |
+                           EVLIST_TIMEOUT | EVLIST_SIGNAL)
+    };
+    return (ev->ev_flags & EVLIST_REGISTERED);
+  }
+};
+
+} // folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/async/NotificationQueue.h
@@ -0,0 +1,676 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements. See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership. The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License. You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied. See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+#pragma once
+
+#include <fcntl.h>
+#include <unistd.h>
+
+#include "folly/io/async/EventBase.h"
+#include "folly/io/async/EventFDWrapper.h"
+#include "folly/io/async/EventHandler.h"
+#include "folly/io/async/Request.h"
+#include "folly/Likely.h"
+#include "folly/SmallLocks.h"
+
+#include <glog/logging.h>
+#include <deque>
+
+namespace folly {
+
+/**
+ * A producer-consumer queue for passing messages between EventBase threads.
+ *
+ * Messages can be added to the queue from any thread.  Multiple consumers may
+ * listen to the queue from multiple EventBase threads.
+ *
+ * A NotificationQueue may not be destroyed while there are still consumers
+ * registered to receive events from the queue.  It is the user's
+ * responsibility to ensure that all consumers are unregistered before the
+ * queue is destroyed.
+ *
+ * MessageT should be MoveConstructible (i.e., must support either a move
+ * constructor or a copy constructor, or both).  Ideally it's move constructor
+ * (or copy constructor if no move constructor is provided) should never throw
+ * exceptions.  If the constructor may throw, the consumers could end up
+ * spinning trying to move a message off the queue and failing, and then
+ * retrying.
+ */
+template<typename MessageT>
+class NotificationQueue {
+ public:
+  /**
+   * A callback interface for consuming messages from the queue as they arrive.
+   */
+  class Consumer : private EventHandler {
+   public:
+    enum : uint16_t { kDefaultMaxReadAtOnce = 10 };
+
+    Consumer()
+      : queue_(nullptr),
+        destroyedFlagPtr_(nullptr),
+        maxReadAtOnce_(kDefaultMaxReadAtOnce) {}
+
+    virtual ~Consumer();
+
+    /**
+     * messageAvailable() will be invoked whenever a new
+     * message is available from the pipe.
+     */
+    virtual void messageAvailable(MessageT&& message) = 0;
+
+    /**
+     * Begin consuming messages from the specified queue.
+     *
+     * messageAvailable() will be called whenever a message is available.  This
+     * consumer will continue to consume messages until stopConsuming() is
+     * called.
+     *
+     * A Consumer may only consume messages from a single NotificationQueue at
+     * a time.  startConsuming() should not be called if this consumer is
+     * already consuming.
+     */
+    void startConsuming(EventBase* eventBase, NotificationQueue* queue) {
+      init(eventBase, queue);
+      registerHandler(READ | PERSIST);
+    }
+
+    /**
+     * Same as above but registers this event handler as internal so that it
+     * doesn't count towards the pending reader count for the IOLoop.
+     */
+    void startConsumingInternal(
+        EventBase* eventBase, NotificationQueue* queue) {
+      init(eventBase, queue);
+      registerInternalHandler(READ | PERSIST);
+    }
+
+    /**
+     * Stop consuming messages.
+     *
+     * startConsuming() may be called again to resume consumption of messages
+     * at a later point in time.
+     */
+    void stopConsuming();
+
+    /**
+     * Get the NotificationQueue that this consumer is currently consuming
+     * messages from.  Returns nullptr if the consumer is not currently
+     * consuming events from any queue.
+     */
+    NotificationQueue* getCurrentQueue() const {
+      return queue_;
+    }
+
+    /**
+     * Set a limit on how many messages this consumer will read each iteration
+     * around the event loop.
+     *
+     * This helps rate-limit how much work the Consumer will do each event loop
+     * iteration, to prevent it from starving other event handlers.
+     *
+     * A limit of 0 means no limit will be enforced.  If unset, the limit
+     * defaults to kDefaultMaxReadAtOnce (defined to 10 above).
+     */
+    void setMaxReadAtOnce(uint32_t maxAtOnce) {
+      maxReadAtOnce_ = maxAtOnce;
+    }
+    uint32_t getMaxReadAtOnce() const {
+      return maxReadAtOnce_;
+    }
+
+    EventBase* getEventBase() {
+      return base_;
+    }
+
+    virtual void handlerReady(uint16_t events) noexcept;
+
+   private:
+    void init(EventBase* eventBase, NotificationQueue* queue);
+
+    NotificationQueue* queue_;
+    bool* destroyedFlagPtr_;
+    uint32_t maxReadAtOnce_;
+    EventBase* base_;
+  };
+
+  enum class FdType {
+    EVENTFD,
+    PIPE
+  };
+
+  /**
+   * Create a new NotificationQueue.
+   *
+   * If the maxSize parameter is specified, this sets the maximum queue size
+   * that will be enforced by tryPutMessage().  (This size is advisory, and may
+   * be exceeded if producers explicitly use putMessage() instead of
+   * tryPutMessage().)
+   *
+   * The fdType parameter determines the type of file descriptor used
+   * internally to signal message availability.  The default (eventfd) is
+   * preferable for performance and because it won't fail when the queue gets
+   * too long.  It is not available on on older and non-linux kernels, however.
+   * In this case the code will fall back to using a pipe, the parameter is
+   * mostly for testing purposes.
+   */
+  explicit NotificationQueue(uint32_t maxSize = 0,
+                              FdType fdType = FdType::EVENTFD)
+    : eventfd_(-1),
+      pipeFds_{-1, -1},
+      advisoryMaxQueueSize_(maxSize),
+      pid_(getpid()),
+      queue_() {
+
+    spinlock_.init();
+
+    RequestContext::getStaticContext();
+
+    if (fdType == FdType::EVENTFD) {
+      eventfd_ = eventfd(0, EFD_CLOEXEC | EFD_NONBLOCK | EFD_SEMAPHORE);
+      if (eventfd_ == -1) {
+        if (errno == ENOSYS || errno == EINVAL) {
+          // eventfd not availalble
+          LOG(ERROR) << "failed to create eventfd for NotificationQueue: "
+                     << errno << ", falling back to pipe mode (is your kernel "
+                     << "> 2.6.30?)";
+          fdType = FdType::PIPE;
+        } else {
+          // some other error
+          folly::throwSystemError("Failed to create eventfd for "
+                                  "NotificationQueue", errno);
+        }
+      }
+    }
+    if (fdType == FdType::PIPE) {
+      if (pipe(pipeFds_)) {
+        folly::throwSystemError("Failed to create pipe for NotificationQueue",
+                                errno);
+      }
+      try {
+        // put both ends of the pipe into non-blocking mode
+        if (fcntl(pipeFds_[0], F_SETFL, O_RDONLY | O_NONBLOCK) != 0) {
+          folly::throwSystemError("failed to put NotificationQueue pipe read "
+                                  "endpoint into non-blocking mode", errno);
+        }
+        if (fcntl(pipeFds_[1], F_SETFL, O_WRONLY | O_NONBLOCK) != 0) {
+          folly::throwSystemError("failed to put NotificationQueue pipe write "
+                                  "endpoint into non-blocking mode", errno);
+        }
+      } catch (...) {
+        ::close(pipeFds_[0]);
+        ::close(pipeFds_[1]);
+        throw;
+      }
+    }
+  }
+
+  ~NotificationQueue() {
+    if (eventfd_ >= 0) {
+      ::close(eventfd_);
+      eventfd_ = -1;
+    }
+    if (pipeFds_[0] >= 0) {
+      ::close(pipeFds_[0]);
+      pipeFds_[0] = -1;
+    }
+    if (pipeFds_[1] >= 0) {
+      ::close(pipeFds_[1]);
+      pipeFds_[1] = -1;
+    }
+  }
+
+  /**
+   * Set the advisory maximum queue size.
+   *
+   * This maximum queue size affects calls to tryPutMessage().  Message
+   * producers can still use the putMessage() call to unconditionally put a
+   * message on the queue, ignoring the configured maximum queue size.  This
+   * can cause the queue size to exceed the configured maximum.
+   */
+  void setMaxQueueSize(uint32_t max) {
+    advisoryMaxQueueSize_ = max;
+  }
+
+  /**
+   * Attempt to put a message on the queue if the queue is not already full.
+   *
+   * If the queue is full, a std::overflow_error will be thrown.  The
+   * setMaxQueueSize() function controls the maximum queue size.
+   *
+   * This method may contend briefly on a spinlock if many threads are
+   * concurrently accessing the queue, but for all intents and purposes it will
+   * immediately place the message on the queue and return.
+   *
+   * tryPutMessage() may throw std::bad_alloc if memory allocation fails, and
+   * may throw any other exception thrown by the MessageT move/copy
+   * constructor.
+   */
+  void tryPutMessage(MessageT&& message) {
+    putMessageImpl(std::move(message), advisoryMaxQueueSize_);
+  }
+  void tryPutMessage(const MessageT& message) {
+    putMessageImpl(message, advisoryMaxQueueSize_);
+  }
+
+  /**
+   * No-throw versions of the above.  Instead returns true on success, false on
+   * failure.
+   *
+   * Only std::overflow_error is prevented from being thrown (since this is the
+   * common exception case), user code must still catch std::bad_alloc errors.
+   */
+  bool tryPutMessageNoThrow(MessageT&& message) {
+    return putMessageImpl(std::move(message), advisoryMaxQueueSize_, false);
+  }
+  bool tryPutMessageNoThrow(const MessageT& message) {
+    return putMessageImpl(message, advisoryMaxQueueSize_, false);
+  }
+
+  /**
+   * Unconditionally put a message on the queue.
+   *
+   * This method is like tryPutMessage(), but ignores the maximum queue size
+   * and always puts the message on the queue, even if the maximum queue size
+   * would be exceeded.
+   *
+   * putMessage() may throw std::bad_alloc if memory allocation fails, and may
+   * throw any other exception thrown by the MessageT move/copy constructor.
+   */
+  void putMessage(MessageT&& message) {
+    putMessageImpl(std::move(message), 0);
+  }
+  void putMessage(const MessageT& message) {
+    putMessageImpl(message, 0);
+  }
+
+  /**
+   * Put several messages on the queue.
+   */
+  template<typename InputIteratorT>
+  void putMessages(InputIteratorT first, InputIteratorT last) {
+    typedef typename std::iterator_traits<InputIteratorT>::iterator_category
+      IterCategory;
+    putMessagesImpl(first, last, IterCategory());
+  }
+
+  /**
+   * Try to immediately pull a message off of the queue, without blocking.
+   *
+   * If a message is immediately available, the result parameter will be
+   * updated to contain the message contents and true will be returned.
+   *
+   * If no message is available, false will be returned and result will be left
+   * unmodified.
+   */
+  bool tryConsume(MessageT& result) {
+    checkPid();
+    if (!tryConsumeEvent()) {
+      return false;
+    }
+
+    try {
+
+      folly::MSLGuard g(spinlock_);
+
+      // This shouldn't happen normally.  See the comments in
+      // Consumer::handlerReady() for more details.
+      if (UNLIKELY(queue_.empty())) {
+        LOG(ERROR) << "found empty queue after signalled event";
+        return false;
+      }
+
+      auto data = std::move(queue_.front());
+      result = data.first;
+      RequestContext::setContext(data.second);
+
+      queue_.pop_front();
+    } catch (...) {
+      // Handle an exception if the assignment operator happens to throw.
+      // We consumed an event but weren't able to pop the message off the
+      // queue.  Signal the event again since the message is still in the
+      // queue.
+      signalEvent(1);
+      throw;
+    }
+
+    return true;
+  }
+
+  int size() {
+    folly::MSLGuard g(spinlock_);
+    return queue_.size();
+  }
+
+  /**
+   * Check that the NotificationQueue is being used from the correct process.
+   *
+   * If you create a NotificationQueue in one process, then fork, and try to
+   * send messages to the queue from the child process, you're going to have a
+   * bad time.  Unfortunately users have (accidentally) run into this.
+   *
+   * Because we use an eventfd/pipe, the child process can actually signal the
+   * parent process that an event is ready.  However, it can't put anything on
+   * the parent's queue, so the parent wakes up and finds an empty queue.  This
+   * check ensures that we catch the problem in the misbehaving child process
+   * code, and crash before signalling the parent process.
+   */
+  void checkPid() const {
+    CHECK_EQ(pid_, getpid());
+  }
+
+ private:
+  // Forbidden copy constructor and assignment operator
+  NotificationQueue(NotificationQueue const &) = delete;
+  NotificationQueue& operator=(NotificationQueue const &) = delete;
+
+  inline bool checkQueueSize(size_t maxSize, bool throws=true) const {
+    DCHECK(0 == spinlock_.try_lock());
+    if (maxSize > 0 && queue_.size() >= maxSize) {
+      if (throws) {
+        throw std::overflow_error("unable to add message to NotificationQueue: "
+                                  "queue is full");
+      }
+      return false;
+    }
+    return true;
+  }
+
+  inline void signalEvent(size_t numAdded = 1) const {
+    static const uint8_t kPipeMessage[] = {
+      1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1
+    };
+
+    ssize_t bytes_written = 0;
+    ssize_t bytes_expected = 0;
+    if (eventfd_ >= 0) {
+      // eventfd(2) dictates that we must write a 64-bit integer
+      uint64_t numAdded64(numAdded);
+      bytes_expected = static_cast<ssize_t>(sizeof(numAdded64));
+      bytes_written = ::write(eventfd_, &numAdded64, sizeof(numAdded64));
+    } else {
+      // pipe semantics, add one message for each numAdded
+      bytes_expected = numAdded;
+      do {
+        size_t messageSize = std::min(numAdded, sizeof(kPipeMessage));
+        ssize_t rc = ::write(pipeFds_[1], kPipeMessage, messageSize);
+        if (rc < 0) {
+          // TODO: if the pipe is full, write will fail with EAGAIN.
+          // See task #1044651 for how this could be handled
+          break;
+        }
+        numAdded -= rc;
+        bytes_written += rc;
+      } while (numAdded > 0);
+    }
+    if (bytes_written != bytes_expected) {
+      folly::throwSystemError("failed to signal NotificationQueue after "
+                              "write", errno);
+    }
+  }
+
+  bool tryConsumeEvent() {
+    uint64_t value = 0;
+    ssize_t rc = -1;
+    if (eventfd_ >= 0) {
+      rc = ::read(eventfd_, &value, sizeof(value));
+    } else {
+      uint8_t value8;
+      rc = ::read(pipeFds_[0], &value8, sizeof(value8));
+      value = value8;
+    }
+    if (rc < 0) {
+      // EAGAIN should pretty much be the only error we can ever get.
+      // This means someone else already processed the only available message.
+      assert(errno == EAGAIN);
+      return false;
+    }
+    assert(value == 1);
+    return true;
+  }
+
+  bool putMessageImpl(MessageT&& message, size_t maxSize, bool throws=true) {
+    checkPid();
+    {
+      folly::MSLGuard g(spinlock_);
+      if (!checkQueueSize(maxSize, throws)) {
+        return false;
+      }
+      queue_.push_back(
+        std::make_pair(std::move(message),
+                       RequestContext::saveContext()));
+    }
+    signalEvent();
+    return true;
+  }
+
+  bool putMessageImpl(
+    const MessageT& message, size_t maxSize, bool throws=true) {
+    checkPid();
+    {
+      folly::MSLGuard g(spinlock_);
+      if (!checkQueueSize(maxSize, throws)) {
+        return false;
+      }
+      queue_.push_back(std::make_pair(message, RequestContext::saveContext()));
+    }
+    signalEvent();
+    return true;
+  }
+
+  template<typename InputIteratorT>
+  void putMessagesImpl(InputIteratorT first, InputIteratorT last,
+                       std::input_iterator_tag) {
+    checkPid();
+    size_t numAdded = 0;
+    {
+      folly::MSLGuard g(spinlock_);
+      while (first != last) {
+        queue_.push_back(std::make_pair(*first, RequestContext::saveContext()));
+        ++first;
+        ++numAdded;
+      }
+    }
+    signalEvent(numAdded);
+  }
+
+  mutable folly::MicroSpinLock spinlock_;
+  int eventfd_;
+  int pipeFds_[2]; // to fallback to on older/non-linux systems
+  uint32_t advisoryMaxQueueSize_;
+  pid_t pid_;
+  std::deque<std::pair<MessageT, std::shared_ptr<RequestContext>>> queue_;
+};
+
+template<typename MessageT>
+NotificationQueue<MessageT>::Consumer::~Consumer() {
+  // If we are in the middle of a call to handlerReady(), destroyedFlagPtr_
+  // will be non-nullptr.  Mark the value that it points to, so that
+  // handlerReady() will know the callback is destroyed, and that it cannot
+  // access any member variables anymore.
+  if (destroyedFlagPtr_) {
+    *destroyedFlagPtr_ = true;
+  }
+}
+
+template<typename MessageT>
+void NotificationQueue<MessageT>::Consumer::handlerReady(uint16_t events)
+    noexcept {
+  uint32_t numProcessed = 0;
+  while (true) {
+    // Try to decrement the eventfd.
+    //
+    // We decrement the eventfd before checking the queue, and only pop a
+    // message off the queue if we read from the eventfd.
+    //
+    // Reading the eventfd first allows us to not have to hold the spinlock
+    // while accessing the eventfd.  If we popped from the queue first, we
+    // would have to hold the lock while reading from or writing to the
+    // eventfd.  (Multiple consumers may be woken up from a single eventfd
+    // notification.  If we popped from the queue first, we could end up
+    // popping a message from the queue before the eventfd has been notified by
+    // the producer, unless the consumer and producer both held the spinlock
+    // around the entire operation.)
+    if (!queue_->tryConsumeEvent()) {
+      // no message available right now
+      return;
+    }
+
+    // Now pop the message off of the queue.
+    // We successfully consumed the eventfd notification.
+    // There should be a message available for us to consume.
+    //
+    // We have to manually acquire and release the spinlock here, rather than
+    // using SpinLockHolder since the MessageT has to be constructed while
+    // holding the spinlock and available after we release it.  SpinLockHolder
+    // unfortunately doesn't provide a release() method.  (We can't construct
+    // MessageT first since we have no guarantee that MessageT has a default
+    // constructor.
+    queue_->spinlock_.lock();
+    bool locked = true;
+
+    try {
+      // The eventfd is incremented once for every message, and only
+      // decremented when a message is popped off.  There should always be a
+      // message here to read.
+      if (UNLIKELY(queue_->queue_.empty())) {
+        // Unfortunately we have seen this happen in practice if a user forks
+        // the process, and then the child tries to send a message to a
+        // NotificationQueue being monitored by a thread in the parent.
+        // The child can signal the parent via the eventfd, but won't have been
+        // able to put anything on the parent's queue since it has a separate
+        // address space.
+        //
+        // This is a bug in the sender's code.  putMessagesImpl() should cause
+        // the sender to crash now before trying to send a message from the
+        // wrong process.  However, just in case let's handle this case in the
+        // consumer without crashing.
+        LOG(ERROR) << "found empty queue after signalled event";
+        queue_->spinlock_.unlock();
+        return;
+      }
+
+      // Pull a message off the queue.
+      auto& data = queue_->queue_.front();
+
+      MessageT msg(std::move(data.first));
+      auto old_ctx =
+        RequestContext::setContext(data.second);
+      queue_->queue_.pop_front();
+
+      // Check to see if the queue is empty now.
+      // We use this as an optimization to see if we should bother trying to
+      // loop again and read another message after invoking this callback.
+      bool wasEmpty = queue_->queue_.empty();
+
+      // Now unlock the spinlock before we invoke the callback.
+      queue_->spinlock_.unlock();
+      locked = false;
+
+      // Call the callback
+      bool callbackDestroyed = false;
+      CHECK(destroyedFlagPtr_ == nullptr);
+      destroyedFlagPtr_ = &callbackDestroyed;
+      messageAvailable(std::move(msg));
+
+      RequestContext::setContext(old_ctx);
+
+      // If the callback was destroyed before it returned, we are done
+      if (callbackDestroyed) {
+        return;
+      }
+      destroyedFlagPtr_ = nullptr;
+
+      // If the callback is no longer installed, we are done.
+      if (queue_ == nullptr) {
+        return;
+      }
+
+      // If we have hit maxReadAtOnce_, we are done.
+      ++numProcessed;
+      if (maxReadAtOnce_ > 0 && numProcessed >= maxReadAtOnce_) {
+        return;
+      }
+
+      // If the queue was empty before we invoked the callback, it's probable
+      // that it is still empty now.  Just go ahead and return, rather than
+      // looping again and trying to re-read from the eventfd.  (If a new
+      // message had in fact arrived while we were invoking the callback, we
+      // will simply be woken up the next time around the event loop and will
+      // process the message then.)
+      if (wasEmpty) {
+        return;
+      }
+    } catch (const std::exception& ex) {
+      // This catch block is really just to handle the case where the MessageT
+      // constructor throws.  The messageAvailable() callback itself is
+      // declared as noexcept and should never throw.
+      //
+      // If the MessageT constructor does throw we try to handle it as best as
+      // we can, but we can't work miracles.  We will just ignore the error for
+      // now and return.  The next time around the event loop we will end up
+      // trying to read the message again.  If MessageT continues to throw we
+      // will never make forward progress and will keep trying each time around
+      // the event loop.
+      if (locked) {
+        // Unlock the spinlock.
+        queue_->spinlock_.unlock();
+
+        // Push a notification back on the eventfd since we didn't actually
+        // read the message off of the queue.
+        queue_->signalEvent(1);
+      }
+
+      return;
+    }
+  }
+}
+
+template<typename MessageT>
+void NotificationQueue<MessageT>::Consumer::init(
+    EventBase* eventBase,
+    NotificationQueue* queue) {
+  assert(eventBase->isInEventBaseThread());
+  assert(queue_ == nullptr);
+  assert(!isHandlerRegistered());
+  queue->checkPid();
+
+  base_ = eventBase;
+
+  queue_ = queue;
+  if (queue_->eventfd_ >= 0) {
+    initHandler(eventBase, queue_->eventfd_);
+  } else {
+    initHandler(eventBase, queue_->pipeFds_[0]);
+  }
+}
+
+template<typename MessageT>
+void NotificationQueue<MessageT>::Consumer::stopConsuming() {
+  if (queue_ == nullptr) {
+    assert(!isHandlerRegistered());
+    return;
+  }
+
+  assert(isHandlerRegistered());
+  unregisterHandler();
+  detachEventBase();
+  queue_ = nullptr;
+}
+
+} // folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/async/Request.cpp
@@ -0,0 +1,34 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements. See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership. The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License. You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied. See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+#include "folly/io/async/Request.h"
+
+#ifndef NO_LIB_GFLAGS
+  DEFINE_bool(enable_request_context, true,
+              "Enable collection of per-request queueing stats for thrift");
+#endif
+
+namespace folly {
+
+#ifdef NO_LIB_GFLAGS
+  bool FLAGS_enable_thrift_request_context = true;
+#endif
+
+RequestContext* defaultContext;
+
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/async/Request.h
@@ -0,0 +1,200 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements. See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership. The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License. You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied. See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+#pragma once
+
+#include <map>
+#include <memory>
+#include <glog/logging.h>
+#include "folly/ThreadLocal.h"
+#include "folly/RWSpinLock.h"
+
+/**
+ * In many cases this header is included as a
+ * dependency to libraries which do not need
+ * command line flags. GFLAGS is a large binary
+ * and thus we do this so that a library which
+ * is size sensitive doesn't have to pull in
+ * GFLAGS if it doesn't want to.
+ */
+#ifndef NO_LIB_GFLAGS
+  #include <gflags/gflags.h>
+  DECLARE_bool(enable_request_context);
+#endif
+
+namespace folly {
+
+#ifdef NO_LIB_GFLAGS
+  extern bool FLAGS_enable_request_context;
+#endif
+
+// Some request context that follows an async request through a process
+// Everything in the context must be thread safe
+
+class RequestData {
+ public:
+  virtual ~RequestData() {}
+};
+
+class RequestContext;
+
+// If you do not call create() to create a unique request context,
+// this default request context will always be returned, and is never
+// copied between threads.
+extern RequestContext* defaultContext;
+
+class RequestContext {
+ public:
+  // Create a unique requext context for this request.
+  // It will be passed between queues / threads (where implemented),
+  // so it should be valid for the lifetime of the request.
+  static bool create() {
+    if(!FLAGS_enable_request_context) {
+      return false;
+    }
+    bool prev = getStaticContext().get() != nullptr;
+    getStaticContext().reset(new std::shared_ptr<RequestContext>(
+                     std::make_shared<RequestContext>()));
+    return prev;
+  }
+
+  // Get the current context.
+  static RequestContext* get() {
+    if (!FLAGS_enable_request_context ||
+        getStaticContext().get() == nullptr) {
+      if (defaultContext == nullptr) {
+        defaultContext = new RequestContext;
+      }
+      return defaultContext;
+    }
+    return getStaticContext().get()->get();
+  }
+
+  // The following API may be used to set per-request data in a thread-safe way.
+  // This access is still performance sensitive, so please ask if you need help
+  // profiling any use of these functions.
+  void setContextData(
+    const std::string& val, std::unique_ptr<RequestData> data) {
+    if (!FLAGS_enable_request_context) {
+      return;
+    }
+
+    folly::RWSpinLock::WriteHolder guard(lock);
+    if (data_.find(val) != data_.end()) {
+      LOG_FIRST_N(WARNING, 1) <<
+        "Called RequestContext::setContextData with data already set";
+
+      data_[val] = nullptr;
+    } else {
+      data_[val] = std::move(data);
+    }
+  }
+
+  bool hasContextData(const std::string& val) {
+    folly::RWSpinLock::ReadHolder guard(lock);
+    return data_.find(val) != data_.end();
+  }
+
+  RequestData* getContextData(const std::string& val) {
+    folly::RWSpinLock::ReadHolder guard(lock);
+    auto r = data_.find(val);
+    if (r == data_.end()) {
+      return nullptr;
+    } else {
+      return r->second.get();
+    }
+  }
+
+  void clearContextData(const std::string& val) {
+    folly::RWSpinLock::WriteHolder guard(lock);
+    data_.erase(val);
+  }
+
+  // The following API is used to pass the context through queues / threads.
+  // saveContext is called to geta shared_ptr to the context, and
+  // setContext is used to reset it on the other side of the queue.
+  //
+  // A shared_ptr is used, because many request may fan out across
+  // multiple threads, or do post-send processing, etc.
+
+  static std::shared_ptr<RequestContext>
+  setContext(std::shared_ptr<RequestContext> ctx) {
+    if (FLAGS_enable_request_context) {
+      std::shared_ptr<RequestContext> old_ctx;
+      if (getStaticContext().get()) {
+        old_ctx = *getStaticContext().get();
+      }
+      if (ctx == nullptr) {
+        getStaticContext().reset(nullptr);
+      } else {
+        getStaticContext().reset(new std::shared_ptr<RequestContext>(ctx));
+      }
+      return old_ctx;
+    }
+    return std::shared_ptr<RequestContext>();
+  }
+
+  static std::shared_ptr<RequestContext> saveContext() {
+    if (!FLAGS_enable_request_context) {
+      return std::shared_ptr<RequestContext>();
+    }
+    if (getStaticContext().get() == nullptr) {
+      return std::shared_ptr<RequestContext>();
+    } else {
+      return *getStaticContext().get();
+    }
+  }
+
+  // Used to solve static destruction ordering issue.  Any static object
+  // that uses RequestContext must call this function in its constructor.
+  //
+  // See below link for more details.
+  // http://stackoverflow.com/questions/335369/
+  // finding-c-static-initialization-order-problems#335746
+  static folly::ThreadLocalPtr<std::shared_ptr<RequestContext>>&
+  getStaticContext() {
+    static folly::ThreadLocalPtr<std::shared_ptr<RequestContext> > context;
+    return context;
+  }
+
+ private:
+  folly::RWSpinLock lock;
+  std::map<std::string, std::unique_ptr<RequestData>> data_;
+};
+
+/**
+ * Set the request context for a specific scope. For example,
+ * if you ran a part of a request in another thread you could
+ * use RequestContextGuard to copy apply the request context
+ * inside the other therad.
+ */
+class RequestContextGuard {
+ public:
+  explicit RequestContextGuard(std::shared_ptr<RequestContext> ctx) {
+    oldctx_ = RequestContext::setContext(std::move(ctx));
+  }
+
+  ~RequestContextGuard() {
+    RequestContext::setContext(std::move(oldctx_));
+  }
+
+ private:
+  std::shared_ptr<RequestContext> oldctx_;
+};
+
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/async/TimeoutManager.h
@@ -0,0 +1,73 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements. See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership. The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License. You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied. See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+#pragma once
+
+#include <chrono>
+#include <stdint.h>
+
+namespace folly {
+
+class AsyncTimeout;
+
+/**
+ * Base interface to be implemented by all classes expecting to manage
+ * timeouts. AsyncTimeout will use implementations of this interface
+ * to schedule/cancel timeouts.
+ */
+class TimeoutManager {
+ public:
+  enum class InternalEnum {
+    INTERNAL,
+    NORMAL
+  };
+
+  virtual ~TimeoutManager() {}
+
+  /**
+   * Attaches/detaches TimeoutManager to AsyncTimeout
+   */
+  virtual void attachTimeoutManager(AsyncTimeout* obj,
+                                    InternalEnum internal) = 0;
+  virtual void detachTimeoutManager(AsyncTimeout* obj) = 0;
+
+  /**
+   * Schedules AsyncTimeout to fire after `timeout` milliseconds
+   */
+  virtual bool scheduleTimeout(AsyncTimeout* obj,
+                               std::chrono::milliseconds timeout) = 0;
+
+  /**
+   * Cancels the AsyncTimeout, if scheduled
+   */
+  virtual void cancelTimeout(AsyncTimeout* obj) = 0;
+
+  /**
+   * This is used to mark the beginning of a new loop cycle by the
+   * first handler fired within that cycle.
+   */
+  virtual bool bumpHandlingTime() = 0;
+
+  /**
+   * Helper method to know whether we are running in the timeout manager
+   * thread
+   */
+  virtual bool isInTimeoutManagerThread() = 0;
+};
+
+} // folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/Compression.cpp
@@ -0,0 +1,898 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/io/Compression.h"
+
+#include <lz4.h>
+#include <lz4hc.h>
+#include <glog/logging.h>
+#include <snappy.h>
+#include <snappy-sinksource.h>
+#include <zlib.h>
+#include <lzma.h>
+
+#include "folly/Conv.h"
+#include "folly/Memory.h"
+#include "folly/Portability.h"
+#include "folly/ScopeGuard.h"
+#include "folly/Varint.h"
+#include "folly/io/Cursor.h"
+
+namespace folly { namespace io {
+
+Codec::Codec(CodecType type) : type_(type) { }
+
+// Ensure consistent behavior in the nullptr case
+std::unique_ptr<IOBuf> Codec::compress(const IOBuf* data) {
+  return !data->empty() ? doCompress(data) : IOBuf::create(0);
+}
+
+std::unique_ptr<IOBuf> Codec::uncompress(const IOBuf* data,
+                                         uint64_t uncompressedLength) {
+  if (uncompressedLength == UNKNOWN_UNCOMPRESSED_LENGTH) {
+    if (needsUncompressedLength()) {
+      throw std::invalid_argument("Codec: uncompressed length required");
+    }
+  } else if (uncompressedLength > maxUncompressedLength()) {
+    throw std::runtime_error("Codec: uncompressed length too large");
+  }
+
+  if (data->empty()) {
+    if (uncompressedLength != UNKNOWN_UNCOMPRESSED_LENGTH &&
+        uncompressedLength != 0) {
+      throw std::runtime_error("Codec: invalid uncompressed length");
+    }
+    return IOBuf::create(0);
+  }
+
+  return doUncompress(data, uncompressedLength);
+}
+
+bool Codec::needsUncompressedLength() const {
+  return doNeedsUncompressedLength();
+}
+
+uint64_t Codec::maxUncompressedLength() const {
+  return doMaxUncompressedLength();
+}
+
+bool Codec::doNeedsUncompressedLength() const {
+  return false;
+}
+
+uint64_t Codec::doMaxUncompressedLength() const {
+  return std::numeric_limits<uint64_t>::max() - 1;
+}
+
+namespace {
+
+/**
+ * No compression
+ */
+class NoCompressionCodec FOLLY_FINAL : public Codec {
+ public:
+  static std::unique_ptr<Codec> create(int level, CodecType type);
+  explicit NoCompressionCodec(int level, CodecType type);
+
+ private:
+  std::unique_ptr<IOBuf> doCompress(const IOBuf* data) FOLLY_OVERRIDE;
+  std::unique_ptr<IOBuf> doUncompress(
+      const IOBuf* data,
+      uint64_t uncompressedLength) FOLLY_OVERRIDE;
+};
+
+std::unique_ptr<Codec> NoCompressionCodec::create(int level, CodecType type) {
+  return make_unique<NoCompressionCodec>(level, type);
+}
+
+NoCompressionCodec::NoCompressionCodec(int level, CodecType type)
+  : Codec(type) {
+  DCHECK(type == CodecType::NO_COMPRESSION);
+  switch (level) {
+  case COMPRESSION_LEVEL_DEFAULT:
+  case COMPRESSION_LEVEL_FASTEST:
+  case COMPRESSION_LEVEL_BEST:
+    level = 0;
+  }
+  if (level != 0) {
+    throw std::invalid_argument(to<std::string>(
+        "NoCompressionCodec: invalid level ", level));
+  }
+}
+
+std::unique_ptr<IOBuf> NoCompressionCodec::doCompress(
+    const IOBuf* data) {
+  return data->clone();
+}
+
+std::unique_ptr<IOBuf> NoCompressionCodec::doUncompress(
+    const IOBuf* data,
+    uint64_t uncompressedLength) {
+  if (uncompressedLength != UNKNOWN_UNCOMPRESSED_LENGTH &&
+      data->computeChainDataLength() != uncompressedLength) {
+    throw std::runtime_error(to<std::string>(
+        "NoCompressionCodec: invalid uncompressed length"));
+  }
+  return data->clone();
+}
+
+/**
+ * LZ4 compression
+ */
+class LZ4Codec FOLLY_FINAL : public Codec {
+ public:
+  static std::unique_ptr<Codec> create(int level, CodecType type);
+  explicit LZ4Codec(int level, CodecType type);
+
+ private:
+  bool doNeedsUncompressedLength() const FOLLY_OVERRIDE;
+  uint64_t doMaxUncompressedLength() const FOLLY_OVERRIDE;
+
+  bool encodeSize() const { return type() == CodecType::LZ4_VARINT_SIZE; }
+
+  std::unique_ptr<IOBuf> doCompress(const IOBuf* data) FOLLY_OVERRIDE;
+  std::unique_ptr<IOBuf> doUncompress(
+      const IOBuf* data,
+      uint64_t uncompressedLength) FOLLY_OVERRIDE;
+
+  bool highCompression_;
+};
+
+std::unique_ptr<Codec> LZ4Codec::create(int level, CodecType type) {
+  return make_unique<LZ4Codec>(level, type);
+}
+
+LZ4Codec::LZ4Codec(int level, CodecType type) : Codec(type) {
+  DCHECK(type == CodecType::LZ4 || type == CodecType::LZ4_VARINT_SIZE);
+
+  switch (level) {
+  case COMPRESSION_LEVEL_FASTEST:
+  case COMPRESSION_LEVEL_DEFAULT:
+    level = 1;
+    break;
+  case COMPRESSION_LEVEL_BEST:
+    level = 2;
+    break;
+  }
+  if (level < 1 || level > 2) {
+    throw std::invalid_argument(to<std::string>(
+        "LZ4Codec: invalid level: ", level));
+  }
+  highCompression_ = (level > 1);
+}
+
+bool LZ4Codec::doNeedsUncompressedLength() const {
+  return !encodeSize();
+}
+
+uint64_t LZ4Codec::doMaxUncompressedLength() const {
+  // From lz4.h: "Max supported value is ~1.9GB"; I wish we had something
+  // more accurate.
+  return 1.8 * (uint64_t(1) << 30);
+}
+
+namespace {
+
+void encodeVarintToIOBuf(uint64_t val, folly::IOBuf* out) {
+  DCHECK_GE(out->tailroom(), kMaxVarintLength64);
+  out->append(encodeVarint(val, out->writableTail()));
+}
+
+uint64_t decodeVarintFromCursor(folly::io::Cursor& cursor) {
+  // Must have enough room in *this* buffer.
+  auto p = cursor.peek();
+  folly::ByteRange range(p.first, p.second);
+  uint64_t val = decodeVarint(range);
+  cursor.skip(range.data() - p.first);
+  return val;
+}
+
+}  // namespace
+
+std::unique_ptr<IOBuf> LZ4Codec::doCompress(const IOBuf* data) {
+  std::unique_ptr<IOBuf> clone;
+  if (data->isChained()) {
+    // LZ4 doesn't support streaming, so we have to coalesce
+    clone = data->clone();
+    clone->coalesce();
+    data = clone.get();
+  }
+
+  uint32_t extraSize = encodeSize() ? kMaxVarintLength64 : 0;
+  auto out = IOBuf::create(extraSize + LZ4_compressBound(data->length()));
+  if (encodeSize()) {
+    encodeVarintToIOBuf(data->length(), out.get());
+  }
+
+  int n;
+  if (highCompression_) {
+    n = LZ4_compressHC(reinterpret_cast<const char*>(data->data()),
+                       reinterpret_cast<char*>(out->writableTail()),
+                       data->length());
+  } else {
+    n = LZ4_compress(reinterpret_cast<const char*>(data->data()),
+                     reinterpret_cast<char*>(out->writableTail()),
+                     data->length());
+  }
+
+  CHECK_GE(n, 0);
+  CHECK_LE(n, out->capacity());
+
+  out->append(n);
+  return out;
+}
+
+std::unique_ptr<IOBuf> LZ4Codec::doUncompress(
+    const IOBuf* data,
+    uint64_t uncompressedLength) {
+  std::unique_ptr<IOBuf> clone;
+  if (data->isChained()) {
+    // LZ4 doesn't support streaming, so we have to coalesce
+    clone = data->clone();
+    clone->coalesce();
+    data = clone.get();
+  }
+
+  folly::io::Cursor cursor(data);
+  uint64_t actualUncompressedLength;
+  if (encodeSize()) {
+    actualUncompressedLength = decodeVarintFromCursor(cursor);
+    if (uncompressedLength != UNKNOWN_UNCOMPRESSED_LENGTH &&
+        uncompressedLength != actualUncompressedLength) {
+      throw std::runtime_error("LZ4Codec: invalid uncompressed length");
+    }
+  } else {
+    actualUncompressedLength = uncompressedLength;
+    DCHECK_NE(actualUncompressedLength, UNKNOWN_UNCOMPRESSED_LENGTH);
+  }
+
+  auto out = IOBuf::create(actualUncompressedLength);
+  auto p = cursor.peek();
+  int n = LZ4_uncompress(reinterpret_cast<const char*>(p.first),
+                         reinterpret_cast<char*>(out->writableTail()),
+                         actualUncompressedLength);
+  if (n != p.second) {
+    throw std::runtime_error(to<std::string>(
+        "LZ4 decompression returned invalid value ", n));
+  }
+  out->append(actualUncompressedLength);
+  return out;
+}
+
+/**
+ * Snappy compression
+ */
+
+/**
+ * Implementation of snappy::Source that reads from a IOBuf chain.
+ */
+class IOBufSnappySource FOLLY_FINAL : public snappy::Source {
+ public:
+  explicit IOBufSnappySource(const IOBuf* data);
+  size_t Available() const FOLLY_OVERRIDE;
+  const char* Peek(size_t* len) FOLLY_OVERRIDE;
+  void Skip(size_t n) FOLLY_OVERRIDE;
+ private:
+  size_t available_;
+  io::Cursor cursor_;
+};
+
+IOBufSnappySource::IOBufSnappySource(const IOBuf* data)
+  : available_(data->computeChainDataLength()),
+    cursor_(data) {
+}
+
+size_t IOBufSnappySource::Available() const {
+  return available_;
+}
+
+const char* IOBufSnappySource::Peek(size_t* len) {
+  auto p = cursor_.peek();
+  *len = p.second;
+  return reinterpret_cast<const char*>(p.first);
+}
+
+void IOBufSnappySource::Skip(size_t n) {
+  CHECK_LE(n, available_);
+  cursor_.skip(n);
+  available_ -= n;
+}
+
+class SnappyCodec FOLLY_FINAL : public Codec {
+ public:
+  static std::unique_ptr<Codec> create(int level, CodecType type);
+  explicit SnappyCodec(int level, CodecType type);
+
+ private:
+  uint64_t doMaxUncompressedLength() const FOLLY_OVERRIDE;
+  std::unique_ptr<IOBuf> doCompress(const IOBuf* data) FOLLY_OVERRIDE;
+  std::unique_ptr<IOBuf> doUncompress(
+      const IOBuf* data,
+      uint64_t uncompressedLength) FOLLY_OVERRIDE;
+};
+
+std::unique_ptr<Codec> SnappyCodec::create(int level, CodecType type) {
+  return make_unique<SnappyCodec>(level, type);
+}
+
+SnappyCodec::SnappyCodec(int level, CodecType type) : Codec(type) {
+  DCHECK(type == CodecType::SNAPPY);
+  switch (level) {
+  case COMPRESSION_LEVEL_FASTEST:
+  case COMPRESSION_LEVEL_DEFAULT:
+  case COMPRESSION_LEVEL_BEST:
+    level = 1;
+  }
+  if (level != 1) {
+    throw std::invalid_argument(to<std::string>(
+        "SnappyCodec: invalid level: ", level));
+  }
+}
+
+uint64_t SnappyCodec::doMaxUncompressedLength() const {
+  // snappy.h uses uint32_t for lengths, so there's that.
+  return std::numeric_limits<uint32_t>::max();
+}
+
+std::unique_ptr<IOBuf> SnappyCodec::doCompress(const IOBuf* data) {
+  IOBufSnappySource source(data);
+  auto out =
+    IOBuf::create(snappy::MaxCompressedLength(source.Available()));
+
+  snappy::UncheckedByteArraySink sink(reinterpret_cast<char*>(
+      out->writableTail()));
+
+  size_t n = snappy::Compress(&source, &sink);
+
+  CHECK_LE(n, out->capacity());
+  out->append(n);
+  return out;
+}
+
+std::unique_ptr<IOBuf> SnappyCodec::doUncompress(const IOBuf* data,
+                                                 uint64_t uncompressedLength) {
+  uint32_t actualUncompressedLength = 0;
+
+  {
+    IOBufSnappySource source(data);
+    if (!snappy::GetUncompressedLength(&source, &actualUncompressedLength)) {
+      throw std::runtime_error("snappy::GetUncompressedLength failed");
+    }
+    if (uncompressedLength != UNKNOWN_UNCOMPRESSED_LENGTH &&
+        uncompressedLength != actualUncompressedLength) {
+      throw std::runtime_error("snappy: invalid uncompressed length");
+    }
+  }
+
+  auto out = IOBuf::create(actualUncompressedLength);
+
+  {
+    IOBufSnappySource source(data);
+    if (!snappy::RawUncompress(&source,
+                               reinterpret_cast<char*>(out->writableTail()))) {
+      throw std::runtime_error("snappy::RawUncompress failed");
+    }
+  }
+
+  out->append(actualUncompressedLength);
+  return out;
+}
+
+/**
+ * Zlib codec
+ */
+class ZlibCodec FOLLY_FINAL : public Codec {
+ public:
+  static std::unique_ptr<Codec> create(int level, CodecType type);
+  explicit ZlibCodec(int level, CodecType type);
+
+ private:
+  std::unique_ptr<IOBuf> doCompress(const IOBuf* data) FOLLY_OVERRIDE;
+  std::unique_ptr<IOBuf> doUncompress(
+      const IOBuf* data,
+      uint64_t uncompressedLength) FOLLY_OVERRIDE;
+
+  std::unique_ptr<IOBuf> addOutputBuffer(z_stream* stream, uint32_t length);
+  bool doInflate(z_stream* stream, IOBuf* head, uint32_t bufferLength);
+
+  int level_;
+};
+
+std::unique_ptr<Codec> ZlibCodec::create(int level, CodecType type) {
+  return make_unique<ZlibCodec>(level, type);
+}
+
+ZlibCodec::ZlibCodec(int level, CodecType type) : Codec(type) {
+  DCHECK(type == CodecType::ZLIB);
+  switch (level) {
+  case COMPRESSION_LEVEL_FASTEST:
+    level = 1;
+    break;
+  case COMPRESSION_LEVEL_DEFAULT:
+    level = Z_DEFAULT_COMPRESSION;
+    break;
+  case COMPRESSION_LEVEL_BEST:
+    level = 9;
+    break;
+  }
+  if (level != Z_DEFAULT_COMPRESSION && (level < 0 || level > 9)) {
+    throw std::invalid_argument(to<std::string>(
+        "ZlibCodec: invalid level: ", level));
+  }
+  level_ = level;
+}
+
+std::unique_ptr<IOBuf> ZlibCodec::addOutputBuffer(z_stream* stream,
+                                                  uint32_t length) {
+  CHECK_EQ(stream->avail_out, 0);
+
+  auto buf = IOBuf::create(length);
+  buf->append(length);
+
+  stream->next_out = buf->writableData();
+  stream->avail_out = buf->length();
+
+  return buf;
+}
+
+bool ZlibCodec::doInflate(z_stream* stream,
+                          IOBuf* head,
+                          uint32_t bufferLength) {
+  if (stream->avail_out == 0) {
+    head->prependChain(addOutputBuffer(stream, bufferLength));
+  }
+
+  int rc = inflate(stream, Z_NO_FLUSH);
+
+  switch (rc) {
+  case Z_OK:
+    break;
+  case Z_STREAM_END:
+    return true;
+  case Z_BUF_ERROR:
+  case Z_NEED_DICT:
+  case Z_DATA_ERROR:
+  case Z_MEM_ERROR:
+    throw std::runtime_error(to<std::string>(
+        "ZlibCodec: inflate error: ", rc, ": ", stream->msg));
+  default:
+    CHECK(false) << rc << ": " << stream->msg;
+  }
+
+  return false;
+}
+
+
+std::unique_ptr<IOBuf> ZlibCodec::doCompress(const IOBuf* data) {
+  z_stream stream;
+  stream.zalloc = nullptr;
+  stream.zfree = nullptr;
+  stream.opaque = nullptr;
+
+  int rc = deflateInit(&stream, level_);
+  if (rc != Z_OK) {
+    throw std::runtime_error(to<std::string>(
+        "ZlibCodec: deflateInit error: ", rc, ": ", stream.msg));
+  }
+
+  stream.next_in = stream.next_out = nullptr;
+  stream.avail_in = stream.avail_out = 0;
+  stream.total_in = stream.total_out = 0;
+
+  bool success = false;
+
+  SCOPE_EXIT {
+    int rc = deflateEnd(&stream);
+    // If we're here because of an exception, it's okay if some data
+    // got dropped.
+    CHECK(rc == Z_OK || (!success && rc == Z_DATA_ERROR))
+      << rc << ": " << stream.msg;
+  };
+
+  uint64_t uncompressedLength = data->computeChainDataLength();
+  uint64_t maxCompressedLength = deflateBound(&stream, uncompressedLength);
+
+  // Max 64MiB in one go
+  constexpr uint32_t maxSingleStepLength = uint32_t(64) << 20;    // 64MiB
+  constexpr uint32_t defaultBufferLength = uint32_t(4) << 20;     // 4MiB
+
+  auto out = addOutputBuffer(
+      &stream,
+      (maxCompressedLength <= maxSingleStepLength ?
+       maxCompressedLength :
+       defaultBufferLength));
+
+  for (auto& range : *data) {
+    if (range.empty()) {
+      continue;
+    }
+
+    stream.next_in = const_cast<uint8_t*>(range.data());
+    stream.avail_in = range.size();
+
+    while (stream.avail_in != 0) {
+      if (stream.avail_out == 0) {
+        out->prependChain(addOutputBuffer(&stream, defaultBufferLength));
+      }
+
+      rc = deflate(&stream, Z_NO_FLUSH);
+
+      CHECK_EQ(rc, Z_OK) << stream.msg;
+    }
+  }
+
+  do {
+    if (stream.avail_out == 0) {
+      out->prependChain(addOutputBuffer(&stream, defaultBufferLength));
+    }
+
+    rc = deflate(&stream, Z_FINISH);
+  } while (rc == Z_OK);
+
+  CHECK_EQ(rc, Z_STREAM_END) << stream.msg;
+
+  out->prev()->trimEnd(stream.avail_out);
+
+  success = true;  // we survived
+
+  return out;
+}
+
+std::unique_ptr<IOBuf> ZlibCodec::doUncompress(const IOBuf* data,
+                                               uint64_t uncompressedLength) {
+  z_stream stream;
+  stream.zalloc = nullptr;
+  stream.zfree = nullptr;
+  stream.opaque = nullptr;
+
+  int rc = inflateInit(&stream);
+  if (rc != Z_OK) {
+    throw std::runtime_error(to<std::string>(
+        "ZlibCodec: inflateInit error: ", rc, ": ", stream.msg));
+  }
+
+  stream.next_in = stream.next_out = nullptr;
+  stream.avail_in = stream.avail_out = 0;
+  stream.total_in = stream.total_out = 0;
+
+  bool success = false;
+
+  SCOPE_EXIT {
+    int rc = inflateEnd(&stream);
+    // If we're here because of an exception, it's okay if some data
+    // got dropped.
+    CHECK(rc == Z_OK || (!success && rc == Z_DATA_ERROR))
+      << rc << ": " << stream.msg;
+  };
+
+  // Max 64MiB in one go
+  constexpr uint32_t maxSingleStepLength = uint32_t(64) << 20;    // 64MiB
+  constexpr uint32_t defaultBufferLength = uint32_t(4) << 20;     // 4MiB
+
+  auto out = addOutputBuffer(
+      &stream,
+      ((uncompressedLength != UNKNOWN_UNCOMPRESSED_LENGTH &&
+        uncompressedLength <= maxSingleStepLength) ?
+       uncompressedLength :
+       defaultBufferLength));
+
+  bool streamEnd = false;
+  for (auto& range : *data) {
+    if (range.empty()) {
+      continue;
+    }
+
+    stream.next_in = const_cast<uint8_t*>(range.data());
+    stream.avail_in = range.size();
+
+    while (stream.avail_in != 0) {
+      if (streamEnd) {
+        throw std::runtime_error(to<std::string>(
+            "ZlibCodec: junk after end of data"));
+      }
+
+      streamEnd = doInflate(&stream, out.get(), defaultBufferLength);
+    }
+  }
+
+  while (!streamEnd) {
+    streamEnd = doInflate(&stream, out.get(), defaultBufferLength);
+  }
+
+  out->prev()->trimEnd(stream.avail_out);
+
+  if (uncompressedLength != UNKNOWN_UNCOMPRESSED_LENGTH &&
+      uncompressedLength != stream.total_out) {
+    throw std::runtime_error(to<std::string>(
+        "ZlibCodec: invalid uncompressed length"));
+  }
+
+  success = true;  // we survived
+
+  return out;
+}
+
+/**
+ * LZMA2 compression
+ */
+class LZMA2Codec FOLLY_FINAL : public Codec {
+ public:
+  static std::unique_ptr<Codec> create(int level, CodecType type);
+  explicit LZMA2Codec(int level, CodecType type);
+
+ private:
+  bool doNeedsUncompressedLength() const FOLLY_OVERRIDE;
+  uint64_t doMaxUncompressedLength() const FOLLY_OVERRIDE;
+
+  bool encodeSize() const { return type() == CodecType::LZMA2_VARINT_SIZE; }
+
+  std::unique_ptr<IOBuf> doCompress(const IOBuf* data) FOLLY_OVERRIDE;
+  std::unique_ptr<IOBuf> doUncompress(
+      const IOBuf* data,
+      uint64_t uncompressedLength) FOLLY_OVERRIDE;
+
+  std::unique_ptr<IOBuf> addOutputBuffer(lzma_stream* stream, size_t length);
+  bool doInflate(lzma_stream* stream, IOBuf* head, size_t bufferLength);
+
+  int level_;
+};
+
+std::unique_ptr<Codec> LZMA2Codec::create(int level, CodecType type) {
+  return make_unique<LZMA2Codec>(level, type);
+}
+
+LZMA2Codec::LZMA2Codec(int level, CodecType type) : Codec(type) {
+  DCHECK(type == CodecType::LZMA2 || type == CodecType::LZMA2_VARINT_SIZE);
+  switch (level) {
+  case COMPRESSION_LEVEL_FASTEST:
+    level = 0;
+    break;
+  case COMPRESSION_LEVEL_DEFAULT:
+    level = LZMA_PRESET_DEFAULT;
+    break;
+  case COMPRESSION_LEVEL_BEST:
+    level = 9;
+    break;
+  }
+  if (level < 0 || level > 9) {
+    throw std::invalid_argument(to<std::string>(
+        "LZMA2Codec: invalid level: ", level));
+  }
+  level_ = level;
+}
+
+bool LZMA2Codec::doNeedsUncompressedLength() const {
+  return !encodeSize();
+}
+
+uint64_t LZMA2Codec::doMaxUncompressedLength() const {
+  // From lzma/base.h: "Stream is roughly 8 EiB (2^63 bytes)"
+  return uint64_t(1) << 63;
+}
+
+std::unique_ptr<IOBuf> LZMA2Codec::addOutputBuffer(
+    lzma_stream* stream,
+    size_t length) {
+
+  CHECK_EQ(stream->avail_out, 0);
+
+  auto buf = IOBuf::create(length);
+  buf->append(length);
+
+  stream->next_out = buf->writableData();
+  stream->avail_out = buf->length();
+
+  return buf;
+}
+
+std::unique_ptr<IOBuf> LZMA2Codec::doCompress(const IOBuf* data) {
+  lzma_ret rc;
+  lzma_stream stream = LZMA_STREAM_INIT;
+
+  rc = lzma_easy_encoder(&stream, level_, LZMA_CHECK_NONE);
+  if (rc != LZMA_OK) {
+    throw std::runtime_error(folly::to<std::string>(
+      "LZMA2Codec: lzma_easy_encoder error: ", rc));
+  }
+
+  SCOPE_EXIT { lzma_end(&stream); };
+
+  uint64_t uncompressedLength = data->computeChainDataLength();
+  uint64_t maxCompressedLength = lzma_stream_buffer_bound(uncompressedLength);
+
+  // Max 64MiB in one go
+  constexpr uint32_t maxSingleStepLength = uint32_t(64) << 20;    // 64MiB
+  constexpr uint32_t defaultBufferLength = uint32_t(4) << 20;     // 4MiB
+
+  auto out = addOutputBuffer(
+    &stream,
+    (maxCompressedLength <= maxSingleStepLength ?
+     maxCompressedLength :
+     defaultBufferLength));
+
+  if (encodeSize()) {
+    auto size = IOBuf::createCombined(kMaxVarintLength64);
+    encodeVarintToIOBuf(uncompressedLength, size.get());
+    size->appendChain(std::move(out));
+    out = std::move(size);
+  }
+
+  for (auto& range : *data) {
+    if (range.empty()) {
+      continue;
+    }
+
+    stream.next_in = const_cast<uint8_t*>(range.data());
+    stream.avail_in = range.size();
+
+    while (stream.avail_in != 0) {
+      if (stream.avail_out == 0) {
+        out->prependChain(addOutputBuffer(&stream, defaultBufferLength));
+      }
+
+      rc = lzma_code(&stream, LZMA_RUN);
+
+      if (rc != LZMA_OK) {
+        throw std::runtime_error(folly::to<std::string>(
+          "LZMA2Codec: lzma_code error: ", rc));
+      }
+    }
+  }
+
+  do {
+    if (stream.avail_out == 0) {
+      out->prependChain(addOutputBuffer(&stream, defaultBufferLength));
+    }
+
+    rc = lzma_code(&stream, LZMA_FINISH);
+  } while (rc == LZMA_OK);
+
+  if (rc != LZMA_STREAM_END) {
+    throw std::runtime_error(folly::to<std::string>(
+      "LZMA2Codec: lzma_code ended with error: ", rc));
+  }
+
+  out->prev()->trimEnd(stream.avail_out);
+
+  return out;
+}
+
+bool LZMA2Codec::doInflate(lzma_stream* stream,
+                          IOBuf* head,
+                          size_t bufferLength) {
+  if (stream->avail_out == 0) {
+    head->prependChain(addOutputBuffer(stream, bufferLength));
+  }
+
+  lzma_ret rc = lzma_code(stream, LZMA_RUN);
+
+  switch (rc) {
+  case LZMA_OK:
+    break;
+  case LZMA_STREAM_END:
+    return true;
+  default:
+    throw std::runtime_error(to<std::string>(
+        "LZMA2Codec: lzma_code error: ", rc));
+  }
+
+  return false;
+}
+
+std::unique_ptr<IOBuf> LZMA2Codec::doUncompress(const IOBuf* data,
+                                               uint64_t uncompressedLength) {
+  lzma_ret rc;
+  lzma_stream stream = LZMA_STREAM_INIT;
+
+  rc = lzma_auto_decoder(&stream, std::numeric_limits<uint64_t>::max(), 0);
+  if (rc != LZMA_OK) {
+    throw std::runtime_error(folly::to<std::string>(
+      "LZMA2Codec: lzma_auto_decoder error: ", rc));
+  }
+
+  SCOPE_EXIT { lzma_end(&stream); };
+
+  // Max 64MiB in one go
+  constexpr uint32_t maxSingleStepLength = uint32_t(64) << 20;    // 64MiB
+  constexpr uint32_t defaultBufferLength = uint32_t(4) << 20;     // 4MiB
+
+  folly::io::Cursor cursor(data);
+  uint64_t actualUncompressedLength;
+  if (encodeSize()) {
+    actualUncompressedLength = decodeVarintFromCursor(cursor);
+    if (uncompressedLength != UNKNOWN_UNCOMPRESSED_LENGTH &&
+        uncompressedLength != actualUncompressedLength) {
+      throw std::runtime_error("LZMA2Codec: invalid uncompressed length");
+    }
+  } else {
+    actualUncompressedLength = uncompressedLength;
+    DCHECK_NE(actualUncompressedLength, UNKNOWN_UNCOMPRESSED_LENGTH);
+  }
+
+  auto out = addOutputBuffer(
+      &stream,
+      (actualUncompressedLength <= maxSingleStepLength ?
+       actualUncompressedLength :
+       defaultBufferLength));
+
+  bool streamEnd = false;
+  auto buf = cursor.peek();
+  while (buf.second != 0) {
+    stream.next_in = const_cast<uint8_t*>(buf.first);
+    stream.avail_in = buf.second;
+
+    while (stream.avail_in != 0) {
+      if (streamEnd) {
+        throw std::runtime_error(to<std::string>(
+            "LZMA2Codec: junk after end of data"));
+      }
+
+      streamEnd = doInflate(&stream, out.get(), defaultBufferLength);
+    }
+
+    cursor.skip(buf.second);
+    buf = cursor.peek();
+  }
+
+  while (!streamEnd) {
+    streamEnd = doInflate(&stream, out.get(), defaultBufferLength);
+  }
+
+  out->prev()->trimEnd(stream.avail_out);
+
+  if (actualUncompressedLength != stream.total_out) {
+    throw std::runtime_error(to<std::string>(
+        "LZMA2Codec: invalid uncompressed length"));
+  }
+
+  return out;
+}
+
+
+typedef std::unique_ptr<Codec> (*CodecFactory)(int, CodecType);
+
+CodecFactory gCodecFactories[
+    static_cast<size_t>(CodecType::NUM_CODEC_TYPES)] = {
+  nullptr,  // USER_DEFINED
+  NoCompressionCodec::create,
+  LZ4Codec::create,
+  SnappyCodec::create,
+  ZlibCodec::create,
+  LZ4Codec::create,
+  LZMA2Codec::create,
+  LZMA2Codec::create,
+};
+
+}  // namespace
+
+std::unique_ptr<Codec> getCodec(CodecType type, int level) {
+  size_t idx = static_cast<size_t>(type);
+  if (idx >= static_cast<size_t>(CodecType::NUM_CODEC_TYPES)) {
+    throw std::invalid_argument(to<std::string>(
+        "Compression type ", idx, " not supported"));
+  }
+  auto factory = gCodecFactories[idx];
+  if (!factory) {
+    throw std::invalid_argument(to<std::string>(
+        "Compression type ", idx, " not supported"));
+  }
+  auto codec = (*factory)(level, type);
+  DCHECK_EQ(static_cast<size_t>(codec->type()), idx);
+  return codec;
+}
+
+}}  // namespaces
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/Compression.h
@@ -0,0 +1,169 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_IO_COMPRESSION_H_
+#define FOLLY_IO_COMPRESSION_H_
+
+#include <cstdint>
+#include <limits>
+#include <memory>
+
+#include "folly/io/IOBuf.h"
+
+/**
+ * Compression / decompression over IOBufs
+ */
+
+namespace folly { namespace io {
+
+enum class CodecType {
+  /**
+   * This codec type is not defined; getCodec() will throw an exception
+   * if used. Useful if deriving your own classes from Codec without
+   * going through the getCodec() interface.
+   */
+  USER_DEFINED = 0,
+
+  /**
+   * Use no compression.
+   * Levels supported: 0
+   */
+  NO_COMPRESSION = 1,
+
+  /**
+   * Use LZ4 compression.
+   * Levels supported: 1 = fast, 2 = best; default = 1
+   */
+  LZ4 = 2,
+
+  /**
+   * Use Snappy compression.
+   * Levels supported: 1
+   */
+  SNAPPY = 3,
+
+  /**
+   * Use zlib compression.
+   * Levels supported: 0 = no compression, 1 = fast, ..., 9 = best; default = 6
+   */
+  ZLIB = 4,
+
+  /**
+   * Use LZ4 compression, prefixed with size (as Varint).
+   */
+  LZ4_VARINT_SIZE = 5,
+
+  /**
+   * Use LZMA2 compression.
+   * Levels supported: 0 = no compression, 1 = fast, ..., 9 = best; default = 6
+   */
+  LZMA2 = 6,
+  LZMA2_VARINT_SIZE = 7,
+
+  NUM_CODEC_TYPES = 8,
+};
+
+class Codec {
+ public:
+  virtual ~Codec() { }
+
+  /**
+   * Return the maximum length of data that may be compressed with this codec.
+   * NO_COMPRESSION and ZLIB support arbitrary lengths;
+   * LZ4 supports up to 1.9GiB; SNAPPY supports up to 4GiB.
+   */
+  uint64_t maxUncompressedLength() const;
+
+  /**
+   * Return the codec's type.
+   */
+  CodecType type() const { return type_; }
+
+  /**
+   * Does this codec need the exact uncompressed length on decompression?
+   */
+  bool needsUncompressedLength() const;
+
+  /**
+   * Compress data, returning an IOBuf (which may share storage with data).
+   * Throws std::invalid_argument if data is larger than
+   * maxUncompressedLength().
+   *
+   * Regardless of the behavior of the underlying compressor, compressing
+   * an empty IOBuf chain will return an empty IOBuf chain.
+   */
+  std::unique_ptr<IOBuf> compress(const folly::IOBuf* data);
+
+  /**
+   * Uncompress data. Throws std::runtime_error on decompression error.
+   *
+   * Some codecs (LZ4) require the exact uncompressed length; this is indicated
+   * by needsUncompressedLength().
+   *
+   * For other codes (zlib), knowing the exact uncompressed length ahead of
+   * time might be faster.
+   *
+   * Regardless of the behavior of the underlying compressor, uncompressing
+   * an empty IOBuf chain will return an empty IOBuf chain.
+   */
+  static constexpr uint64_t UNKNOWN_UNCOMPRESSED_LENGTH = uint64_t(-1);
+
+  std::unique_ptr<IOBuf> uncompress(
+      const IOBuf* data,
+      uint64_t uncompressedLength = UNKNOWN_UNCOMPRESSED_LENGTH);
+
+ protected:
+  explicit Codec(CodecType type);
+
+ private:
+  // default: no limits (save for special value UNKNOWN_UNCOMPRESSED_LENGTH)
+  virtual uint64_t doMaxUncompressedLength() const;
+  // default: doesn't need uncompressed length
+  virtual bool doNeedsUncompressedLength() const;
+  virtual std::unique_ptr<IOBuf> doCompress(const folly::IOBuf* data) = 0;
+  virtual std::unique_ptr<IOBuf> doUncompress(const folly::IOBuf* data,
+                                              uint64_t uncompressedLength) = 0;
+
+  CodecType type_;
+};
+
+constexpr int COMPRESSION_LEVEL_FASTEST = -1;
+constexpr int COMPRESSION_LEVEL_DEFAULT = -2;
+constexpr int COMPRESSION_LEVEL_BEST = -3;
+
+/**
+ * Return a codec for the given type. Throws on error.  The level
+ * is a non-negative codec-dependent integer indicating the level of
+ * compression desired, or one of the following constants:
+ *
+ * COMPRESSION_LEVEL_FASTEST is fastest (uses least CPU / memory,
+ *   worst compression)
+ * COMPRESSION_LEVEL_DEFAULT is the default (likely a tradeoff between
+ *   FASTEST and BEST)
+ * COMPRESSION_LEVEL_BEST is the best compression (uses most CPU / memory,
+ *   best compression)
+ *
+ * When decompressing, the compression level is ignored. All codecs will
+ * decompress all data compressed with the a codec of the same type, regardless
+ * of compression level.
+ */
+std::unique_ptr<Codec> getCodec(CodecType type,
+                                int level = COMPRESSION_LEVEL_DEFAULT);
+
+}}  // namespaces
+
+#endif /* FOLLY_IO_COMPRESSION_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/Cursor.h
@@ -0,0 +1,748 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_CURSOR_H
+#define FOLLY_CURSOR_H
+
+#include <assert.h>
+#include <stdexcept>
+#include <string.h>
+#include <type_traits>
+#include <memory>
+
+#include "folly/Bits.h"
+#include "folly/io/IOBuf.h"
+#include "folly/io/IOBufQueue.h"
+#include "folly/Likely.h"
+#include "folly/Memory.h"
+
+/**
+ * Cursor class for fast iteration over IOBuf chains.
+ *
+ * Cursor - Read-only access
+ *
+ * RWPrivateCursor - Read-write access, assumes private access to IOBuf chain
+ * RWUnshareCursor - Read-write access, calls unshare on write (COW)
+ * Appender        - Write access, assumes private access to IOBuf chian
+ *
+ * Note that RW cursors write in the preallocated part of buffers (that is,
+ * between the buffer's data() and tail()), while Appenders append to the end
+ * of the buffer (between the buffer's tail() and bufferEnd()).  Appenders
+ * automatically adjust the buffer pointers, so you may only use one
+ * Appender with a buffer chain; for this reason, Appenders assume private
+ * access to the buffer (you need to call unshare() yourself if necessary).
+ **/
+namespace folly { namespace io {
+namespace detail {
+
+template <class Derived, typename BufType>
+class CursorBase {
+ public:
+  const uint8_t* data() const {
+    return crtBuf_->data() + offset_;
+  }
+
+  /*
+   * Return the remaining space available in the current IOBuf.
+   *
+   * May return 0 if the cursor is at the end of an IOBuf.  Use peek() instead
+   * if you want to avoid this.  peek() will advance to the next non-empty
+   * IOBuf (up to the end of the chain) if the cursor is currently pointing at
+   * the end of a buffer.
+   */
+  size_t length() const {
+    return crtBuf_->length() - offset_;
+  }
+
+  /*
+   * Return the space available until the end of the entire IOBuf chain.
+   */
+  size_t totalLength() const {
+    if (crtBuf_ == buffer_) {
+      return crtBuf_->computeChainDataLength() - offset_;
+    }
+    CursorBase end(buffer_->prev());
+    end.offset_ = end.buffer_->length();
+    return end - *this;
+  }
+
+  Derived& operator+=(size_t offset) {
+    Derived* p = static_cast<Derived*>(this);
+    p->skip(offset);
+    return *p;
+  }
+  Derived operator+(size_t offset) const {
+    Derived other(*this);
+    other.skip(offset);
+    return other;
+  }
+
+  /**
+   * Compare cursors for equality/inequality.
+   *
+   * Two cursors are equal if they are pointing to the same location in the
+   * same IOBuf chain.
+   */
+  bool operator==(const Derived& other) const {
+    return (offset_ == other.offset_) && (crtBuf_ == other.crtBuf_);
+  }
+  bool operator!=(const Derived& other) const {
+    return !operator==(other);
+  }
+
+  template <class T>
+  typename std::enable_if<std::is_arithmetic<T>::value, T>::type
+  read() {
+    T val;
+    pull(&val, sizeof(T));
+    return val;
+  }
+
+  template <class T>
+  T readBE() {
+    return Endian::big(read<T>());
+  }
+
+  template <class T>
+  T readLE() {
+    return Endian::little(read<T>());
+  }
+
+  /**
+   * Read a fixed-length string.
+   *
+   * The std::string-based APIs should probably be avoided unless you
+   * ultimately want the data to live in an std::string. You're better off
+   * using the pull() APIs to copy into a raw buffer otherwise.
+   */
+  std::string readFixedString(size_t len) {
+    std::string str;
+
+    str.reserve(len);
+    for (;;) {
+      // Fast path: it all fits in one buffer.
+      size_t available = length();
+      if (LIKELY(available >= len)) {
+        str.append(reinterpret_cast<const char*>(data()), len);
+        offset_ += len;
+        return str;
+      }
+
+      str.append(reinterpret_cast<const char*>(data()), available);
+      if (UNLIKELY(!tryAdvanceBuffer())) {
+        throw std::out_of_range("string underflow");
+      }
+      len -= available;
+    }
+  }
+
+  /**
+   * Read a string consisting of bytes until the given terminator character is
+   * seen. Raises an std::length_error if maxLength bytes have been processed
+   * before the terminator is seen.
+   *
+   * See comments in readFixedString() about when it's appropriate to use this
+   * vs. using pull().
+   */
+  std::string readTerminatedString(
+    char termChar = '\0',
+    size_t maxLength = std::numeric_limits<size_t>::max()) {
+    std::string str;
+
+    for (;;) {
+      const uint8_t* buf = data();
+      size_t buflen = length();
+
+      size_t i = 0;
+      while (i < buflen && buf[i] != termChar) {
+        ++i;
+
+        // Do this check after incrementing 'i', as even though we start at the
+        // 0 byte, it still represents a single character
+        if (str.length() + i >= maxLength) {
+          throw std::length_error("string overflow");
+        }
+      }
+
+      str.append(reinterpret_cast<const char*>(buf), i);
+      if (i < buflen) {
+        skip(i + 1);
+        return str;
+      }
+
+      skip(i);
+
+      if (UNLIKELY(!tryAdvanceBuffer())) {
+        throw std::out_of_range("string underflow");
+      }
+    }
+  }
+
+  explicit CursorBase(BufType* buf)
+    : crtBuf_(buf)
+    , offset_(0)
+    , buffer_(buf) {}
+
+  // Make all the templated classes friends for copy constructor.
+  template <class D, typename B> friend class CursorBase;
+
+  /*
+   * Copy constructor.
+   *
+   * This also allows constructing a CursorBase from other derived types.
+   * For instance, this allows constructing a Cursor from an RWPrivateCursor.
+   */
+  template <class OtherDerived, class OtherBuf>
+  explicit CursorBase(const CursorBase<OtherDerived, OtherBuf>& cursor)
+    : crtBuf_(cursor.crtBuf_),
+      offset_(cursor.offset_),
+      buffer_(cursor.buffer_) {}
+
+  // reset cursor to point to a new buffer.
+  void reset(BufType* buf) {
+    crtBuf_ = buf;
+    buffer_ = buf;
+    offset_ = 0;
+  }
+
+  /**
+   * Return the available data in the current buffer.
+   * If you want to gather more data from the chain into a contiguous region
+   * (for hopefully zero-copy access), use gather() before peek().
+   */
+  std::pair<const uint8_t*, size_t> peek() {
+    // Ensure that we're pointing to valid data
+    size_t available = length();
+    while (UNLIKELY(available == 0 && tryAdvanceBuffer())) {
+      available = length();
+    }
+
+    return std::make_pair(data(), available);
+  }
+
+  void pull(void* buf, size_t len) {
+    if (UNLIKELY(pullAtMost(buf, len) != len)) {
+      throw std::out_of_range("underflow");
+    }
+  }
+
+  void clone(std::unique_ptr<folly::IOBuf>& buf, size_t len) {
+    if (UNLIKELY(cloneAtMost(buf, len) != len)) {
+      throw std::out_of_range("underflow");
+    }
+  }
+
+  void clone(folly::IOBuf& buf, size_t len) {
+    if (UNLIKELY(cloneAtMost(buf, len) != len)) {
+      throw std::out_of_range("underflow");
+    }
+  }
+
+  void skip(size_t len) {
+    if (UNLIKELY(skipAtMost(len) != len)) {
+      throw std::out_of_range("underflow");
+    }
+  }
+
+  size_t pullAtMost(void* buf, size_t len) {
+    uint8_t* p = reinterpret_cast<uint8_t*>(buf);
+    size_t copied = 0;
+    for (;;) {
+      // Fast path: it all fits in one buffer.
+      size_t available = length();
+      if (LIKELY(available >= len)) {
+        memcpy(p, data(), len);
+        offset_ += len;
+        return copied + len;
+      }
+
+      memcpy(p, data(), available);
+      copied += available;
+      if (UNLIKELY(!tryAdvanceBuffer())) {
+        return copied;
+      }
+      p += available;
+      len -= available;
+    }
+  }
+
+  size_t cloneAtMost(folly::IOBuf& buf, size_t len) {
+    buf = folly::IOBuf();
+
+    std::unique_ptr<folly::IOBuf> tmp;
+    size_t copied = 0;
+    for (int loopCount = 0; true; ++loopCount) {
+      // Fast path: it all fits in one buffer.
+      size_t available = length();
+      if (LIKELY(available >= len)) {
+        if (loopCount == 0) {
+          crtBuf_->cloneOneInto(buf);
+          buf.trimStart(offset_);
+          buf.trimEnd(buf.length() - len);
+        } else {
+          tmp = crtBuf_->cloneOne();
+          tmp->trimStart(offset_);
+          tmp->trimEnd(tmp->length() - len);
+          buf.prependChain(std::move(tmp));
+        }
+
+        offset_ += len;
+        return copied + len;
+      }
+
+
+      if (loopCount == 0) {
+        crtBuf_->cloneOneInto(buf);
+        buf.trimStart(offset_);
+      } else {
+        tmp = crtBuf_->cloneOne();
+        tmp->trimStart(offset_);
+        buf.prependChain(std::move(tmp));
+      }
+
+      copied += available;
+      if (UNLIKELY(!tryAdvanceBuffer())) {
+        return copied;
+      }
+      len -= available;
+    }
+  }
+
+  size_t cloneAtMost(std::unique_ptr<folly::IOBuf>& buf, size_t len) {
+    if (!buf) {
+      buf = make_unique<folly::IOBuf>();
+    }
+
+    return cloneAtMost(*buf, len);
+  }
+
+  size_t skipAtMost(size_t len) {
+    size_t skipped = 0;
+    for (;;) {
+      // Fast path: it all fits in one buffer.
+      size_t available = length();
+      if (LIKELY(available >= len)) {
+        offset_ += len;
+        return skipped + len;
+      }
+
+      skipped += available;
+      if (UNLIKELY(!tryAdvanceBuffer())) {
+        return skipped;
+      }
+      len -= available;
+    }
+  }
+
+  /**
+   * Return the distance between two cursors.
+   */
+  size_t operator-(const CursorBase& other) const {
+    BufType *otherBuf = other.crtBuf_;
+    size_t len = 0;
+
+    if (otherBuf != crtBuf_) {
+      len += otherBuf->length() - other.offset_;
+
+      for (otherBuf = otherBuf->next();
+           otherBuf != crtBuf_ && otherBuf != other.buffer_;
+           otherBuf = otherBuf->next()) {
+        len += otherBuf->length();
+      }
+
+      if (otherBuf == other.buffer_) {
+        throw std::out_of_range("wrap-around");
+      }
+
+      len += offset_;
+    } else {
+      if (offset_ < other.offset_) {
+        throw std::out_of_range("underflow");
+      }
+
+      len += offset_ - other.offset_;
+    }
+
+    return len;
+  }
+
+  /**
+   * Return the distance from the given IOBuf to the this cursor.
+   */
+  size_t operator-(const BufType* buf) const {
+    size_t len = 0;
+
+    BufType *curBuf = buf;
+    while (curBuf != crtBuf_) {
+      len += curBuf->length();
+      curBuf = curBuf->next();
+      if (curBuf == buf || curBuf == buffer_) {
+        throw std::out_of_range("wrap-around");
+      }
+    }
+
+    len += offset_;
+    return len;
+  }
+
+ protected:
+  BufType* crtBuf_;
+  size_t offset_;
+
+  ~CursorBase(){}
+
+  BufType* head() {
+    return buffer_;
+  }
+
+  bool tryAdvanceBuffer() {
+    BufType* nextBuf = crtBuf_->next();
+    if (UNLIKELY(nextBuf == buffer_)) {
+      offset_ = crtBuf_->length();
+      return false;
+    }
+
+    offset_ = 0;
+    crtBuf_ = nextBuf;
+    static_cast<Derived*>(this)->advanceDone();
+    return true;
+  }
+
+ private:
+  void advanceDone() {
+  }
+
+  BufType* buffer_;
+};
+
+template <class Derived>
+class Writable {
+ public:
+  template <class T>
+  typename std::enable_if<std::is_arithmetic<T>::value>::type
+  write(T value) {
+    const uint8_t* u8 = reinterpret_cast<const uint8_t*>(&value);
+    Derived* d = static_cast<Derived*>(this);
+    d->push(u8, sizeof(T));
+  }
+
+  template <class T>
+  void writeBE(T value) {
+    Derived* d = static_cast<Derived*>(this);
+    d->write(Endian::big(value));
+  }
+
+  template <class T>
+  void writeLE(T value) {
+    Derived* d = static_cast<Derived*>(this);
+    d->write(Endian::little(value));
+  }
+
+  void push(const uint8_t* buf, size_t len) {
+    Derived* d = static_cast<Derived*>(this);
+    if (d->pushAtMost(buf, len) != len) {
+      throw std::out_of_range("overflow");
+    }
+  }
+};
+
+} // namespace detail
+
+class Cursor : public detail::CursorBase<Cursor, const IOBuf> {
+ public:
+  explicit Cursor(const IOBuf* buf)
+    : detail::CursorBase<Cursor, const IOBuf>(buf) {}
+
+  template <class OtherDerived, class OtherBuf>
+  explicit Cursor(const detail::CursorBase<OtherDerived, OtherBuf>& cursor)
+    : detail::CursorBase<Cursor, const IOBuf>(cursor) {}
+};
+
+enum class CursorAccess {
+  PRIVATE,
+  UNSHARE
+};
+
+template <CursorAccess access>
+class RWCursor
+  : public detail::CursorBase<RWCursor<access>, IOBuf>,
+    public detail::Writable<RWCursor<access>> {
+  friend class detail::CursorBase<RWCursor<access>, IOBuf>;
+ public:
+  explicit RWCursor(IOBuf* buf)
+    : detail::CursorBase<RWCursor<access>, IOBuf>(buf),
+      maybeShared_(true) {}
+
+  template <class OtherDerived, class OtherBuf>
+  explicit RWCursor(const detail::CursorBase<OtherDerived, OtherBuf>& cursor)
+    : detail::CursorBase<RWCursor<access>, IOBuf>(cursor),
+      maybeShared_(true) {}
+  /**
+   * Gather at least n bytes contiguously into the current buffer,
+   * by coalescing subsequent buffers from the chain as necessary.
+   */
+  void gather(size_t n) {
+    // Forbid attempts to gather beyond the end of this IOBuf chain.
+    // Otherwise we could try to coalesce the head of the chain and end up
+    // accidentally freeing it, invalidating the pointer owned by external
+    // code.
+    //
+    // If crtBuf_ == head() then IOBuf::gather() will perform all necessary
+    // checking.  We only have to perform an explicit check here when calling
+    // gather() on a non-head element.
+    if (this->crtBuf_ != this->head() && this->totalLength() < n) {
+      throw std::overflow_error("cannot gather() past the end of the chain");
+    }
+    this->crtBuf_->gather(this->offset_ + n);
+  }
+  void gatherAtMost(size_t n) {
+    size_t size = std::min(n, this->totalLength());
+    return this->crtBuf_->gather(this->offset_ + size);
+  }
+
+  size_t pushAtMost(const uint8_t* buf, size_t len) {
+    size_t copied = 0;
+    for (;;) {
+      // Fast path: the current buffer is big enough.
+      size_t available = this->length();
+      if (LIKELY(available >= len)) {
+        if (access == CursorAccess::UNSHARE) {
+          maybeUnshare();
+        }
+        memcpy(writableData(), buf, len);
+        this->offset_ += len;
+        return copied + len;
+      }
+
+      if (access == CursorAccess::UNSHARE) {
+        maybeUnshare();
+      }
+      memcpy(writableData(), buf, available);
+      copied += available;
+      if (UNLIKELY(!this->tryAdvanceBuffer())) {
+        return copied;
+      }
+      buf += available;
+      len -= available;
+    }
+  }
+
+  void insert(std::unique_ptr<folly::IOBuf> buf) {
+    folly::IOBuf* nextBuf;
+    if (this->offset_ == 0) {
+      // Can just prepend
+      nextBuf = this->crtBuf_;
+      this->crtBuf_->prependChain(std::move(buf));
+    } else {
+      std::unique_ptr<folly::IOBuf> remaining;
+      if (this->crtBuf_->length() - this->offset_ > 0) {
+        // Need to split current IOBuf in two.
+        remaining = this->crtBuf_->cloneOne();
+        remaining->trimStart(this->offset_);
+        nextBuf = remaining.get();
+        buf->prependChain(std::move(remaining));
+      } else {
+        // Can just append
+        nextBuf = this->crtBuf_->next();
+      }
+      this->crtBuf_->trimEnd(this->length());
+      this->crtBuf_->appendChain(std::move(buf));
+    }
+    // Jump past the new links
+    this->offset_ = 0;
+    this->crtBuf_ = nextBuf;
+  }
+
+  uint8_t* writableData() {
+    return this->crtBuf_->writableData() + this->offset_;
+  }
+
+ private:
+  void maybeUnshare() {
+    if (UNLIKELY(maybeShared_)) {
+      this->crtBuf_->unshareOne();
+      maybeShared_ = false;
+    }
+  }
+
+  void advanceDone() {
+    maybeShared_ = true;
+  }
+
+  bool maybeShared_;
+};
+
+typedef RWCursor<CursorAccess::PRIVATE> RWPrivateCursor;
+typedef RWCursor<CursorAccess::UNSHARE> RWUnshareCursor;
+
+/**
+ * Append to the end of a buffer chain, growing the chain (by allocating new
+ * buffers) in increments of at least growth bytes every time.  Won't grow
+ * (and push() and ensure() will throw) if growth == 0.
+ *
+ * TODO(tudorb): add a flavor of Appender that reallocates one IOBuf instead
+ * of chaining.
+ */
+class Appender : public detail::Writable<Appender> {
+ public:
+  Appender(IOBuf* buf, uint64_t growth)
+    : buffer_(buf),
+      crtBuf_(buf->prev()),
+      growth_(growth) {
+  }
+
+  uint8_t* writableData() {
+    return crtBuf_->writableTail();
+  }
+
+  size_t length() const {
+    return crtBuf_->tailroom();
+  }
+
+  /**
+   * Mark n bytes (must be <= length()) as appended, as per the
+   * IOBuf::append() method.
+   */
+  void append(size_t n) {
+    crtBuf_->append(n);
+  }
+
+  /**
+   * Ensure at least n contiguous bytes available to write.
+   * Postcondition: length() >= n.
+   */
+  void ensure(uint64_t n) {
+    if (LIKELY(length() >= n)) {
+      return;
+    }
+
+    // Waste the rest of the current buffer and allocate a new one.
+    // Don't make it too small, either.
+    if (growth_ == 0) {
+      throw std::out_of_range("can't grow buffer chain");
+    }
+
+    n = std::max(n, growth_);
+    buffer_->prependChain(IOBuf::create(n));
+    crtBuf_ = buffer_->prev();
+  }
+
+  size_t pushAtMost(const uint8_t* buf, size_t len) {
+    size_t copied = 0;
+    for (;;) {
+      // Fast path: it all fits in one buffer.
+      size_t available = length();
+      if (LIKELY(available >= len)) {
+        memcpy(writableData(), buf, len);
+        append(len);
+        return copied + len;
+      }
+
+      memcpy(writableData(), buf, available);
+      append(available);
+      copied += available;
+      if (UNLIKELY(!tryGrowChain())) {
+        return copied;
+      }
+      buf += available;
+      len -= available;
+    }
+  }
+
+ private:
+  bool tryGrowChain() {
+    assert(crtBuf_->next() == buffer_);
+    if (growth_ == 0) {
+      return false;
+    }
+
+    buffer_->prependChain(IOBuf::create(growth_));
+    crtBuf_ = buffer_->prev();
+    return true;
+  }
+
+  IOBuf* buffer_;
+  IOBuf* crtBuf_;
+  uint64_t growth_;
+};
+
+class QueueAppender : public detail::Writable<QueueAppender> {
+ public:
+  /**
+   * Create an Appender that writes to a IOBufQueue.  When we allocate
+   * space in the queue, we grow no more than growth bytes at once
+   * (unless you call ensure() with a bigger value yourself).
+   */
+  QueueAppender(IOBufQueue* queue, uint64_t growth) {
+    reset(queue, growth);
+  }
+
+  void reset(IOBufQueue* queue, uint64_t growth) {
+    queue_ = queue;
+    growth_ = growth;
+  }
+
+  uint8_t* writableData() {
+    return static_cast<uint8_t*>(queue_->writableTail());
+  }
+
+  size_t length() const { return queue_->tailroom(); }
+
+  void append(size_t n) { queue_->postallocate(n); }
+
+  // Ensure at least n contiguous; can go above growth_, throws if
+  // not enough room.
+  void ensure(uint64_t n) { queue_->preallocate(n, growth_); }
+
+  template <class T>
+  typename std::enable_if<std::is_arithmetic<T>::value>::type
+  write(T value) {
+    // We can't fail.
+    auto p = queue_->preallocate(sizeof(T), growth_);
+    storeUnaligned(p.first, value);
+    queue_->postallocate(sizeof(T));
+  }
+
+
+  size_t pushAtMost(const uint8_t* buf, size_t len) {
+    size_t remaining = len;
+    while (remaining != 0) {
+      auto p = queue_->preallocate(std::min(remaining, growth_),
+                                   growth_,
+                                   remaining);
+      memcpy(p.first, buf, p.second);
+      queue_->postallocate(p.second);
+      buf += p.second;
+      remaining -= p.second;
+    }
+
+    return len;
+  }
+
+  void insert(std::unique_ptr<folly::IOBuf> buf) {
+    if (buf) {
+      queue_->append(std::move(buf), true);
+    }
+  }
+
+ private:
+  folly::IOBufQueue* queue_;
+  size_t growth_;
+};
+
+}}  // folly::io
+
+#endif // FOLLY_CURSOR_H
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/IOBuf.cpp
@@ -0,0 +1,892 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define __STDC_LIMIT_MACROS
+
+#include "folly/io/IOBuf.h"
+
+#include "folly/Conv.h"
+#include "folly/Likely.h"
+#include "folly/Malloc.h"
+#include "folly/Memory.h"
+#include "folly/ScopeGuard.h"
+
+#include <stdexcept>
+#include <assert.h>
+#include <stdint.h>
+#include <stdlib.h>
+
+using std::unique_ptr;
+
+namespace {
+
+enum : uint16_t {
+  kHeapMagic = 0xa5a5,
+  // This memory segment contains an IOBuf that is still in use
+  kIOBufInUse = 0x01,
+  // This memory segment contains buffer data that is still in use
+  kDataInUse = 0x02,
+};
+
+enum : uint64_t {
+  // When create() is called for buffers less than kDefaultCombinedBufSize,
+  // we allocate a single combined memory segment for the IOBuf and the data
+  // together.  See the comments for createCombined()/createSeparate() for more
+  // details.
+  //
+  // (The size of 1k is largely just a guess here.  We could could probably do
+  // benchmarks of real applications to see if adjusting this number makes a
+  // difference.  Callers that know their exact use case can also explicitly
+  // call createCombined() or createSeparate().)
+  kDefaultCombinedBufSize = 1024
+};
+
+// Helper function for IOBuf::takeOwnership()
+void takeOwnershipError(bool freeOnError, void* buf,
+                        folly::IOBuf::FreeFunction freeFn,
+                        void* userData) {
+  if (!freeOnError) {
+    return;
+  }
+  if (!freeFn) {
+    free(buf);
+    return;
+  }
+  try {
+    freeFn(buf, userData);
+  } catch (...) {
+    // The user's free function is not allowed to throw.
+    // (We are already in the middle of throwing an exception, so
+    // we cannot let this exception go unhandled.)
+    abort();
+  }
+}
+
+} // unnamed namespace
+
+namespace folly {
+
+struct IOBuf::HeapPrefix {
+  HeapPrefix(uint16_t flg)
+    : magic(kHeapMagic),
+      flags(flg) {}
+  ~HeapPrefix() {
+    // Reset magic to 0 on destruction.  This is solely for debugging purposes
+    // to help catch bugs where someone tries to use HeapStorage after it has
+    // been deleted.
+    magic = 0;
+  }
+
+  uint16_t magic;
+  std::atomic<uint16_t> flags;
+};
+
+struct IOBuf::HeapStorage {
+  HeapPrefix prefix;
+  // The IOBuf is last in the HeapStorage object.
+  // This way operator new will work even if allocating a subclass of IOBuf
+  // that requires more space.
+  folly::IOBuf buf;
+};
+
+struct IOBuf::HeapFullStorage {
+  // Make sure jemalloc allocates from the 64-byte class.  Putting this here
+  // because HeapStorage is private so it can't be at namespace level.
+  static_assert(sizeof(HeapStorage) <= 64,
+                "IOBuf may not grow over 56 bytes!");
+
+  HeapStorage hs;
+  SharedInfo shared;
+  MaxAlign align;
+};
+
+IOBuf::SharedInfo::SharedInfo()
+  : freeFn(NULL),
+    userData(NULL) {
+  // Use relaxed memory ordering here.  Since we are creating a new SharedInfo,
+  // no other threads should be referring to it yet.
+  refcount.store(1, std::memory_order_relaxed);
+}
+
+IOBuf::SharedInfo::SharedInfo(FreeFunction fn, void* arg)
+  : freeFn(fn),
+    userData(arg) {
+  // Use relaxed memory ordering here.  Since we are creating a new SharedInfo,
+  // no other threads should be referring to it yet.
+  refcount.store(1, std::memory_order_relaxed);
+}
+
+void* IOBuf::operator new(size_t size) {
+  size_t fullSize = offsetof(HeapStorage, buf) + size;
+  auto* storage = static_cast<HeapStorage*>(malloc(fullSize));
+  // operator new is not allowed to return NULL
+  if (UNLIKELY(storage == nullptr)) {
+    throw std::bad_alloc();
+  }
+
+  new (&storage->prefix) HeapPrefix(kIOBufInUse);
+  return &(storage->buf);
+}
+
+void* IOBuf::operator new(size_t size, void* ptr) {
+  return ptr;
+}
+
+void IOBuf::operator delete(void* ptr) {
+  auto* storageAddr = static_cast<uint8_t*>(ptr) - offsetof(HeapStorage, buf);
+  auto* storage = reinterpret_cast<HeapStorage*>(storageAddr);
+  releaseStorage(storage, kIOBufInUse);
+}
+
+void IOBuf::releaseStorage(HeapStorage* storage, uint16_t freeFlags) {
+  CHECK_EQ(storage->prefix.magic, static_cast<uint16_t>(kHeapMagic));
+
+  // Use relaxed memory order here.  If we are unlucky and happen to get
+  // out-of-date data the compare_exchange_weak() call below will catch
+  // it and load new data with memory_order_acq_rel.
+  auto flags = storage->prefix.flags.load(std::memory_order_acquire);
+  DCHECK_EQ((flags & freeFlags), freeFlags);
+
+  while (true) {
+    uint16_t newFlags = (flags & ~freeFlags);
+    if (newFlags == 0) {
+      // The storage space is now unused.  Free it.
+      storage->prefix.HeapPrefix::~HeapPrefix();
+      free(storage);
+      return;
+    }
+
+    // This storage segment still contains portions that are in use.
+    // Just clear the flags specified in freeFlags for now.
+    auto ret = storage->prefix.flags.compare_exchange_weak(
+        flags, newFlags, std::memory_order_acq_rel);
+    if (ret) {
+      // We successfully updated the flags.
+      return;
+    }
+
+    // We failed to update the flags.  Some other thread probably updated them
+    // and cleared some of the other bits.  Continue around the loop to see if
+    // we are the last user now, or if we need to try updating the flags again.
+  }
+}
+
+void IOBuf::freeInternalBuf(void* buf, void* userData) {
+  auto* storage = static_cast<HeapStorage*>(userData);
+  releaseStorage(storage, kDataInUse);
+}
+
+IOBuf::IOBuf(CreateOp, uint64_t capacity)
+  : next_(this),
+    prev_(this),
+    data_(nullptr),
+    length_(0),
+    flagsAndSharedInfo_(0) {
+  SharedInfo* info;
+  allocExtBuffer(capacity, &buf_, &info, &capacity_);
+  setSharedInfo(info);
+  data_ = buf_;
+}
+
+IOBuf::IOBuf(CopyBufferOp op, const void* buf, uint64_t size,
+             uint64_t headroom, uint64_t minTailroom)
+  : IOBuf(CREATE, headroom + size + minTailroom) {
+  advance(headroom);
+  memcpy(writableData(), buf, size);
+  append(size);
+}
+
+IOBuf::IOBuf(CopyBufferOp op, ByteRange br,
+             uint64_t headroom, uint64_t minTailroom)
+  : IOBuf(op, br.data(), br.size(), headroom, minTailroom) {
+}
+
+unique_ptr<IOBuf> IOBuf::create(uint64_t capacity) {
+  // For smaller-sized buffers, allocate the IOBuf, SharedInfo, and the buffer
+  // all with a single allocation.
+  //
+  // We don't do this for larger buffers since it can be wasteful if the user
+  // needs to reallocate the buffer but keeps using the same IOBuf object.
+  // In this case we can't free the data space until the IOBuf is also
+  // destroyed.  Callers can explicitly call createCombined() or
+  // createSeparate() if they know their use case better, and know if they are
+  // likely to reallocate the buffer later.
+  if (capacity <= kDefaultCombinedBufSize) {
+    return createCombined(capacity);
+  }
+  return createSeparate(capacity);
+}
+
+unique_ptr<IOBuf> IOBuf::createCombined(uint64_t capacity) {
+  // To save a memory allocation, allocate space for the IOBuf object, the
+  // SharedInfo struct, and the data itself all with a single call to malloc().
+  size_t requiredStorage = offsetof(HeapFullStorage, align) + capacity;
+  size_t mallocSize = goodMallocSize(requiredStorage);
+  auto* storage = static_cast<HeapFullStorage*>(malloc(mallocSize));
+
+  new (&storage->hs.prefix) HeapPrefix(kIOBufInUse | kDataInUse);
+  new (&storage->shared) SharedInfo(freeInternalBuf, storage);
+
+  uint8_t* bufAddr = reinterpret_cast<uint8_t*>(&storage->align);
+  uint8_t* storageEnd = reinterpret_cast<uint8_t*>(storage) + mallocSize;
+  size_t actualCapacity = storageEnd - bufAddr;
+  unique_ptr<IOBuf> ret(new (&storage->hs.buf) IOBuf(
+        InternalConstructor(), packFlagsAndSharedInfo(0, &storage->shared),
+        bufAddr, actualCapacity, bufAddr, 0));
+  return ret;
+}
+
+unique_ptr<IOBuf> IOBuf::createSeparate(uint64_t capacity) {
+  return make_unique<IOBuf>(CREATE, capacity);
+}
+
+unique_ptr<IOBuf> IOBuf::createChain(
+    size_t totalCapacity, uint64_t maxBufCapacity) {
+  unique_ptr<IOBuf> out = create(
+      std::min(totalCapacity, size_t(maxBufCapacity)));
+  size_t allocatedCapacity = out->capacity();
+
+  while (allocatedCapacity < totalCapacity) {
+    unique_ptr<IOBuf> newBuf = create(
+        std::min(totalCapacity - allocatedCapacity, size_t(maxBufCapacity)));
+    allocatedCapacity += newBuf->capacity();
+    out->prependChain(std::move(newBuf));
+  }
+
+  return out;
+}
+
+IOBuf::IOBuf(TakeOwnershipOp, void* buf, uint64_t capacity, uint64_t length,
+             FreeFunction freeFn, void* userData,
+             bool freeOnError)
+  : next_(this),
+    prev_(this),
+    data_(static_cast<uint8_t*>(buf)),
+    buf_(static_cast<uint8_t*>(buf)),
+    length_(length),
+    capacity_(capacity),
+    flagsAndSharedInfo_(packFlagsAndSharedInfo(kFlagFreeSharedInfo, nullptr)) {
+  try {
+    setSharedInfo(new SharedInfo(freeFn, userData));
+  } catch (...) {
+    takeOwnershipError(freeOnError, buf, freeFn, userData);
+    throw;
+  }
+}
+
+unique_ptr<IOBuf> IOBuf::takeOwnership(void* buf, uint64_t capacity,
+                                       uint64_t length,
+                                       FreeFunction freeFn,
+                                       void* userData,
+                                       bool freeOnError) {
+  try {
+    // TODO: We could allocate the IOBuf object and SharedInfo all in a single
+    // memory allocation.  We could use the existing HeapStorage class, and
+    // define a new kSharedInfoInUse flag.  We could change our code to call
+    // releaseStorage(kFlagFreeSharedInfo) when this kFlagFreeSharedInfo,
+    // rather than directly calling delete.
+    //
+    // Note that we always pass freeOnError as false to the constructor.
+    // If the constructor throws we'll handle it below.  (We have to handle
+    // allocation failures from make_unique too.)
+    return make_unique<IOBuf>(TAKE_OWNERSHIP, buf, capacity, length,
+                              freeFn, userData, false);
+  } catch (...) {
+    takeOwnershipError(freeOnError, buf, freeFn, userData);
+    throw;
+  }
+}
+
+IOBuf::IOBuf(WrapBufferOp, const void* buf, uint64_t capacity)
+  : IOBuf(InternalConstructor(), 0,
+          // We cast away the const-ness of the buffer here.
+          // This is okay since IOBuf users must use unshare() to create a copy
+          // of this buffer before writing to the buffer.
+          static_cast<uint8_t*>(const_cast<void*>(buf)), capacity,
+          static_cast<uint8_t*>(const_cast<void*>(buf)), capacity) {
+}
+
+IOBuf::IOBuf(WrapBufferOp op, ByteRange br)
+  : IOBuf(op, br.data(), br.size()) {
+}
+
+unique_ptr<IOBuf> IOBuf::wrapBuffer(const void* buf, uint64_t capacity) {
+  return make_unique<IOBuf>(WRAP_BUFFER, buf, capacity);
+}
+
+IOBuf::IOBuf() noexcept {
+}
+
+IOBuf::IOBuf(IOBuf&& other) noexcept {
+  *this = std::move(other);
+}
+
+IOBuf::IOBuf(InternalConstructor,
+             uintptr_t flagsAndSharedInfo,
+             uint8_t* buf,
+             uint64_t capacity,
+             uint8_t* data,
+             uint64_t length)
+  : next_(this),
+    prev_(this),
+    data_(data),
+    buf_(buf),
+    length_(length),
+    capacity_(capacity),
+    flagsAndSharedInfo_(flagsAndSharedInfo) {
+  assert(data >= buf);
+  assert(data + length <= buf + capacity);
+}
+
+IOBuf::~IOBuf() {
+  // Destroying an IOBuf destroys the entire chain.
+  // Users of IOBuf should only explicitly delete the head of any chain.
+  // The other elements in the chain will be automatically destroyed.
+  while (next_ != this) {
+    // Since unlink() returns unique_ptr() and we don't store it,
+    // it will automatically delete the unlinked element.
+    (void)next_->unlink();
+  }
+
+  decrementRefcount();
+}
+
+IOBuf& IOBuf::operator=(IOBuf&& other) noexcept {
+  // If we are part of a chain, delete the rest of the chain.
+  while (next_ != this) {
+    // Since unlink() returns unique_ptr() and we don't store it,
+    // it will automatically delete the unlinked element.
+    (void)next_->unlink();
+  }
+
+  // Decrement our refcount on the current buffer
+  decrementRefcount();
+
+  // Take ownership of the other buffer's data
+  data_ = other.data_;
+  buf_ = other.buf_;
+  length_ = other.length_;
+  capacity_ = other.capacity_;
+  flagsAndSharedInfo_ = other.flagsAndSharedInfo_;
+  // Reset other so it is a clean state to be destroyed.
+  other.data_ = nullptr;
+  other.buf_ = nullptr;
+  other.length_ = 0;
+  other.capacity_ = 0;
+  other.flagsAndSharedInfo_ = 0;
+
+  // If other was part of the chain, assume ownership of the rest of its chain.
+  // (It's only valid to perform move assignment on the head of a chain.)
+  if (other.next_ != &other) {
+    next_ = other.next_;
+    next_->prev_ = this;
+    other.next_ = &other;
+
+    prev_ = other.prev_;
+    prev_->next_ = this;
+    other.prev_ = &other;
+  }
+
+  // Sanity check to make sure that other is in a valid state to be destroyed.
+  DCHECK_EQ(other.prev_, &other);
+  DCHECK_EQ(other.next_, &other);
+
+  return *this;
+}
+
+bool IOBuf::empty() const {
+  const IOBuf* current = this;
+  do {
+    if (current->length() != 0) {
+      return false;
+    }
+    current = current->next_;
+  } while (current != this);
+  return true;
+}
+
+size_t IOBuf::countChainElements() const {
+  size_t numElements = 1;
+  for (IOBuf* current = next_; current != this; current = current->next_) {
+    ++numElements;
+  }
+  return numElements;
+}
+
+uint64_t IOBuf::computeChainDataLength() const {
+  uint64_t fullLength = length_;
+  for (IOBuf* current = next_; current != this; current = current->next_) {
+    fullLength += current->length_;
+  }
+  return fullLength;
+}
+
+void IOBuf::prependChain(unique_ptr<IOBuf>&& iobuf) {
+  // Take ownership of the specified IOBuf
+  IOBuf* other = iobuf.release();
+
+  // Remember the pointer to the tail of the other chain
+  IOBuf* otherTail = other->prev_;
+
+  // Hook up prev_->next_ to point at the start of the other chain,
+  // and other->prev_ to point at prev_
+  prev_->next_ = other;
+  other->prev_ = prev_;
+
+  // Hook up otherTail->next_ to point at us,
+  // and prev_ to point back at otherTail,
+  otherTail->next_ = this;
+  prev_ = otherTail;
+}
+
+unique_ptr<IOBuf> IOBuf::clone() const {
+  unique_ptr<IOBuf> ret = make_unique<IOBuf>();
+  cloneInto(*ret);
+  return ret;
+}
+
+unique_ptr<IOBuf> IOBuf::cloneOne() const {
+  unique_ptr<IOBuf> ret = make_unique<IOBuf>();
+  cloneOneInto(*ret);
+  return ret;
+}
+
+void IOBuf::cloneInto(IOBuf& other) const {
+  IOBuf tmp;
+  cloneOneInto(tmp);
+
+  for (IOBuf* current = next_; current != this; current = current->next_) {
+    tmp.prependChain(current->cloneOne());
+  }
+
+  other = std::move(tmp);
+}
+
+void IOBuf::cloneOneInto(IOBuf& other) const {
+  SharedInfo* info = sharedInfo();
+  if (info) {
+    setFlags(kFlagMaybeShared);
+  }
+  other = IOBuf(InternalConstructor(),
+                flagsAndSharedInfo_, buf_, capacity_,
+                data_, length_);
+  if (info) {
+    info->refcount.fetch_add(1, std::memory_order_acq_rel);
+  }
+}
+
+void IOBuf::unshareOneSlow() {
+  // Allocate a new buffer for the data
+  uint8_t* buf;
+  SharedInfo* sharedInfo;
+  uint64_t actualCapacity;
+  allocExtBuffer(capacity_, &buf, &sharedInfo, &actualCapacity);
+
+  // Copy the data
+  // Maintain the same amount of headroom.  Since we maintained the same
+  // minimum capacity we also maintain at least the same amount of tailroom.
+  uint64_t headlen = headroom();
+  memcpy(buf + headlen, data_, length_);
+
+  // Release our reference on the old buffer
+  decrementRefcount();
+  // Make sure kFlagMaybeShared and kFlagFreeSharedInfo are all cleared.
+  setFlagsAndSharedInfo(0, sharedInfo);
+
+  // Update the buffer pointers to point to the new buffer
+  data_ = buf + headlen;
+  buf_ = buf;
+}
+
+void IOBuf::unshareChained() {
+  // unshareChained() should only be called if we are part of a chain of
+  // multiple IOBufs.  The caller should have already verified this.
+  assert(isChained());
+
+  IOBuf* current = this;
+  while (true) {
+    if (current->isSharedOne()) {
+      // we have to unshare
+      break;
+    }
+
+    current = current->next_;
+    if (current == this) {
+      // None of the IOBufs in the chain are shared,
+      // so return without doing anything
+      return;
+    }
+  }
+
+  // We have to unshare.  Let coalesceSlow() do the work.
+  coalesceSlow();
+}
+
+void IOBuf::coalesceSlow() {
+  // coalesceSlow() should only be called if we are part of a chain of multiple
+  // IOBufs.  The caller should have already verified this.
+  DCHECK(isChained());
+
+  // Compute the length of the entire chain
+  uint64_t newLength = 0;
+  IOBuf* end = this;
+  do {
+    newLength += end->length_;
+    end = end->next_;
+  } while (end != this);
+
+  coalesceAndReallocate(newLength, end);
+  // We should be only element left in the chain now
+  DCHECK(!isChained());
+}
+
+void IOBuf::coalesceSlow(size_t maxLength) {
+  // coalesceSlow() should only be called if we are part of a chain of multiple
+  // IOBufs.  The caller should have already verified this.
+  DCHECK(isChained());
+  DCHECK_LT(length_, maxLength);
+
+  // Compute the length of the entire chain
+  uint64_t newLength = 0;
+  IOBuf* end = this;
+  while (true) {
+    newLength += end->length_;
+    end = end->next_;
+    if (newLength >= maxLength) {
+      break;
+    }
+    if (end == this) {
+      throw std::overflow_error("attempted to coalesce more data than "
+                                "available");
+    }
+  }
+
+  coalesceAndReallocate(newLength, end);
+  // We should have the requested length now
+  DCHECK_GE(length_, maxLength);
+}
+
+void IOBuf::coalesceAndReallocate(size_t newHeadroom,
+                                  size_t newLength,
+                                  IOBuf* end,
+                                  size_t newTailroom) {
+  uint64_t newCapacity = newLength + newHeadroom + newTailroom;
+  if (newCapacity > UINT32_MAX) {
+    throw std::overflow_error("IOBuf chain too large to coalesce");
+  }
+
+  // Allocate space for the coalesced buffer.
+  // We always convert to an external buffer, even if we happened to be an
+  // internal buffer before.
+  uint8_t* newBuf;
+  SharedInfo* newInfo;
+  uint64_t actualCapacity;
+  allocExtBuffer(newCapacity, &newBuf, &newInfo, &actualCapacity);
+
+  // Copy the data into the new buffer
+  uint8_t* newData = newBuf + newHeadroom;
+  uint8_t* p = newData;
+  IOBuf* current = this;
+  size_t remaining = newLength;
+  do {
+    assert(current->length_ <= remaining);
+    remaining -= current->length_;
+    memcpy(p, current->data_, current->length_);
+    p += current->length_;
+    current = current->next_;
+  } while (current != end);
+  assert(remaining == 0);
+
+  // Point at the new buffer
+  decrementRefcount();
+
+  // Make sure kFlagMaybeShared and kFlagFreeSharedInfo are all cleared.
+  setFlagsAndSharedInfo(0, newInfo);
+
+  capacity_ = actualCapacity;
+  buf_ = newBuf;
+  data_ = newData;
+  length_ = newLength;
+
+  // Separate from the rest of our chain.
+  // Since we don't store the unique_ptr returned by separateChain(),
+  // this will immediately delete the returned subchain.
+  if (isChained()) {
+    (void)separateChain(next_, current->prev_);
+  }
+}
+
+void IOBuf::decrementRefcount() {
+  // Externally owned buffers don't have a SharedInfo object and aren't managed
+  // by the reference count
+  SharedInfo* info = sharedInfo();
+  if (!info) {
+    return;
+  }
+
+  // Decrement the refcount
+  uint32_t newcnt = info->refcount.fetch_sub(
+      1, std::memory_order_acq_rel);
+  // Note that fetch_sub() returns the value before we decremented.
+  // If it is 1, we were the only remaining user; if it is greater there are
+  // still other users.
+  if (newcnt > 1) {
+    return;
+  }
+
+  // We were the last user.  Free the buffer
+  freeExtBuffer();
+
+  // Free the SharedInfo if it was allocated separately.
+  //
+  // This is only used by takeOwnership().
+  //
+  // To avoid this special case handling in decrementRefcount(), we could have
+  // takeOwnership() set a custom freeFn() that calls the user's free function
+  // then frees the SharedInfo object.  (This would require that
+  // takeOwnership() store the user's free function with its allocated
+  // SharedInfo object.)  However, handling this specially with a flag seems
+  // like it shouldn't be problematic.
+  if (flags() & kFlagFreeSharedInfo) {
+    delete sharedInfo();
+  }
+}
+
+void IOBuf::reserveSlow(uint64_t minHeadroom, uint64_t minTailroom) {
+  size_t newCapacity = (size_t)length_ + minHeadroom + minTailroom;
+  DCHECK_LT(newCapacity, UINT32_MAX);
+
+  // reserveSlow() is dangerous if anyone else is sharing the buffer, as we may
+  // reallocate and free the original buffer.  It should only ever be called if
+  // we are the only user of the buffer.
+  DCHECK(!isSharedOne());
+
+  // We'll need to reallocate the buffer.
+  // There are a few options.
+  // - If we have enough total room, move the data around in the buffer
+  //   and adjust the data_ pointer.
+  // - If we're using an internal buffer, we'll switch to an external
+  //   buffer with enough headroom and tailroom.
+  // - If we have enough headroom (headroom() >= minHeadroom) but not too much
+  //   (so we don't waste memory), we can try one of two things, depending on
+  //   whether we use jemalloc or not:
+  //   - If using jemalloc, we can try to expand in place, avoiding a memcpy()
+  //   - If not using jemalloc and we don't have too much to copy,
+  //     we'll use realloc() (note that realloc might have to copy
+  //     headroom + data + tailroom, see smartRealloc in folly/Malloc.h)
+  // - Otherwise, bite the bullet and reallocate.
+  if (headroom() + tailroom() >= minHeadroom + minTailroom) {
+    uint8_t* newData = writableBuffer() + minHeadroom;
+    memmove(newData, data_, length_);
+    data_ = newData;
+    return;
+  }
+
+  size_t newAllocatedCapacity = goodExtBufferSize(newCapacity);
+  uint8_t* newBuffer = nullptr;
+  uint64_t newHeadroom = 0;
+  uint64_t oldHeadroom = headroom();
+
+  // If we have a buffer allocated with malloc and we just need more tailroom,
+  // try to use realloc()/rallocm() to grow the buffer in place.
+  SharedInfo* info = sharedInfo();
+  if (info && (info->freeFn == nullptr) && length_ != 0 &&
+      oldHeadroom >= minHeadroom) {
+    if (usingJEMalloc()) {
+      size_t headSlack = oldHeadroom - minHeadroom;
+      // We assume that tailroom is more useful and more important than
+      // headroom (not least because realloc / rallocm allow us to grow the
+      // buffer at the tail, but not at the head)  So, if we have more headroom
+      // than we need, we consider that "wasted".  We arbitrarily define "too
+      // much" headroom to be 25% of the capacity.
+      if (headSlack * 4 <= newCapacity) {
+        size_t allocatedCapacity = capacity() + sizeof(SharedInfo);
+        void* p = buf_;
+        if (allocatedCapacity >= jemallocMinInPlaceExpandable) {
+          // rallocm can write to its 2nd arg even if it returns
+          // ALLOCM_ERR_NOT_MOVED. So, we pass a temporary to its 2nd arg and
+          // update newAllocatedCapacity only on success.
+          size_t allocatedSize;
+          int r = rallocm(&p, &allocatedSize, newAllocatedCapacity,
+                          0, ALLOCM_NO_MOVE);
+          if (r == ALLOCM_SUCCESS) {
+            newBuffer = static_cast<uint8_t*>(p);
+            newHeadroom = oldHeadroom;
+            newAllocatedCapacity = allocatedSize;
+          } else if (r == ALLOCM_ERR_OOM) {
+            // shouldn't happen as we don't actually allocate new memory
+            // (due to ALLOCM_NO_MOVE)
+            throw std::bad_alloc();
+          }
+          // if ALLOCM_ERR_NOT_MOVED, do nothing, fall back to
+          // malloc/memcpy/free
+        }
+      }
+    } else {  // Not using jemalloc
+      size_t copySlack = capacity() - length_;
+      if (copySlack * 2 <= length_) {
+        void* p = realloc(buf_, newAllocatedCapacity);
+        if (UNLIKELY(p == nullptr)) {
+          throw std::bad_alloc();
+        }
+        newBuffer = static_cast<uint8_t*>(p);
+        newHeadroom = oldHeadroom;
+      }
+    }
+  }
+
+  // None of the previous reallocation strategies worked (or we're using
+  // an internal buffer).  malloc/copy/free.
+  if (newBuffer == nullptr) {
+    void* p = malloc(newAllocatedCapacity);
+    if (UNLIKELY(p == nullptr)) {
+      throw std::bad_alloc();
+    }
+    newBuffer = static_cast<uint8_t*>(p);
+    memcpy(newBuffer + minHeadroom, data_, length_);
+    if (sharedInfo()) {
+      freeExtBuffer();
+    }
+    newHeadroom = minHeadroom;
+  }
+
+  uint64_t cap;
+  initExtBuffer(newBuffer, newAllocatedCapacity, &info, &cap);
+
+  if (flags() & kFlagFreeSharedInfo) {
+    delete sharedInfo();
+  }
+
+  setFlagsAndSharedInfo(0, info);
+  capacity_ = cap;
+  buf_ = newBuffer;
+  data_ = newBuffer + newHeadroom;
+  // length_ is unchanged
+}
+
+void IOBuf::freeExtBuffer() {
+  SharedInfo* info = sharedInfo();
+  DCHECK(info);
+
+  if (info->freeFn) {
+    try {
+      info->freeFn(buf_, info->userData);
+    } catch (...) {
+      // The user's free function should never throw.  Otherwise we might
+      // throw from the IOBuf destructor.  Other code paths like coalesce()
+      // also assume that decrementRefcount() cannot throw.
+      abort();
+    }
+  } else {
+    free(buf_);
+  }
+}
+
+void IOBuf::allocExtBuffer(uint64_t minCapacity,
+                           uint8_t** bufReturn,
+                           SharedInfo** infoReturn,
+                           uint64_t* capacityReturn) {
+  size_t mallocSize = goodExtBufferSize(minCapacity);
+  uint8_t* buf = static_cast<uint8_t*>(malloc(mallocSize));
+  if (UNLIKELY(buf == NULL)) {
+    throw std::bad_alloc();
+  }
+  initExtBuffer(buf, mallocSize, infoReturn, capacityReturn);
+  *bufReturn = buf;
+}
+
+size_t IOBuf::goodExtBufferSize(uint64_t minCapacity) {
+  // Determine how much space we should allocate.  We'll store the SharedInfo
+  // for the external buffer just after the buffer itself.  (We store it just
+  // after the buffer rather than just before so that the code can still just
+  // use free(buf_) to free the buffer.)
+  size_t minSize = static_cast<size_t>(minCapacity) + sizeof(SharedInfo);
+  // Add room for padding so that the SharedInfo will be aligned on an 8-byte
+  // boundary.
+  minSize = (minSize + 7) & ~7;
+
+  // Use goodMallocSize() to bump up the capacity to a decent size to request
+  // from malloc, so we can use all of the space that malloc will probably give
+  // us anyway.
+  return goodMallocSize(minSize);
+}
+
+void IOBuf::initExtBuffer(uint8_t* buf, size_t mallocSize,
+                          SharedInfo** infoReturn,
+                          uint64_t* capacityReturn) {
+  // Find the SharedInfo storage at the end of the buffer
+  // and construct the SharedInfo.
+  uint8_t* infoStart = (buf + mallocSize) - sizeof(SharedInfo);
+  SharedInfo* sharedInfo = new(infoStart) SharedInfo;
+
+  *capacityReturn = infoStart - buf;
+  *infoReturn = sharedInfo;
+}
+
+fbstring IOBuf::moveToFbString() {
+  // malloc-allocated buffers are just fine, everything else needs
+  // to be turned into one.
+  if (!sharedInfo() ||         // user owned, not ours to give up
+      sharedInfo()->freeFn ||  // not malloc()-ed
+      headroom() != 0 ||       // malloc()-ed block doesn't start at beginning
+      tailroom() == 0 ||       // no room for NUL terminator
+      isShared() ||            // shared
+      isChained()) {           // chained
+    // We might as well get rid of all head and tailroom if we're going
+    // to reallocate; we need 1 byte for NUL terminator.
+    coalesceAndReallocate(0, computeChainDataLength(), this, 1);
+  }
+
+  // Ensure NUL terminated
+  *writableTail() = 0;
+  fbstring str(reinterpret_cast<char*>(writableData()),
+               length(),  capacity(),
+               AcquireMallocatedString());
+
+  if (flags() & kFlagFreeSharedInfo) {
+    delete sharedInfo();
+  }
+
+  // Reset to a state where we can be deleted cleanly
+  flagsAndSharedInfo_ = 0;
+  buf_ = nullptr;
+  clear();
+  return str;
+}
+
+IOBuf::Iterator IOBuf::cbegin() const {
+  return Iterator(this, this);
+}
+
+IOBuf::Iterator IOBuf::cend() const {
+  return Iterator(nullptr, nullptr);
+}
+
+folly::fbvector<struct iovec> IOBuf::getIov() const {
+  folly::fbvector<struct iovec> iov;
+  iov.reserve(countChainElements());
+  IOBuf const* p = this;
+  do {
+    // some code can get confused by empty iovs, so skip them
+    if (p->length() > 0) {
+      iov.push_back({(void*)p->data(), p->length()});
+    }
+    p = p->next();
+  } while (p != this);
+  return iov;
+}
+
+} // folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/IOBuf.h
@@ -0,0 +1,1369 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_IO_IOBUF_H_
+#define FOLLY_IO_IOBUF_H_
+
+#include <glog/logging.h>
+#include <atomic>
+#include <cassert>
+#include <cinttypes>
+#include <cstddef>
+#include <cstring>
+#include <memory>
+#include <limits>
+#include <sys/uio.h>
+#include <type_traits>
+
+#include <boost/iterator/iterator_facade.hpp>
+
+#include "folly/FBString.h"
+#include "folly/Range.h"
+#include "folly/FBVector.h"
+
+// Ignore shadowing warnings within this file, so includers can use -Wshadow.
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wshadow"
+
+namespace folly {
+
+/**
+ * An IOBuf is a pointer to a buffer of data.
+ *
+ * IOBuf objects are intended to be used primarily for networking code, and are
+ * modelled somewhat after FreeBSD's mbuf data structure, and Linux's sk_buff
+ * structure.
+ *
+ * IOBuf objects facilitate zero-copy network programming, by allowing multiple
+ * IOBuf objects to point to the same underlying buffer of data, using a
+ * reference count to track when the buffer is no longer needed and can be
+ * freed.
+ *
+ *
+ * Data Layout
+ * -----------
+ *
+ * The IOBuf itself is a small object containing a pointer to the buffer and
+ * information about which segment of the buffer contains valid data.
+ *
+ * The data layout looks like this:
+ *
+ *  +-------+
+ *  | IOBuf |
+ *  +-------+
+ *   /
+ *  |
+ *  v
+ *  +------------+--------------------+-----------+
+ *  | headroom   |        data        |  tailroom |
+ *  +------------+--------------------+-----------+
+ *  ^            ^                    ^           ^
+ *  buffer()   data()               tail()      bufferEnd()
+ *
+ *  The length() method returns the length of the valid data; capacity()
+ *  returns the entire capacity of the buffer (from buffer() to bufferEnd()).
+ *  The headroom() and tailroom() methods return the amount of unused capacity
+ *  available before and after the data.
+ *
+ *
+ * Buffer Sharing
+ * --------------
+ *
+ * The buffer itself is reference counted, and multiple IOBuf objects may point
+ * to the same buffer.  Each IOBuf may point to a different section of valid
+ * data within the underlying buffer.  For example, if multiple protocol
+ * requests are read from the network into a single buffer, a separate IOBuf
+ * may be created for each request, all sharing the same underlying buffer.
+ *
+ * In other words, when multiple IOBufs share the same underlying buffer, the
+ * data() and tail() methods on each IOBuf may point to a different segment of
+ * the data.  However, the buffer() and bufferEnd() methods will point to the
+ * same location for all IOBufs sharing the same underlying buffer.
+ *
+ *       +-----------+     +---------+
+ *       |  IOBuf 1  |     | IOBuf 2 |
+ *       +-----------+     +---------+
+ *        |         | _____/        |
+ *   data |    tail |/    data      | tail
+ *        v         v               v
+ *  +-------------------------------------+
+ *  |     |         |               |     |
+ *  +-------------------------------------+
+ *
+ * If you only read data from an IOBuf, you don't need to worry about other
+ * IOBuf objects possibly sharing the same underlying buffer.  However, if you
+ * ever write to the buffer you need to first ensure that no other IOBufs point
+ * to the same buffer.  The unshare() method may be used to ensure that you
+ * have an unshared buffer.
+ *
+ *
+ * IOBuf Chains
+ * ------------
+ *
+ * IOBuf objects also contain pointers to next and previous IOBuf objects.
+ * This can be used to represent a single logical piece of data that its stored
+ * in non-contiguous chunks in separate buffers.
+ *
+ * A single IOBuf object can only belong to one chain at a time.
+ *
+ * IOBuf chains are always circular.  The "prev" pointer in the head of the
+ * chain points to the tail of the chain.  However, it is up to the user to
+ * decide which IOBuf is the head.  Internally the IOBuf code does not care
+ * which element is the head.
+ *
+ * The lifetime of all IOBufs in the chain are linked: when one element in the
+ * chain is deleted, all other chained elements are also deleted.  Conceptually
+ * it is simplest to treat this as if the head of the chain owns all other
+ * IOBufs in the chain.  When you delete the head of the chain, it will delete
+ * the other elements as well.  For this reason, prependChain() and
+ * appendChain() take ownership of of the new elements being added to this
+ * chain.
+ *
+ * When the coalesce() method is used to coalesce an entire IOBuf chain into a
+ * single IOBuf, all other IOBufs in the chain are eliminated and automatically
+ * deleted.  The unshare() method may coalesce the chain; if it does it will
+ * similarly delete all IOBufs eliminated from the chain.
+ *
+ * As discussed in the following section, it is up to the user to maintain a
+ * lock around the entire IOBuf chain if multiple threads need to access the
+ * chain.  IOBuf does not provide any internal locking.
+ *
+ *
+ * Synchronization
+ * ---------------
+ *
+ * When used in multithread programs, a single IOBuf object should only be used
+ * in a single thread at a time.  If a caller uses a single IOBuf across
+ * multiple threads the caller is responsible for using an external lock to
+ * synchronize access to the IOBuf.
+ *
+ * Two separate IOBuf objects may be accessed concurrently in separate threads
+ * without locking, even if they point to the same underlying buffer.  The
+ * buffer reference count is always accessed atomically, and no other
+ * operations should affect other IOBufs that point to the same data segment.
+ * The caller is responsible for using unshare() to ensure that the data buffer
+ * is not shared by other IOBufs before writing to it, and this ensures that
+ * the data itself is not modified in one thread while also being accessed from
+ * another thread.
+ *
+ * For IOBuf chains, no two IOBufs in the same chain should be accessed
+ * simultaneously in separate threads.  The caller must maintain a lock around
+ * the entire chain if the chain, or individual IOBufs in the chain, may be
+ * accessed by multiple threads.
+ *
+ *
+ * IOBuf Object Allocation
+ * -----------------------
+ *
+ * IOBuf objects themselves exist separately from the data buffer they point
+ * to.  Therefore one must also consider how to allocate and manage the IOBuf
+ * objects.
+ *
+ * It is more common to allocate IOBuf objects on the heap, using the create(),
+ * takeOwnership(), or wrapBuffer() factory functions.  The clone()/cloneOne()
+ * functions also return new heap-allocated IOBufs.  The createCombined()
+ * function allocates the IOBuf object and data storage space together, in a
+ * single memory allocation.  This can improve performance, particularly if you
+ * know that the data buffer and the IOBuf itself will have similar lifetimes.
+ *
+ * That said, it is also possible to allocate IOBufs on the stack or inline
+ * inside another object as well.  This is useful for cases where the IOBuf is
+ * short-lived, or when the overhead of allocating the IOBuf on the heap is
+ * undesirable.
+ *
+ * However, note that stack-allocated IOBufs may only be used as the head of a
+ * chain (or standalone as the only IOBuf in a chain).  All non-head members of
+ * an IOBuf chain must be heap allocated.  (All functions to add nodes to a
+ * chain require a std::unique_ptr<IOBuf>, which enforces this requrement.)
+ *
+ * Additionally, no copy-constructor or assignment operator currently exists,
+ * so stack-allocated IOBufs may only be moved, not copied.  (Technically
+ * nothing is preventing us from adding a copy constructor and assignment
+ * operator.  However, it seems like this would add the possibility for some
+ * confusion.  We would need to determine if these functions would copy just a
+ * single buffer, or the entire chain.)
+ *
+ *
+ * IOBuf Sharing
+ * -------------
+ *
+ * The IOBuf class manages sharing of the underlying buffer that it points to,
+ * maintaining a reference count if multiple IOBufs are pointing at the same
+ * buffer.
+ *
+ * However, it is the callers responsibility to manage sharing and ownership of
+ * IOBuf objects themselves.  The IOBuf structure does not provide room for an
+ * intrusive refcount on the IOBuf object itself, only the underlying data
+ * buffer is reference counted.  If users want to share the same IOBuf object
+ * between multiple parts of the code, they are responsible for managing this
+ * sharing on their own.  (For example, by using a shared_ptr.  Alternatively,
+ * users always have the option of using clone() to create a second IOBuf that
+ * points to the same underlying buffer.)
+ */
+namespace detail {
+// Is T a unique_ptr<> to a standard-layout type?
+template <class T, class Enable=void> struct IsUniquePtrToSL
+  : public std::false_type { };
+template <class T, class D>
+struct IsUniquePtrToSL<
+  std::unique_ptr<T, D>,
+  typename std::enable_if<std::is_standard_layout<T>::value>::type>
+  : public std::true_type { };
+}  // namespace detail
+
+class IOBuf {
+ public:
+  class Iterator;
+
+  enum CreateOp { CREATE };
+  enum WrapBufferOp { WRAP_BUFFER };
+  enum TakeOwnershipOp { TAKE_OWNERSHIP };
+  enum CopyBufferOp { COPY_BUFFER };
+
+  typedef ByteRange value_type;
+  typedef Iterator iterator;
+  typedef Iterator const_iterator;
+
+  typedef void (*FreeFunction)(void* buf, void* userData);
+
+  /**
+   * Allocate a new IOBuf object with the requested capacity.
+   *
+   * Returns a new IOBuf object that must be (eventually) deleted by the
+   * caller.  The returned IOBuf may actually have slightly more capacity than
+   * requested.
+   *
+   * The data pointer will initially point to the start of the newly allocated
+   * buffer, and will have a data length of 0.
+   *
+   * Throws std::bad_alloc on error.
+   */
+  static std::unique_ptr<IOBuf> create(uint64_t capacity);
+  IOBuf(CreateOp, uint64_t capacity);
+
+  /**
+   * Create a new IOBuf, using a single memory allocation to allocate space
+   * for both the IOBuf object and the data storage space.
+   *
+   * This saves one memory allocation.  However, it can be wasteful if you
+   * later need to grow the buffer using reserve().  If the buffer needs to be
+   * reallocated, the space originally allocated will not be freed() until the
+   * IOBuf object itself is also freed.  (It can also be slightly wasteful in
+   * some cases where you clone this IOBuf and then free the original IOBuf.)
+   */
+  static std::unique_ptr<IOBuf> createCombined(uint64_t capacity);
+
+  /**
+   * Create a new IOBuf, using separate memory allocations for the IOBuf object
+   * for the IOBuf and the data storage space.
+   *
+   * This requires two memory allocations, but saves space in the long run
+   * if you know that you will need to reallocate the data buffer later.
+   */
+  static std::unique_ptr<IOBuf> createSeparate(uint64_t capacity);
+
+  /**
+   * Allocate a new IOBuf chain with the requested total capacity, allocating
+   * no more than maxBufCapacity to each buffer.
+   */
+  static std::unique_ptr<IOBuf> createChain(
+      size_t totalCapacity, uint64_t maxBufCapacity);
+
+  /**
+   * Create a new IOBuf pointing to an existing data buffer.
+   *
+   * The new IOBuffer will assume ownership of the buffer, and free it by
+   * calling the specified FreeFunction when the last IOBuf pointing to this
+   * buffer is destroyed.  The function will be called with a pointer to the
+   * buffer as the first argument, and the supplied userData value as the
+   * second argument.  The free function must never throw exceptions.
+   *
+   * If no FreeFunction is specified, the buffer will be freed using free()
+   * which will result in undefined behavior if the memory was allocated
+   * using 'new'.
+   *
+   * The IOBuf data pointer will initially point to the start of the buffer,
+   *
+   * In the first version of this function, the length of data is unspecified
+   * and is initialized to the capacity of the buffer
+   *
+   * In the second version, the user specifies the valid length of data
+   * in the buffer
+   *
+   * On error, std::bad_alloc will be thrown.  If freeOnError is true (the
+   * default) the buffer will be freed before throwing the error.
+   */
+  static std::unique_ptr<IOBuf> takeOwnership(void* buf, uint64_t capacity,
+                                              FreeFunction freeFn = nullptr,
+                                              void* userData = nullptr,
+                                              bool freeOnError = true) {
+    return takeOwnership(buf, capacity, capacity, freeFn,
+                         userData, freeOnError);
+  }
+  IOBuf(TakeOwnershipOp op, void* buf, uint64_t capacity,
+        FreeFunction freeFn = nullptr, void* userData = nullptr,
+        bool freeOnError = true)
+    : IOBuf(op, buf, capacity, capacity, freeFn, userData, freeOnError) {}
+
+  static std::unique_ptr<IOBuf> takeOwnership(void* buf, uint64_t capacity,
+                                              uint64_t length,
+                                              FreeFunction freeFn = nullptr,
+                                              void* userData = nullptr,
+                                              bool freeOnError = true);
+  IOBuf(TakeOwnershipOp, void* buf, uint64_t capacity, uint64_t length,
+        FreeFunction freeFn = nullptr, void* userData = nullptr,
+        bool freeOnError = true);
+
+  /**
+   * Create a new IOBuf pointing to an existing data buffer made up of
+   * count objects of a given standard-layout type.
+   *
+   * This is dangerous -- it is essentially equivalent to doing
+   * reinterpret_cast<unsigned char*> on your data -- but it's often useful
+   * for serialization / deserialization.
+   *
+   * The new IOBuffer will assume ownership of the buffer, and free it
+   * appropriately (by calling the UniquePtr's custom deleter, or by calling
+   * delete or delete[] appropriately if there is no custom deleter)
+   * when the buffer is destroyed.  The custom deleter, if any, must never
+   * throw exceptions.
+   *
+   * The IOBuf data pointer will initially point to the start of the buffer,
+   * and the length will be the full capacity of the buffer (count *
+   * sizeof(T)).
+   *
+   * On error, std::bad_alloc will be thrown, and the buffer will be freed
+   * before throwing the error.
+   */
+  template <class UniquePtr>
+  static typename std::enable_if<detail::IsUniquePtrToSL<UniquePtr>::value,
+                                 std::unique_ptr<IOBuf>>::type
+  takeOwnership(UniquePtr&& buf, size_t count=1);
+
+  /**
+   * Create a new IOBuf object that points to an existing user-owned buffer.
+   *
+   * This should only be used when the caller knows the lifetime of the IOBuf
+   * object ahead of time and can ensure that all IOBuf objects that will point
+   * to this buffer will be destroyed before the buffer itself is destroyed.
+   *
+   * This buffer will not be freed automatically when the last IOBuf
+   * referencing it is destroyed.  It is the caller's responsibility to free
+   * the buffer after the last IOBuf has been destroyed.
+   *
+   * The IOBuf data pointer will initially point to the start of the buffer,
+   * and the length will be the full capacity of the buffer.
+   *
+   * An IOBuf created using wrapBuffer() will always be reported as shared.
+   * unshare() may be used to create a writable copy of the buffer.
+   *
+   * On error, std::bad_alloc will be thrown.
+   */
+  static std::unique_ptr<IOBuf> wrapBuffer(const void* buf, uint64_t capacity);
+  static std::unique_ptr<IOBuf> wrapBuffer(ByteRange br) {
+    return wrapBuffer(br.data(), br.size());
+  }
+  IOBuf(WrapBufferOp op, const void* buf, uint64_t capacity);
+  IOBuf(WrapBufferOp op, ByteRange br);
+
+  /**
+   * Convenience function to create a new IOBuf object that copies data from a
+   * user-supplied buffer, optionally allocating a given amount of
+   * headroom and tailroom.
+   */
+  static std::unique_ptr<IOBuf> copyBuffer(const void* buf, uint64_t size,
+                                           uint64_t headroom=0,
+                                           uint64_t minTailroom=0);
+  static std::unique_ptr<IOBuf> copyBuffer(ByteRange br,
+                                           uint64_t headroom=0,
+                                           uint64_t minTailroom=0) {
+    return copyBuffer(br.data(), br.size(), headroom, minTailroom);
+  }
+  IOBuf(CopyBufferOp op, const void* buf, uint64_t size,
+        uint64_t headroom=0, uint64_t minTailroom=0);
+  IOBuf(CopyBufferOp op, ByteRange br,
+        uint64_t headroom=0, uint64_t minTailroom=0);
+
+  /**
+   * Convenience function to create a new IOBuf object that copies data from a
+   * user-supplied string, optionally allocating a given amount of
+   * headroom and tailroom.
+   *
+   * Beware when attempting to invoke this function with a constant string
+   * literal and a headroom argument: you will likely end up invoking the
+   * version of copyBuffer() above.  IOBuf::copyBuffer("hello", 3) will treat
+   * the first argument as a const void*, and will invoke the version of
+   * copyBuffer() above, with the size argument of 3.
+   */
+  static std::unique_ptr<IOBuf> copyBuffer(const std::string& buf,
+                                           uint64_t headroom=0,
+                                           uint64_t minTailroom=0);
+  IOBuf(CopyBufferOp op, const std::string& buf,
+        uint64_t headroom=0, uint64_t minTailroom=0)
+    : IOBuf(op, buf.data(), buf.size(), headroom, minTailroom) {}
+
+  /**
+   * A version of copyBuffer() that returns a null pointer if the input string
+   * is empty.
+   */
+  static std::unique_ptr<IOBuf> maybeCopyBuffer(const std::string& buf,
+                                                uint64_t headroom=0,
+                                                uint64_t minTailroom=0);
+
+  /**
+   * Convenience function to free a chain of IOBufs held by a unique_ptr.
+   */
+  static void destroy(std::unique_ptr<IOBuf>&& data) {
+    auto destroyer = std::move(data);
+  }
+
+  /**
+   * Destroy this IOBuf.
+   *
+   * Deleting an IOBuf will automatically destroy all IOBufs in the chain.
+   * (See the comments above regarding the ownership model of IOBuf chains.
+   * All subsequent IOBufs in the chain are considered to be owned by the head
+   * of the chain.  Users should only explicitly delete the head of a chain.)
+   *
+   * When each individual IOBuf is destroyed, it will release its reference
+   * count on the underlying buffer.  If it was the last user of the buffer,
+   * the buffer will be freed.
+   */
+  ~IOBuf();
+
+  /**
+   * Check whether the chain is empty (i.e., whether the IOBufs in the
+   * chain have a total data length of zero).
+   *
+   * This method is semantically equivalent to
+   *   i->computeChainDataLength()==0
+   * but may run faster because it can short-circuit as soon as it
+   * encounters a buffer with length()!=0
+   */
+  bool empty() const;
+
+  /**
+   * Get the pointer to the start of the data.
+   */
+  const uint8_t* data() const {
+    return data_;
+  }
+
+  /**
+   * Get a writable pointer to the start of the data.
+   *
+   * The caller is responsible for calling unshare() first to ensure that it is
+   * actually safe to write to the buffer.
+   */
+  uint8_t* writableData() {
+    return data_;
+  }
+
+  /**
+   * Get the pointer to the end of the data.
+   */
+  const uint8_t* tail() const {
+    return data_ + length_;
+  }
+
+  /**
+   * Get a writable pointer to the end of the data.
+   *
+   * The caller is responsible for calling unshare() first to ensure that it is
+   * actually safe to write to the buffer.
+   */
+  uint8_t* writableTail() {
+    return data_ + length_;
+  }
+
+  /**
+   * Get the data length.
+   */
+  uint64_t length() const {
+    return length_;
+  }
+
+  /**
+   * Get the amount of head room.
+   *
+   * Returns the number of bytes in the buffer before the start of the data.
+   */
+  uint64_t headroom() const {
+    return data_ - buffer();
+  }
+
+  /**
+   * Get the amount of tail room.
+   *
+   * Returns the number of bytes in the buffer after the end of the data.
+   */
+  uint64_t tailroom() const {
+    return bufferEnd() - tail();
+  }
+
+  /**
+   * Get the pointer to the start of the buffer.
+   *
+   * Note that this is the pointer to the very beginning of the usable buffer,
+   * not the start of valid data within the buffer.  Use the data() method to
+   * get a pointer to the start of the data within the buffer.
+   */
+  const uint8_t* buffer() const {
+    return buf_;
+  }
+
+  /**
+   * Get a writable pointer to the start of the buffer.
+   *
+   * The caller is responsible for calling unshare() first to ensure that it is
+   * actually safe to write to the buffer.
+   */
+  uint8_t* writableBuffer() {
+    return buf_;
+  }
+
+  /**
+   * Get the pointer to the end of the buffer.
+   *
+   * Note that this is the pointer to the very end of the usable buffer,
+   * not the end of valid data within the buffer.  Use the tail() method to
+   * get a pointer to the end of the data within the buffer.
+   */
+  const uint8_t* bufferEnd() const {
+    return buf_ + capacity_;
+  }
+
+  /**
+   * Get the total size of the buffer.
+   *
+   * This returns the total usable length of the buffer.  Use the length()
+   * method to get the length of the actual valid data in this IOBuf.
+   */
+  uint64_t capacity() const {
+    return capacity_;
+  }
+
+  /**
+   * Get a pointer to the next IOBuf in this chain.
+   */
+  IOBuf* next() {
+    return next_;
+  }
+  const IOBuf* next() const {
+    return next_;
+  }
+
+  /**
+   * Get a pointer to the previous IOBuf in this chain.
+   */
+  IOBuf* prev() {
+    return prev_;
+  }
+  const IOBuf* prev() const {
+    return prev_;
+  }
+
+  /**
+   * Shift the data forwards in the buffer.
+   *
+   * This shifts the data pointer forwards in the buffer to increase the
+   * headroom.  This is commonly used to increase the headroom in a newly
+   * allocated buffer.
+   *
+   * The caller is responsible for ensuring that there is sufficient
+   * tailroom in the buffer before calling advance().
+   *
+   * If there is a non-zero data length, advance() will use memmove() to shift
+   * the data forwards in the buffer.  In this case, the caller is responsible
+   * for making sure the buffer is unshared, so it will not affect other IOBufs
+   * that may be sharing the same underlying buffer.
+   */
+  void advance(uint64_t amount) {
+    // In debug builds, assert if there is a problem.
+    assert(amount <= tailroom());
+
+    if (length_ > 0) {
+      memmove(data_ + amount, data_, length_);
+    }
+    data_ += amount;
+  }
+
+  /**
+   * Shift the data backwards in the buffer.
+   *
+   * The caller is responsible for ensuring that there is sufficient headroom
+   * in the buffer before calling retreat().
+   *
+   * If there is a non-zero data length, retreat() will use memmove() to shift
+   * the data backwards in the buffer.  In this case, the caller is responsible
+   * for making sure the buffer is unshared, so it will not affect other IOBufs
+   * that may be sharing the same underlying buffer.
+   */
+  void retreat(uint64_t amount) {
+    // In debug builds, assert if there is a problem.
+    assert(amount <= headroom());
+
+    if (length_ > 0) {
+      memmove(data_ - amount, data_, length_);
+    }
+    data_ -= amount;
+  }
+
+  /**
+   * Adjust the data pointer to include more valid data at the beginning.
+   *
+   * This moves the data pointer backwards to include more of the available
+   * buffer.  The caller is responsible for ensuring that there is sufficient
+   * headroom for the new data.  The caller is also responsible for populating
+   * this section with valid data.
+   *
+   * This does not modify any actual data in the buffer.
+   */
+  void prepend(uint64_t amount) {
+    DCHECK_LE(amount, headroom());
+    data_ -= amount;
+    length_ += amount;
+  }
+
+  /**
+   * Adjust the tail pointer to include more valid data at the end.
+   *
+   * This moves the tail pointer forwards to include more of the available
+   * buffer.  The caller is responsible for ensuring that there is sufficient
+   * tailroom for the new data.  The caller is also responsible for populating
+   * this section with valid data.
+   *
+   * This does not modify any actual data in the buffer.
+   */
+  void append(uint64_t amount) {
+    DCHECK_LE(amount, tailroom());
+    length_ += amount;
+  }
+
+  /**
+   * Adjust the data pointer forwards to include less valid data.
+   *
+   * This moves the data pointer forwards so that the first amount bytes are no
+   * longer considered valid data.  The caller is responsible for ensuring that
+   * amount is less than or equal to the actual data length.
+   *
+   * This does not modify any actual data in the buffer.
+   */
+  void trimStart(uint64_t amount) {
+    DCHECK_LE(amount, length_);
+    data_ += amount;
+    length_ -= amount;
+  }
+
+  /**
+   * Adjust the tail pointer backwards to include less valid data.
+   *
+   * This moves the tail pointer backwards so that the last amount bytes are no
+   * longer considered valid data.  The caller is responsible for ensuring that
+   * amount is less than or equal to the actual data length.
+   *
+   * This does not modify any actual data in the buffer.
+   */
+  void trimEnd(uint64_t amount) {
+    DCHECK_LE(amount, length_);
+    length_ -= amount;
+  }
+
+  /**
+   * Clear the buffer.
+   *
+   * Postcondition: headroom() == 0, length() == 0, tailroom() == capacity()
+   */
+  void clear() {
+    data_ = writableBuffer();
+    length_ = 0;
+  }
+
+  /**
+   * Ensure that this buffer has at least minHeadroom headroom bytes and at
+   * least minTailroom tailroom bytes.  The buffer must be writable
+   * (you must call unshare() before this, if necessary).
+   *
+   * Postcondition: headroom() >= minHeadroom, tailroom() >= minTailroom,
+   * the data (between data() and data() + length()) is preserved.
+   */
+  void reserve(uint64_t minHeadroom, uint64_t minTailroom) {
+    // Maybe we don't need to do anything.
+    if (headroom() >= minHeadroom && tailroom() >= minTailroom) {
+      return;
+    }
+    // If the buffer is empty but we have enough total room (head + tail),
+    // move the data_ pointer around.
+    if (length() == 0 &&
+        headroom() + tailroom() >= minHeadroom + minTailroom) {
+      data_ = writableBuffer() + minHeadroom;
+      return;
+    }
+    // Bah, we have to do actual work.
+    reserveSlow(minHeadroom, minTailroom);
+  }
+
+  /**
+   * Return true if this IOBuf is part of a chain of multiple IOBufs, or false
+   * if this is the only IOBuf in its chain.
+   */
+  bool isChained() const {
+    assert((next_ == this) == (prev_ == this));
+    return next_ != this;
+  }
+
+  /**
+   * Get the number of IOBufs in this chain.
+   *
+   * Beware that this method has to walk the entire chain.
+   * Use isChained() if you just want to check if this IOBuf is part of a chain
+   * or not.
+   */
+  size_t countChainElements() const;
+
+  /**
+   * Get the length of all the data in this IOBuf chain.
+   *
+   * Beware that this method has to walk the entire chain.
+   */
+  uint64_t computeChainDataLength() const;
+
+  /**
+   * Insert another IOBuf chain immediately before this IOBuf.
+   *
+   * For example, if there are two IOBuf chains (A, B, C) and (D, E, F),
+   * and B->prependChain(D) is called, the (D, E, F) chain will be subsumed
+   * and become part of the chain starting at A, which will now look like
+   * (A, D, E, F, B, C)
+   *
+   * Note that since IOBuf chains are circular, head->prependChain(other) can
+   * be used to append the other chain at the very end of the chain pointed to
+   * by head.  For example, if there are two IOBuf chains (A, B, C) and
+   * (D, E, F), and A->prependChain(D) is called, the chain starting at A will
+   * now consist of (A, B, C, D, E, F)
+   *
+   * The elements in the specified IOBuf chain will become part of this chain,
+   * and will be owned by the head of this chain.  When this chain is
+   * destroyed, all elements in the supplied chain will also be destroyed.
+   *
+   * For this reason, appendChain() only accepts an rvalue-reference to a
+   * unique_ptr(), to make it clear that it is taking ownership of the supplied
+   * chain.  If you have a raw pointer, you can pass in a new temporary
+   * unique_ptr around the raw pointer.  If you have an existing,
+   * non-temporary unique_ptr, you must call std::move(ptr) to make it clear
+   * that you are destroying the original pointer.
+   */
+  void prependChain(std::unique_ptr<IOBuf>&& iobuf);
+
+  /**
+   * Append another IOBuf chain immediately after this IOBuf.
+   *
+   * For example, if there are two IOBuf chains (A, B, C) and (D, E, F),
+   * and B->appendChain(D) is called, the (D, E, F) chain will be subsumed
+   * and become part of the chain starting at A, which will now look like
+   * (A, B, D, E, F, C)
+   *
+   * The elements in the specified IOBuf chain will become part of this chain,
+   * and will be owned by the head of this chain.  When this chain is
+   * destroyed, all elements in the supplied chain will also be destroyed.
+   *
+   * For this reason, appendChain() only accepts an rvalue-reference to a
+   * unique_ptr(), to make it clear that it is taking ownership of the supplied
+   * chain.  If you have a raw pointer, you can pass in a new temporary
+   * unique_ptr around the raw pointer.  If you have an existing,
+   * non-temporary unique_ptr, you must call std::move(ptr) to make it clear
+   * that you are destroying the original pointer.
+   */
+  void appendChain(std::unique_ptr<IOBuf>&& iobuf) {
+    // Just use prependChain() on the next element in our chain
+    next_->prependChain(std::move(iobuf));
+  }
+
+  /**
+   * Remove this IOBuf from its current chain.
+   *
+   * Since ownership of all elements an IOBuf chain is normally maintained by
+   * the head of the chain, unlink() transfers ownership of this IOBuf from the
+   * chain and gives it to the caller.  A new unique_ptr to the IOBuf is
+   * returned to the caller.  The caller must store the returned unique_ptr (or
+   * call release() on it) to take ownership, otherwise the IOBuf will be
+   * immediately destroyed.
+   *
+   * Since unlink transfers ownership of the IOBuf to the caller, be careful
+   * not to call unlink() on the head of a chain if you already maintain
+   * ownership on the head of the chain via other means.  The pop() method
+   * is a better choice for that situation.
+   */
+  std::unique_ptr<IOBuf> unlink() {
+    next_->prev_ = prev_;
+    prev_->next_ = next_;
+    prev_ = this;
+    next_ = this;
+    return std::unique_ptr<IOBuf>(this);
+  }
+
+  /**
+   * Remove this IOBuf from its current chain and return a unique_ptr to
+   * the IOBuf that formerly followed it in the chain.
+   */
+  std::unique_ptr<IOBuf> pop() {
+    IOBuf *next = next_;
+    next_->prev_ = prev_;
+    prev_->next_ = next_;
+    prev_ = this;
+    next_ = this;
+    return std::unique_ptr<IOBuf>((next == this) ? nullptr : next);
+  }
+
+  /**
+   * Remove a subchain from this chain.
+   *
+   * Remove the subchain starting at head and ending at tail from this chain.
+   *
+   * Returns a unique_ptr pointing to head.  (In other words, ownership of the
+   * head of the subchain is transferred to the caller.)  If the caller ignores
+   * the return value and lets the unique_ptr be destroyed, the subchain will
+   * be immediately destroyed.
+   *
+   * The subchain referenced by the specified head and tail must be part of the
+   * same chain as the current IOBuf, but must not contain the current IOBuf.
+   * However, the specified head and tail may be equal to each other (i.e.,
+   * they may be a subchain of length 1).
+   */
+  std::unique_ptr<IOBuf> separateChain(IOBuf* head, IOBuf* tail) {
+    assert(head != this);
+    assert(tail != this);
+
+    head->prev_->next_ = tail->next_;
+    tail->next_->prev_ = head->prev_;
+
+    head->prev_ = tail;
+    tail->next_ = head;
+
+    return std::unique_ptr<IOBuf>(head);
+  }
+
+  /**
+   * Return true if at least one of the IOBufs in this chain are shared,
+   * or false if all of the IOBufs point to unique buffers.
+   *
+   * Use isSharedOne() to only check this IOBuf rather than the entire chain.
+   */
+  bool isShared() const {
+    const IOBuf* current = this;
+    while (true) {
+      if (current->isSharedOne()) {
+        return true;
+      }
+      current = current->next_;
+      if (current == this) {
+        return false;
+      }
+    }
+  }
+
+  /**
+   * Return true if other IOBufs are also pointing to the buffer used by this
+   * IOBuf, and false otherwise.
+   *
+   * If this IOBuf points at a buffer owned by another (non-IOBuf) part of the
+   * code (i.e., if the IOBuf was created using wrapBuffer(), or was cloned
+   * from such an IOBuf), it is always considered shared.
+   *
+   * This only checks the current IOBuf, and not other IOBufs in the chain.
+   */
+  bool isSharedOne() const {
+    // If this is a user-owned buffer, it is always considered shared
+    if (UNLIKELY(!sharedInfo())) {
+      return true;
+    }
+
+    if (LIKELY(!(flags() & kFlagMaybeShared))) {
+      return false;
+    }
+
+    // kFlagMaybeShared is set, so we need to check the reference count.
+    // (Checking the reference count requires an atomic operation, which is why
+    // we prefer to only check kFlagMaybeShared if possible.)
+    bool shared = sharedInfo()->refcount.load(std::memory_order_acquire) > 1;
+    if (!shared) {
+      // we're the last one left
+      clearFlags(kFlagMaybeShared);
+    }
+    return shared;
+  }
+
+  /**
+   * Ensure that this IOBuf has a unique buffer that is not shared by other
+   * IOBufs.
+   *
+   * unshare() operates on an entire chain of IOBuf objects.  If the chain is
+   * shared, it may also coalesce the chain when making it unique.  If the
+   * chain is coalesced, subsequent IOBuf objects in the current chain will be
+   * automatically deleted.
+   *
+   * Note that buffers owned by other (non-IOBuf) users are automatically
+   * considered shared.
+   *
+   * Throws std::bad_alloc on error.  On error the IOBuf chain will be
+   * unmodified.
+   *
+   * Currently unshare may also throw std::overflow_error if it tries to
+   * coalesce.  (TODO: In the future it would be nice if unshare() were smart
+   * enough not to coalesce the entire buffer if the data is too large.
+   * However, in practice this seems unlikely to become an issue.)
+   */
+  void unshare() {
+    if (isChained()) {
+      unshareChained();
+    } else {
+      unshareOne();
+    }
+  }
+
+  /**
+   * Ensure that this IOBuf has a unique buffer that is not shared by other
+   * IOBufs.
+   *
+   * unshareOne() operates on a single IOBuf object.  This IOBuf will have a
+   * unique buffer after unshareOne() returns, but other IOBufs in the chain
+   * may still be shared after unshareOne() returns.
+   *
+   * Throws std::bad_alloc on error.  On error the IOBuf will be unmodified.
+   */
+  void unshareOne() {
+    if (isSharedOne()) {
+      unshareOneSlow();
+    }
+  }
+
+  /**
+   * Coalesce this IOBuf chain into a single buffer.
+   *
+   * This method moves all of the data in this IOBuf chain into a single
+   * contiguous buffer, if it is not already in one buffer.  After coalesce()
+   * returns, this IOBuf will be a chain of length one.  Other IOBufs in the
+   * chain will be automatically deleted.
+   *
+   * After coalescing, the IOBuf will have at least as much headroom as the
+   * first IOBuf in the chain, and at least as much tailroom as the last IOBuf
+   * in the chain.
+   *
+   * Throws std::bad_alloc on error.  On error the IOBuf chain will be
+   * unmodified.
+   *
+   * Returns ByteRange that points to the data IOBuf stores.
+   */
+  ByteRange coalesce() {
+    if (isChained()) {
+      coalesceSlow();
+    }
+    return ByteRange(data_, length_);
+  }
+
+  /**
+   * Ensure that this chain has at least maxLength bytes available as a
+   * contiguous memory range.
+   *
+   * This method coalesces whole buffers in the chain into this buffer as
+   * necessary until this buffer's length() is at least maxLength.
+   *
+   * After coalescing, the IOBuf will have at least as much headroom as the
+   * first IOBuf in the chain, and at least as much tailroom as the last IOBuf
+   * that was coalesced.
+   *
+   * Throws std::bad_alloc or std::overflow_error on error.  On error the IOBuf
+   * chain will be unmodified.  Throws std::overflow_error if maxLength is
+   * longer than the total chain length.
+   *
+   * Upon return, either enough of the chain was coalesced into a contiguous
+   * region, or the entire chain was coalesced.  That is,
+   * length() >= maxLength || !isChained() is true.
+   */
+  void gather(uint64_t maxLength) {
+    if (!isChained() || length_ >= maxLength) {
+      return;
+    }
+    coalesceSlow(maxLength);
+  }
+
+  /**
+   * Return a new IOBuf chain sharing the same data as this chain.
+   *
+   * The new IOBuf chain will normally point to the same underlying data
+   * buffers as the original chain.  (The one exception to this is if some of
+   * the IOBufs in this chain contain small internal data buffers which cannot
+   * be shared.)
+   */
+  std::unique_ptr<IOBuf> clone() const;
+
+  /**
+   * Return a new IOBuf with the same data as this IOBuf.
+   *
+   * The new IOBuf returned will not be part of a chain (even if this IOBuf is
+   * part of a larger chain).
+   */
+  std::unique_ptr<IOBuf> cloneOne() const;
+
+  /**
+   * Similar to Clone(). But use other as the head node. Other nodes in the
+   * chain (if any) will be allocted on heap.
+   */
+  void cloneInto(IOBuf& other) const;
+
+  /**
+   * Similar to CloneOne(). But to fill an existing IOBuf instead of a new
+   * IOBuf.
+   */
+  void cloneOneInto(IOBuf& other) const;
+
+  /**
+   * Return an iovector suitable for e.g. writev()
+   *
+   *   auto iov = buf->getIov();
+   *   auto xfer = writev(fd, iov.data(), iov.size());
+   *
+   * Naturally, the returned iovector is invalid if you modify the buffer
+   * chain.
+   */
+  folly::fbvector<struct iovec> getIov() const;
+
+  /*
+   * Overridden operator new and delete.
+   * These perform specialized memory management to help support
+   * createCombined(), which allocates IOBuf objects together with the buffer
+   * data.
+   */
+  void* operator new(size_t size);
+  void* operator new(size_t size, void* ptr);
+  void operator delete(void* ptr);
+
+  /**
+   * Destructively convert this IOBuf to a fbstring efficiently.
+   * We rely on fbstring's AcquireMallocatedString constructor to
+   * transfer memory.
+   */
+  fbstring moveToFbString();
+
+  /**
+   * Iteration support: a chain of IOBufs may be iterated through using
+   * STL-style iterators over const ByteRanges.  Iterators are only invalidated
+   * if the IOBuf that they currently point to is removed.
+   */
+  Iterator cbegin() const;
+  Iterator cend() const;
+  Iterator begin() const;
+  Iterator end() const;
+
+  /**
+   * Allocate a new null buffer.
+   *
+   * This can be used to allocate an empty IOBuf on the stack.  It will have no
+   * space allocated for it.  This is generally useful only to later use move
+   * assignment to fill out the IOBuf.
+   */
+  IOBuf() noexcept;
+
+  /**
+   * Move constructor and assignment operator.
+   *
+   * In general, you should only ever move the head of an IOBuf chain.
+   * Internal nodes in an IOBuf chain are owned by the head of the chain, and
+   * should not be moved from.  (Technically, nothing prevents you from moving
+   * a non-head node, but the moved-to node will replace the moved-from node in
+   * the chain.  This has implications for ownership, since non-head nodes are
+   * owned by the chain head.  You are then responsible for relinquishing
+   * ownership of the moved-to node, and manually deleting the moved-from
+   * node.)
+   *
+   * With the move assignment operator, the destination of the move should be
+   * the head of an IOBuf chain or a solitary IOBuf not part of a chain.  If
+   * the move destination is part of a chain, all other IOBufs in the chain
+   * will be deleted.
+   *
+   * (We currently don't provide a copy constructor or assignment operator.
+   * The main reason is because it is not clear these operations should copy
+   * the entire chain or just the single IOBuf.)
+   */
+  IOBuf(IOBuf&& other) noexcept;
+  IOBuf& operator=(IOBuf&& other) noexcept;
+
+ private:
+  enum FlagsEnum : uintptr_t {
+    // Adding any more flags would not work on 32-bit architectures,
+    // as these flags are stashed in the least significant 2 bits of a
+    // max-align-aligned pointer.
+    kFlagFreeSharedInfo = 0x1,
+    kFlagMaybeShared = 0x2,
+    kFlagMask = kFlagFreeSharedInfo | kFlagMaybeShared
+  };
+
+  struct SharedInfo {
+    SharedInfo();
+    SharedInfo(FreeFunction fn, void* arg);
+
+    // A pointer to a function to call to free the buffer when the refcount
+    // hits 0.  If this is null, free() will be used instead.
+    FreeFunction freeFn;
+    void* userData;
+    std::atomic<uint32_t> refcount;
+  };
+  // Helper structs for use by operator new and delete
+  struct HeapPrefix;
+  struct HeapStorage;
+  struct HeapFullStorage;
+
+  // Forbidden copy constructor and assignment opererator
+  IOBuf(IOBuf const &);
+  IOBuf& operator=(IOBuf const &);
+
+  /**
+   * Create a new IOBuf pointing to an external buffer.
+   *
+   * The caller is responsible for holding a reference count for this new
+   * IOBuf.  The IOBuf constructor does not automatically increment the
+   * reference count.
+   */
+  struct InternalConstructor {};  // avoid conflicts
+  IOBuf(InternalConstructor, uintptr_t flagsAndSharedInfo,
+        uint8_t* buf, uint64_t capacity,
+        uint8_t* data, uint64_t length);
+
+  void unshareOneSlow();
+  void unshareChained();
+  void coalesceSlow();
+  void coalesceSlow(size_t maxLength);
+  // newLength must be the entire length of the buffers between this and
+  // end (no truncation)
+  void coalesceAndReallocate(
+      size_t newHeadroom,
+      size_t newLength,
+      IOBuf* end,
+      size_t newTailroom);
+  void coalesceAndReallocate(size_t newLength, IOBuf* end) {
+    coalesceAndReallocate(headroom(), newLength, end, end->prev_->tailroom());
+  }
+  void decrementRefcount();
+  void reserveSlow(uint64_t minHeadroom, uint64_t minTailroom);
+  void freeExtBuffer();
+
+  static size_t goodExtBufferSize(uint64_t minCapacity);
+  static void initExtBuffer(uint8_t* buf, size_t mallocSize,
+                            SharedInfo** infoReturn,
+                            uint64_t* capacityReturn);
+  static void allocExtBuffer(uint64_t minCapacity,
+                             uint8_t** bufReturn,
+                             SharedInfo** infoReturn,
+                             uint64_t* capacityReturn);
+  static void releaseStorage(HeapStorage* storage, uint16_t freeFlags);
+  static void freeInternalBuf(void* buf, void* userData);
+
+  /*
+   * Member variables
+   */
+
+  /*
+   * Links to the next and the previous IOBuf in this chain.
+   *
+   * The chain is circularly linked (the last element in the chain points back
+   * at the head), and next_ and prev_ can never be null.  If this IOBuf is the
+   * only element in the chain, next_ and prev_ will both point to this.
+   */
+  IOBuf* next_{this};
+  IOBuf* prev_{this};
+
+  /*
+   * A pointer to the start of the data referenced by this IOBuf, and the
+   * length of the data.
+   *
+   * This may refer to any subsection of the actual buffer capacity.
+   */
+  uint8_t* data_{nullptr};
+  uint8_t* buf_{nullptr};
+  uint64_t length_{0};
+  uint64_t capacity_{0};
+
+  // Pack flags in least significant 2 bits, sharedInfo in the rest
+  mutable uintptr_t flagsAndSharedInfo_{0};
+
+  static inline uintptr_t packFlagsAndSharedInfo(uintptr_t flags,
+                                                 SharedInfo* info) {
+    uintptr_t uinfo = reinterpret_cast<uintptr_t>(info);
+    DCHECK_EQ(flags & ~kFlagMask, 0);
+    DCHECK_EQ(uinfo & kFlagMask, 0);
+    return flags | uinfo;
+  }
+
+  inline SharedInfo* sharedInfo() const {
+    return reinterpret_cast<SharedInfo*>(flagsAndSharedInfo_ & ~kFlagMask);
+  }
+
+  inline void setSharedInfo(SharedInfo* info) {
+    uintptr_t uinfo = reinterpret_cast<uintptr_t>(info);
+    DCHECK_EQ(uinfo & kFlagMask, 0);
+    flagsAndSharedInfo_ = (flagsAndSharedInfo_ & kFlagMask) | uinfo;
+  }
+
+  inline uintptr_t flags() const {
+    return flagsAndSharedInfo_ & kFlagMask;
+  }
+
+  // flags_ are changed from const methods
+  inline void setFlags(uintptr_t flags) const {
+    DCHECK_EQ(flags & ~kFlagMask, 0);
+    flagsAndSharedInfo_ |= flags;
+  }
+
+  inline void clearFlags(uintptr_t flags) const {
+    DCHECK_EQ(flags & ~kFlagMask, 0);
+    flagsAndSharedInfo_ &= ~flags;
+  }
+
+  inline void setFlagsAndSharedInfo(uintptr_t flags, SharedInfo* info) {
+    flagsAndSharedInfo_ = packFlagsAndSharedInfo(flags, info);
+  }
+
+  struct DeleterBase {
+    virtual ~DeleterBase() { }
+    virtual void dispose(void* p) = 0;
+  };
+
+  template <class UniquePtr>
+  struct UniquePtrDeleter : public DeleterBase {
+    typedef typename UniquePtr::pointer Pointer;
+    typedef typename UniquePtr::deleter_type Deleter;
+
+    explicit UniquePtrDeleter(Deleter deleter) : deleter_(std::move(deleter)){ }
+    void dispose(void* p) {
+      try {
+        deleter_(static_cast<Pointer>(p));
+        delete this;
+      } catch (...) {
+        abort();
+      }
+    }
+
+   private:
+    Deleter deleter_;
+  };
+
+  static void freeUniquePtrBuffer(void* ptr, void* userData) {
+    static_cast<DeleterBase*>(userData)->dispose(ptr);
+  }
+};
+
+template <class UniquePtr>
+typename std::enable_if<detail::IsUniquePtrToSL<UniquePtr>::value,
+                        std::unique_ptr<IOBuf>>::type
+IOBuf::takeOwnership(UniquePtr&& buf, size_t count) {
+  size_t size = count * sizeof(typename UniquePtr::element_type);
+  auto deleter = new UniquePtrDeleter<UniquePtr>(buf.get_deleter());
+  return takeOwnership(buf.release(),
+                       size,
+                       &IOBuf::freeUniquePtrBuffer,
+                       deleter);
+}
+
+inline std::unique_ptr<IOBuf> IOBuf::copyBuffer(
+    const void* data, uint64_t size, uint64_t headroom,
+    uint64_t minTailroom) {
+  uint64_t capacity = headroom + size + minTailroom;
+  std::unique_ptr<IOBuf> buf = create(capacity);
+  buf->advance(headroom);
+  memcpy(buf->writableData(), data, size);
+  buf->append(size);
+  return buf;
+}
+
+inline std::unique_ptr<IOBuf> IOBuf::copyBuffer(const std::string& buf,
+                                                uint64_t headroom,
+                                                uint64_t minTailroom) {
+  return copyBuffer(buf.data(), buf.size(), headroom, minTailroom);
+}
+
+inline std::unique_ptr<IOBuf> IOBuf::maybeCopyBuffer(const std::string& buf,
+                                                     uint64_t headroom,
+                                                     uint64_t minTailroom) {
+  if (buf.empty()) {
+    return nullptr;
+  }
+  return copyBuffer(buf.data(), buf.size(), headroom, minTailroom);
+}
+
+class IOBuf::Iterator : public boost::iterator_facade<
+    IOBuf::Iterator,  // Derived
+    const ByteRange,  // Value
+    boost::forward_traversal_tag  // Category or traversal
+  > {
+  friend class boost::iterator_core_access;
+ public:
+  // Note that IOBufs are stored as a circular list without a guard node,
+  // so pos == end is ambiguous (it may mean "begin" or "end").  To solve
+  // the ambiguity (at the cost of one extra comparison in the "increment"
+  // code path), we define end iterators as having pos_ == end_ == nullptr
+  // and we only allow forward iteration.
+  explicit Iterator(const IOBuf* pos, const IOBuf* end)
+    : pos_(pos),
+      end_(end) {
+    // Sadly, we must return by const reference, not by value.
+    if (pos_) {
+      setVal();
+    }
+  }
+
+ private:
+  void setVal() {
+    val_ = ByteRange(pos_->data(), pos_->tail());
+  }
+
+  void adjustForEnd() {
+    if (pos_ == end_) {
+      pos_ = end_ = nullptr;
+      val_ = ByteRange();
+    } else {
+      setVal();
+    }
+  }
+
+  const ByteRange& dereference() const {
+    return val_;
+  }
+
+  bool equal(const Iterator& other) const {
+    // We must compare end_ in addition to pos_, because forward traversal
+    // requires that if two iterators are equal (a == b) and dereferenceable,
+    // then ++a == ++b.
+    return pos_ == other.pos_ && end_ == other.end_;
+  }
+
+  void increment() {
+    pos_ = pos_->next();
+    adjustForEnd();
+  }
+
+  const IOBuf* pos_;
+  const IOBuf* end_;
+  ByteRange val_;
+};
+
+inline IOBuf::Iterator IOBuf::begin() const { return cbegin(); }
+inline IOBuf::Iterator IOBuf::end() const { return cend(); }
+
+} // folly
+
+#pragma GCC diagnostic pop
+
+#endif // FOLLY_IO_IOBUF_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/IOBufQueue.cpp
@@ -0,0 +1,276 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/io/IOBufQueue.h"
+
+#include <string.h>
+
+#include <stdexcept>
+
+using std::make_pair;
+using std::pair;
+using std::unique_ptr;
+
+namespace {
+
+using folly::IOBuf;
+
+const size_t MIN_ALLOC_SIZE = 2000;
+const size_t MAX_ALLOC_SIZE = 8000;
+const size_t MAX_PACK_COPY = 4096;
+
+/**
+ * Convenience function to append chain src to chain dst.
+ */
+void
+appendToChain(unique_ptr<IOBuf>& dst, unique_ptr<IOBuf>&& src, bool pack) {
+  if (dst == nullptr) {
+    dst = std::move(src);
+  } else {
+    IOBuf* tail = dst->prev();
+    if (pack) {
+      // Copy up to MAX_PACK_COPY bytes if we can free buffers; this helps
+      // reduce wastage (the tail's tailroom and the head's headroom) when
+      // joining two IOBufQueues together.
+      size_t copyRemaining = MAX_PACK_COPY;
+      uint64_t n;
+      while (src &&
+             (n = src->length()) < copyRemaining &&
+             n < tail->tailroom()) {
+        memcpy(tail->writableTail(), src->data(), n);
+        tail->append(n);
+        copyRemaining -= n;
+        src = src->pop();
+      }
+    }
+    if (src) {
+      tail->appendChain(std::move(src));
+    }
+  }
+}
+
+} // anonymous namespace
+
+namespace folly {
+
+IOBufQueue::IOBufQueue(const Options& options)
+  : options_(options),
+    chainLength_(0) {
+}
+
+IOBufQueue::IOBufQueue(IOBufQueue&& other)
+  : options_(other.options_),
+    chainLength_(other.chainLength_),
+    head_(std::move(other.head_)) {
+  other.chainLength_ = 0;
+}
+
+IOBufQueue& IOBufQueue::operator=(IOBufQueue&& other) {
+  if (&other != this) {
+    options_ = other.options_;
+    chainLength_ = other.chainLength_;
+    head_ = std::move(other.head_);
+    other.chainLength_ = 0;
+  }
+  return *this;
+}
+
+std::pair<void*, uint64_t>
+IOBufQueue::headroom() {
+  if (head_) {
+    return std::make_pair(head_->writableBuffer(), head_->headroom());
+  } else {
+    return std::make_pair(nullptr, 0);
+  }
+}
+
+void
+IOBufQueue::markPrepended(uint64_t n) {
+  if (n == 0) {
+    return;
+  }
+  assert(head_);
+  head_->prepend(n);
+  chainLength_ += n;
+}
+
+void
+IOBufQueue::prepend(const void* buf, uint64_t n) {
+  auto p = headroom();
+  if (n > p.second) {
+    throw std::overflow_error("Not enough room to prepend");
+  }
+  memcpy(static_cast<char*>(p.first) + p.second - n, buf, n);
+  markPrepended(n);
+}
+
+void
+IOBufQueue::append(unique_ptr<IOBuf>&& buf, bool pack) {
+  if (!buf) {
+    return;
+  }
+  if (options_.cacheChainLength) {
+    chainLength_ += buf->computeChainDataLength();
+  }
+  appendToChain(head_, std::move(buf), pack);
+}
+
+void
+IOBufQueue::append(IOBufQueue& other, bool pack) {
+  if (!other.head_) {
+    return;
+  }
+  if (options_.cacheChainLength) {
+    if (other.options_.cacheChainLength) {
+      chainLength_ += other.chainLength_;
+    } else {
+      chainLength_ += other.head_->computeChainDataLength();
+    }
+  }
+  appendToChain(head_, std::move(other.head_), pack);
+  other.chainLength_ = 0;
+}
+
+void
+IOBufQueue::append(const void* buf, size_t len) {
+  auto src = static_cast<const uint8_t*>(buf);
+  while (len != 0) {
+    if ((head_ == nullptr) || head_->prev()->isSharedOne() ||
+        (head_->prev()->tailroom() == 0)) {
+      appendToChain(head_, std::move(
+          IOBuf::create(std::max(MIN_ALLOC_SIZE,
+              std::min(len, MAX_ALLOC_SIZE)))),
+          false);
+    }
+    IOBuf* last = head_->prev();
+    uint64_t copyLen = std::min(len, (size_t)last->tailroom());
+    memcpy(last->writableTail(), src, copyLen);
+    src += copyLen;
+    last->append(copyLen);
+    chainLength_ += copyLen;
+    len -= copyLen;
+  }
+}
+
+void
+IOBufQueue::wrapBuffer(const void* buf, size_t len, uint64_t blockSize) {
+  auto src = static_cast<const uint8_t*>(buf);
+  while (len != 0) {
+    size_t n = std::min(len, size_t(blockSize));
+    append(IOBuf::wrapBuffer(src, n));
+    src += n;
+    len -= n;
+  }
+}
+
+pair<void*,uint64_t>
+IOBufQueue::preallocateSlow(uint64_t min, uint64_t newAllocationSize,
+                            uint64_t max) {
+  // Allocate a new buffer of the requested max size.
+  unique_ptr<IOBuf> newBuf(IOBuf::create(std::max(min, newAllocationSize)));
+  appendToChain(head_, std::move(newBuf), false);
+  IOBuf* last = head_->prev();
+  return make_pair(last->writableTail(),
+                   std::min(max, last->tailroom()));
+}
+
+unique_ptr<IOBuf>
+IOBufQueue::split(size_t n) {
+  unique_ptr<IOBuf> result;
+  while (n != 0) {
+    if (head_ == nullptr) {
+      throw std::underflow_error(
+          "Attempt to remove more bytes than are present in IOBufQueue");
+    } else if (head_->length() <= n) {
+      n -= head_->length();
+      chainLength_ -= head_->length();
+      unique_ptr<IOBuf> remainder = head_->pop();
+      appendToChain(result, std::move(head_), false);
+      head_ = std::move(remainder);
+    } else {
+      unique_ptr<IOBuf> clone = head_->cloneOne();
+      clone->trimEnd(clone->length() - n);
+      appendToChain(result, std::move(clone), false);
+      head_->trimStart(n);
+      chainLength_ -= n;
+      break;
+    }
+  }
+  return std::move(result);
+}
+
+void IOBufQueue::trimStart(size_t amount) {
+  while (amount > 0) {
+    if (!head_) {
+      throw std::underflow_error(
+        "Attempt to trim more bytes than are present in IOBufQueue");
+    }
+    if (head_->length() > amount) {
+      head_->trimStart(amount);
+      chainLength_ -= amount;
+      break;
+    }
+    amount -= head_->length();
+    chainLength_ -= head_->length();
+    head_ = head_->pop();
+  }
+}
+
+void IOBufQueue::trimEnd(size_t amount) {
+  while (amount > 0) {
+    if (!head_) {
+      throw std::underflow_error(
+        "Attempt to trim more bytes than are present in IOBufQueue");
+    }
+    if (head_->prev()->length() > amount) {
+      head_->prev()->trimEnd(amount);
+      chainLength_ -= amount;
+      break;
+    }
+    amount -= head_->prev()->length();
+    chainLength_ -= head_->prev()->length();
+
+    if (head_->isChained()) {
+      head_->prev()->unlink();
+    } else {
+      head_.reset();
+    }
+  }
+}
+
+std::unique_ptr<folly::IOBuf> IOBufQueue::pop_front() {
+  if (!head_) {
+    return nullptr;
+  }
+  chainLength_ -= head_->length();
+  std::unique_ptr<folly::IOBuf> retBuf = std::move(head_);
+  head_ = retBuf->pop();
+  return retBuf;
+}
+
+void IOBufQueue::clear() {
+  if (!head_) {
+    return;
+  }
+  IOBuf* buf = head_.get();
+  do {
+    buf->clear();
+    buf = buf->next();
+  } while (buf != head_.get());
+  chainLength_ = 0;
+}
+
+} // folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/IOBufQueue.h
@@ -0,0 +1,294 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_IO_IOBUF_QUEUE_H
+#define FOLLY_IO_IOBUF_QUEUE_H
+
+#include "folly/io/IOBuf.h"
+
+#include <stdexcept>
+#include <string>
+
+namespace folly {
+
+/**
+ * An IOBufQueue encapsulates a chain of IOBufs and provides
+ * convenience functions to append data to the back of the chain
+ * and remove data from the front.
+ *
+ * You may also prepend data into the headroom of the first buffer in the
+ * chain, if any.
+ */
+class IOBufQueue {
+ public:
+  struct Options {
+    Options() : cacheChainLength(false) { }
+    bool cacheChainLength;
+  };
+
+  /**
+   * Commonly used Options, currently the only possible value other than
+   * the default.
+   */
+  static Options cacheChainLength() {
+    Options options;
+    options.cacheChainLength = true;
+    return options;
+  }
+
+  explicit IOBufQueue(const Options& options = Options());
+
+  /**
+   * Return a space to prepend bytes and the amount of headroom available.
+   */
+  std::pair<void*, uint64_t> headroom();
+
+  /**
+   * Indicate that n bytes from the headroom have been used.
+   */
+  void markPrepended(uint64_t n);
+
+  /**
+   * Prepend an existing range; throws std::overflow_error if not enough
+   * room.
+   */
+  void prepend(const void* buf, uint64_t n);
+
+  /**
+   * Add a buffer or buffer chain to the end of this queue. The
+   * queue takes ownership of buf.
+   *
+   * If pack is true, we try to reduce wastage at the end of this queue
+   * by copying some data from the first buffers in the buf chain (and
+   * releasing the buffers), if possible.  If pack is false, we leave
+   * the chain topology unchanged.
+   */
+  void append(std::unique_ptr<folly::IOBuf>&& buf,
+              bool pack=false);
+
+  /**
+   * Add a queue to the end of this queue. The queue takes ownership of
+   * all buffers from the other queue.
+   */
+  void append(IOBufQueue& other, bool pack=false);
+  void append(IOBufQueue&& other, bool pack=false) {
+    append(other, pack);  // call lvalue reference overload, above
+  }
+
+  /**
+   * Copy len bytes, starting at buf, to the end of this queue.
+   * The caller retains ownership of the source data.
+   */
+  void append(const void* buf, size_t len);
+
+  /**
+   * Copy a string to the end of this queue.
+   * The caller retains ownership of the source data.
+   */
+  void append(const std::string& buf) {
+    append(buf.data(), buf.length());
+  }
+
+  /**
+   * Append a chain of IOBuf objects that point to consecutive regions
+   * within buf.
+   *
+   * Just like IOBuf::wrapBuffer, this should only be used when the caller
+   * knows ahead of time and can ensure that all IOBuf objects that will point
+   * to this buffer will be destroyed before the buffer itself is destroyed;
+   * all other caveats from wrapBuffer also apply.
+   *
+   * Every buffer except for the last will wrap exactly blockSize bytes.
+   * Importantly, this method may be used to wrap buffers larger than 4GB.
+   */
+  void wrapBuffer(const void* buf, size_t len,
+                  uint64_t blockSize=(1U << 31));  // default block size: 2GB
+
+  /**
+   * Obtain a writable block of contiguous bytes at the end of this
+   * queue, allocating more space if necessary.  The amount of space
+   * reserved will be at least min.  If min contiguous space is not
+   * available at the end of the queue, and IOBuf with size newAllocationSize
+   * is appended to the chain and returned.  The actual available space
+   * may be larger than newAllocationSize, but will be truncated to max,
+   * if specified.
+   *
+   * If the caller subsequently writes anything into the returned space,
+   * it must call the postallocate() method.
+   *
+   * @return The starting address of the block and the length in bytes.
+   *
+   * @note The point of the preallocate()/postallocate() mechanism is
+   *       to support I/O APIs such as Thrift's TAsyncSocket::ReadCallback
+   *       that request a buffer from the application and then, in a later
+   *       callback, tell the application how much of the buffer they've
+   *       filled with data.
+   */
+  std::pair<void*,uint64_t> preallocate(
+    uint64_t min, uint64_t newAllocationSize,
+    uint64_t max = std::numeric_limits<uint64_t>::max()) {
+    auto buf = tailBuf();
+    if (LIKELY(buf && buf->tailroom() >= min)) {
+      return std::make_pair(buf->writableTail(),
+                            std::min(max, buf->tailroom()));
+    }
+
+    return preallocateSlow(min, newAllocationSize, max);
+  }
+
+  /**
+   * Tell the queue that the caller has written data into the first n
+   * bytes provided by the previous preallocate() call.
+   *
+   * @note n should be less than or equal to the size returned by
+   *       preallocate().  If n is zero, the caller may skip the call
+   *       to postallocate().  If n is nonzero, the caller must not
+   *       invoke any other non-const methods on this IOBufQueue between
+   *       the call to preallocate and the call to postallocate().
+   */
+  void postallocate(uint64_t n) {
+    head_->prev()->append(n);
+    chainLength_ += n;
+  }
+
+  /**
+   * Obtain a writable block of n contiguous bytes, allocating more space
+   * if necessary, and mark it as used.  The caller can fill it later.
+   */
+  void* allocate(uint64_t n) {
+    void* p = preallocate(n, n).first;
+    postallocate(n);
+    return p;
+  }
+
+  void* writableTail() const {
+    auto buf = tailBuf();
+    return buf ? buf->writableTail() : nullptr;
+  }
+
+  size_t tailroom() const {
+    auto buf = tailBuf();
+    return buf ? buf->tailroom() : 0;
+  }
+
+  /**
+   * Split off the first n bytes of the queue into a separate IOBuf chain,
+   * and transfer ownership of the new chain to the caller.  The IOBufQueue
+   * retains ownership of everything after the split point.
+   *
+   * @warning If the split point lies in the middle of some IOBuf within
+   *          the chain, this function may, as an implementation detail,
+   *          clone that IOBuf.
+   *
+   * @throws std::underflow_error if n exceeds the number of bytes
+   *         in the queue.
+   */
+  std::unique_ptr<folly::IOBuf> split(size_t n);
+
+  /**
+   * Similar to IOBuf::trimStart, but works on the whole queue.  Will
+   * pop off buffers that have been completely trimmed.
+   */
+  void trimStart(size_t amount);
+
+  /**
+   * Similar to IOBuf::trimEnd, but works on the whole queue.  Will
+   * pop off buffers that have been completely trimmed.
+   */
+  void trimEnd(size_t amount);
+
+  /**
+   * Transfer ownership of the queue's entire IOBuf chain to the caller.
+   */
+  std::unique_ptr<folly::IOBuf> move() {
+    chainLength_ = 0;
+    return std::move(head_);
+  }
+
+  /**
+   * Access
+   */
+  const folly::IOBuf* front() const {
+    return head_.get();
+  }
+
+  /**
+   * returns the first IOBuf in the chain and removes it from the chain
+   *
+   * @return first IOBuf in the chain or nullptr if none.
+   */
+  std::unique_ptr<folly::IOBuf> pop_front();
+
+  /**
+   * Total chain length, only valid if cacheLength was specified in the
+   * constructor.
+   */
+  size_t chainLength() const {
+    if (UNLIKELY(!options_.cacheChainLength)) {
+      throw std::invalid_argument("IOBufQueue: chain length not cached");
+    }
+    return chainLength_;
+  }
+
+  /**
+   * Returns true iff the IOBuf chain length is 0.
+   */
+  bool empty() const {
+    return !head_ || head_->empty();
+  }
+
+  const Options& options() const {
+    return options_;
+  }
+
+  /**
+   * Clear the queue.  Note that this does not release the buffers, it
+   * just sets their length to zero; useful if you want to reuse the
+   * same queue without reallocating.
+   */
+  void clear();
+
+  /** Movable */
+  IOBufQueue(IOBufQueue&&);
+  IOBufQueue& operator=(IOBufQueue&&);
+
+ private:
+  IOBuf* tailBuf() const {
+    if (UNLIKELY(!head_)) return nullptr;
+    IOBuf* buf = head_->prev();
+    return LIKELY(!buf->isSharedOne()) ? buf : nullptr;
+  }
+  std::pair<void*,uint64_t> preallocateSlow(
+    uint64_t min, uint64_t newAllocationSize, uint64_t max);
+
+  static const size_t kChainLengthNotCached = (size_t)-1;
+  /** Not copyable */
+  IOBufQueue(const IOBufQueue&) = delete;
+  IOBufQueue& operator=(const IOBufQueue&) = delete;
+
+  Options options_;
+
+  // NOTE that chainLength_ is still updated even if !options_.cacheChainLength
+  // because doing it unchecked in postallocate() is faster (no (mis)predicted
+  // branch)
+  size_t chainLength_;
+  /** Everything that has been appended but not yet discarded or moved out */
+  std::unique_ptr<folly::IOBuf> head_;
+};
+
+} // folly
+
+#endif // FOLLY_IO_IOBUF_QUEUE_H
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/RecordIO.cpp
@@ -0,0 +1,233 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/io/RecordIO.h"
+
+#include <sys/types.h>
+#include <unistd.h>
+
+#include "folly/Exception.h"
+#include "folly/FileUtil.h"
+#include "folly/Memory.h"
+#include "folly/Portability.h"
+#include "folly/ScopeGuard.h"
+#include "folly/String.h"
+
+namespace folly {
+
+using namespace recordio_helpers;
+
+RecordIOWriter::RecordIOWriter(File file, uint32_t fileId)
+  : file_(std::move(file)),
+    fileId_(fileId),
+    writeLock_(file_, std::defer_lock),
+    filePos_(0) {
+  if (!writeLock_.try_lock()) {
+    throw std::runtime_error("RecordIOWriter: file locked by another process");
+  }
+
+  struct stat st;
+  checkUnixError(fstat(file_.fd(), &st), "fstat() failed");
+
+  filePos_ = st.st_size;
+}
+
+void RecordIOWriter::write(std::unique_ptr<IOBuf> buf) {
+  size_t totalLength = prependHeader(buf, fileId_);
+  if (totalLength == 0) {
+    return;  // nothing to do
+  }
+
+  DCHECK_EQ(buf->computeChainDataLength(), totalLength);
+
+  // We're going to write.  Reserve space for ourselves.
+  off_t pos = filePos_.fetch_add(totalLength);
+
+#if FOLLY_HAVE_PWRITEV
+  auto iov = buf->getIov();
+  ssize_t bytes = pwritevFull(file_.fd(), iov.data(), iov.size(), pos);
+#else
+  buf->unshare();
+  buf->coalesce();
+  ssize_t bytes = pwriteFull(file_.fd(), buf->data(), buf->length(), pos);
+#endif
+
+  checkUnixError(bytes, "pwrite() failed");
+  DCHECK_EQ(bytes, totalLength);
+}
+
+RecordIOReader::RecordIOReader(File file, uint32_t fileId)
+  : map_(std::move(file)),
+    fileId_(fileId) {
+}
+
+RecordIOReader::Iterator::Iterator(ByteRange range, uint32_t fileId, off_t pos)
+  : range_(range),
+    fileId_(fileId),
+    recordAndPos_(ByteRange(), 0) {
+  if (pos >= range_.size()) {
+    recordAndPos_.second = off_t(-1);
+    range_.clear();
+  } else {
+    recordAndPos_.second = pos;
+    range_.advance(pos);
+    advanceToValid();
+  }
+}
+
+void RecordIOReader::Iterator::advanceToValid() {
+  ByteRange record = findRecord(range_, fileId_).record;
+  if (record.empty()) {
+    recordAndPos_ = std::make_pair(ByteRange(), off_t(-1));
+    range_.clear();  // at end
+  } else {
+    size_t skipped = record.begin() - range_.begin();
+    DCHECK_GE(skipped, headerSize());
+    skipped -= headerSize();
+    range_.advance(skipped);
+    recordAndPos_.first = record;
+    recordAndPos_.second += skipped;
+  }
+}
+
+namespace recordio_helpers {
+
+using namespace detail;
+
+namespace {
+
+constexpr uint32_t kHashSeed = 0xdeadbeef;  // for mcurtiss
+
+uint32_t headerHash(const Header& header) {
+  return hash::SpookyHashV2::Hash32(&header, offsetof(Header, headerHash),
+                                    kHashSeed);
+}
+
+std::pair<size_t, uint64_t> dataLengthAndHash(const IOBuf* buf) {
+  size_t len = 0;
+  hash::SpookyHashV2 hasher;
+  hasher.Init(kHashSeed, kHashSeed);
+  for (auto br : *buf) {
+    len += br.size();
+    hasher.Update(br.data(), br.size());
+  }
+  uint64_t hash1;
+  uint64_t hash2;
+  hasher.Final(&hash1, &hash2);
+  if (len + headerSize() >= std::numeric_limits<uint32_t>::max()) {
+    throw std::invalid_argument("Record length must fit in 32 bits");
+  }
+  return std::make_pair(len, hash1);
+}
+
+uint64_t dataHash(ByteRange range) {
+  return hash::SpookyHashV2::Hash64(range.data(), range.size(), kHashSeed);
+}
+
+}  // namespace
+
+size_t prependHeader(std::unique_ptr<IOBuf>& buf, uint32_t fileId) {
+  if (fileId == 0) {
+    throw std::invalid_argument("invalid file id");
+  }
+  auto lengthAndHash = dataLengthAndHash(buf.get());
+  if (lengthAndHash.first == 0) {
+    return 0;  // empty, nothing to do, no zero-length records
+  }
+
+  // Prepend to the first buffer in the chain if we have room, otherwise
+  // prepend a new buffer.
+  if (buf->headroom() >= headerSize()) {
+    buf->unshareOne();
+    buf->prepend(headerSize());
+  } else {
+    auto b = IOBuf::create(headerSize());
+    b->append(headerSize());
+    b->appendChain(std::move(buf));
+    buf = std::move(b);
+  }
+  detail::Header* header =
+    reinterpret_cast<detail::Header*>(buf->writableData());
+  memset(header, 0, sizeof(Header));
+  header->magic = detail::Header::kMagic;
+  header->fileId = fileId;
+  header->dataLength = lengthAndHash.first;
+  header->dataHash = lengthAndHash.second;
+  header->headerHash = headerHash(*header);
+
+  return lengthAndHash.first + headerSize();
+}
+
+RecordInfo validateRecord(ByteRange range, uint32_t fileId) {
+  if (range.size() <= headerSize()) {  // records may not be empty
+    return {0};
+  }
+  const Header* header = reinterpret_cast<const Header*>(range.begin());
+  range.advance(sizeof(Header));
+  if (header->magic != Header::kMagic ||
+      header->version != 0 ||
+      header->hashFunction != 0 ||
+      header->flags != 0 ||
+      (fileId != 0 && header->fileId != fileId) ||
+      header->dataLength > range.size()) {
+    return {0};
+  }
+  if (headerHash(*header) != header->headerHash) {
+    return {0};
+  }
+  range.reset(range.begin(), header->dataLength);
+  if (dataHash(range) != header->dataHash) {
+    return {0};
+  }
+  return {header->fileId, range};
+}
+
+RecordInfo findRecord(ByteRange searchRange,
+                      ByteRange wholeRange,
+                      uint32_t fileId) {
+  static const uint32_t magic = Header::kMagic;
+  static const ByteRange magicRange(reinterpret_cast<const uint8_t*>(&magic),
+                                    sizeof(magic));
+
+  DCHECK_GE(searchRange.begin(), wholeRange.begin());
+  DCHECK_LE(searchRange.end(), wholeRange.end());
+
+  const uint8_t* start = searchRange.begin();
+  const uint8_t* end = std::min(searchRange.end(),
+                                wholeRange.end() - sizeof(Header));
+  // end-1: the last place where a Header could start
+  while (start < end) {
+    auto p = ByteRange(start, end + sizeof(magic)).find(magicRange);
+    if (p == ByteRange::npos) {
+      break;
+    }
+
+    start += p;
+    auto r = validateRecord(ByteRange(start, wholeRange.end()), fileId);
+    if (!r.record.empty()) {
+      return r;
+    }
+
+    // No repeated prefix in magic, so we can do better than start++
+    start += sizeof(magic);
+  }
+
+  return {0};
+}
+
+}  // namespace
+
+}  // namespaces
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/RecordIO.h
@@ -0,0 +1,181 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * RecordIO: self-synchronizing stream of variable length records
+ *
+ * RecordIO gives you the ability to write a stream of variable length records
+ * and read them later even in the face of data corruption -- randomly inserted
+ * or deleted chunks of the file, or modified data.  When reading, you may lose
+ * corrupted records, but the stream will resynchronize automatically.
+ */
+#ifndef FOLLY_IO_RECORDIO_H_
+#define FOLLY_IO_RECORDIO_H_
+
+#include <atomic>
+#include <memory>
+#include <mutex>
+
+#include "folly/File.h"
+#include "folly/Range.h"
+#include "folly/MemoryMapping.h"
+#include "folly/io/IOBuf.h"
+
+namespace folly {
+
+/**
+ * Class to write a stream of RecordIO records to a file.
+ *
+ * RecordIOWriter is thread-safe
+ */
+class RecordIOWriter {
+ public:
+  /**
+   * Create a RecordIOWriter around a file; will append to the end of
+   * file if it exists.
+   *
+   * Each file must have a non-zero file id, which is embedded in all
+   * record headers.  Readers will only return records with the requested
+   * file id (or, if the reader is created with fileId=0 in the constructor,
+   * the reader will return all records).  File ids are only used to allow
+   * resynchronization if you store RecordIO records (with headers) inside
+   * other RecordIO records (for example, if a record consists of a fragment
+   * from another RecordIO file).  If you're not planning to do that,
+   * the defaults are fine.
+   */
+  explicit RecordIOWriter(File file, uint32_t fileId = 1);
+
+  /**
+   * Write a record.  We will use at most headerSize() bytes of headroom,
+   * you might want to arrange that before copying your data into it.
+   */
+  void write(std::unique_ptr<IOBuf> buf);
+
+  /**
+   * Return the position in the file where the next byte will be written.
+   * Conservative, as stuff can be written at any time from another thread.
+   */
+  off_t filePos() const { return filePos_; }
+
+ private:
+  File file_;
+  uint32_t fileId_;
+  std::unique_lock<File> writeLock_;
+  std::atomic<off_t> filePos_;
+};
+
+/**
+ * Class to read from a RecordIO file.  Will skip invalid records.
+ */
+class RecordIOReader {
+ public:
+  class Iterator;
+
+  /**
+   * RecordIOReader is iterable, returning pairs of ByteRange (record content)
+   * and position in file where the record (including header) begins.
+   * Note that the position includes the header, that is, it can be passed back
+   * to seek().
+   */
+  typedef Iterator iterator;
+  typedef Iterator const_iterator;
+  typedef std::pair<ByteRange, off_t> value_type;
+  typedef value_type& reference;
+  typedef const value_type& const_reference;
+
+  /**
+   * A record reader with a fileId of 0 will return all records.
+   * A record reader with a non-zero fileId will only return records where
+   * the fileId matches.
+   */
+  explicit RecordIOReader(File file, uint32_t fileId = 0);
+
+  Iterator cbegin() const;
+  Iterator begin() const;
+  Iterator cend() const;
+  Iterator end() const;
+
+  /**
+   * Create an iterator to the first valid record after pos.
+   */
+  Iterator seek(off_t pos) const;
+
+ private:
+  MemoryMapping map_;
+  uint32_t fileId_;
+};
+
+namespace recordio_helpers {
+
+// We're exposing the guts of the RecordIO implementation for two reasons:
+// 1. It makes unit testing easier, and
+// 2. It allows you to build different RecordIO readers / writers that use
+// different storage systems underneath (not standard files)
+
+/**
+ * Header size.
+ */
+constexpr size_t headerSize();  // defined in RecordIO-inl.h
+
+/**
+ * Write a header in the buffer.  We will prepend the header to the front
+ * of the chain.  Do not write the buffer if empty (we don't allow empty
+ * records).  Returns the total length, including header (0 if empty)
+ * (same as buf->computeChainDataLength(), but likely faster)
+ *
+ * The fileId should be unique per stream and allows you to have RecordIO
+ * headers stored inside the data (for example, have an entire RecordIO
+ * file stored as a record inside another RecordIO file).  The fileId may
+ * not be 0.
+ */
+size_t prependHeader(std::unique_ptr<IOBuf>& buf, uint32_t fileId = 1);
+
+/**
+ * Search for the first valid record that begins in searchRange (which must be
+ * a subrange of wholeRange).  Returns the record data (not the header) if
+ * found, ByteRange() otherwise.
+ *
+ * The fileId may be 0, in which case we'll return the first valid record for
+ * *any* fileId, or non-zero, in which case we'll only look for records with
+ * the requested fileId.
+ */
+struct RecordInfo {
+  uint32_t fileId;
+  ByteRange record;
+};
+RecordInfo findRecord(ByteRange searchRange,
+                      ByteRange wholeRange,
+                      uint32_t fileId);
+
+/**
+ * Search for the first valid record in range.
+ */
+RecordInfo findRecord(ByteRange range, uint32_t fileId);
+
+/**
+ * Check if there is a valid record at the beginning of range.  Returns the
+ * record data (not the header) if the record is valid, ByteRange() otherwise.
+ */
+RecordInfo validateRecord(ByteRange range, uint32_t fileId);
+
+}  // namespace recordio_helpers
+
+}  // namespaces
+
+#include "folly/io/RecordIO-inl.h"
+
+#endif /* FOLLY_IO_RECORDIO_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/RecordIO-inl.h
@@ -0,0 +1,94 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_IO_RECORDIO_H_
+#error This file may only be included from folly/io/RecordIO.h
+#endif
+
+#include <boost/iterator/iterator_facade.hpp>
+
+#include "folly/SpookyHashV2.h"
+
+namespace folly {
+
+class RecordIOReader::Iterator : public boost::iterator_facade<
+    RecordIOReader::Iterator,
+    const std::pair<ByteRange, off_t>,
+    boost::forward_traversal_tag> {
+  friend class boost::iterator_core_access;
+  friend class RecordIOReader;
+ private:
+  Iterator(ByteRange range, uint32_t fileId, off_t pos);
+
+  reference dereference() const { return recordAndPos_; }
+  bool equal(const Iterator& other) const { return range_ == other.range_; }
+  void increment() {
+    size_t skip = recordio_helpers::headerSize() + recordAndPos_.first.size();
+    recordAndPos_.second += skip;
+    range_.advance(skip);
+    advanceToValid();
+  }
+
+  void advanceToValid();
+  ByteRange range_;
+  uint32_t fileId_;
+  // stored as a pair so we can return by reference in dereference()
+  std::pair<ByteRange, off_t> recordAndPos_;
+};
+
+inline auto RecordIOReader::cbegin() const -> Iterator { return seek(0); }
+inline auto RecordIOReader::begin() const -> Iterator { return cbegin(); }
+inline auto RecordIOReader::cend() const -> Iterator { return seek(off_t(-1)); }
+inline auto RecordIOReader::end() const -> Iterator { return cend(); }
+inline auto RecordIOReader::seek(off_t pos) const -> Iterator {
+  return Iterator(map_.range(), fileId_, pos);
+}
+
+namespace recordio_helpers {
+
+namespace detail {
+
+struct Header {
+  // First 4 bytes of SHA1("zuck"), big-endian
+  // Any values will do, except that the sequence must not have a
+  // repeated prefix (that is, if we see kMagic, we know that the next
+  // occurrence must start at least 4 bytes later)
+  static constexpr uint32_t kMagic = 0xeac313a1;
+  uint32_t magic;
+  uint8_t  version;       // backwards incompatible version, currently 0
+  uint8_t  hashFunction;  // 0 = SpookyHashV2
+  uint16_t flags;         // reserved (must be 0)
+  uint32_t fileId;        // unique file ID
+  uint32_t dataLength;
+  uint64_t dataHash;
+  uint32_t headerHash;  // must be last
+} __attribute__((packed));
+
+static_assert(offsetof(Header, headerHash) + sizeof(Header::headerHash) ==
+              sizeof(Header), "invalid header layout");
+
+}  // namespace detail
+
+constexpr size_t headerSize() { return sizeof(detail::Header); }
+
+inline RecordInfo findRecord(ByteRange range, uint32_t fileId) {
+  return findRecord(range, range, fileId);
+}
+
+}  // namespace recordio_helpers
+
+}  // namespaces
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/test/CompressionTest.cpp
@@ -0,0 +1,258 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/io/Compression.h"
+
+// Yes, tr1, as that's what gtest requires
+#include <random>
+#include <thread>
+#include <tr1/tuple>
+#include <unordered_map>
+
+#include <boost/noncopyable.hpp>
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+#include "folly/Benchmark.h"
+#include "folly/Hash.h"
+#include "folly/Random.h"
+#include "folly/io/IOBufQueue.h"
+
+namespace folly { namespace io { namespace test {
+
+class DataHolder : private boost::noncopyable {
+ public:
+  uint64_t hash(size_t size) const;
+  ByteRange data(size_t size) const;
+
+ protected:
+  explicit DataHolder(size_t sizeLog2);
+  const size_t size_;
+  std::unique_ptr<uint8_t[]> data_;
+  mutable std::unordered_map<uint64_t, uint64_t> hashCache_;
+};
+
+DataHolder::DataHolder(size_t sizeLog2)
+  : size_(size_t(1) << sizeLog2),
+    data_(new uint8_t[size_]) {
+}
+
+uint64_t DataHolder::hash(size_t size) const {
+  CHECK_LE(size, size_);
+  auto p = hashCache_.find(size);
+  if (p != hashCache_.end()) {
+    return p->second;
+  }
+
+  uint64_t h = folly::hash::fnv64_buf(data_.get(), size);
+  hashCache_[size] = h;
+  return h;
+}
+
+ByteRange DataHolder::data(size_t size) const {
+  CHECK_LE(size, size_);
+  return ByteRange(data_.get(), size);
+}
+
+uint64_t hashIOBuf(const IOBuf* buf) {
+  uint64_t h = folly::hash::FNV_64_HASH_START;
+  for (auto& range : *buf) {
+    h = folly::hash::fnv64_buf(range.data(), range.size(), h);
+  }
+  return h;
+}
+
+class RandomDataHolder : public DataHolder {
+ public:
+  explicit RandomDataHolder(size_t sizeLog2);
+};
+
+RandomDataHolder::RandomDataHolder(size_t sizeLog2)
+  : DataHolder(sizeLog2) {
+  constexpr size_t numThreadsLog2 = 3;
+  constexpr size_t numThreads = size_t(1) << numThreadsLog2;
+
+  uint32_t seed = randomNumberSeed();
+
+  std::vector<std::thread> threads;
+  threads.reserve(numThreads);
+  for (size_t t = 0; t < numThreads; ++t) {
+    threads.emplace_back(
+        [this, seed, t, numThreadsLog2, sizeLog2] () {
+          std::mt19937 rng(seed + t);
+          size_t countLog2 = size_t(1) << (sizeLog2 - numThreadsLog2);
+          size_t start = size_t(t) << countLog2;
+          for (size_t i = 0; i < countLog2; ++i) {
+            this->data_[start + i] = rng();
+          }
+        });
+  }
+
+  for (auto& t : threads) {
+    t.join();
+  }
+}
+
+class ConstantDataHolder : public DataHolder {
+ public:
+  explicit ConstantDataHolder(size_t sizeLog2);
+};
+
+ConstantDataHolder::ConstantDataHolder(size_t sizeLog2)
+  : DataHolder(sizeLog2) {
+  memset(data_.get(), 'a', size_);
+}
+
+constexpr size_t dataSizeLog2 = 27;  // 128MiB
+RandomDataHolder randomDataHolder(dataSizeLog2);
+ConstantDataHolder constantDataHolder(dataSizeLog2);
+
+TEST(CompressionTestNeedsUncompressedLength, Simple) {
+  EXPECT_FALSE(getCodec(CodecType::NO_COMPRESSION)->needsUncompressedLength());
+  EXPECT_TRUE(getCodec(CodecType::LZ4)->needsUncompressedLength());
+  EXPECT_FALSE(getCodec(CodecType::SNAPPY)->needsUncompressedLength());
+  EXPECT_FALSE(getCodec(CodecType::ZLIB)->needsUncompressedLength());
+  EXPECT_FALSE(getCodec(CodecType::LZ4_VARINT_SIZE)->needsUncompressedLength());
+  EXPECT_TRUE(getCodec(CodecType::LZMA2)->needsUncompressedLength());
+  EXPECT_FALSE(getCodec(CodecType::LZMA2_VARINT_SIZE)
+    ->needsUncompressedLength());
+}
+
+class CompressionTest : public testing::TestWithParam<
+    std::tr1::tuple<int, CodecType>> {
+  protected:
+   void SetUp() {
+     auto tup = GetParam();
+     uncompressedLength_ = uint64_t(1) << std::tr1::get<0>(tup);
+     codec_ = getCodec(std::tr1::get<1>(tup));
+   }
+
+   void runSimpleTest(const DataHolder& dh);
+
+   uint64_t uncompressedLength_;
+   std::unique_ptr<Codec> codec_;
+};
+
+void CompressionTest::runSimpleTest(const DataHolder& dh) {
+  auto original = IOBuf::wrapBuffer(dh.data(uncompressedLength_));
+  auto compressed = codec_->compress(original.get());
+  if (!codec_->needsUncompressedLength()) {
+    auto uncompressed = codec_->uncompress(compressed.get());
+    EXPECT_EQ(uncompressedLength_, uncompressed->computeChainDataLength());
+    EXPECT_EQ(dh.hash(uncompressedLength_), hashIOBuf(uncompressed.get()));
+  }
+  {
+    auto uncompressed = codec_->uncompress(compressed.get(),
+                                           uncompressedLength_);
+    EXPECT_EQ(uncompressedLength_, uncompressed->computeChainDataLength());
+    EXPECT_EQ(dh.hash(uncompressedLength_), hashIOBuf(uncompressed.get()));
+  }
+}
+
+TEST_P(CompressionTest, RandomData) {
+  runSimpleTest(randomDataHolder);
+}
+
+TEST_P(CompressionTest, ConstantData) {
+  runSimpleTest(constantDataHolder);
+}
+
+INSTANTIATE_TEST_CASE_P(
+    CompressionTest,
+    CompressionTest,
+    testing::Combine(
+        testing::Values(0, 1, 12, 22, 25, 27),
+        testing::Values(CodecType::NO_COMPRESSION,
+                        CodecType::LZ4,
+                        CodecType::SNAPPY,
+                        CodecType::ZLIB,
+                        CodecType::LZ4_VARINT_SIZE,
+                        CodecType::LZMA2,
+                        CodecType::LZMA2_VARINT_SIZE)));
+
+class CompressionCorruptionTest : public testing::TestWithParam<CodecType> {
+ protected:
+  void SetUp() {
+    codec_ = getCodec(GetParam());
+  }
+
+  void runSimpleTest(const DataHolder& dh);
+
+  std::unique_ptr<Codec> codec_;
+};
+
+void CompressionCorruptionTest::runSimpleTest(const DataHolder& dh) {
+  constexpr uint64_t uncompressedLength = 42;
+  auto original = IOBuf::wrapBuffer(dh.data(uncompressedLength));
+  auto compressed = codec_->compress(original.get());
+
+  if (!codec_->needsUncompressedLength()) {
+    auto uncompressed = codec_->uncompress(compressed.get());
+    EXPECT_EQ(uncompressedLength, uncompressed->computeChainDataLength());
+    EXPECT_EQ(dh.hash(uncompressedLength), hashIOBuf(uncompressed.get()));
+  }
+  {
+    auto uncompressed = codec_->uncompress(compressed.get(),
+                                           uncompressedLength);
+    EXPECT_EQ(uncompressedLength, uncompressed->computeChainDataLength());
+    EXPECT_EQ(dh.hash(uncompressedLength), hashIOBuf(uncompressed.get()));
+  }
+
+  EXPECT_THROW(codec_->uncompress(compressed.get(), uncompressedLength + 1),
+               std::runtime_error);
+
+  // Corrupt the first character
+  ++(compressed->writableData()[0]);
+
+  if (!codec_->needsUncompressedLength()) {
+    EXPECT_THROW(codec_->uncompress(compressed.get()),
+                 std::runtime_error);
+  }
+
+  EXPECT_THROW(codec_->uncompress(compressed.get(), uncompressedLength),
+               std::runtime_error);
+}
+
+TEST_P(CompressionCorruptionTest, RandomData) {
+  runSimpleTest(randomDataHolder);
+}
+
+TEST_P(CompressionCorruptionTest, ConstantData) {
+  runSimpleTest(constantDataHolder);
+}
+
+INSTANTIATE_TEST_CASE_P(
+    CompressionCorruptionTest,
+    CompressionCorruptionTest,
+    testing::Values(
+        // NO_COMPRESSION can't detect corruption
+        // LZ4 can't detect corruption reliably (sigh)
+        CodecType::SNAPPY,
+        CodecType::ZLIB));
+
+}}}  // namespaces
+
+int main(int argc, char *argv[]) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+
+  auto ret = RUN_ALL_TESTS();
+  if (!ret) {
+    folly::runBenchmarksOnFlag();
+  }
+  return ret;
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/test/IOBufCursorTest.cpp
@@ -0,0 +1,665 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/io/IOBuf.h"
+
+#include <gflags/gflags.h>
+#include <boost/random.hpp>
+#include <gtest/gtest.h>
+#include "folly/Benchmark.h"
+#include "folly/Range.h"
+#include "folly/io/Cursor.h"
+
+DECLARE_bool(benchmark);
+
+using folly::IOBuf;
+using std::unique_ptr;
+using namespace folly::io;
+
+TEST(IOBuf, RWCursor) {
+  unique_ptr<IOBuf> iobuf1(IOBuf::create(20));
+  iobuf1->append(20);
+  unique_ptr<IOBuf> iobuf2(IOBuf::create(20));
+  iobuf2->append(20);
+
+  IOBuf* iob2ptr = iobuf2.get();
+  iobuf1->prependChain(std::move(iobuf2));
+
+  EXPECT_TRUE(iobuf1->isChained());
+
+  RWPrivateCursor wcursor(iobuf1.get());
+  Cursor rcursor(iobuf1.get());
+  wcursor.writeLE((uint64_t)1);
+  wcursor.writeLE((uint64_t)1);
+  wcursor.writeLE((uint64_t)1);
+  wcursor.write((uint8_t)1);
+
+  EXPECT_EQ(1, rcursor.readLE<uint64_t>());
+  rcursor.skip(8);
+  EXPECT_EQ(1, rcursor.readLE<uint32_t>());
+  rcursor.skip(0);
+  EXPECT_EQ(0, rcursor.read<uint8_t>());
+  EXPECT_EQ(0, rcursor.read<uint8_t>());
+  EXPECT_EQ(0, rcursor.read<uint8_t>());
+  EXPECT_EQ(0, rcursor.read<uint8_t>());
+  EXPECT_EQ(1, rcursor.read<uint8_t>());
+}
+
+TEST(IOBuf, skip) {
+  unique_ptr<IOBuf> iobuf1(IOBuf::create(20));
+  iobuf1->append(20);
+  RWPrivateCursor wcursor(iobuf1.get());
+  wcursor.write((uint8_t)1);
+  wcursor.write((uint8_t)2);
+  Cursor cursor(iobuf1.get());
+  cursor.skip(1);
+  EXPECT_EQ(2, cursor.read<uint8_t>());
+}
+
+TEST(IOBuf, reset) {
+  unique_ptr<IOBuf> iobuf1(IOBuf::create(20));
+  iobuf1->append(20);
+  RWPrivateCursor wcursor(iobuf1.get());
+  wcursor.write((uint8_t)1);
+  wcursor.write((uint8_t)2);
+  wcursor.reset(iobuf1.get());
+  EXPECT_EQ(1, wcursor.read<uint8_t>());
+}
+
+TEST(IOBuf, copy_assign_convert) {
+  unique_ptr<IOBuf> iobuf1(IOBuf::create(20));
+  iobuf1->append(20);
+  RWPrivateCursor wcursor(iobuf1.get());
+  RWPrivateCursor cursor2(wcursor);
+  RWPrivateCursor cursor3(iobuf1.get());
+
+  wcursor.write((uint8_t)1);
+  cursor3 = wcursor;
+  wcursor.write((uint8_t)2);
+  Cursor cursor4(wcursor);
+  RWPrivateCursor cursor5(wcursor);
+  wcursor.write((uint8_t)3);
+
+  EXPECT_EQ(1, cursor2.read<uint8_t>());
+  EXPECT_EQ(2, cursor3.read<uint8_t>());
+  EXPECT_EQ(3, cursor4.read<uint8_t>());
+}
+
+TEST(IOBuf, arithmetic) {
+  IOBuf iobuf1(IOBuf::CREATE, 20);
+  iobuf1.append(20);
+  RWPrivateCursor wcursor(&iobuf1);
+  wcursor += 1;
+  wcursor.write((uint8_t)1);
+  Cursor cursor(&iobuf1);
+  cursor += 1;
+  EXPECT_EQ(1, cursor.read<uint8_t>());
+
+  Cursor start(&iobuf1);
+  Cursor cursor2 = start + 9;
+  EXPECT_EQ(7, cursor2 - cursor);
+  EXPECT_NE(cursor, cursor2);
+  cursor += 8;
+  cursor2 = cursor2 + 1;
+  EXPECT_EQ(cursor, cursor2);
+}
+
+TEST(IOBuf, endian) {
+  unique_ptr<IOBuf> iobuf1(IOBuf::create(20));
+  iobuf1->append(20);
+  RWPrivateCursor wcursor(iobuf1.get());
+  Cursor rcursor(iobuf1.get());
+  uint16_t v = 1;
+  int16_t vu = -1;
+  wcursor.writeBE(v);
+  wcursor.writeBE(vu);
+  // Try a couple combinations to ensure they were generated correctly
+  wcursor.writeBE(vu);
+  wcursor.writeLE(vu);
+  wcursor.writeLE(vu);
+  wcursor.writeLE(v);
+  EXPECT_EQ(v, rcursor.readBE<uint16_t>());
+}
+
+TEST(IOBuf, Cursor) {
+  unique_ptr<IOBuf> iobuf1(IOBuf::create(1));
+  iobuf1->append(1);
+  RWPrivateCursor c(iobuf1.get());
+  c.write((uint8_t)40); // OK
+  try {
+    c.write((uint8_t)10); // Bad write, checked should except.
+    EXPECT_EQ(true, false);
+  } catch (...) {
+  }
+}
+
+TEST(IOBuf, UnshareCursor) {
+  uint8_t buf = 0;
+  unique_ptr<IOBuf> iobuf1(IOBuf::wrapBuffer(&buf, 1));
+  unique_ptr<IOBuf> iobuf2(IOBuf::wrapBuffer(&buf, 1));
+  RWUnshareCursor c1(iobuf1.get());
+  RWUnshareCursor c2(iobuf2.get());
+
+  c1.write((uint8_t)10); // This should duplicate the two buffers.
+  uint8_t t = c2.read<uint8_t>();
+  EXPECT_EQ(0, t);
+
+  iobuf1 = IOBuf::wrapBuffer(&buf, 1);
+  iobuf2 = IOBuf::wrapBuffer(&buf, 1);
+  RWPrivateCursor c3(iobuf1.get());
+  RWPrivateCursor c4(iobuf2.get());
+
+  c3.write((uint8_t)10); // This should _not_ duplicate the two buffers.
+  t = c4.read<uint8_t>();
+  EXPECT_EQ(10, t);
+}
+
+namespace {
+void append(std::unique_ptr<IOBuf>& buf, folly::StringPiece data) {
+  EXPECT_LE(data.size(), buf->tailroom());
+  memcpy(buf->writableData(), data.data(), data.size());
+  buf->append(data.size());
+}
+
+void append(Appender& appender, folly::StringPiece data) {
+  appender.push(reinterpret_cast<const uint8_t*>(data.data()), data.size());
+}
+
+std::string toString(const IOBuf& buf) {
+  std::string str;
+  Cursor cursor(&buf);
+  std::pair<const uint8_t*, size_t> p;
+  while ((p = cursor.peek()).second) {
+    str.append(reinterpret_cast<const char*>(p.first), p.second);
+    cursor.skip(p.second);
+  }
+  return str;
+}
+
+}  // namespace
+
+TEST(IOBuf, PullAndPeek) {
+  std::unique_ptr<IOBuf> iobuf1(IOBuf::create(10));
+  append(iobuf1, "he");
+  std::unique_ptr<IOBuf> iobuf2(IOBuf::create(10));
+  append(iobuf2, "llo ");
+  std::unique_ptr<IOBuf> iobuf3(IOBuf::create(10));
+  append(iobuf3, "world");
+  iobuf1->prependChain(std::move(iobuf2));
+  iobuf1->prependChain(std::move(iobuf3));
+  EXPECT_EQ(3, iobuf1->countChainElements());
+  EXPECT_EQ(11, iobuf1->computeChainDataLength());
+
+  char buf[12];
+  memset(buf, 0, sizeof(buf));
+  Cursor(iobuf1.get()).pull(buf, 11);
+  EXPECT_EQ("hello world", std::string(buf));
+
+  memset(buf, 0, sizeof(buf));
+  EXPECT_EQ(11, Cursor(iobuf1.get()).pullAtMost(buf, 20));
+  EXPECT_EQ("hello world", std::string(buf));
+
+  EXPECT_THROW({Cursor(iobuf1.get()).pull(buf, 20);},
+               std::out_of_range);
+
+  {
+    RWPrivateCursor cursor(iobuf1.get());
+    auto p = cursor.peek();
+    EXPECT_EQ("he", std::string(reinterpret_cast<const char*>(p.first),
+                                p.second));
+    cursor.skip(p.second);
+    p = cursor.peek();
+    EXPECT_EQ("llo ", std::string(reinterpret_cast<const char*>(p.first),
+                                  p.second));
+    cursor.skip(p.second);
+    p = cursor.peek();
+    EXPECT_EQ("world", std::string(reinterpret_cast<const char*>(p.first),
+                                   p.second));
+    cursor.skip(p.second);
+    EXPECT_EQ(3, iobuf1->countChainElements());
+    EXPECT_EQ(11, iobuf1->computeChainDataLength());
+  }
+
+  {
+    RWPrivateCursor cursor(iobuf1.get());
+    cursor.gather(11);
+    auto p = cursor.peek();
+    EXPECT_EQ("hello world", std::string(reinterpret_cast<const
+                                         char*>(p.first), p.second));
+    EXPECT_EQ(1, iobuf1->countChainElements());
+    EXPECT_EQ(11, iobuf1->computeChainDataLength());
+  }
+}
+
+TEST(IOBuf, Gather) {
+  std::unique_ptr<IOBuf> iobuf1(IOBuf::create(10));
+  append(iobuf1, "he");
+  std::unique_ptr<IOBuf> iobuf2(IOBuf::create(10));
+  append(iobuf2, "llo ");
+  std::unique_ptr<IOBuf> iobuf3(IOBuf::create(10));
+  append(iobuf3, "world");
+  iobuf1->prependChain(std::move(iobuf2));
+  iobuf1->prependChain(std::move(iobuf3));
+  EXPECT_EQ(3, iobuf1->countChainElements());
+  EXPECT_EQ(11, iobuf1->computeChainDataLength());
+
+  // Attempting to gather() more data than available in the chain should fail.
+  // Try from the very beginning of the chain.
+  RWPrivateCursor cursor(iobuf1.get());
+  EXPECT_THROW(cursor.gather(15), std::overflow_error);
+  // Now try from the middle of the chain
+  cursor += 3;
+  EXPECT_THROW(cursor.gather(10), std::overflow_error);
+
+  // Calling gatherAtMost() should succeed, however, and just gather
+  // as much as it can
+  cursor.gatherAtMost(10);
+  EXPECT_EQ(8, cursor.length());
+  EXPECT_EQ(8, cursor.totalLength());
+  EXPECT_EQ("lo world",
+            folly::StringPiece(reinterpret_cast<const char*>(cursor.data()),
+                               cursor.length()));
+  EXPECT_EQ(2, iobuf1->countChainElements());
+  EXPECT_EQ(11, iobuf1->computeChainDataLength());
+
+  // Now try gather again on the chain head
+  cursor = RWPrivateCursor(iobuf1.get());
+  cursor.gather(5);
+  // Since gather() doesn't split buffers, everything should be collapsed into
+  // a single buffer now.
+  EXPECT_EQ(1, iobuf1->countChainElements());
+  EXPECT_EQ(11, iobuf1->computeChainDataLength());
+  EXPECT_EQ(11, cursor.length());
+  EXPECT_EQ(11, cursor.totalLength());
+}
+
+TEST(IOBuf, cloneAndInsert) {
+  std::unique_ptr<IOBuf> iobuf1(IOBuf::create(10));
+  append(iobuf1, "he");
+  std::unique_ptr<IOBuf> iobuf2(IOBuf::create(10));
+  append(iobuf2, "llo ");
+  std::unique_ptr<IOBuf> iobuf3(IOBuf::create(10));
+  append(iobuf3, "world");
+  iobuf1->prependChain(std::move(iobuf2));
+  iobuf1->prependChain(std::move(iobuf3));
+  EXPECT_EQ(3, iobuf1->countChainElements());
+  EXPECT_EQ(11, iobuf1->computeChainDataLength());
+
+  std::unique_ptr<IOBuf> cloned;
+
+  Cursor(iobuf1.get()).clone(cloned, 3);
+  EXPECT_EQ(2, cloned->countChainElements());
+  EXPECT_EQ(3, cloned->computeChainDataLength());
+
+
+  EXPECT_EQ(11, Cursor(iobuf1.get()).cloneAtMost(cloned, 20));
+  EXPECT_EQ(3, cloned->countChainElements());
+  EXPECT_EQ(11, cloned->computeChainDataLength());
+
+
+  EXPECT_THROW({Cursor(iobuf1.get()).clone(cloned, 20);},
+               std::out_of_range);
+
+  {
+    // Check that inserting in the middle of an iobuf splits
+    RWPrivateCursor cursor(iobuf1.get());
+    Cursor(iobuf1.get()).clone(cloned, 3);
+    EXPECT_EQ(2, cloned->countChainElements());
+    EXPECT_EQ(3, cloned->computeChainDataLength());
+
+    cursor.skip(1);
+
+    cursor.insert(std::move(cloned));
+    cursor.insert(folly::IOBuf::create(0));
+    EXPECT_EQ(7, iobuf1->countChainElements());
+    EXPECT_EQ(14, iobuf1->computeChainDataLength());
+    // Check that nextBuf got set correctly to the buffer with 1 byte left
+    EXPECT_EQ(1, cursor.peek().second);
+    cursor.read<uint8_t>();
+  }
+
+  {
+    // Check that inserting at the end doesn't create empty buf
+    RWPrivateCursor cursor(iobuf1.get());
+    Cursor(iobuf1.get()).clone(cloned, 1);
+    EXPECT_EQ(1, cloned->countChainElements());
+    EXPECT_EQ(1, cloned->computeChainDataLength());
+
+    cursor.skip(1);
+
+    cursor.insert(std::move(cloned));
+    EXPECT_EQ(8, iobuf1->countChainElements());
+    EXPECT_EQ(15, iobuf1->computeChainDataLength());
+    // Check that nextBuf got set correctly
+    cursor.read<uint8_t>();
+  }
+  {
+    // Check that inserting at the beginning doesn't create empty buf
+    RWPrivateCursor cursor(iobuf1.get());
+    Cursor(iobuf1.get()).clone(cloned, 1);
+    EXPECT_EQ(1, cloned->countChainElements());
+    EXPECT_EQ(1, cloned->computeChainDataLength());
+
+    cursor.insert(std::move(cloned));
+    EXPECT_EQ(9, iobuf1->countChainElements());
+    EXPECT_EQ(16, iobuf1->computeChainDataLength());
+    // Check that nextBuf got set correctly
+    cursor.read<uint8_t>();
+  }
+}
+
+TEST(IOBuf, Appender) {
+  std::unique_ptr<IOBuf> head(IOBuf::create(10));
+  append(head, "hello");
+
+  Appender app(head.get(), 10);
+  uint32_t cap = head->capacity();
+  uint32_t len1 = app.length();
+  EXPECT_EQ(cap - 5, len1);
+  app.ensure(len1);  // won't grow
+  EXPECT_EQ(len1, app.length());
+  app.ensure(len1 + 1);  // will grow
+  EXPECT_LE(len1 + 1, app.length());
+
+  append(app, " world");
+  EXPECT_EQ("hello world", toString(*head));
+}
+
+TEST(IOBuf, QueueAppender) {
+  folly::IOBufQueue queue;
+
+  // Allocate 100 bytes at once, but don't grow past 1024
+  QueueAppender app(&queue, 100);
+  size_t n = 1024 / sizeof(uint32_t);
+  for (uint32_t i = 0; i < n; ++i) {
+    app.writeBE(i);
+  }
+
+  // There must be a goodMallocSize between 100 and 1024...
+  EXPECT_LT(1, queue.front()->countChainElements());
+  const IOBuf* buf = queue.front();
+  do {
+    EXPECT_LE(100, buf->capacity());
+    buf = buf->next();
+  } while (buf != queue.front());
+
+  Cursor cursor(queue.front());
+  for (uint32_t i = 0; i < n; ++i) {
+    EXPECT_EQ(i, cursor.readBE<uint32_t>());
+  }
+
+  EXPECT_THROW({cursor.readBE<uint32_t>();}, std::out_of_range);
+}
+
+TEST(IOBuf, CursorOperators) {
+  // Test operators on a single-item chain
+  {
+    std::unique_ptr<IOBuf> chain1(IOBuf::create(20));
+    chain1->append(10);
+
+    Cursor curs1(chain1.get());
+    EXPECT_EQ(0, curs1 - chain1.get());
+    curs1.skip(3);
+    EXPECT_EQ(3, curs1 - chain1.get());
+    curs1.skip(7);
+    EXPECT_EQ(10, curs1 - chain1.get());
+
+    Cursor curs2(chain1.get());
+    EXPECT_EQ(0, curs2 - chain1.get());
+    EXPECT_EQ(10, curs1 - curs2);
+    EXPECT_THROW(curs2 - curs1, std::out_of_range);
+  }
+
+  // Test cross-chain operations
+  {
+    std::unique_ptr<IOBuf> chain1(IOBuf::create(20));
+    chain1->append(10);
+    std::unique_ptr<IOBuf> chain2 = chain1->clone();
+
+    Cursor curs1(chain1.get());
+    Cursor curs2(chain2.get());
+    EXPECT_THROW(curs1 - curs2, std::out_of_range);
+    EXPECT_THROW(curs1 - chain2.get(), std::out_of_range);
+  }
+
+  // Test operations on multi-item chains
+  {
+    std::unique_ptr<IOBuf> chain(IOBuf::create(20));
+    chain->append(10);
+    chain->appendChain(chain->clone());
+    EXPECT_EQ(20, chain->computeChainDataLength());
+
+    Cursor curs1(chain.get());
+    curs1.skip(5);
+    Cursor curs2(chain.get());
+    curs2.skip(3);
+    EXPECT_EQ(2, curs1 - curs2);
+    EXPECT_EQ(5, curs1 - chain.get());
+    EXPECT_THROW(curs2 - curs1, std::out_of_range);
+
+    curs1.skip(7);
+    EXPECT_EQ(9, curs1 - curs2);
+    EXPECT_EQ(12, curs1 - chain.get());
+    EXPECT_THROW(curs2 - curs1, std::out_of_range);
+
+    curs2.skip(7);
+    EXPECT_EQ(2, curs1 - curs2);
+    EXPECT_THROW(curs2 - curs1, std::out_of_range);
+  }
+}
+
+TEST(IOBuf, StringOperations) {
+  // Test a single buffer with two null-terminated strings and an extra uint8_t
+  // at the end
+  {
+    std::unique_ptr<IOBuf> chain(IOBuf::create(16));
+    Appender app(chain.get(), 0);
+    app.push(reinterpret_cast<const uint8_t*>("hello\0world\0\x01"), 13);
+
+    Cursor curs(chain.get());
+    EXPECT_STREQ("hello", curs.readTerminatedString().c_str());
+    EXPECT_STREQ("world", curs.readTerminatedString().c_str());
+    EXPECT_EQ(1, curs.read<uint8_t>());
+  }
+
+  // Test multiple buffers where the first is empty and the string starts in
+  // the second buffer.
+  {
+    std::unique_ptr<IOBuf> chain(IOBuf::create(8));
+    chain->prependChain(IOBuf::create(12));
+    Appender app(chain.get(), 0);
+    app.push(reinterpret_cast<const uint8_t*>("hello world\0"), 12);
+
+    Cursor curs(chain.get());
+    EXPECT_STREQ("hello world", curs.readTerminatedString().c_str());
+  }
+
+  // Test multiple buffers with a single null-terminated string spanning them
+  {
+    std::unique_ptr<IOBuf> chain(IOBuf::create(8));
+    chain->prependChain(IOBuf::create(8));
+    chain->append(8);
+    chain->next()->append(4);
+    RWPrivateCursor rwc(chain.get());
+    rwc.push(reinterpret_cast<const uint8_t*>("hello world\0"), 12);
+
+    Cursor curs(chain.get());
+    EXPECT_STREQ("hello world", curs.readTerminatedString().c_str());
+  }
+
+  // Test a reading a null-terminated string that's longer than the maximum
+  // allowable length
+  {
+    std::unique_ptr<IOBuf> chain(IOBuf::create(16));
+    Appender app(chain.get(), 0);
+    app.push(reinterpret_cast<const uint8_t*>("hello world\0"), 12);
+
+    Cursor curs(chain.get());
+    EXPECT_THROW(curs.readTerminatedString('\0', 5), std::length_error);
+  }
+
+  // Test reading a null-terminated string from a chain with an empty buffer at
+  // the front
+  {
+    std::unique_ptr<IOBuf> buf(IOBuf::create(8));
+    Appender app(buf.get(), 0);
+    app.push(reinterpret_cast<const uint8_t*>("hello\0"), 6);
+    std::unique_ptr<IOBuf> chain(IOBuf::create(8));
+    chain->prependChain(std::move(buf));
+
+    Cursor curs(chain.get());
+    EXPECT_STREQ("hello", curs.readTerminatedString().c_str());
+  }
+
+  // Test reading a two fixed-length strings from a single buffer with an extra
+  // uint8_t at the end
+  {
+    std::unique_ptr<IOBuf> chain(IOBuf::create(16));
+    Appender app(chain.get(), 0);
+    app.push(reinterpret_cast<const uint8_t*>("helloworld\x01"), 11);
+
+    Cursor curs(chain.get());
+    EXPECT_STREQ("hello", curs.readFixedString(5).c_str());
+    EXPECT_STREQ("world", curs.readFixedString(5).c_str());
+    EXPECT_EQ(1, curs.read<uint8_t>());
+  }
+
+  // Test multiple buffers where the first is empty and a fixed-length string
+  // starts in the second buffer.
+  {
+    std::unique_ptr<IOBuf> chain(IOBuf::create(8));
+    chain->prependChain(IOBuf::create(16));
+    Appender app(chain.get(), 0);
+    app.push(reinterpret_cast<const uint8_t*>("hello world"), 11);
+
+    Cursor curs(chain.get());
+    EXPECT_STREQ("hello world", curs.readFixedString(11).c_str());
+  }
+
+  // Test multiple buffers with a single fixed-length string spanning them
+  {
+    std::unique_ptr<IOBuf> chain(IOBuf::create(8));
+    chain->prependChain(IOBuf::create(8));
+    chain->append(7);
+    chain->next()->append(4);
+    RWPrivateCursor rwc(chain.get());
+    rwc.push(reinterpret_cast<const uint8_t*>("hello world"), 11);
+
+    Cursor curs(chain.get());
+    EXPECT_STREQ("hello world", curs.readFixedString(11).c_str());
+  }
+
+  // Test reading a fixed-length string from a chain with an empty buffer at
+  // the front
+  {
+    std::unique_ptr<IOBuf> buf(IOBuf::create(8));
+    Appender app(buf.get(), 0);
+    app.push(reinterpret_cast<const uint8_t*>("hello"), 5);
+    std::unique_ptr<IOBuf> chain(IOBuf::create(8));
+    chain->prependChain(std::move(buf));
+
+    Cursor curs(chain.get());
+    EXPECT_STREQ("hello", curs.readFixedString(5).c_str());
+  }
+}
+
+int benchmark_size = 1000;
+unique_ptr<IOBuf> iobuf_benchmark;
+
+unique_ptr<IOBuf> iobuf_read_benchmark;
+
+template <class CursClass>
+void runBenchmark() {
+  CursClass c(iobuf_benchmark.get());
+
+  for(int i = 0; i < benchmark_size; i++) {
+    c.write((uint8_t)0);
+  }
+}
+
+BENCHMARK(rwPrivateCursorBenchmark, iters) {
+  while (iters--) {
+    runBenchmark<RWPrivateCursor>();
+  }
+}
+
+BENCHMARK(rwUnshareCursorBenchmark, iters) {
+  while (iters--) {
+    runBenchmark<RWUnshareCursor>();
+  }
+}
+
+
+BENCHMARK(cursorBenchmark, iters) {
+  while (iters--) {
+    Cursor c(iobuf_read_benchmark.get());
+    for(int i = 0; i < benchmark_size ; i++) {
+      c.read<uint8_t>();
+    }
+  }
+}
+
+BENCHMARK(skipBenchmark, iters) {
+  uint8_t buf;
+  while (iters--) {
+    Cursor c(iobuf_read_benchmark.get());
+    for(int i = 0; i < benchmark_size ; i++) {
+      c.peek();
+      c.skip(1);
+    }
+  }
+}
+
+// fbmake opt
+// _bin/folly/experimental/io/test/iobuf_cursor_test -benchmark
+//
+// Benchmark                               Iters   Total t    t/iter iter/sec
+// ---------------------------------------------------------------------------
+// rwPrivateCursorBenchmark               100000  142.9 ms  1.429 us  683.5 k
+// rwUnshareCursorBenchmark               100000  309.3 ms  3.093 us  315.7 k
+// cursorBenchmark                        100000  741.4 ms  7.414 us  131.7 k
+// skipBenchmark                          100000  738.9 ms  7.389 us  132.2 k
+//
+// uname -a:
+//
+// Linux dev2159.snc6.facebook.com 2.6.33-7_fbk15_104e4d0 #1 SMP
+// Tue Oct 19 22:40:30 PDT 2010 x86_64 x86_64 x86_64 GNU/Linux
+//
+// 72GB RAM, 2 CPUs (Intel(R) Xeon(R) CPU L5630  @ 2.13GHz)
+// hyperthreading disabled
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+
+  auto ret = RUN_ALL_TESTS();
+
+  if (ret == 0 && FLAGS_benchmark) {
+    iobuf_benchmark = IOBuf::create(benchmark_size);
+    iobuf_benchmark->append(benchmark_size);
+
+    iobuf_read_benchmark = IOBuf::create(1);
+    for (int i = 0; i < benchmark_size; i++) {
+      unique_ptr<IOBuf> iobuf2(IOBuf::create(1));
+      iobuf2->append(1);
+      iobuf_read_benchmark->prependChain(std::move(iobuf2));
+    }
+
+    folly::runBenchmarks();
+  }
+
+  return ret;
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/test/IOBufQueueTest.cpp
@@ -0,0 +1,371 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/io/IOBufQueue.h"
+#include "folly/Range.h"
+
+#include <gflags/gflags.h>
+#include <gtest/gtest.h>
+
+#include <iostream>
+#include <stdexcept>
+#include <string.h>
+
+using folly::IOBuf;
+using folly::IOBufQueue;
+using folly::StringPiece;
+using std::pair;
+using std::string;
+using std::unique_ptr;
+
+// String Comma Length macro for string literals
+#define SCL(x) (x), sizeof(x) - 1
+
+namespace {
+
+IOBufQueue::Options clOptions;
+struct Initializer {
+  Initializer() {
+    clOptions.cacheChainLength = true;
+  }
+};
+Initializer initializer;
+
+unique_ptr<IOBuf>
+stringToIOBuf(const char* s, uint32_t len) {
+  unique_ptr<IOBuf> buf = IOBuf::create(len);
+  memcpy(buf->writableTail(), s, len);
+  buf->append(len);
+  return std::move(buf);
+}
+
+void checkConsistency(const IOBufQueue& queue) {
+  if (queue.options().cacheChainLength) {
+    size_t len = queue.front() ? queue.front()->computeChainDataLength() : 0;
+    EXPECT_EQ(len, queue.chainLength());
+  }
+}
+
+}
+
+TEST(IOBufQueue, Simple) {
+  IOBufQueue queue(clOptions);
+  EXPECT_EQ(NULL, queue.front());
+  queue.append(SCL(""));
+  EXPECT_EQ(NULL, queue.front());
+  queue.append(unique_ptr<IOBuf>());
+  EXPECT_EQ(NULL, queue.front());
+  string emptyString;
+  queue.append(emptyString);
+  EXPECT_EQ(NULL, queue.front());
+}
+
+TEST(IOBufQueue, Append) {
+  IOBufQueue queue(clOptions);
+  queue.append(SCL("Hello"));
+  IOBufQueue queue2(clOptions);
+  queue2.append(SCL(", "));
+  queue2.append(SCL("World"));
+  checkConsistency(queue);
+  checkConsistency(queue2);
+  queue.append(queue2.move());
+  checkConsistency(queue);
+  checkConsistency(queue2);
+  const IOBuf* chain = queue.front();
+  EXPECT_NE((IOBuf*)NULL, chain);
+  EXPECT_EQ(12, chain->computeChainDataLength());
+  EXPECT_EQ(NULL, queue2.front());
+}
+
+TEST(IOBufQueue, Append2) {
+  IOBufQueue queue(clOptions);
+  queue.append(SCL("Hello"));
+  IOBufQueue queue2(clOptions);
+  queue2.append(SCL(", "));
+  queue2.append(SCL("World"));
+  checkConsistency(queue);
+  checkConsistency(queue2);
+  queue.append(queue2);
+  checkConsistency(queue);
+  checkConsistency(queue2);
+  const IOBuf* chain = queue.front();
+  EXPECT_NE((IOBuf*)NULL, chain);
+  EXPECT_EQ(12, chain->computeChainDataLength());
+  EXPECT_EQ(NULL, queue2.front());
+}
+
+TEST(IOBufQueue, Split) {
+  IOBufQueue queue(clOptions);
+  queue.append(stringToIOBuf(SCL("Hello")));
+  queue.append(stringToIOBuf(SCL(",")));
+  queue.append(stringToIOBuf(SCL(" ")));
+  queue.append(stringToIOBuf(SCL("")));
+  queue.append(stringToIOBuf(SCL("World")));
+  checkConsistency(queue);
+  EXPECT_EQ(12, queue.front()->computeChainDataLength());
+
+  unique_ptr<IOBuf> prefix(queue.split(1));
+  checkConsistency(queue);
+  EXPECT_EQ(1, prefix->computeChainDataLength());
+  EXPECT_EQ(11, queue.front()->computeChainDataLength());
+  prefix = queue.split(2);
+  checkConsistency(queue);
+  EXPECT_EQ(2, prefix->computeChainDataLength());
+  EXPECT_EQ(9, queue.front()->computeChainDataLength());
+  prefix = queue.split(3);
+  checkConsistency(queue);
+  EXPECT_EQ(3, prefix->computeChainDataLength());
+  EXPECT_EQ(6, queue.front()->computeChainDataLength());
+  prefix = queue.split(1);
+  checkConsistency(queue);
+  EXPECT_EQ(1, prefix->computeChainDataLength());
+  EXPECT_EQ(5, queue.front()->computeChainDataLength());
+  prefix = queue.split(5);
+  checkConsistency(queue);
+  EXPECT_EQ(5, prefix->computeChainDataLength());
+  EXPECT_EQ((IOBuf*)NULL, queue.front());
+
+  queue.append(stringToIOBuf(SCL("Hello,")));
+  queue.append(stringToIOBuf(SCL(" World")));
+  checkConsistency(queue);
+  bool exceptionFired = false;
+  EXPECT_THROW({prefix = queue.split(13);}, std::underflow_error);
+  checkConsistency(queue);
+}
+
+TEST(IOBufQueue, Preallocate) {
+  IOBufQueue queue(clOptions);
+  queue.append(string("Hello"));
+  pair<void*,uint32_t> writable = queue.preallocate(2, 64, 64);
+  checkConsistency(queue);
+  EXPECT_NE((void*)NULL, writable.first);
+  EXPECT_LE(2, writable.second);
+  EXPECT_GE(64, writable.second);
+  memcpy(writable.first, SCL(", "));
+  queue.postallocate(2);
+  checkConsistency(queue);
+  EXPECT_EQ(7, queue.front()->computeChainDataLength());
+  queue.append(SCL("World"));
+  checkConsistency(queue);
+  EXPECT_EQ(12, queue.front()->computeChainDataLength());
+  // There are not 2048 bytes available, this will alloc a new buf
+  writable = queue.preallocate(2048, 4096);
+  checkConsistency(queue);
+  EXPECT_LE(2048, writable.second);
+  // IOBuf allocates more than newAllocationSize, and we didn't cap it
+  EXPECT_GE(writable.second, 4096);
+  queue.postallocate(writable.second);
+  // queue has no empty space, make sure we allocate at least min, even if
+  // newAllocationSize < min
+  writable = queue.preallocate(1024, 1, 1024);
+  checkConsistency(queue);
+  EXPECT_EQ(1024, writable.second);
+}
+
+TEST(IOBufQueue, Wrap) {
+  IOBufQueue queue(clOptions);
+  const char* buf = "hello world goodbye";
+  size_t len = strlen(buf);
+  queue.wrapBuffer(buf, len, 6);
+  auto iob = queue.move();
+  EXPECT_EQ((len - 1) / 6 + 1, iob->countChainElements());
+  iob->unshare();
+  iob->coalesce();
+  EXPECT_EQ(StringPiece(buf),
+            StringPiece(reinterpret_cast<const char*>(iob->data()),
+                        iob->length()));
+}
+
+TEST(IOBufQueue, Trim) {
+  IOBufQueue queue(clOptions);
+  unique_ptr<IOBuf> a = IOBuf::create(4);
+  a->append(4);
+  queue.append(std::move(a));
+  checkConsistency(queue);
+  a = IOBuf::create(6);
+  a->append(6);
+  queue.append(std::move(a));
+  checkConsistency(queue);
+  a = IOBuf::create(8);
+  a->append(8);
+  queue.append(std::move(a));
+  checkConsistency(queue);
+  a = IOBuf::create(10);
+  a->append(10);
+  queue.append(std::move(a));
+  checkConsistency(queue);
+
+  EXPECT_EQ(4, queue.front()->countChainElements());
+  EXPECT_EQ(28, queue.front()->computeChainDataLength());
+  EXPECT_EQ(4, queue.front()->length());
+
+  queue.trimStart(1);
+  checkConsistency(queue);
+  EXPECT_EQ(4, queue.front()->countChainElements());
+  EXPECT_EQ(27, queue.front()->computeChainDataLength());
+  EXPECT_EQ(3, queue.front()->length());
+
+  queue.trimStart(5);
+  checkConsistency(queue);
+  EXPECT_EQ(3, queue.front()->countChainElements());
+  EXPECT_EQ(22, queue.front()->computeChainDataLength());
+  EXPECT_EQ(4, queue.front()->length());
+
+  queue.trimEnd(1);
+  checkConsistency(queue);
+  EXPECT_EQ(3, queue.front()->countChainElements());
+  EXPECT_EQ(21, queue.front()->computeChainDataLength());
+  EXPECT_EQ(9, queue.front()->prev()->length());
+
+  queue.trimEnd(20);
+  checkConsistency(queue);
+  EXPECT_EQ(1, queue.front()->countChainElements());
+  EXPECT_EQ(1, queue.front()->computeChainDataLength());
+  EXPECT_EQ(1, queue.front()->prev()->length());
+
+  queue.trimEnd(1);
+  checkConsistency(queue);
+  EXPECT_EQ(NULL, queue.front());
+
+  EXPECT_THROW(queue.trimStart(2), std::underflow_error);
+  checkConsistency(queue);
+
+  EXPECT_THROW(queue.trimEnd(30), std::underflow_error);
+  checkConsistency(queue);
+}
+
+TEST(IOBufQueue, TrimPack) {
+  IOBufQueue queue(clOptions);
+  unique_ptr<IOBuf> a = IOBuf::create(64);
+  a->append(4);
+  queue.append(std::move(a), true);
+  checkConsistency(queue);
+  a = IOBuf::create(6);
+  a->append(6);
+  queue.append(std::move(a), true);
+  checkConsistency(queue);
+  a = IOBuf::create(8);
+  a->append(8);
+  queue.append(std::move(a), true);
+  checkConsistency(queue);
+  a = IOBuf::create(10);
+  a->append(10);
+  queue.append(std::move(a), true);
+  checkConsistency(queue);
+
+  EXPECT_EQ(1, queue.front()->countChainElements());
+  EXPECT_EQ(28, queue.front()->computeChainDataLength());
+  EXPECT_EQ(28, queue.front()->length());
+
+  queue.trimStart(1);
+  checkConsistency(queue);
+  EXPECT_EQ(1, queue.front()->countChainElements());
+  EXPECT_EQ(27, queue.front()->computeChainDataLength());
+  EXPECT_EQ(27, queue.front()->length());
+
+  queue.trimStart(5);
+  checkConsistency(queue);
+  EXPECT_EQ(1, queue.front()->countChainElements());
+  EXPECT_EQ(22, queue.front()->computeChainDataLength());
+  EXPECT_EQ(22, queue.front()->length());
+
+  queue.trimEnd(1);
+  checkConsistency(queue);
+  EXPECT_EQ(1, queue.front()->countChainElements());
+  EXPECT_EQ(21, queue.front()->computeChainDataLength());
+  EXPECT_EQ(21, queue.front()->prev()->length());
+
+  queue.trimEnd(20);
+  checkConsistency(queue);
+  EXPECT_EQ(1, queue.front()->countChainElements());
+  EXPECT_EQ(1, queue.front()->computeChainDataLength());
+  EXPECT_EQ(1, queue.front()->prev()->length());
+
+  queue.trimEnd(1);
+  checkConsistency(queue);
+  EXPECT_EQ(NULL, queue.front());
+
+  EXPECT_THROW(queue.trimStart(2), std::underflow_error);
+  checkConsistency(queue);
+
+  EXPECT_THROW(queue.trimEnd(30), std::underflow_error);
+  checkConsistency(queue);
+}
+
+TEST(IOBufQueue, Prepend) {
+  folly::IOBufQueue queue;
+
+  auto buf = folly::IOBuf::create(10);
+  buf->advance(5);
+  queue.append(std::move(buf));
+
+  queue.append(SCL(" World"));
+  queue.prepend(SCL("Hello"));
+
+  EXPECT_THROW(queue.prepend(SCL("x")), std::overflow_error);
+
+  auto out = queue.move();
+  out->coalesce();
+  EXPECT_EQ("Hello World",
+            StringPiece(reinterpret_cast<const char*>(out->data()),
+                        out->length()));
+}
+
+TEST(IOBufQueue, PopFirst) {
+  IOBufQueue queue(IOBufQueue::cacheChainLength());
+  const char * strings[] = {
+    "Hello",
+    ",",
+    " ",
+    "",
+    "World"
+  };
+
+  const size_t numStrings=sizeof(strings)/sizeof(*strings);
+  size_t chainLength = 0;
+  for(ssize_t i=0; i<numStrings; ++i) {
+    queue.append(stringToIOBuf(strings[i], strlen(strings[i])));
+    checkConsistency(queue);
+    chainLength += strlen(strings[i]);
+  }
+
+  unique_ptr<IOBuf> first;
+  for(ssize_t i=0; i<numStrings; ++i) {
+    checkConsistency(queue);
+    EXPECT_EQ(chainLength, queue.front()->computeChainDataLength());
+    EXPECT_EQ(chainLength, queue.chainLength());
+    first = queue.pop_front();
+    chainLength-=strlen(strings[i]);
+    EXPECT_EQ(strlen(strings[i]), first->computeChainDataLength());
+  }
+  checkConsistency(queue);
+  EXPECT_EQ(chainLength, queue.chainLength());
+
+  EXPECT_EQ((IOBuf*)NULL, queue.front());
+  first = queue.pop_front();
+  EXPECT_EQ((IOBuf*)NULL, first.get());
+
+  checkConsistency(queue);
+  EXPECT_EQ((IOBuf*)NULL, queue.front());
+  EXPECT_EQ(0, queue.chainLength());
+}
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+
+  return RUN_ALL_TESTS();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/test/IOBufTest.cpp
@@ -0,0 +1,1014 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/io/IOBuf.h"
+#include "folly/io/TypedIOBuf.h"
+
+// googletest requires std::tr1::tuple, not std::tuple
+#include <tr1/tuple>
+
+#include <gflags/gflags.h>
+#include <boost/random.hpp>
+#include <gtest/gtest.h>
+
+#include "folly/Malloc.h"
+#include "folly/Range.h"
+
+using folly::fbstring;
+using folly::fbvector;
+using folly::IOBuf;
+using folly::TypedIOBuf;
+using folly::StringPiece;
+using folly::ByteRange;
+using std::unique_ptr;
+
+void append(std::unique_ptr<IOBuf>& buf, StringPiece str) {
+  EXPECT_LE(str.size(), buf->tailroom());
+  memcpy(buf->writableData(), str.data(), str.size());
+  buf->append(str.size());
+}
+
+void prepend(std::unique_ptr<IOBuf>& buf, StringPiece str) {
+  EXPECT_LE(str.size(), buf->headroom());
+  memcpy(buf->writableData() - str.size(), str.data(), str.size());
+  buf->prepend(str.size());
+}
+
+TEST(IOBuf, Simple) {
+  unique_ptr<IOBuf> buf(IOBuf::create(100));
+  uint32_t cap = buf->capacity();
+  EXPECT_LE(100, cap);
+  EXPECT_EQ(0, buf->headroom());
+  EXPECT_EQ(0, buf->length());
+  EXPECT_EQ(cap, buf->tailroom());
+
+  append(buf, "world");
+  buf->advance(10);
+  EXPECT_EQ(10, buf->headroom());
+  EXPECT_EQ(5, buf->length());
+  EXPECT_EQ(cap - 15, buf->tailroom());
+
+  prepend(buf, "hello ");
+  EXPECT_EQ(4, buf->headroom());
+  EXPECT_EQ(11, buf->length());
+  EXPECT_EQ(cap - 15, buf->tailroom());
+
+  const char* p = reinterpret_cast<const char*>(buf->data());
+  EXPECT_EQ("hello world", std::string(p, buf->length()));
+
+  buf->clear();
+  EXPECT_EQ(0, buf->headroom());
+  EXPECT_EQ(0, buf->length());
+  EXPECT_EQ(cap, buf->tailroom());
+}
+
+
+void testAllocSize(uint32_t requestedCapacity) {
+  unique_ptr<IOBuf> iobuf(IOBuf::create(requestedCapacity));
+  EXPECT_GE(iobuf->capacity(), requestedCapacity);
+}
+
+TEST(IOBuf, AllocSizes) {
+  // Try with a small allocation size that should fit in the internal buffer
+  testAllocSize(28);
+
+  // Try with a large allocation size that will require an external buffer.
+  testAllocSize(9000);
+
+  // 220 bytes is currently the cutoff
+  // (It would be nice to use the IOBuf::kMaxInternalDataSize constant,
+  // but it's private and it doesn't seem worth making it public just for this
+  // test code.)
+  testAllocSize(220);
+  testAllocSize(219);
+  testAllocSize(221);
+}
+
+void deleteArrayBuffer(void *buf, void* arg) {
+  uint32_t* deleteCount = static_cast<uint32_t*>(arg);
+  ++(*deleteCount);
+  uint8_t* bufPtr = static_cast<uint8_t*>(buf);
+  delete[] bufPtr;
+}
+
+TEST(IOBuf, TakeOwnership) {
+  uint32_t size1 = 99;
+  uint8_t *buf1 = static_cast<uint8_t*>(malloc(size1));
+  unique_ptr<IOBuf> iobuf1(IOBuf::takeOwnership(buf1, size1));
+  EXPECT_EQ(buf1, iobuf1->data());
+  EXPECT_EQ(size1, iobuf1->length());
+  EXPECT_EQ(buf1, iobuf1->buffer());
+  EXPECT_EQ(size1, iobuf1->capacity());
+
+  uint32_t deleteCount = 0;
+  uint32_t size2 = 4321;
+  uint8_t *buf2 = new uint8_t[size2];
+  unique_ptr<IOBuf> iobuf2(IOBuf::takeOwnership(buf2, size2,
+                                                deleteArrayBuffer,
+                                                &deleteCount));
+  EXPECT_EQ(buf2, iobuf2->data());
+  EXPECT_EQ(size2, iobuf2->length());
+  EXPECT_EQ(buf2, iobuf2->buffer());
+  EXPECT_EQ(size2, iobuf2->capacity());
+  EXPECT_EQ(0, deleteCount);
+  iobuf2.reset();
+  EXPECT_EQ(1, deleteCount);
+
+  deleteCount = 0;
+  uint32_t size3 = 3456;
+  uint8_t *buf3 = new uint8_t[size3];
+  uint32_t length3 = 48;
+  unique_ptr<IOBuf> iobuf3(IOBuf::takeOwnership(buf3, size3, length3,
+                                                deleteArrayBuffer,
+                                                &deleteCount));
+  EXPECT_EQ(buf3, iobuf3->data());
+  EXPECT_EQ(length3, iobuf3->length());
+  EXPECT_EQ(buf3, iobuf3->buffer());
+  EXPECT_EQ(size3, iobuf3->capacity());
+  EXPECT_EQ(0, deleteCount);
+  iobuf3.reset();
+  EXPECT_EQ(1, deleteCount);
+
+  deleteCount = 0;
+  {
+    uint32_t size4 = 1234;
+    uint8_t *buf4 = new uint8_t[size4];
+    uint32_t length4 = 48;
+    IOBuf iobuf4(IOBuf::TAKE_OWNERSHIP, buf4, size4, length4,
+                 deleteArrayBuffer, &deleteCount);
+    EXPECT_EQ(buf4, iobuf4.data());
+    EXPECT_EQ(length4, iobuf4.length());
+    EXPECT_EQ(buf4, iobuf4.buffer());
+    EXPECT_EQ(size4, iobuf4.capacity());
+
+    IOBuf iobuf5 = std::move(iobuf4);
+    EXPECT_EQ(buf4, iobuf5.data());
+    EXPECT_EQ(length4, iobuf5.length());
+    EXPECT_EQ(buf4, iobuf5.buffer());
+    EXPECT_EQ(size4, iobuf5.capacity());
+    EXPECT_EQ(0, deleteCount);
+  }
+  EXPECT_EQ(1, deleteCount);
+}
+
+TEST(IOBuf, WrapBuffer) {
+  const uint32_t size1 = 1234;
+  uint8_t buf1[size1];
+  unique_ptr<IOBuf> iobuf1(IOBuf::wrapBuffer(buf1, size1));
+  EXPECT_EQ(buf1, iobuf1->data());
+  EXPECT_EQ(size1, iobuf1->length());
+  EXPECT_EQ(buf1, iobuf1->buffer());
+  EXPECT_EQ(size1, iobuf1->capacity());
+
+  uint32_t size2 = 0x1234;
+  unique_ptr<uint8_t[]> buf2(new uint8_t[size2]);
+  unique_ptr<IOBuf> iobuf2(IOBuf::wrapBuffer(buf2.get(), size2));
+  EXPECT_EQ(buf2.get(), iobuf2->data());
+  EXPECT_EQ(size2, iobuf2->length());
+  EXPECT_EQ(buf2.get(), iobuf2->buffer());
+  EXPECT_EQ(size2, iobuf2->capacity());
+
+  uint32_t size3 = 4321;
+  unique_ptr<uint8_t[]> buf3(new uint8_t[size3]);
+  IOBuf iobuf3(IOBuf::WRAP_BUFFER, buf3.get(), size3);
+  EXPECT_EQ(buf3.get(), iobuf3.data());
+  EXPECT_EQ(size3, iobuf3.length());
+  EXPECT_EQ(buf3.get(), iobuf3.buffer());
+  EXPECT_EQ(size3, iobuf3.capacity());
+}
+
+TEST(IOBuf, CreateCombined) {
+  // Create a combined IOBuf, then destroy it.
+  // The data buffer and IOBuf both become unused as part of the destruction
+  {
+    auto buf = IOBuf::createCombined(256);
+    EXPECT_FALSE(buf->isShared());
+  }
+
+  // Create a combined IOBuf, clone from it, and then destroy the original
+  // IOBuf.  The data buffer cannot be deleted until the clone is also
+  // destroyed.
+  {
+    auto bufA = IOBuf::createCombined(256);
+    EXPECT_FALSE(bufA->isShared());
+    auto bufB = bufA->clone();
+    EXPECT_TRUE(bufA->isShared());
+    EXPECT_TRUE(bufB->isShared());
+    bufA.reset();
+    EXPECT_FALSE(bufB->isShared());
+  }
+
+  // Create a combined IOBuf, then call reserve() to get a larger buffer.
+  // The IOBuf no longer points to the combined data buffer, but the
+  // overall memory segment cannot be deleted until the IOBuf is also
+  // destroyed.
+  {
+    auto buf = IOBuf::createCombined(256);
+    buf->reserve(0, buf->capacity() + 100);
+  }
+
+  // Create a combined IOBuf, clone from it, then call unshare() on the original
+  // buffer.  This creates a situation where bufB is pointing at the combined
+  // buffer associated with bufA, but bufA is now using a different buffer.
+  auto testSwap = [](bool resetAFirst) {
+    auto bufA = IOBuf::createCombined(256);
+    EXPECT_FALSE(bufA->isShared());
+    auto bufB = bufA->clone();
+    EXPECT_TRUE(bufA->isShared());
+    EXPECT_TRUE(bufB->isShared());
+    bufA->unshare();
+    EXPECT_FALSE(bufA->isShared());
+    EXPECT_FALSE(bufB->isShared());
+
+    if (resetAFirst) {
+      bufA.reset();
+      bufB.reset();
+    } else {
+      bufB.reset();
+      bufA.reset();
+    }
+  };
+  testSwap(true);
+  testSwap(false);
+}
+
+void fillBuf(uint8_t* buf, uint32_t length, boost::mt19937& gen) {
+  for (uint32_t n = 0; n < length; ++n) {
+    buf[n] = static_cast<uint8_t>(gen() & 0xff);
+  }
+}
+
+void fillBuf(IOBuf* buf, boost::mt19937& gen) {
+  buf->unshare();
+  fillBuf(buf->writableData(), buf->length(), gen);
+}
+
+void checkBuf(const uint8_t* buf, uint32_t length, boost::mt19937& gen) {
+  // Rather than using EXPECT_EQ() to check each character,
+  // count the number of differences and the first character that differs.
+  // This way on error we'll report just that information, rather than tons of
+  // failed checks for each byte in the buffer.
+  uint32_t numDifferences = 0;
+  uint32_t firstDiffIndex = 0;
+  uint8_t firstDiffExpected = 0;
+  for (uint32_t n = 0; n < length; ++n) {
+    uint8_t expected = static_cast<uint8_t>(gen() & 0xff);
+    if (buf[n] == expected) {
+      continue;
+    }
+
+    if (numDifferences == 0) {
+      firstDiffIndex = n;
+      firstDiffExpected = expected;
+    }
+    ++numDifferences;
+  }
+
+  EXPECT_EQ(0, numDifferences);
+  if (numDifferences > 0) {
+    // Cast to int so it will be printed numerically
+    // rather than as a char if the check fails
+    EXPECT_EQ(static_cast<int>(buf[firstDiffIndex]),
+              static_cast<int>(firstDiffExpected));
+  }
+}
+
+void checkBuf(IOBuf* buf, boost::mt19937& gen) {
+  checkBuf(buf->data(), buf->length(), gen);
+}
+
+void checkBuf(ByteRange buf, boost::mt19937& gen) {
+  checkBuf(buf.data(), buf.size(), gen);
+}
+
+void checkChain(IOBuf* buf, boost::mt19937& gen) {
+  IOBuf *current = buf;
+  do {
+    checkBuf(current->data(), current->length(), gen);
+    current = current->next();
+  } while (current != buf);
+}
+
+TEST(IOBuf, Chaining) {
+  uint32_t fillSeed = 0x12345678;
+  boost::mt19937 gen(fillSeed);
+
+  // An IOBuf with external storage
+  uint32_t headroom = 123;
+  unique_ptr<IOBuf> iob1(IOBuf::create(2048));
+  iob1->advance(headroom);
+  iob1->append(1500);
+  fillBuf(iob1.get(), gen);
+
+  // An IOBuf with internal storage
+  unique_ptr<IOBuf> iob2(IOBuf::create(20));
+  iob2->append(20);
+  fillBuf(iob2.get(), gen);
+
+  // An IOBuf around a buffer it doesn't own
+  uint8_t localbuf[1234];
+  fillBuf(localbuf, 1234, gen);
+  unique_ptr<IOBuf> iob3(IOBuf::wrapBuffer(localbuf, sizeof(localbuf)));
+
+  // An IOBuf taking ownership of a user-supplied buffer
+  uint32_t heapBufSize = 900;
+  uint8_t* heapBuf = static_cast<uint8_t*>(malloc(heapBufSize));
+  fillBuf(heapBuf, heapBufSize, gen);
+  unique_ptr<IOBuf> iob4(IOBuf::takeOwnership(heapBuf, heapBufSize));
+
+  // An IOBuf taking ownership of a user-supplied buffer with
+  // a custom free function
+  uint32_t arrayBufSize = 321;
+  uint8_t* arrayBuf = new uint8_t[arrayBufSize];
+  fillBuf(arrayBuf, arrayBufSize, gen);
+  uint32_t arrayBufFreeCount = 0;
+  unique_ptr<IOBuf> iob5(IOBuf::takeOwnership(arrayBuf, arrayBufSize,
+                                              deleteArrayBuffer,
+                                              &arrayBufFreeCount));
+
+  EXPECT_FALSE(iob1->isChained());
+  EXPECT_FALSE(iob2->isChained());
+  EXPECT_FALSE(iob3->isChained());
+  EXPECT_FALSE(iob4->isChained());
+  EXPECT_FALSE(iob5->isChained());
+
+  EXPECT_FALSE(iob1->isSharedOne());
+  EXPECT_FALSE(iob2->isSharedOne());
+  EXPECT_TRUE(iob3->isSharedOne()); // since we own the buffer
+  EXPECT_FALSE(iob4->isSharedOne());
+  EXPECT_FALSE(iob5->isSharedOne());
+
+  // Chain the buffers all together
+  // Since we are going to relinquish ownership of iob2-5 to the chain,
+  // store raw pointers to them so we can reference them later.
+  IOBuf* iob2ptr = iob2.get();
+  IOBuf* iob3ptr = iob3.get();
+  IOBuf* iob4ptr = iob4.get();
+  IOBuf* iob5ptr = iob5.get();
+
+  iob1->prependChain(std::move(iob2));
+  iob1->prependChain(std::move(iob4));
+  iob2ptr->appendChain(std::move(iob3));
+  iob1->prependChain(std::move(iob5));
+
+  EXPECT_EQ(iob2ptr, iob1->next());
+  EXPECT_EQ(iob3ptr, iob2ptr->next());
+  EXPECT_EQ(iob4ptr, iob3ptr->next());
+  EXPECT_EQ(iob5ptr, iob4ptr->next());
+  EXPECT_EQ(iob1.get(), iob5ptr->next());
+
+  EXPECT_EQ(iob5ptr, iob1->prev());
+  EXPECT_EQ(iob1.get(), iob2ptr->prev());
+  EXPECT_EQ(iob2ptr, iob3ptr->prev());
+  EXPECT_EQ(iob3ptr, iob4ptr->prev());
+  EXPECT_EQ(iob4ptr, iob5ptr->prev());
+
+  EXPECT_TRUE(iob1->isChained());
+  EXPECT_TRUE(iob2ptr->isChained());
+  EXPECT_TRUE(iob3ptr->isChained());
+  EXPECT_TRUE(iob4ptr->isChained());
+  EXPECT_TRUE(iob5ptr->isChained());
+
+  uint64_t fullLength = (iob1->length() + iob2ptr->length() +
+                         iob3ptr->length() + iob4ptr->length() +
+                        iob5ptr->length());
+  EXPECT_EQ(5, iob1->countChainElements());
+  EXPECT_EQ(fullLength, iob1->computeChainDataLength());
+
+  // Since iob3 is shared, the entire buffer should report itself as shared
+  EXPECT_TRUE(iob1->isShared());
+  // Unshare just iob3
+  iob3ptr->unshareOne();
+  EXPECT_FALSE(iob3ptr->isSharedOne());
+  // Now everything in the chain should be unshared.
+  // Check on all members of the chain just for good measure
+  EXPECT_FALSE(iob1->isShared());
+  EXPECT_FALSE(iob2ptr->isShared());
+  EXPECT_FALSE(iob3ptr->isShared());
+  EXPECT_FALSE(iob4ptr->isShared());
+  EXPECT_FALSE(iob5ptr->isShared());
+
+  // Check iteration
+  gen.seed(fillSeed);
+  size_t count = 0;
+  for (auto buf : *iob1) {
+    checkBuf(buf, gen);
+    ++count;
+  }
+  EXPECT_EQ(5, count);
+
+  // Clone one of the IOBufs in the chain
+  unique_ptr<IOBuf> iob4clone = iob4ptr->cloneOne();
+  gen.seed(fillSeed);
+  checkBuf(iob1.get(), gen);
+  checkBuf(iob2ptr, gen);
+  checkBuf(iob3ptr, gen);
+  checkBuf(iob4clone.get(), gen);
+  checkBuf(iob5ptr, gen);
+
+  EXPECT_TRUE(iob1->isShared());
+  EXPECT_TRUE(iob2ptr->isShared());
+  EXPECT_TRUE(iob3ptr->isShared());
+  EXPECT_TRUE(iob4ptr->isShared());
+  EXPECT_TRUE(iob5ptr->isShared());
+
+  EXPECT_FALSE(iob1->isSharedOne());
+  EXPECT_FALSE(iob2ptr->isSharedOne());
+  EXPECT_FALSE(iob3ptr->isSharedOne());
+  EXPECT_TRUE(iob4ptr->isSharedOne());
+  EXPECT_FALSE(iob5ptr->isSharedOne());
+
+  // Unshare that clone
+  EXPECT_TRUE(iob4clone->isSharedOne());
+  iob4clone->unshare();
+  EXPECT_FALSE(iob4clone->isSharedOne());
+  EXPECT_FALSE(iob4ptr->isSharedOne());
+  EXPECT_FALSE(iob1->isShared());
+  iob4clone.reset();
+
+
+  // Create a clone of a different IOBuf
+  EXPECT_FALSE(iob1->isShared());
+  EXPECT_FALSE(iob3ptr->isSharedOne());
+
+  unique_ptr<IOBuf> iob3clone = iob3ptr->cloneOne();
+  gen.seed(fillSeed);
+  checkBuf(iob1.get(), gen);
+  checkBuf(iob2ptr, gen);
+  checkBuf(iob3clone.get(), gen);
+  checkBuf(iob4ptr, gen);
+  checkBuf(iob5ptr, gen);
+
+  EXPECT_TRUE(iob1->isShared());
+  EXPECT_TRUE(iob3ptr->isSharedOne());
+  EXPECT_FALSE(iob1->isSharedOne());
+
+  // Delete the clone and make sure the original is unshared
+  iob3clone.reset();
+  EXPECT_FALSE(iob1->isShared());
+  EXPECT_FALSE(iob3ptr->isSharedOne());
+
+
+  // Clone the entire chain
+  unique_ptr<IOBuf> chainClone = iob1->clone();
+  // Verify that the data is correct.
+  EXPECT_EQ(fullLength, chainClone->computeChainDataLength());
+  gen.seed(fillSeed);
+  checkChain(chainClone.get(), gen);
+
+  // Check that the buffers report sharing correctly
+  EXPECT_TRUE(chainClone->isShared());
+  EXPECT_TRUE(iob1->isShared());
+
+  EXPECT_TRUE(iob1->isSharedOne());
+  EXPECT_TRUE(iob2ptr->isSharedOne());
+  EXPECT_TRUE(iob3ptr->isSharedOne());
+  EXPECT_TRUE(iob4ptr->isSharedOne());
+  EXPECT_TRUE(iob5ptr->isSharedOne());
+
+  // Unshare the cloned chain
+  chainClone->unshare();
+  EXPECT_FALSE(chainClone->isShared());
+  EXPECT_FALSE(iob1->isShared());
+
+  // Make sure the unshared result still has the same data
+  EXPECT_EQ(fullLength, chainClone->computeChainDataLength());
+  gen.seed(fillSeed);
+  checkChain(chainClone.get(), gen);
+
+  // Destroy this chain
+  chainClone.reset();
+
+
+  // Clone a new chain
+  EXPECT_FALSE(iob1->isShared());
+  chainClone = iob1->clone();
+  EXPECT_TRUE(iob1->isShared());
+  EXPECT_TRUE(chainClone->isShared());
+
+  // Delete the original chain
+  iob1.reset();
+  EXPECT_FALSE(chainClone->isShared());
+
+  // Coalesce the chain
+  //
+  // Coalescing this chain will create a new buffer and release the last
+  // refcount on the original buffers we created.  Also make sure
+  // that arrayBufFreeCount increases to one to indicate that arrayBuf was
+  // freed.
+  EXPECT_EQ(5, chainClone->countChainElements());
+  EXPECT_EQ(0, arrayBufFreeCount);
+
+  // Buffer lengths: 1500 20 1234 900 321
+  // Attempting to gather more data than available should fail
+  EXPECT_THROW(chainClone->gather(4000), std::overflow_error);
+  // Coalesce the first 3 buffers
+  chainClone->gather(1521);
+  EXPECT_EQ(3, chainClone->countChainElements());
+  EXPECT_EQ(0, arrayBufFreeCount);
+
+  // Make sure the data is still the same after coalescing
+  EXPECT_EQ(fullLength, chainClone->computeChainDataLength());
+  gen.seed(fillSeed);
+  checkChain(chainClone.get(), gen);
+
+  // Coalesce the entire chain
+  chainClone->coalesce();
+  EXPECT_EQ(1, chainClone->countChainElements());
+  EXPECT_EQ(1, arrayBufFreeCount);
+
+  // Make sure the data is still the same after coalescing
+  EXPECT_EQ(fullLength, chainClone->computeChainDataLength());
+  gen.seed(fillSeed);
+  checkChain(chainClone.get(), gen);
+
+  // Make a new chain to test the unlink and pop operations
+  iob1 = IOBuf::create(1);
+  iob1->append(1);
+  IOBuf *iob1ptr = iob1.get();
+  iob2 = IOBuf::create(3);
+  iob2->append(3);
+  iob2ptr = iob2.get();
+  iob3 = IOBuf::create(5);
+  iob3->append(5);
+  iob3ptr = iob3.get();
+  iob4 = IOBuf::create(7);
+  iob4->append(7);
+  iob4ptr = iob4.get();
+  iob1->appendChain(std::move(iob2));
+  iob1->prev()->appendChain(std::move(iob3));
+  iob1->prev()->appendChain(std::move(iob4));
+  EXPECT_EQ(4, iob1->countChainElements());
+  EXPECT_EQ(16, iob1->computeChainDataLength());
+
+  // Unlink from the middle of the chain
+  iob3 = iob3ptr->unlink();
+  EXPECT_TRUE(iob3.get() == iob3ptr);
+  EXPECT_EQ(3, iob1->countChainElements());
+  EXPECT_EQ(11, iob1->computeChainDataLength());
+
+  // Unlink from the end of the chain
+  iob4 = iob1->prev()->unlink();
+  EXPECT_TRUE(iob4.get() == iob4ptr);
+  EXPECT_EQ(2, iob1->countChainElements());
+  EXPECT_TRUE(iob1->next() == iob2ptr);
+  EXPECT_EQ(4, iob1->computeChainDataLength());
+
+  // Pop from the front of the chain
+  iob2 = iob1->pop();
+  EXPECT_TRUE(iob1.get() == iob1ptr);
+  EXPECT_EQ(1, iob1->countChainElements());
+  EXPECT_EQ(1, iob1->computeChainDataLength());
+  EXPECT_TRUE(iob2.get() == iob2ptr);
+  EXPECT_EQ(1, iob2->countChainElements());
+  EXPECT_EQ(3, iob2->computeChainDataLength());
+}
+
+void testFreeFn(void* buffer, void* ptr) {
+  uint32_t* freeCount = static_cast<uint32_t*>(ptr);;
+  delete[] static_cast<uint8_t*>(buffer);
+  if (freeCount) {
+    ++(*freeCount);
+  }
+};
+
+TEST(IOBuf, Reserve) {
+  uint32_t fillSeed = 0x23456789;
+  boost::mt19937 gen(fillSeed);
+
+  // Reserve does nothing if empty and doesn't have to grow the buffer
+  {
+    gen.seed(fillSeed);
+    unique_ptr<IOBuf> iob(IOBuf::create(2000));
+    EXPECT_EQ(0, iob->headroom());
+    const void* p1 = iob->buffer();
+    iob->reserve(5, 15);
+    EXPECT_LE(5, iob->headroom());
+    EXPECT_EQ(p1, iob->buffer());
+  }
+
+  // Reserve doesn't reallocate if we have enough total room
+  {
+    gen.seed(fillSeed);
+    unique_ptr<IOBuf> iob(IOBuf::create(2000));
+    iob->append(100);
+    fillBuf(iob.get(), gen);
+    EXPECT_EQ(0, iob->headroom());
+    EXPECT_EQ(100, iob->length());
+    const void* p1 = iob->buffer();
+    const uint8_t* d1 = iob->data();
+    iob->reserve(100, 1800);
+    EXPECT_LE(100, iob->headroom());
+    EXPECT_EQ(p1, iob->buffer());
+    EXPECT_EQ(d1 + 100, iob->data());
+    gen.seed(fillSeed);
+    checkBuf(iob.get(), gen);
+  }
+
+  // Reserve reallocates if we don't have enough total room.
+  // NOTE that, with jemalloc, we know that this won't reallocate in place
+  // as the size is less than jemallocMinInPlaceExpanadable
+  {
+    gen.seed(fillSeed);
+    unique_ptr<IOBuf> iob(IOBuf::create(2000));
+    iob->append(100);
+    fillBuf(iob.get(), gen);
+    EXPECT_EQ(0, iob->headroom());
+    EXPECT_EQ(100, iob->length());
+    const void* p1 = iob->buffer();
+    const uint8_t* d1 = iob->data();
+    iob->reserve(100, 2512);  // allocation sizes are multiples of 256
+    EXPECT_LE(100, iob->headroom());
+    if (folly::usingJEMalloc()) {
+      EXPECT_NE(p1, iob->buffer());
+    }
+    gen.seed(fillSeed);
+    checkBuf(iob.get(), gen);
+  }
+
+  // Test reserve from internal buffer, this used to segfault
+  {
+    unique_ptr<IOBuf> iob(IOBuf::create(0));
+    iob->reserve(0, 2000);
+    EXPECT_EQ(0, iob->headroom());
+    EXPECT_LE(2000, iob->tailroom());
+  }
+
+  // Test reserving from a user-allocated buffer.
+  {
+    uint8_t* buf = static_cast<uint8_t*>(malloc(100));
+    auto iob = IOBuf::takeOwnership(buf, 100);
+    iob->reserve(0, 2000);
+    EXPECT_EQ(0, iob->headroom());
+    EXPECT_LE(2000, iob->tailroom());
+  }
+
+  // Test reserving from a user-allocated with a custom free function.
+  {
+    uint32_t freeCount{0};
+    uint8_t* buf = new uint8_t[100];
+    auto iob = IOBuf::takeOwnership(buf, 100, testFreeFn, &freeCount);
+    iob->reserve(0, 2000);
+    EXPECT_EQ(0, iob->headroom());
+    EXPECT_LE(2000, iob->tailroom());
+    EXPECT_EQ(1, freeCount);
+  }
+}
+
+TEST(IOBuf, copyBuffer) {
+  std::string s("hello");
+  auto buf = IOBuf::copyBuffer(s.data(), s.size(), 1, 2);
+  EXPECT_EQ(1, buf->headroom());
+  EXPECT_EQ(s, std::string(reinterpret_cast<const char*>(buf->data()),
+                           buf->length()));
+  EXPECT_LE(2, buf->tailroom());
+
+  buf = IOBuf::copyBuffer(s, 5, 7);
+  EXPECT_EQ(5, buf->headroom());
+  EXPECT_EQ(s, std::string(reinterpret_cast<const char*>(buf->data()),
+                           buf->length()));
+  EXPECT_LE(7, buf->tailroom());
+
+  std::string empty;
+  buf = IOBuf::copyBuffer(empty, 3, 6);
+  EXPECT_EQ(3, buf->headroom());
+  EXPECT_EQ(0, buf->length());
+  EXPECT_LE(6, buf->tailroom());
+
+  // A stack-allocated version
+  IOBuf stackBuf(IOBuf::COPY_BUFFER, s, 1, 2);
+  EXPECT_EQ(1, stackBuf.headroom());
+  EXPECT_EQ(s, std::string(reinterpret_cast<const char*>(stackBuf.data()),
+                           stackBuf.length()));
+  EXPECT_LE(2, stackBuf.tailroom());
+}
+
+TEST(IOBuf, maybeCopyBuffer) {
+  std::string s("this is a test");
+  auto buf = IOBuf::maybeCopyBuffer(s, 1, 2);
+  EXPECT_EQ(1, buf->headroom());
+  EXPECT_EQ(s, std::string(reinterpret_cast<const char*>(buf->data()),
+                           buf->length()));
+  EXPECT_LE(2, buf->tailroom());
+
+  std::string empty;
+  buf = IOBuf::maybeCopyBuffer("", 5, 7);
+  EXPECT_EQ(nullptr, buf.get());
+
+  buf = IOBuf::maybeCopyBuffer("");
+  EXPECT_EQ(nullptr, buf.get());
+}
+
+namespace {
+
+int customDeleterCount = 0;
+int destructorCount = 0;
+struct OwnershipTestClass {
+  explicit OwnershipTestClass(int v = 0) : val(v) { }
+  ~OwnershipTestClass() {
+    ++destructorCount;
+  }
+  int val;
+};
+
+typedef std::function<void(OwnershipTestClass*)> CustomDeleter;
+
+void customDelete(OwnershipTestClass* p) {
+  ++customDeleterCount;
+  delete p;
+}
+
+void customDeleteArray(OwnershipTestClass* p) {
+  ++customDeleterCount;
+  delete[] p;
+}
+
+}  // namespace
+
+TEST(IOBuf, takeOwnershipUniquePtr) {
+  destructorCount = 0;
+  {
+    std::unique_ptr<OwnershipTestClass> p(new OwnershipTestClass());
+  }
+  EXPECT_EQ(1, destructorCount);
+
+  destructorCount = 0;
+  {
+    std::unique_ptr<OwnershipTestClass[]> p(new OwnershipTestClass[2]);
+  }
+  EXPECT_EQ(2, destructorCount);
+
+  destructorCount = 0;
+  {
+    std::unique_ptr<OwnershipTestClass> p(new OwnershipTestClass());
+    std::unique_ptr<IOBuf> buf(IOBuf::takeOwnership(std::move(p)));
+    EXPECT_EQ(sizeof(OwnershipTestClass), buf->length());
+    EXPECT_EQ(0, destructorCount);
+  }
+  EXPECT_EQ(1, destructorCount);
+
+  destructorCount = 0;
+  {
+    std::unique_ptr<OwnershipTestClass[]> p(new OwnershipTestClass[2]);
+    std::unique_ptr<IOBuf> buf(IOBuf::takeOwnership(std::move(p), 2));
+    EXPECT_EQ(2 * sizeof(OwnershipTestClass), buf->length());
+    EXPECT_EQ(0, destructorCount);
+  }
+  EXPECT_EQ(2, destructorCount);
+
+  customDeleterCount = 0;
+  destructorCount = 0;
+  {
+    std::unique_ptr<OwnershipTestClass, CustomDeleter>
+      p(new OwnershipTestClass(), customDelete);
+    std::unique_ptr<IOBuf> buf(IOBuf::takeOwnership(std::move(p)));
+    EXPECT_EQ(sizeof(OwnershipTestClass), buf->length());
+    EXPECT_EQ(0, destructorCount);
+  }
+  EXPECT_EQ(1, destructorCount);
+  EXPECT_EQ(1, customDeleterCount);
+
+  customDeleterCount = 0;
+  destructorCount = 0;
+  {
+    std::unique_ptr<OwnershipTestClass[], CustomDeleter>
+      p(new OwnershipTestClass[2], CustomDeleter(customDeleteArray));
+    std::unique_ptr<IOBuf> buf(IOBuf::takeOwnership(std::move(p), 2));
+    EXPECT_EQ(2 * sizeof(OwnershipTestClass), buf->length());
+    EXPECT_EQ(0, destructorCount);
+  }
+  EXPECT_EQ(2, destructorCount);
+  EXPECT_EQ(1, customDeleterCount);
+}
+
+TEST(IOBuf, Alignment) {
+  // max_align_t doesn't exist in gcc 4.6.2
+  struct MaxAlign {
+    char c;
+  } __attribute__((aligned));
+  size_t alignment = alignof(MaxAlign);
+
+  std::vector<size_t> sizes {0, 1, 64, 256, 1024, 1 << 10};
+  for (size_t size : sizes) {
+    auto buf = IOBuf::create(size);
+    uintptr_t p = reinterpret_cast<uintptr_t>(buf->data());
+    EXPECT_EQ(0, p & (alignment - 1)) << "size=" << size;
+  }
+}
+
+TEST(TypedIOBuf, Simple) {
+  auto buf = IOBuf::create(0);
+  TypedIOBuf<uint64_t> typed(buf.get());
+  const uint64_t n = 10000;
+  typed.reserve(0, n);
+  EXPECT_LE(n, typed.capacity());
+  for (uint64_t i = 0; i < n; i++) {
+    *typed.writableTail() = i;
+    typed.append(1);
+  }
+  EXPECT_EQ(n, typed.length());
+  for (uint64_t i = 0; i < n; i++) {
+    EXPECT_EQ(i, typed.data()[i]);
+  }
+}
+enum BufType {
+  CREATE,
+  TAKE_OWNERSHIP_MALLOC,
+  TAKE_OWNERSHIP_CUSTOM,
+  USER_OWNED,
+};
+
+// chain element size, number of elements in chain, shared
+class MoveToFbStringTest
+  : public ::testing::TestWithParam<std::tr1::tuple<int, int, bool, BufType>> {
+ protected:
+  void SetUp() {
+    std::tr1::tie(elementSize_, elementCount_, shared_, type_) = GetParam();
+    buf_ = makeBuf();
+    for (int i = 0; i < elementCount_ - 1; ++i) {
+      buf_->prependChain(makeBuf());
+    }
+    EXPECT_EQ(elementCount_, buf_->countChainElements());
+    EXPECT_EQ(elementCount_ * elementSize_, buf_->computeChainDataLength());
+    if (shared_) {
+      buf2_ = buf_->clone();
+      EXPECT_EQ(elementCount_, buf2_->countChainElements());
+      EXPECT_EQ(elementCount_ * elementSize_, buf2_->computeChainDataLength());
+    }
+  }
+
+  std::unique_ptr<IOBuf> makeBuf() {
+    unique_ptr<IOBuf> buf;
+    switch (type_) {
+      case CREATE:
+        buf = IOBuf::create(elementSize_);
+        buf->append(elementSize_);
+        break;
+      case TAKE_OWNERSHIP_MALLOC: {
+        void* data = malloc(elementSize_);
+        if (!data) {
+          throw std::bad_alloc();
+        }
+        buf = IOBuf::takeOwnership(data, elementSize_);
+        break;
+      }
+      case TAKE_OWNERSHIP_CUSTOM: {
+        uint8_t* data = new uint8_t[elementSize_];
+        buf = IOBuf::takeOwnership(data, elementSize_, testFreeFn);
+        break;
+      }
+      case USER_OWNED: {
+        unique_ptr<uint8_t[]> data(new uint8_t[elementSize_]);
+        buf = IOBuf::wrapBuffer(data.get(), elementSize_);
+        ownedBuffers_.emplace_back(std::move(data));
+        break;
+      }
+      default:
+        throw std::invalid_argument("unexpected buffer type parameter");
+        break;
+    }
+    memset(buf->writableData(), 'x', elementSize_);
+    return buf;
+  }
+
+  void check(std::unique_ptr<IOBuf>& buf) {
+    fbstring str = buf->moveToFbString();
+    EXPECT_EQ(elementCount_ * elementSize_, str.size());
+    EXPECT_EQ(elementCount_ * elementSize_, strspn(str.c_str(), "x"));
+    EXPECT_EQ(0, buf->length());
+    EXPECT_EQ(1, buf->countChainElements());
+    EXPECT_EQ(0, buf->computeChainDataLength());
+    EXPECT_FALSE(buf->isChained());
+  }
+
+  int elementSize_;
+  int elementCount_;
+  bool shared_;
+  BufType type_;
+  std::unique_ptr<IOBuf> buf_;
+  std::unique_ptr<IOBuf> buf2_;
+  std::vector<std::unique_ptr<uint8_t[]>> ownedBuffers_;
+};
+
+TEST_P(MoveToFbStringTest, Simple) {
+  check(buf_);
+  if (shared_) {
+    check(buf2_);
+  }
+}
+
+INSTANTIATE_TEST_CASE_P(
+    MoveToFbString,
+    MoveToFbStringTest,
+    ::testing::Combine(
+        ::testing::Values(0, 1, 24, 256, 1 << 10, 1 << 20),  // element size
+        ::testing::Values(1, 2, 10),                         // element count
+        ::testing::Bool(),                                   // shared
+        ::testing::Values(CREATE, TAKE_OWNERSHIP_MALLOC,
+                          TAKE_OWNERSHIP_CUSTOM, USER_OWNED)));
+
+TEST(IOBuf, getIov) {
+  uint32_t fillSeed = 0xdeadbeef;
+  boost::mt19937 gen(fillSeed);
+
+  size_t len = 4096;
+  size_t count = 32;
+  auto buf = IOBuf::create(len + 1);
+  buf->append(rand() % len + 1);
+  fillBuf(buf.get(), gen);
+
+  for (size_t i = 0; i < count - 1; i++) {
+    auto buf2 = IOBuf::create(len + 1);
+    buf2->append(rand() % len + 1);
+    fillBuf(buf2.get(), gen);
+    buf->prependChain(std::move(buf2));
+  }
+  EXPECT_EQ(count, buf->countChainElements());
+
+  auto iov = buf->getIov();
+  EXPECT_EQ(count, iov.size());
+
+  IOBuf const* p = buf.get();
+  for (size_t i = 0; i < count; i++, p = p->next()) {
+    EXPECT_EQ(p->data(), iov[i].iov_base);
+    EXPECT_EQ(p->length(), iov[i].iov_len);
+  }
+
+  // an empty buf should be skipped in the iov.
+  buf->next()->clear();
+  iov = buf->getIov();
+  EXPECT_EQ(count - 1, iov.size());
+  EXPECT_EQ(buf->next()->next()->data(), iov[1].iov_base);
+
+  // same for the first one being empty
+  buf->clear();
+  iov = buf->getIov();
+  EXPECT_EQ(count - 2, iov.size());
+  EXPECT_EQ(buf->next()->next()->data(), iov[0].iov_base);
+
+  // and the last one
+  buf->prev()->clear();
+  iov = buf->getIov();
+  EXPECT_EQ(count - 3, iov.size());
+}
+
+TEST(IOBuf, move) {
+  // Default allocate an IOBuf on the stack
+  IOBuf outerBuf;
+  char data[] = "foobar";
+  uint32_t length = sizeof(data);
+  uint32_t actualCapacity{0};
+  const void* ptr{nullptr};
+
+  {
+    // Create a small IOBuf on the stack.
+    // Note that IOBufs created on the stack always use an external buffer.
+    IOBuf b1(IOBuf::CREATE, 10);
+    actualCapacity = b1.capacity();
+    EXPECT_GE(actualCapacity, 10);
+    EXPECT_EQ(0, b1.length());
+    EXPECT_FALSE(b1.isShared());
+    ptr = b1.data();
+    ASSERT_TRUE(ptr != nullptr);
+    memcpy(b1.writableTail(), data, length);
+    b1.append(length);
+    EXPECT_EQ(length, b1.length());
+
+    // Use the move constructor
+    IOBuf b2(std::move(b1));
+    EXPECT_EQ(ptr, b2.data());
+    EXPECT_EQ(length, b2.length());
+    EXPECT_EQ(actualCapacity, b2.capacity());
+    EXPECT_FALSE(b2.isShared());
+
+    // Use the move assignment operator
+    outerBuf = std::move(b2);
+    // Close scope, destroying b1 and b2
+    // (which are both be invalid now anyway after moving out of them)
+  }
+
+  EXPECT_EQ(ptr, outerBuf.data());
+  EXPECT_EQ(length, outerBuf.length());
+  EXPECT_EQ(actualCapacity, outerBuf.capacity());
+  EXPECT_FALSE(outerBuf.isShared());
+}
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+
+  return RUN_ALL_TESTS();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/test/NetworkBenchmark.cpp
@@ -0,0 +1,172 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/io/IOBuf.h"
+
+#include <gflags/gflags.h>
+#include "folly/Benchmark.h"
+#include "folly/io/Cursor.h"
+
+#include <vector>
+
+using folly::IOBuf;
+using std::unique_ptr;
+using namespace folly::io;
+using namespace std;
+
+size_t buf_size = 0;
+size_t num_bufs = 0;
+
+BENCHMARK(reserveBenchmark, iters) {
+  while (iters--) {
+    unique_ptr<IOBuf> iobuf1(IOBuf::create(buf_size));
+    iobuf1->append(buf_size);
+    for (size_t bufs = num_bufs; bufs > 1; bufs --) {
+      iobuf1->reserve(0, buf_size);
+      iobuf1->append(buf_size);
+    }
+  }
+}
+
+BENCHMARK(chainBenchmark, iters) {
+  while (iters--) {
+    unique_ptr<IOBuf> iobuf1(IOBuf::create(buf_size));
+    iobuf1->append(buf_size);
+    for (size_t bufs = num_bufs; bufs > 1; bufs --) {
+      unique_ptr<IOBuf> iobufNext(IOBuf::create(buf_size));
+      iobuf1->prependChain(std::move(iobufNext));
+    }
+  }
+}
+
+vector<unique_ptr<IOBuf>> bufPool;
+inline unique_ptr<IOBuf> poolGetIOBuf() {
+  if (bufPool.size() > 0) {
+    unique_ptr<IOBuf> ret = std::move(bufPool.back());
+    bufPool.pop_back();
+    return std::move(ret);
+  } else {
+    unique_ptr<IOBuf> iobuf(IOBuf::create(buf_size));
+    iobuf->append(buf_size);
+    return std::move(iobuf);
+  }
+}
+
+inline void poolPutIOBuf(unique_ptr<IOBuf>&& buf) {
+  unique_ptr<IOBuf> head = std::move(buf);
+  while (head) {
+    unique_ptr<IOBuf> next = std::move(head->pop());
+    bufPool.push_back(std::move(head));
+    head = std::move(next);
+  }
+}
+
+BENCHMARK(poolBenchmark, iters) {
+  while (iters--) {
+    unique_ptr<IOBuf> head = std::move(poolGetIOBuf());
+    for (size_t bufs = num_bufs; bufs > 1; bufs --) {
+      unique_ptr<IOBuf> iobufNext = std::move(poolGetIOBuf());
+      head->prependChain(std::move(iobufNext));
+    }
+    // cleanup
+    poolPutIOBuf(std::move(head));
+  }
+}
+
+void setNumbers(size_t size, size_t num) {
+  buf_size = size;
+  num_bufs = num;
+  bufPool.clear();
+
+  printf("\nBuffer size: %zu, number of buffers: %zu\n\n", size, num);
+}
+
+/*
+------------------------------------------------------------------------------
+reserveBenchmark                       100000  9.186 ms  91.86 ns  10.38 M
+chainBenchmark                         100000  59.44 ms  594.4 ns  1.604 M
+poolBenchmark                          100000  15.87 ms  158.7 ns   6.01 M
+
+Buffer size: 100, number of buffers: 10
+
+Benchmark                               Iters   Total t    t/iter iter/sec
+------------------------------------------------------------------------------
+reserveBenchmark                       100000     62 ms    620 ns  1.538 M
+chainBenchmark                         100000  59.48 ms  594.8 ns  1.603 M
+poolBenchmark                          100000  16.07 ms  160.7 ns  5.933 M
+
+Buffer size: 2048, number of buffers: 10
+
+Benchmark                               Iters   Total t    t/iter iter/sec
+------------------------------------------------------------------------------
+reserveBenchmark                       100000  148.4 ms  1.484 us  658.2 k
+chainBenchmark                         100000  140.9 ms  1.409 us    693 k
+poolBenchmark                          100000  16.73 ms  167.3 ns    5.7 M
+
+Buffer size: 10000, number of buffers: 10
+
+Benchmark                               Iters   Total t    t/iter iter/sec
+------------------------------------------------------------------------------
+reserveBenchmark                       100000    234 ms   2.34 us  417.3 k
+chainBenchmark                         100000  142.3 ms  1.423 us  686.1 k
+poolBenchmark                          100000  16.78 ms  167.8 ns  5.684 M
+
+Buffer size: 100000, number of buffers: 10
+
+Benchmark                               Iters   Total t    t/iter iter/sec
+------------------------------------------------------------------------------
+reserveBenchmark                       100000  186.5 ms  1.865 us  523.5 k
+chainBenchmark                         100000  360.5 ms  3.605 us  270.9 k
+poolBenchmark                          100000  16.52 ms  165.2 ns  5.772 M
+
+Buffer size: 1000000, number of buffers: 10
+
+Benchmark                               Iters   Total t    t/iter iter/sec
+------------------------------------------------------------------------------
+reserveBenchmark                          156  2.084 s   13.36 ms  74.84
+chainBenchmark                          30082  2.001 s    66.5 us  14.68 k
+poolBenchmark                          100000  18.18 ms  181.8 ns  5.244 M
+
+
+Buffer size: 10, number of buffers: 20
+
+Benchmark                               Iters   Total t    t/iter iter/sec
+------------------------------------------------------------------------------
+reserveBenchmark                       100000  12.54 ms  125.4 ns  7.603 M
+chainBenchmark                         100000  118.6 ms  1.186 us  823.2 k
+poolBenchmark                          100000   32.2 ms    322 ns  2.962 M
+*/
+int main(int argc, char** argv) {
+  google::ParseCommandLineFlags(&argc, &argv, true);
+
+  setNumbers(10, 10);
+  folly::runBenchmarks();
+  setNumbers(100, 10);
+  folly::runBenchmarks();
+  setNumbers(2048, 10);
+  folly::runBenchmarks();
+  setNumbers(10000, 10);
+  folly::runBenchmarks();
+  setNumbers(100000, 10);
+  folly::runBenchmarks();
+  setNumbers(1000000, 10);
+  folly::runBenchmarks();
+
+  setNumbers(10, 20);
+  folly::runBenchmarks();
+
+  return 0;
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/test/RecordIOTest.cpp
@@ -0,0 +1,272 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/io/RecordIO.h"
+
+#include <sys/types.h>
+#include <unistd.h>
+
+#include <random>
+
+#include <gflags/gflags.h>
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+#include "folly/Conv.h"
+#include "folly/FBString.h"
+#include "folly/Random.h"
+#include "folly/experimental/TestUtil.h"
+#include "folly/io/IOBufQueue.h"
+
+DEFINE_int32(random_seed, folly::randomNumberSeed(), "random seed");
+
+namespace folly { namespace test {
+
+namespace {
+// shortcut
+StringPiece sp(ByteRange br) { return StringPiece(br); }
+
+template <class T>
+std::unique_ptr<IOBuf> iobufs(std::initializer_list<T> ranges) {
+  IOBufQueue queue;
+  for (auto& range : ranges) {
+    StringPiece r(range);
+    queue.append(IOBuf::wrapBuffer(r.data(), r.size()));
+  }
+  return queue.move();
+}
+
+}  // namespace
+
+TEST(RecordIOTest, Simple) {
+  TemporaryFile file;
+  {
+    RecordIOWriter writer(File(file.fd()));
+    writer.write(iobufs({"hello ", "world"}));
+    writer.write(iobufs({"goodbye"}));
+  }
+  {
+    RecordIOReader reader(File(file.fd()));
+    auto it = reader.begin();
+    ASSERT_FALSE(it == reader.end());
+    EXPECT_EQ("hello world", sp((it++)->first));
+    ASSERT_FALSE(it == reader.end());
+    EXPECT_EQ("goodbye", sp((it++)->first));
+    EXPECT_TRUE(it == reader.end());
+  }
+  {
+    RecordIOWriter writer(File(file.fd()));
+    writer.write(iobufs({"meow"}));
+    writer.write(iobufs({"woof"}));
+  }
+  {
+    RecordIOReader reader(File(file.fd()));
+    auto it = reader.begin();
+    ASSERT_FALSE(it == reader.end());
+    EXPECT_EQ("hello world", sp((it++)->first));
+    ASSERT_FALSE(it == reader.end());
+    EXPECT_EQ("goodbye", sp((it++)->first));
+    ASSERT_FALSE(it == reader.end());
+    EXPECT_EQ("meow", sp((it++)->first));
+    ASSERT_FALSE(it == reader.end());
+    EXPECT_EQ("woof", sp((it++)->first));
+    EXPECT_TRUE(it == reader.end());
+  }
+}
+
+TEST(RecordIOTest, SmallRecords) {
+  constexpr size_t kSize = 10;
+  char tmp[kSize];
+  memset(tmp, 'x', kSize);
+  TemporaryFile file;
+  {
+    RecordIOWriter writer(File(file.fd()));
+    for (int i = 0; i < kSize; ++i) {  // record of size 0 should be ignored
+      writer.write(IOBuf::wrapBuffer(tmp, i));
+    }
+  }
+  {
+    RecordIOReader reader(File(file.fd()));
+    auto it = reader.begin();
+    for (int i = 1; i < kSize; ++i) {
+      ASSERT_FALSE(it == reader.end());
+      EXPECT_EQ(StringPiece(tmp, i), sp((it++)->first));
+    }
+    EXPECT_TRUE(it == reader.end());
+  }
+}
+
+TEST(RecordIOTest, MultipleFileIds) {
+  TemporaryFile file;
+  {
+    RecordIOWriter writer(File(file.fd()), 1);
+    writer.write(iobufs({"hello"}));
+  }
+  {
+    RecordIOWriter writer(File(file.fd()), 2);
+    writer.write(iobufs({"world"}));
+  }
+  {
+    RecordIOWriter writer(File(file.fd()), 1);
+    writer.write(iobufs({"goodbye"}));
+  }
+  {
+    RecordIOReader reader(File(file.fd()), 0);  // return all
+    auto it = reader.begin();
+    ASSERT_FALSE(it == reader.end());
+    EXPECT_EQ("hello", sp((it++)->first));
+    ASSERT_FALSE(it == reader.end());
+    EXPECT_EQ("world", sp((it++)->first));
+    ASSERT_FALSE(it == reader.end());
+    EXPECT_EQ("goodbye", sp((it++)->first));
+    EXPECT_TRUE(it == reader.end());
+  }
+  {
+    RecordIOReader reader(File(file.fd()), 1);
+    auto it = reader.begin();
+    ASSERT_FALSE(it == reader.end());
+    EXPECT_EQ("hello", sp((it++)->first));
+    ASSERT_FALSE(it == reader.end());
+    EXPECT_EQ("goodbye", sp((it++)->first));
+    EXPECT_TRUE(it == reader.end());
+  }
+  {
+    RecordIOReader reader(File(file.fd()), 2);
+    auto it = reader.begin();
+    ASSERT_FALSE(it == reader.end());
+    EXPECT_EQ("world", sp((it++)->first));
+    EXPECT_TRUE(it == reader.end());
+  }
+  {
+    RecordIOReader reader(File(file.fd()), 3);
+    auto it = reader.begin();
+    EXPECT_TRUE(it == reader.end());
+  }
+}
+
+TEST(RecordIOTest, ExtraMagic) {
+  TemporaryFile file;
+  {
+    RecordIOWriter writer(File(file.fd()));
+    writer.write(iobufs({"hello"}));
+  }
+  uint8_t buf[recordio_helpers::headerSize() + 5];
+  EXPECT_EQ(0, lseek(file.fd(), 0, SEEK_SET));
+  EXPECT_EQ(sizeof(buf), read(file.fd(), buf, sizeof(buf)));
+  // Append an extra magic
+  const uint32_t magic = recordio_helpers::detail::Header::kMagic;
+  EXPECT_EQ(sizeof(magic), write(file.fd(), &magic, sizeof(magic)));
+  // and an extra record
+  EXPECT_EQ(sizeof(buf), write(file.fd(), buf, sizeof(buf)));
+  {
+    RecordIOReader reader(File(file.fd()));
+    auto it = reader.begin();
+    ASSERT_FALSE(it == reader.end());
+    EXPECT_EQ("hello", sp((it++)->first));
+    ASSERT_FALSE(it == reader.end());
+    EXPECT_EQ("hello", sp((it++)->first));
+    EXPECT_TRUE(it == reader.end());
+  }
+}
+
+namespace {
+void corrupt(int fd, off_t pos) {
+  uint8_t val = 0;
+  EXPECT_EQ(1, pread(fd, &val, 1, pos));
+  ++val;
+  EXPECT_EQ(1, pwrite(fd, &val, 1, pos));
+}
+}  // namespace
+
+TEST(RecordIOTest, Randomized) {
+  SCOPED_TRACE(to<std::string>("Random seed is ", FLAGS_random_seed));
+  std::mt19937 rnd(FLAGS_random_seed);
+
+  size_t recordCount =
+    std::uniform_int_distribution<uint32_t>(30, 300)(rnd);
+
+  std::uniform_int_distribution<uint32_t> recordSizeDist(1, 3 << 16);
+  std::uniform_int_distribution<uint32_t> charDist(0, 255);
+  std::uniform_int_distribution<uint32_t> junkDist(0, 1 << 20);
+  // corrupt 1/5 of all records
+  std::uniform_int_distribution<uint32_t> corruptDist(0, 4);
+
+  std::vector<std::pair<fbstring, off_t>> records;
+  std::vector<off_t> corruptPositions;
+  records.reserve(recordCount);
+  TemporaryFile file;
+
+  fbstring record;
+  // Recreate the writer multiple times so we test that we create a
+  // continuous stream
+  for (size_t i = 0; i < 3; ++i) {
+    RecordIOWriter writer(File(file.fd()));
+    for (size_t j = 0; j < recordCount; ++j) {
+      off_t beginPos = writer.filePos();
+      record.clear();
+      size_t recordSize = recordSizeDist(rnd);
+      record.reserve(recordSize);
+      for (size_t k = 0; k < recordSize; ++k) {
+        record.push_back(charDist(rnd));
+      }
+      writer.write(iobufs({record}));
+
+      bool corrupt = (corruptDist(rnd) == 0);
+      if (corrupt) {
+        // Corrupt one random byte in the record (including header)
+        std::uniform_int_distribution<uint32_t> corruptByteDist(
+            0, recordSize + recordio_helpers::headerSize() - 1);
+        off_t corruptRel = corruptByteDist(rnd);
+        VLOG(1) << "n=" << records.size() << " bpos=" << beginPos
+                << " rsize=" << record.size()
+                << " corrupt rel=" << corruptRel
+                << " abs=" << beginPos + corruptRel;
+        corruptPositions.push_back(beginPos + corruptRel);
+      } else {
+        VLOG(2) << "n=" << records.size() << " bpos=" << beginPos
+                << " rsize=" << record.size()
+                << " good";
+        records.emplace_back(std::move(record), beginPos);
+      }
+    }
+    VLOG(1) << "n=" << records.size() << " close abs=" << writer.filePos();
+  }
+
+  for (auto& pos : corruptPositions) {
+    corrupt(file.fd(), pos);
+  }
+
+  {
+    size_t i = 0;
+    RecordIOReader reader(File(file.fd()));
+    for (auto& r : reader) {
+      SCOPED_TRACE(i);
+      ASSERT_LT(i, records.size());
+      EXPECT_EQ(records[i].first, sp(r.first));
+      EXPECT_EQ(records[i].second, r.second);
+      ++i;
+    }
+    EXPECT_EQ(records.size(), i);
+  }
+}
+
+}}  // namespaces
+
+int main(int argc, char *argv[]) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  return RUN_ALL_TESTS();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/io/TypedIOBuf.h
@@ -0,0 +1,214 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_IO_TYPEDIOBUF_H_
+#define FOLLY_IO_TYPEDIOBUF_H_
+
+#include <algorithm>
+#include <iterator>
+#include <type_traits>
+
+#include "folly/Malloc.h"
+#include "folly/io/IOBuf.h"
+
+namespace folly {
+
+/**
+ * Wrapper class to handle a IOBuf as a typed buffer (to a standard layout
+ * class).
+ *
+ * This class punts on alignment, and assumes that you know what you're doing.
+ *
+ * All methods are wrappers around the corresponding IOBuf methods.  The
+ * TypedIOBuf object is stateless, so it's perfectly okay to access the
+ * underlying IOBuf in between TypedIOBuf method calls.
+ */
+template <class T>
+class TypedIOBuf {
+  static_assert(std::is_standard_layout<T>::value, "must be standard layout");
+ public:
+  typedef T value_type;
+  typedef value_type& reference;
+  typedef const value_type& const_reference;
+  typedef uint32_t size_type;
+  typedef value_type* iterator;
+  typedef const value_type* const_iterator;
+
+  explicit TypedIOBuf(IOBuf* buf) : buf_(buf) { }
+
+  IOBuf* ioBuf() {
+    return buf_;
+  }
+  const IOBuf* ioBuf() const {
+    return buf_;
+  }
+
+  bool empty() const {
+    return buf_->empty();
+  }
+  const T* data() const {
+    return cast(buf_->data());
+  }
+  T* writableData() {
+    return cast(buf_->writableData());
+  }
+  const T* tail() const {
+    return cast(buf_->tail());
+  }
+  T* writableTail() {
+    return cast(buf_->writableTail());
+  }
+  uint32_t length() const {
+    return sdiv(buf_->length());
+  }
+  uint32_t size() const { return length(); }
+
+  uint32_t headroom() const {
+    return sdiv(buf_->headroom());
+  }
+  uint32_t tailroom() const {
+    return sdiv(buf_->tailroom());
+  }
+  const T* buffer() const {
+    return cast(buf_->buffer());
+  }
+  T* writableBuffer() {
+    return cast(buf_->writableBuffer());
+  }
+  const T* bufferEnd() const {
+    return cast(buf_->bufferEnd());
+  }
+  uint32_t capacity() const {
+    return sdiv(buf_->capacity());
+  }
+  void advance(uint32_t n) {
+    buf_->advance(smul(n));
+  }
+  void retreat(uint32_t n) {
+    buf_->retreat(smul(n));
+  }
+  void prepend(uint32_t n) {
+    buf_->prepend(smul(n));
+  }
+  void append(uint32_t n) {
+    buf_->append(smul(n));
+  }
+  void trimStart(uint32_t n) {
+    buf_->trimStart(smul(n));
+  }
+  void trimEnd(uint32_t n) {
+    buf_->trimEnd(smul(n));
+  }
+  void clear() {
+    buf_->clear();
+  }
+  void reserve(uint32_t minHeadroom, uint32_t minTailroom) {
+    buf_->reserve(smul(minHeadroom), smul(minTailroom));
+  }
+  void reserve(uint32_t minTailroom) { reserve(0, minTailroom); }
+
+  const T* cbegin() const { return data(); }
+  const T* cend() const { return tail(); }
+  const T* begin() const { return cbegin(); }
+  const T* end() const { return cend(); }
+  T* begin() { return writableData(); }
+  T* end() { return writableTail(); }
+
+  const T& front() const {
+    assert(!empty());
+    return *begin();
+  }
+  T& front() {
+    assert(!empty());
+    return *begin();
+  }
+  const T& back() const {
+    assert(!empty());
+    return end()[-1];
+  }
+  T& back() {
+    assert(!empty());
+    return end()[-1];
+  }
+
+  /**
+   * Simple wrapper to make it easier to treat this TypedIOBuf as an array of
+   * T.
+   */
+  const T& operator[](ssize_t idx) const {
+    assert(idx >= 0 && idx < length());
+    return data()[idx];
+  }
+
+  /**
+   * Append one element.
+   */
+  void push(const T& data) {
+    push(&data, &data + 1);
+  }
+  void push_back(const T& data) { push(data); }
+
+  /**
+   * Append multiple elements in a sequence; will call distance().
+   */
+  template <class IT>
+  void push(IT begin, IT end) {
+    uint32_t n = std::distance(begin, end);
+    if (usingJEMalloc()) {
+      // Rely on rallocm() and avoid exponential growth to limit
+      // amount of memory wasted.
+      reserve(headroom(), n);
+    } else if (tailroom() < n) {
+      reserve(headroom(), std::max(n, 3 + size() / 2));
+    }
+    std::copy(begin, end, writableTail());
+    append(n);
+  }
+
+  // Movable
+  TypedIOBuf(TypedIOBuf&&) = default;
+  TypedIOBuf& operator=(TypedIOBuf&&) = default;
+
+ private:
+  // Non-copyable
+  TypedIOBuf(const TypedIOBuf&) = delete;
+  TypedIOBuf& operator=(const TypedIOBuf&) = delete;
+
+  // cast to T*
+  static T* cast(uint8_t* p) {
+    return reinterpret_cast<T*>(p);
+  }
+  static const T* cast(const uint8_t* p) {
+    return reinterpret_cast<const T*>(p);
+  }
+  // divide by size
+  static uint32_t sdiv(uint32_t n) {
+    return n / sizeof(T);
+  }
+  // multiply by size
+  static uint32_t smul(uint32_t n) {
+    // In debug mode, check for overflow
+    assert((uint64_t(n) * sizeof(T)) < (uint64_t(1) << 32));
+    return n * sizeof(T);
+  }
+
+  IOBuf* buf_;
+};
+
+}  // namespace folly
+
+#endif /* FOLLY_IO_TYPEDIOBUF_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/json.cpp
@@ -0,0 +1,765 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/json.h"
+#include <cassert>
+#include <boost/next_prior.hpp>
+#include <boost/algorithm/string.hpp>
+
+#include "folly/Range.h"
+#include "folly/Unicode.h"
+#include "folly/Conv.h"
+
+namespace folly {
+
+//////////////////////////////////////////////////////////////////////
+
+namespace json {
+namespace {
+
+char32_t decodeUtf8(
+    const unsigned char*& p,
+    const unsigned char* const e,
+    bool skipOnError) {
+  /* The following encodings are valid, except for the 5 and 6 byte
+   * combinations:
+   * 0xxxxxxx
+   * 110xxxxx 10xxxxxx
+   * 1110xxxx 10xxxxxx 10xxxxxx
+   * 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx
+   * 111110xx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx
+   * 1111110x 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx
+   */
+
+  auto skip = [&] { ++p; return U'\ufffd'; };
+
+  if (p >= e) {
+    if (skipOnError) return skip();
+    throw std::runtime_error("folly::decodeUtf8 empty/invalid string");
+  }
+
+  unsigned char fst = *p;
+  if (!(fst & 0x80)) {
+    // trivial case
+    return *p++;
+  }
+
+  static const uint32_t bitMask[] = {
+    (1 << 7) - 1,
+    (1 << 11) - 1,
+    (1 << 16) - 1,
+    (1 << 21) - 1
+  };
+
+  // upper control bits are masked out later
+  uint32_t d = fst;
+
+  if ((fst & 0xC0) != 0xC0) {
+    if (skipOnError) return skip();
+    throw std::runtime_error(to<std::string>("folly::decodeUtf8 i=0 d=", d));
+  }
+
+  fst <<= 1;
+
+  for (unsigned int i = 1; i != 3 && p + i < e; ++i) {
+    unsigned char tmp = p[i];
+
+    if ((tmp & 0xC0) != 0x80) {
+      if (skipOnError) return skip();
+      throw std::runtime_error(
+        to<std::string>("folly::decodeUtf8 i=", i, " tmp=", (uint32_t)tmp));
+    }
+
+    d = (d << 6) | (tmp & 0x3F);
+    fst <<= 1;
+
+    if (!(fst & 0x80)) {
+      d &= bitMask[i];
+
+      // overlong, could have been encoded with i bytes
+      if ((d & ~bitMask[i - 1]) == 0) {
+        if (skipOnError) return skip();
+        throw std::runtime_error(
+          to<std::string>("folly::decodeUtf8 i=", i, " d=", d));
+      }
+
+      // check for surrogates only needed for 3 bytes
+      if (i == 2) {
+        if ((d >= 0xD800 && d <= 0xDFFF) || d > 0x10FFFF) {
+          if (skipOnError) return skip();
+          throw std::runtime_error(
+            to<std::string>("folly::decodeUtf8 i=", i, " d=", d));
+        }
+      }
+
+      p += i + 1;
+      return d;
+    }
+  }
+
+  if (skipOnError) return skip();
+  throw std::runtime_error("folly::decodeUtf8 encoding length maxed out");
+}
+
+struct Printer {
+  explicit Printer(fbstring& out,
+                   unsigned* indentLevel,
+                   serialization_opts const* opts)
+    : out_(out)
+    , indentLevel_(indentLevel)
+    , opts_(*opts)
+  {}
+
+  void operator()(dynamic const& v) const {
+    switch (v.type()) {
+    case dynamic::DOUBLE:
+      if (!opts_.allow_nan_inf &&
+          (std::isnan(v.asDouble()) || std::isinf(v.asDouble()))) {
+        throw std::runtime_error("folly::toJson: JSON object value was a "
+          "NaN or INF");
+      }
+      toAppend(v.asDouble(), &out_, opts_.double_mode, opts_.double_num_digits);
+      break;
+    case dynamic::INT64: {
+      auto intval = v.asInt();
+      if (opts_.javascript_safe) {
+        // Use folly::to to check that this integer can be represented
+        // as a double without loss of precision.
+        intval = int64_t(to<double>(intval));
+      }
+      toAppend(intval, &out_);
+      break;
+    }
+    case dynamic::BOOL:
+      out_ += v.asBool() ? "true" : "false";
+      break;
+    case dynamic::NULLT:
+      out_ += "null";
+      break;
+    case dynamic::STRING:
+      escapeString(v.asString(), out_, opts_);
+      break;
+    case dynamic::OBJECT:
+      printObject(v);
+      break;
+    case dynamic::ARRAY:
+      printArray(v);
+      break;
+    default:
+      CHECK(0) << "Bad type " << v.type();
+    }
+  }
+
+private:
+  void printKV(const std::pair<const dynamic, dynamic>& p) const {
+    if (!opts_.allow_non_string_keys && !p.first.isString()) {
+      throw std::runtime_error("folly::toJson: JSON object key was not a "
+        "string");
+    }
+    (*this)(p.first);
+    mapColon();
+    (*this)(p.second);
+  }
+
+  template <typename Iterator>
+  void printKVPairs(Iterator begin, Iterator end) const {
+    printKV(*begin);
+    for (++begin; begin != end; ++begin) {
+      out_ += ',';
+      newline();
+      printKV(*begin);
+    }
+  }
+
+  void printObject(dynamic const& o) const {
+    if (o.empty()) {
+      out_ += "{}";
+      return;
+    }
+
+    out_ += '{';
+    indent();
+    newline();
+    if (opts_.sort_keys) {
+      std::vector<std::pair<dynamic, dynamic>> items(
+        o.items().begin(), o.items().end());
+      std::sort(items.begin(), items.end());
+      printKVPairs(items.begin(), items.end());
+    } else {
+      printKVPairs(o.items().begin(), o.items().end());
+    }
+    outdent();
+    newline();
+    out_ += '}';
+  }
+
+  void printArray(dynamic const& a) const {
+    if (a.empty()) {
+      out_ += "[]";
+      return;
+    }
+
+    out_ += '[';
+    indent();
+    newline();
+    (*this)(a[0]);
+    for (auto& val : range(boost::next(a.begin()), a.end())) {
+      out_ += ',';
+      newline();
+      (*this)(val);
+    }
+    outdent();
+    newline();
+    out_ += ']';
+  }
+
+private:
+  void outdent() const {
+    if (indentLevel_) {
+      --*indentLevel_;
+    }
+  }
+
+  void indent() const {
+    if (indentLevel_) {
+      ++*indentLevel_;
+    }
+  }
+
+  void newline() const {
+    if (indentLevel_) {
+      out_ += to<fbstring>('\n', fbstring(*indentLevel_ * 2, ' '));
+    }
+  }
+
+  void mapColon() const {
+    out_ += indentLevel_ ? " : " : ":";
+  }
+
+private:
+  fbstring& out_;
+  unsigned* const indentLevel_;
+  serialization_opts const& opts_;
+};
+
+//////////////////////////////////////////////////////////////////////
+
+struct ParseError : std::runtime_error {
+  explicit ParseError(int line)
+    : std::runtime_error(to<std::string>("json parse error on line ", line))
+  {}
+
+  explicit ParseError(int line, std::string const& context,
+      std::string const& expected)
+    : std::runtime_error(to<std::string>("json parse error on line ", line,
+        !context.empty() ? to<std::string>(" near `", context, '\'')
+                        : "",
+        ": ", expected))
+  {}
+
+  explicit ParseError(std::string const& what)
+    : std::runtime_error("json parse error: " + what)
+  {}
+};
+
+// Wraps our input buffer with some helper functions.
+struct Input {
+  explicit Input(StringPiece range, json::serialization_opts const* opts)
+      : range_(range)
+      , opts_(*opts)
+      , lineNum_(0)
+  {
+    storeCurrent();
+  }
+
+  Input(Input const&) = delete;
+  Input& operator=(Input const&) = delete;
+
+  char const* begin() const { return range_.begin(); }
+
+  // Parse ahead for as long as the supplied predicate is satisfied,
+  // returning a range of what was skipped.
+  template<class Predicate>
+  StringPiece skipWhile(const Predicate& p) {
+    std::size_t skipped = 0;
+    for (; skipped < range_.size(); ++skipped) {
+      if (!p(range_[skipped])) {
+        break;
+      }
+      if (range_[skipped] == '\n') {
+        ++lineNum_;
+      }
+    }
+    auto ret = range_.subpiece(0, skipped);
+    range_.advance(skipped);
+    storeCurrent();
+    return ret;
+  }
+
+  StringPiece skipDigits() {
+    return skipWhile([] (char c) { return c >= '0' && c <= '9'; });
+  }
+
+  StringPiece skipMinusAndDigits() {
+    bool firstChar = true;
+    return skipWhile([&firstChar] (char c) {
+        bool result = (c >= '0' && c <= '9') || (firstChar && c == '-');
+        firstChar = false;
+        return result;
+      });
+  }
+
+  void skipWhitespace() {
+    // Spaces other than ' ' characters are less common but should be
+    // checked.  This configuration where we loop on the ' '
+    // separately from oddspaces was empirically fastest.
+    auto oddspace = [] (char c) {
+      return c == '\n' || c == '\t' || c == '\r';
+    };
+
+  loop:
+    for (; !range_.empty() && range_.front() == ' '; range_.pop_front()) {
+    }
+    if (!range_.empty() && oddspace(range_.front())) {
+      range_.pop_front();
+      goto loop;
+    }
+    storeCurrent();
+  }
+
+  void expect(char c) {
+    if (**this != c) {
+      throw ParseError(lineNum_, context(),
+        to<std::string>("expected '", c, '\''));
+    }
+    ++*this;
+  }
+
+  std::size_t size() const {
+    return range_.size();
+  }
+
+  int operator*() const {
+    return current_;
+  }
+
+  void operator++() {
+    range_.pop_front();
+    storeCurrent();
+  }
+
+  template<class T>
+  T extract() {
+    try {
+      return to<T>(&range_);
+    } catch (std::exception const& e) {
+      error(e.what());
+    }
+  }
+
+  bool consume(StringPiece str) {
+    if (boost::starts_with(range_, str)) {
+      range_.advance(str.size());
+      storeCurrent();
+      return true;
+    }
+    return false;
+  }
+
+  std::string context() const {
+    return range_.subpiece(0, 16 /* arbitrary */).toString();
+  }
+
+  dynamic error(char const* what) const {
+    throw ParseError(lineNum_, context(), what);
+  }
+
+  json::serialization_opts const& getOpts() {
+    return opts_;
+  }
+
+private:
+  void storeCurrent() {
+    current_ = range_.empty() ? EOF : range_.front();
+  }
+
+private:
+  StringPiece range_;
+  json::serialization_opts const& opts_;
+  unsigned lineNum_;
+  int current_;
+};
+
+dynamic parseValue(Input& in);
+fbstring parseString(Input& in);
+dynamic parseNumber(Input& in);
+
+dynamic parseObject(Input& in) {
+  assert(*in == '{');
+  ++in;
+
+  dynamic ret = dynamic::object;
+
+  in.skipWhitespace();
+  if (*in == '}') {
+    ++in;
+    return ret;
+  }
+
+  for (;;) {
+    if (in.getOpts().allow_trailing_comma && *in == '}') {
+      break;
+    }
+    if (*in == '\"') { // string
+      auto key = parseString(in);
+      in.skipWhitespace();
+      in.expect(':');
+      in.skipWhitespace();
+      ret.insert(std::move(key), parseValue(in));
+    } else if (!in.getOpts().allow_non_string_keys) {
+      in.error("expected string for object key name");
+    } else {
+      auto key = parseValue(in);
+      in.skipWhitespace();
+      in.expect(':');
+      in.skipWhitespace();
+      ret.insert(std::move(key), parseValue(in));
+    }
+
+    in.skipWhitespace();
+    if (*in != ',') {
+      break;
+    }
+    ++in;
+    in.skipWhitespace();
+  }
+  in.expect('}');
+
+  return ret;
+}
+
+dynamic parseArray(Input& in) {
+  assert(*in == '[');
+  ++in;
+
+  dynamic ret = {};
+
+  in.skipWhitespace();
+  if (*in == ']') {
+    ++in;
+    return ret;
+  }
+
+  for (;;) {
+    if (in.getOpts().allow_trailing_comma && *in == ']') {
+      break;
+    }
+    ret.push_back(parseValue(in));
+    in.skipWhitespace();
+    if (*in != ',') {
+      break;
+    }
+    ++in;
+    in.skipWhitespace();
+  }
+  in.expect(']');
+
+  return ret;
+}
+
+dynamic parseNumber(Input& in) {
+  bool const negative = (*in == '-');
+  if (negative) {
+    if (in.consume("-Infinity")) {
+      return -std::numeric_limits<double>::infinity();
+    }
+  }
+
+  auto integral = in.skipMinusAndDigits();
+  if (negative && integral.size() < 2) {
+    in.error("expected digits after `-'");
+  }
+
+  auto const wasE = *in == 'e' || *in == 'E';
+  if (*in != '.' && !wasE) {
+    auto val = to<int64_t>(integral);
+    in.skipWhitespace();
+    return val;
+  }
+
+  auto end = !wasE ? (++in, in.skipDigits().end()) : in.begin();
+  if (*in == 'e' || *in == 'E') {
+    ++in;
+    if (*in == '+' || *in == '-') {
+      ++in;
+    }
+    auto expPart = in.skipDigits();
+    end = expPart.end();
+  }
+  auto fullNum = range(integral.begin(), end);
+
+  auto val = to<double>(fullNum);
+  return val;
+}
+
+fbstring decodeUnicodeEscape(Input& in) {
+  auto hexVal = [&] (char c) -> unsigned {
+    return c >= '0' && c <= '9' ? c - '0' :
+           c >= 'a' && c <= 'f' ? c - 'a' + 10 :
+           c >= 'A' && c <= 'F' ? c - 'A' + 10 :
+           (in.error("invalid hex digit"), 0);
+  };
+
+  auto readHex = [&]() -> uint16_t {
+    if (in.size() < 4) {
+      in.error("expected 4 hex digits");
+    }
+
+    uint16_t ret = hexVal(*in) * 4096;
+    ++in;
+    ret += hexVal(*in) * 256;
+    ++in;
+    ret += hexVal(*in) * 16;
+    ++in;
+    ret += hexVal(*in);
+    ++in;
+    return ret;
+  };
+
+  /*
+   * If the value encoded is in the surrogate pair range, we need to
+   * make sure there is another escape that we can use also.
+   */
+  uint32_t codePoint = readHex();
+  if (codePoint >= 0xd800 && codePoint <= 0xdbff) {
+    if (!in.consume("\\u")) {
+      in.error("expected another unicode escape for second half of "
+        "surrogate pair");
+    }
+    uint16_t second = readHex();
+    if (second >= 0xdc00 && second <= 0xdfff) {
+      codePoint = 0x10000 + ((codePoint & 0x3ff) << 10) +
+                  (second & 0x3ff);
+    } else {
+      in.error("second character in surrogate pair is invalid");
+    }
+  } else if (codePoint >= 0xdc00 && codePoint <= 0xdfff) {
+    in.error("invalid unicode code point (in range [0xdc00,0xdfff])");
+  }
+
+  return codePointToUtf8(codePoint);
+}
+
+fbstring parseString(Input& in) {
+  assert(*in == '\"');
+  ++in;
+
+  fbstring ret;
+  for (;;) {
+    auto range = in.skipWhile(
+      [] (char c) { return c != '\"' && c != '\\'; }
+    );
+    ret.append(range.begin(), range.end());
+
+    if (*in == '\"') {
+      ++in;
+      break;
+    }
+    if (*in == '\\') {
+      ++in;
+      switch (*in) {
+      case '\"':    ret.push_back('\"'); ++in; break;
+      case '\\':    ret.push_back('\\'); ++in; break;
+      case '/':     ret.push_back('/');  ++in; break;
+      case 'b':     ret.push_back('\b'); ++in; break;
+      case 'f':     ret.push_back('\f'); ++in; break;
+      case 'n':     ret.push_back('\n'); ++in; break;
+      case 'r':     ret.push_back('\r'); ++in; break;
+      case 't':     ret.push_back('\t'); ++in; break;
+      case 'u':     ++in; ret += decodeUnicodeEscape(in); break;
+      default:      in.error(to<fbstring>("unknown escape ", *in,
+                                          " in string").c_str());
+      }
+      continue;
+    }
+    if (*in == EOF) {
+      in.error("unterminated string");
+    }
+    if (!*in) {
+      /*
+       * Apparently we're actually supposed to ban all control
+       * characters from strings.  This seems unnecessarily
+       * restrictive, so we're only banning zero bytes.  (Since the
+       * string is presumed to be UTF-8 encoded it's fine to just
+       * check this way.)
+       */
+      in.error("null byte in string");
+    }
+
+    ret.push_back(*in);
+    ++in;
+  }
+
+  return ret;
+}
+
+dynamic parseValue(Input& in) {
+  in.skipWhitespace();
+  return *in == '[' ? parseArray(in) :
+         *in == '{' ? parseObject(in) :
+         *in == '\"' ? parseString(in) :
+         (*in == '-' || (*in >= '0' && *in <= '9')) ? parseNumber(in) :
+         in.consume("true") ? true :
+         in.consume("false") ? false :
+         in.consume("null") ? nullptr :
+         in.consume("Infinity") ? std::numeric_limits<double>::infinity() :
+         in.consume("NaN") ? std::numeric_limits<double>::quiet_NaN() :
+         in.error("expected json value");
+}
+
+}
+
+//////////////////////////////////////////////////////////////////////
+
+fbstring serialize(dynamic const& dyn, serialization_opts const& opts) {
+  fbstring ret;
+  unsigned indentLevel = 0;
+  Printer p(ret, opts.pretty_formatting ? &indentLevel : nullptr, &opts);
+  p(dyn);
+  return ret;
+}
+
+// Escape a string so that it is legal to print it in JSON text.
+void escapeString(StringPiece input,
+                  fbstring& out,
+                  const serialization_opts& opts) {
+  auto hexDigit = [] (int c) -> char {
+    return c < 10 ? c + '0' : c - 10 + 'a';
+  };
+
+  out.reserve(out.size() + input.size() + 2);
+  out.push_back('\"');
+
+  auto* p = reinterpret_cast<const unsigned char*>(input.begin());
+  auto* q = reinterpret_cast<const unsigned char*>(input.begin());
+  auto* e = reinterpret_cast<const unsigned char*>(input.end());
+
+  while (p < e) {
+    // Since non-ascii encoding inherently does utf8 validation
+    // we explicitly validate utf8 only if non-ascii encoding is disabled.
+    if ((opts.validate_utf8 || opts.skip_invalid_utf8)
+        && !opts.encode_non_ascii) {
+      // to achieve better spatial and temporal coherence
+      // we do utf8 validation progressively along with the
+      // string-escaping instead of two separate passes
+
+      // as the encoding progresses, q will stay at or ahead of p
+      CHECK(q >= p);
+
+      // as p catches up with q, move q forward
+      if (q == p) {
+        // calling utf8_decode has the side effect of
+        // checking that utf8 encodings are valid
+        char32_t v = decodeUtf8(q, e, opts.skip_invalid_utf8);
+        if (opts.skip_invalid_utf8 && v == U'\ufffd') {
+          out.append("\ufffd");
+          p = q;
+          continue;
+        }
+      }
+    }
+    if (opts.encode_non_ascii && (*p & 0x80)) {
+      // note that this if condition captures utf8 chars
+      // with value > 127, so size > 1 byte
+      char32_t v = decodeUtf8(p, e, opts.skip_invalid_utf8);
+      out.append("\\u");
+      out.push_back(hexDigit(v >> 12));
+      out.push_back(hexDigit((v >> 8) & 0x0f));
+      out.push_back(hexDigit((v >> 4) & 0x0f));
+      out.push_back(hexDigit(v & 0x0f));
+    } else if (*p == '\\' || *p == '\"') {
+      out.push_back('\\');
+      out.push_back(*p++);
+    } else if (*p <= 0x1f) {
+      switch (*p) {
+        case '\b': out.append("\\b"); p++; break;
+        case '\f': out.append("\\f"); p++; break;
+        case '\n': out.append("\\n"); p++; break;
+        case '\r': out.append("\\r"); p++; break;
+        case '\t': out.append("\\t"); p++; break;
+        default:
+          // note that this if condition captures non readable chars
+          // with value < 32, so size = 1 byte (e.g control chars).
+          out.append("\\u00");
+          out.push_back(hexDigit((*p & 0xf0) >> 4));
+          out.push_back(hexDigit(*p & 0xf));
+          p++;
+      }
+    } else {
+      out.push_back(*p++);
+    }
+  }
+
+  out.push_back('\"');
+}
+
+}
+
+//////////////////////////////////////////////////////////////////////
+
+dynamic parseJson(StringPiece range) {
+  return parseJson(range, json::serialization_opts());
+}
+
+dynamic parseJson(
+    StringPiece range,
+    json::serialization_opts const& opts) {
+
+  json::Input in(range, &opts);
+
+  auto ret = parseValue(in);
+  in.skipWhitespace();
+  if (in.size() && *in != '\0') {
+    in.error("parsing didn't consume all input");
+  }
+  return ret;
+}
+
+fbstring toJson(dynamic const& dyn) {
+  return json::serialize(dyn, json::serialization_opts());
+}
+
+fbstring toPrettyJson(dynamic const& dyn) {
+  json::serialization_opts opts;
+  opts.pretty_formatting = true;
+  return json::serialize(dyn, opts);
+}
+
+//////////////////////////////////////////////////////////////////////
+// dynamic::print_as_pseudo_json() is implemented here for header
+// ordering reasons (most of the dynamic implementation is in
+// dynamic-inl.h, which we don't want to include json.h).
+
+void dynamic::print_as_pseudo_json(std::ostream& out) const {
+  json::serialization_opts opts;
+  opts.allow_non_string_keys = true;
+  opts.allow_nan_inf = true;
+  out << json::serialize(*this, opts);
+}
+
+//////////////////////////////////////////////////////////////////////
+
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/json.h
@@ -0,0 +1,150 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ *
+ * Serialize and deserialize folly::dynamic values as JSON.
+ *
+ * Before you use this you should probably understand the basic
+ * concepts in the JSON type system:
+ *
+ *    Value  : String | Bool | Null | Object | Array | Number
+ *    String : UTF-8 sequence
+ *    Object : (String, Value) pairs, with unique String keys
+ *    Array  : ordered list of Values
+ *    Null   : null
+ *    Bool   : true | false
+ *    Number : (representation unspecified)
+ *
+ * ... That's about it.  For more information see http://json.org or
+ * look up RFC 4627.
+ *
+ * If your dynamic has anything illegal with regard to this type
+ * system, the serializer will throw.
+ *
+ * @author Jordan DeLong <delong.j@fb.com>
+ */
+
+#ifndef FOLLY_JSON_H_
+#define FOLLY_JSON_H_
+
+#include "folly/dynamic.h"
+#include "folly/FBString.h"
+#include "folly/Range.h"
+
+namespace folly {
+
+//////////////////////////////////////////////////////////////////////
+
+namespace json {
+
+  struct serialization_opts {
+    explicit serialization_opts()
+      : allow_non_string_keys(false)
+      , javascript_safe(false)
+      , pretty_formatting(false)
+      , encode_non_ascii(false)
+      , validate_utf8(false)
+      , allow_trailing_comma(false)
+      , sort_keys(false)
+      , skip_invalid_utf8(false)
+      , allow_nan_inf(false)
+      , double_mode(double_conversion::DoubleToStringConverter::SHORTEST)
+      , double_num_digits(0) // ignored when mode is SHORTEST
+    {}
+
+    // If true, keys in an object can be non-strings.  (In strict
+    // JSON, object keys must be strings.)  This is used by dynamic's
+    // operator<<.
+    bool allow_non_string_keys;
+
+    /*
+     * If true, refuse to serialize 64-bit numbers that cannot be
+     * precisely represented by fit a double---instead, throws an
+     * exception if the document contains this.
+     */
+    bool javascript_safe;
+
+    // If true, the serialized json will contain space and newlines to
+    // try to be minimally "pretty".
+    bool pretty_formatting;
+
+    // If true, non-ASCII utf8 characters would be encoded as \uXXXX.
+    bool encode_non_ascii;
+
+    // Check that strings are valid utf8
+    bool validate_utf8;
+
+    // Allow trailing comma in lists of values / items
+    bool allow_trailing_comma;
+
+    // Sort keys of all objects before printing out (potentially slow)
+    bool sort_keys;
+
+    // Replace invalid utf8 characters with U+FFFD and continue
+    bool skip_invalid_utf8;
+
+    // true to allow NaN or INF values
+    bool allow_nan_inf;
+
+    // Options for how to print floating point values.  See Conv.h
+    // toAppend implementation for floating point for more info
+    double_conversion::DoubleToStringConverter::DtoaMode double_mode;
+    unsigned int double_num_digits;
+  };
+
+  /*
+   * Main JSON serialization routine taking folly::dynamic parameters.
+   * For the most common use cases there are simpler functions in the
+   * main folly namespace below.
+   */
+  fbstring serialize(dynamic const&, serialization_opts const&);
+
+  /*
+   * Escape a string so that it is legal to print it in JSON text and
+   * append the result to out.
+   */
+
+  void escapeString(StringPiece input,
+                    fbstring& out,
+                    const serialization_opts& opts);
+}
+
+//////////////////////////////////////////////////////////////////////
+
+/*
+ * Parse a json blob out of a range and produce a dynamic representing
+ * it.
+ */
+dynamic parseJson(StringPiece, json::serialization_opts const&);
+dynamic parseJson(StringPiece);
+
+/*
+ * Serialize a dynamic into a json string.
+ */
+fbstring toJson(dynamic const&);
+
+/*
+ * Same as the above, except format the json with some minimal
+ * indentation.
+ */
+fbstring toPrettyJson(dynamic const&);
+
+//////////////////////////////////////////////////////////////////////
+
+}
+
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/Lazy.h
@@ -0,0 +1,135 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#ifndef FOLLY_LAZY_H_
+#define FOLLY_LAZY_H_
+
+#include <utility>
+#include <type_traits>
+
+#include "folly/Optional.h"
+
+namespace folly {
+
+//////////////////////////////////////////////////////////////////////
+
+/*
+ * Lazy -- for delayed initialization of a value.  The value's
+ * initialization will be computed on demand at its first use, but
+ * will not be recomputed if its value is requested again.  The value
+ * may still be mutated after its initialization if the lazy is not
+ * declared const.
+ *
+ * The value is created using folly::lazy, usually with a lambda, and
+ * its value is requested using operator().
+ *
+ * Note that the value is not safe for concurrent accesses by multiple
+ * threads, even if you declare it const.  See note below.
+ *
+ *
+ * Example Usage:
+ *
+ *   void foo() {
+ *     auto const val = folly::lazy([&]{
+ *       return something_expensive(blah());
+ *     });
+ *
+ *     if (condition1) {
+ *       use(val());
+ *     }
+ *     if (condition2) {
+ *       useMaybeAgain(val());
+ *     } else {
+ *       // Unneeded in this branch.
+ *     }
+ *   }
+ *
+ *
+ * Rationale:
+ *
+ *    - operator() is used to request the value instead of an implicit
+ *      conversion because the slight syntactic overhead in common
+ *      seems worth the increased clarity.
+ *
+ *    - Lazy values do not model CopyConstructible because it is
+ *      unclear what semantics would be desirable.  Either copies
+ *      should share the cached value (adding overhead to cases that
+ *      don't need to support copies), or they could recompute the
+ *      value unnecessarily.  Sharing with mutable lazies would also
+ *      leave them with non-value semantics despite looking
+ *      value-like.
+ *
+ *    - Not thread safe for const accesses.  Many use cases for lazy
+ *      values are local variables on the stack, where multiple
+ *      threads shouldn't even be able to reach the value.  It still
+ *      is useful to indicate/check that the value doesn't change with
+ *      const, particularly when it is captured by a large family of
+ *      lambdas.  Adding internal synchronization seems like it would
+ *      pessimize the most common use case in favor of less likely use
+ *      cases.
+ *
+ */
+
+//////////////////////////////////////////////////////////////////////
+
+namespace detail {
+
+template<class Func>
+struct Lazy {
+  typedef typename std::result_of<Func()>::type result_type;
+
+  explicit Lazy(Func&& f) : func_(std::move(f)) {}
+  explicit Lazy(Func& f)  : func_(f) {}
+
+  Lazy(Lazy&& o)
+    : value_(std::move(o.value_))
+    , func_(std::move(o.func_))
+  {}
+
+  Lazy(const Lazy&) = delete;
+  Lazy& operator=(const Lazy&) = delete;
+  Lazy& operator=(Lazy&&) = delete;
+
+  const result_type& operator()() const {
+    return const_cast<Lazy&>(*this)();
+  }
+
+  result_type& operator()() {
+    if (!value_) value_ = func_();
+    return *value_;
+  }
+
+private:
+  Optional<result_type> value_;
+  Func func_;
+};
+
+}
+
+//////////////////////////////////////////////////////////////////////
+
+template<class Func>
+detail::Lazy<typename std::remove_reference<Func>::type>
+lazy(Func&& fun) {
+  return detail::Lazy<typename std::remove_reference<Func>::type>(
+    std::forward<Func>(fun)
+  );
+}
+
+//////////////////////////////////////////////////////////////////////
+
+}
+
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/Likely.h
@@ -0,0 +1,39 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Compiler hints to indicate the fast path of an "if" branch: whether
+ * the if condition is likely to be true or false.
+ *
+ * @author Tudor Bosman (tudorb@fb.com)
+ */
+
+#ifndef FOLLY_BASE_LIKELY_H_
+#define FOLLY_BASE_LIKELY_H_
+
+#undef LIKELY
+#undef UNLIKELY
+
+#if defined(__GNUC__) && __GNUC__ >= 4
+#define LIKELY(x)   (__builtin_expect((x), 1))
+#define UNLIKELY(x) (__builtin_expect((x), 0))
+#else
+#define LIKELY(x)   (x)
+#define UNLIKELY(x) (x)
+#endif
+
+#endif /* FOLLY_BASE_LIKELY_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/Logging.h
@@ -0,0 +1,54 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_LOGGING_H_
+#define FOLLY_LOGGING_H_
+
+#include <atomic>
+#include <chrono>
+#include <glog/logging.h>
+
+#ifndef FB_LOG_EVERY_MS
+/**
+ * Issues a LOG(severity) no more often than every
+ * milliseconds. Example:
+ *
+ * FB_LOG_EVERY_MS(INFO, 10000) << "At least ten seconds passed"
+ *   " since you last saw this.";
+ *
+ * The implementation uses for statements to introduce variables in
+ * a nice way that doesn't mess surrounding statements.  It is thread
+ * safe.
+ */
+#define FB_LOG_EVERY_MS(severity, milli_interval)                            \
+  for (bool FB_LEM_once = true; FB_LEM_once; )                               \
+    for (::std::chrono::milliseconds::rep FB_LEM_prev, FB_LEM_now =          \
+             ::std::chrono::duration_cast< ::std::chrono::milliseconds>(     \
+                 ::std::chrono::system_clock::now().time_since_epoch()       \
+                 ).count();                                                  \
+         FB_LEM_once; )                                                      \
+      for (static ::std::atomic< ::std::chrono::milliseconds::rep>           \
+               FB_LEM_hist; FB_LEM_once; FB_LEM_once = false)                \
+        if (FB_LEM_now - (FB_LEM_prev =                                      \
+                          FB_LEM_hist.load(std::memory_order_acquire)) <     \
+                milli_interval ||                                            \
+            !FB_LEM_hist.compare_exchange_strong(FB_LEM_prev, FB_LEM_now)) { \
+        } else                                                               \
+          LOG(severity)
+
+#endif
+
+#endif  // FOLLY_LOGGING_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/m4/ac_cxx_compile_stdcxx_0x.m4
@@ -0,0 +1,110 @@
+# ===========================================================================
+#        http://autoconf-archive.cryp.to/ac_cxx_compile_stdcxx_0x.html
+# ===========================================================================
+#
+# SYNOPSIS
+#
+#   AC_CXX_COMPILE_STDCXX_0X
+#
+# DESCRIPTION
+#
+#   Check for baseline language coverage in the compiler for the C++0x
+#   standard.
+#
+# LAST MODIFICATION
+#
+#   2008-04-17
+#
+# COPYLEFT
+#
+#   Copyright (c) 2008 Benjamin Kosnik <bkoz@redhat.com>
+#
+#   Copying and distribution of this file, with or without modification, are
+#   permitted in any medium without royalty provided the copyright notice
+#   and this notice are preserved.
+
+AC_DEFUN([AC_CXX_COMPILE_STDCXX_0X], [
+  AC_CACHE_CHECK(if g++ supports C++0x features without additional flags,
+  ac_cv_cxx_compile_cxx0x_native,
+  [AC_LANG_SAVE
+  AC_LANG_CPLUSPLUS
+  AC_TRY_COMPILE([
+  template <typename T>
+    struct check
+    {
+      static_assert(sizeof(int) <= sizeof(T), "not big enough");
+    };
+
+    typedef check<check<bool>> right_angle_brackets;
+
+    int a;
+    decltype(a) b;
+
+    typedef check<int> check_type;
+    check_type c;
+    check_type&& cr = static_cast<check_type&&>(c);],,
+  ac_cv_cxx_compile_cxx0x_native=yes, ac_cv_cxx_compile_cxx0x_native=no)
+  AC_LANG_RESTORE
+  ])
+
+  AC_CACHE_CHECK(if g++ supports C++0x features with -std=c++0x,
+  ac_cv_cxx_compile_cxx0x_cxx,
+  [AC_LANG_SAVE
+  AC_LANG_CPLUSPLUS
+  ac_save_CXXFLAGS="$CXXFLAGS"
+  CXXFLAGS="$CXXFLAGS -std=c++0x"
+  AC_TRY_COMPILE([
+  template <typename T>
+    struct check
+    {
+      static_assert(sizeof(int) <= sizeof(T), "not big enough");
+    };
+
+    typedef check<check<bool>> right_angle_brackets;
+
+    int a;
+    decltype(a) b;
+
+    typedef check<int> check_type;
+    check_type c;
+    check_type&& cr = static_cast<check_type&&>(c);],,
+  ac_cv_cxx_compile_cxx0x_cxx=yes, ac_cv_cxx_compile_cxx0x_cxx=no)
+  CXXFLAGS="$ac_save_CXXFLAGS"
+  AC_LANG_RESTORE
+  ])
+
+  AC_CACHE_CHECK(if g++ supports C++0x features with -std=gnu++0x,
+  ac_cv_cxx_compile_cxx0x_gxx,
+  [AC_LANG_SAVE
+  AC_LANG_CPLUSPLUS
+  ac_save_CXXFLAGS="$CXXFLAGS"
+  CXXFLAGS="$CXXFLAGS -std=gnu++0x"
+  AC_TRY_COMPILE([
+  template <typename T>
+    struct check
+    {
+      static_assert(sizeof(int) <= sizeof(T), "not big enough");
+    };
+
+    typedef check<check<bool>> right_angle_brackets;
+
+    int a;
+    decltype(a) b;
+
+    typedef check<int> check_type;
+    check_type c;
+    check_type&& cr = static_cast<check_type&&>(c);],,
+  ac_cv_cxx_compile_cxx0x_gxx=yes, ac_cv_cxx_compile_cxx0x_gxx=no)
+  CXXFLAGS="$ac_save_CXXFLAGS"
+  AC_LANG_RESTORE
+  ])
+
+  if test "$ac_cv_cxx_compile_cxx0x_native" = yes ||
+     test "$ac_cv_cxx_compile_cxx0x_cxx" = yes ||
+     test "$ac_cv_cxx_compile_cxx0x_gxx" = yes; then
+    AC_DEFINE(HAVE_STDCXX_0X,,[Define if g++ supports C++0x features. ])
+  else
+    AC_MSG_ERROR([Could not find cxx0x support in g++])				
+  fi
+])
+ 
\ No newline at end of file
--- /dev/null
+++ b/hphp/submodules/folly/folly/m4/ax_boost_base.m4
@@ -0,0 +1,258 @@
+# ===========================================================================
+#       http://www.gnu.org/software/autoconf-archive/ax_boost_base.html
+# ===========================================================================
+#
+# SYNOPSIS
+#
+#   AX_BOOST_BASE([MINIMUM-VERSION], [ACTION-IF-FOUND], [ACTION-IF-NOT-FOUND])
+#
+# DESCRIPTION
+#
+#   Test for the Boost C++ libraries of a particular version (or newer)
+#
+#   If no path to the installed boost library is given the macro searchs
+#   under /usr, /usr/local, /opt and /opt/local and evaluates the
+#   $BOOST_ROOT environment variable. Further documentation is available at
+#   <http://randspringer.de/boost/index.html>.
+#
+#   This macro calls:
+#
+#     AC_SUBST(BOOST_CPPFLAGS) / AC_SUBST(BOOST_LDFLAGS)
+#
+#   And sets:
+#
+#     HAVE_BOOST
+#
+# LICENSE
+#
+#   Copyright (c) 2008 Thomas Porschberg <thomas@randspringer.de>
+#   Copyright (c) 2009 Peter Adolphs
+#
+#   Copying and distribution of this file, with or without modification, are
+#   permitted in any medium without royalty provided the copyright notice
+#   and this notice are preserved. This file is offered as-is, without any
+#   warranty.
+
+#serial 20
+
+AC_DEFUN([AX_BOOST_BASE],
+[
+AC_ARG_WITH([boost],
+  [AS_HELP_STRING([--with-boost@<:@=ARG@:>@],
+    [use Boost library from a standard location (ARG=yes),
+     from the specified location (ARG=<path>),
+     or disable it (ARG=no)
+     @<:@ARG=yes@:>@ ])],
+    [
+    if test "$withval" = "no"; then
+        want_boost="no"
+    elif test "$withval" = "yes"; then
+        want_boost="yes"
+        ac_boost_path=""
+    else
+        want_boost="yes"
+        ac_boost_path="$withval"
+    fi
+    ],
+    [want_boost="yes"])
+
+
+AC_ARG_WITH([boost-libdir],
+        AS_HELP_STRING([--with-boost-libdir=LIB_DIR],
+        [Force given directory for boost libraries. Note that this will override library path detection, so use this parameter only if default library detection fails and you know exactly where your boost libraries are located.]),
+        [
+        if test -d "$withval"
+        then
+                ac_boost_lib_path="$withval"
+        else
+                AC_MSG_ERROR(--with-boost-libdir expected directory name)
+        fi
+        ],
+        [ac_boost_lib_path=""]
+)
+
+if test "x$want_boost" = "xyes"; then
+    boost_lib_version_req=ifelse([$1], ,1.20.0,$1)
+    boost_lib_version_req_shorten=`expr $boost_lib_version_req : '\([[0-9]]*\.[[0-9]]*\)'`
+    boost_lib_version_req_major=`expr $boost_lib_version_req : '\([[0-9]]*\)'`
+    boost_lib_version_req_minor=`expr $boost_lib_version_req : '[[0-9]]*\.\([[0-9]]*\)'`
+    boost_lib_version_req_sub_minor=`expr $boost_lib_version_req : '[[0-9]]*\.[[0-9]]*\.\([[0-9]]*\)'`
+    if test "x$boost_lib_version_req_sub_minor" = "x" ; then
+        boost_lib_version_req_sub_minor="0"
+        fi
+    WANT_BOOST_VERSION=`expr $boost_lib_version_req_major \* 100000 \+  $boost_lib_version_req_minor \* 100 \+ $boost_lib_version_req_sub_minor`
+    AC_MSG_CHECKING(for boostlib >= $boost_lib_version_req)
+    succeeded=no
+
+    dnl On 64-bit systems check for system libraries in both lib64 and lib.
+    dnl The former is specified by FHS, but e.g. Debian does not adhere to
+    dnl this (as it rises problems for generic multi-arch support).
+    dnl The last entry in the list is chosen by default when no libraries
+    dnl are found, e.g. when only header-only libraries are installed!
+    libsubdirs="lib"
+    ax_arch=`uname -m`
+    if test $ax_arch = x86_64 -o $ax_arch = ppc64 -o $ax_arch = s390x -o $ax_arch = sparc64; then
+        libsubdirs="lib64 lib lib64"
+    fi
+
+    dnl first we check the system location for boost libraries
+    dnl this location ist chosen if boost libraries are installed with the --layout=system option
+    dnl or if you install boost with RPM
+    if test "$ac_boost_path" != ""; then
+        BOOST_CPPFLAGS="-I$ac_boost_path/include"
+        for ac_boost_path_tmp in $libsubdirs; do
+                if test -d "$ac_boost_path"/"$ac_boost_path_tmp" ; then
+                        BOOST_LDFLAGS="-L$ac_boost_path/$ac_boost_path_tmp"
+                        break
+                fi
+        done
+    elif test "$cross_compiling" != yes; then
+        for ac_boost_path_tmp in /usr /usr/local /opt /opt/local ; do
+            if test -d "$ac_boost_path_tmp/include/boost" && test -r "$ac_boost_path_tmp/include/boost"; then
+                for libsubdir in $libsubdirs ; do
+                    if ls "$ac_boost_path_tmp/$libsubdir/libboost_"* >/dev/null 2>&1 ; then break; fi
+                done
+                BOOST_LDFLAGS="-L$ac_boost_path_tmp/$libsubdir"
+                BOOST_CPPFLAGS="-I$ac_boost_path_tmp/include"
+                break;
+            fi
+        done
+    fi
+
+    dnl overwrite ld flags if we have required special directory with
+    dnl --with-boost-libdir parameter
+    if test "$ac_boost_lib_path" != ""; then
+       BOOST_LDFLAGS="-L$ac_boost_lib_path"
+    fi
+
+    CPPFLAGS_SAVED="$CPPFLAGS"
+    CPPFLAGS="$CPPFLAGS $BOOST_CPPFLAGS"
+    export CPPFLAGS
+
+    LDFLAGS_SAVED="$LDFLAGS"
+    LDFLAGS="$LDFLAGS $BOOST_LDFLAGS"
+    export LDFLAGS
+
+    AC_REQUIRE([AC_PROG_CXX])
+    AC_LANG_PUSH(C++)
+        AC_COMPILE_IFELSE([AC_LANG_PROGRAM([[
+    @%:@include <boost/version.hpp>
+    ]], [[
+    #if BOOST_VERSION >= $WANT_BOOST_VERSION
+    // Everything is okay
+    #else
+    #  error Boost version is too old
+    #endif
+    ]])],[
+        AC_MSG_RESULT(yes)
+    succeeded=yes
+    found_system=yes
+        ],[
+        ])
+    AC_LANG_POP([C++])
+
+
+
+    dnl if we found no boost with system layout we search for boost libraries
+    dnl built and installed without the --layout=system option or for a staged(not installed) version
+    if test "x$succeeded" != "xyes"; then
+        _version=0
+        if test "$ac_boost_path" != ""; then
+            if test -d "$ac_boost_path" && test -r "$ac_boost_path"; then
+                for i in `ls -d $ac_boost_path/include/boost-* 2>/dev/null`; do
+                    _version_tmp=`echo $i | sed "s#$ac_boost_path##" | sed 's/\/include\/boost-//' | sed 's/_/./'`
+                    V_CHECK=`expr $_version_tmp \> $_version`
+                    if test "$V_CHECK" = "1" ; then
+                        _version=$_version_tmp
+                    fi
+                    VERSION_UNDERSCORE=`echo $_version | sed 's/\./_/'`
+                    BOOST_CPPFLAGS="-I$ac_boost_path/include/boost-$VERSION_UNDERSCORE"
+                done
+            fi
+        else
+            if test "$cross_compiling" != yes; then
+                for ac_boost_path in /usr /usr/local /opt /opt/local ; do
+                    if test -d "$ac_boost_path" && test -r "$ac_boost_path"; then
+                        for i in `ls -d $ac_boost_path/include/boost-* 2>/dev/null`; do
+                            _version_tmp=`echo $i | sed "s#$ac_boost_path##" | sed 's/\/include\/boost-//' | sed 's/_/./'`
+                            V_CHECK=`expr $_version_tmp \> $_version`
+                            if test "$V_CHECK" = "1" ; then
+                                _version=$_version_tmp
+                                best_path=$ac_boost_path
+                            fi
+                        done
+                    fi
+                done
+
+                VERSION_UNDERSCORE=`echo $_version | sed 's/\./_/'`
+                BOOST_CPPFLAGS="-I$best_path/include/boost-$VERSION_UNDERSCORE"
+                if test "$ac_boost_lib_path" = ""; then
+                    for libsubdir in $libsubdirs ; do
+                        if ls "$best_path/$libsubdir/libboost_"* >/dev/null 2>&1 ; then break; fi
+                    done
+                    BOOST_LDFLAGS="-L$best_path/$libsubdir"
+                fi
+            fi
+
+            if test "x$BOOST_ROOT" != "x"; then
+                for libsubdir in $libsubdirs ; do
+                    if ls "$BOOST_ROOT/stage/$libsubdir/libboost_"* >/dev/null 2>&1 ; then break; fi
+                done
+                if test -d "$BOOST_ROOT" && test -r "$BOOST_ROOT" && test -d "$BOOST_ROOT/stage/$libsubdir" && test -r "$BOOST_ROOT/stage/$libsubdir"; then
+                    version_dir=`expr //$BOOST_ROOT : '.*/\(.*\)'`
+                    stage_version=`echo $version_dir | sed 's/boost_//' | sed 's/_/./g'`
+                        stage_version_shorten=`expr $stage_version : '\([[0-9]]*\.[[0-9]]*\)'`
+                    V_CHECK=`expr $stage_version_shorten \>\= $_version`
+                    if test "$V_CHECK" = "1" -a "$ac_boost_lib_path" = "" ; then
+                        AC_MSG_NOTICE(We will use a staged boost library from $BOOST_ROOT)
+                        BOOST_CPPFLAGS="-I$BOOST_ROOT"
+                        BOOST_LDFLAGS="-L$BOOST_ROOT/stage/$libsubdir"
+                    fi
+                fi
+            fi
+        fi
+
+        CPPFLAGS="$CPPFLAGS $BOOST_CPPFLAGS"
+        export CPPFLAGS
+        LDFLAGS="$LDFLAGS $BOOST_LDFLAGS"
+        export LDFLAGS
+
+        AC_LANG_PUSH(C++)
+            AC_COMPILE_IFELSE([AC_LANG_PROGRAM([[
+        @%:@include <boost/version.hpp>
+        ]], [[
+        #if BOOST_VERSION >= $WANT_BOOST_VERSION
+        // Everything is okay
+        #else
+        #  error Boost version is too old
+        #endif
+        ]])],[
+            AC_MSG_RESULT(yes)
+        succeeded=yes
+        found_system=yes
+            ],[
+            ])
+        AC_LANG_POP([C++])
+    fi
+
+    if test "$succeeded" != "yes" ; then
+        if test "$_version" = "0" ; then
+            AC_MSG_NOTICE([[We could not detect the boost libraries (version $boost_lib_version_req_shorten or higher). If you have a staged boost library (still not installed) please specify \$BOOST_ROOT in your environment and do not give a PATH to --with-boost option.  If you are sure you have boost installed, then check your version number looking in <boost/version.hpp>. See http://randspringer.de/boost for more documentation.]])
+        else
+            AC_MSG_NOTICE([Your boost libraries seems to old (version $_version).])
+        fi
+        # execute ACTION-IF-NOT-FOUND (if present):
+        ifelse([$3], , :, [$3])
+    else
+        AC_SUBST(BOOST_CPPFLAGS)
+        AC_SUBST(BOOST_LDFLAGS)
+        AC_DEFINE(HAVE_BOOST,,[define if the Boost library is available])
+        # execute ACTION-IF-FOUND (if present):
+        ifelse([$2], , :, [$2])
+    fi
+
+    CPPFLAGS="$CPPFLAGS_SAVED"
+    LDFLAGS="$LDFLAGS_SAVED"
+fi
+
+])
\ No newline at end of file
--- /dev/null
+++ b/hphp/submodules/folly/folly/m4/ax_boost_regex.m4
@@ -0,0 +1,111 @@
+# ===========================================================================
+#      http://www.gnu.org/software/autoconf-archive/ax_boost_regex.html
+# ===========================================================================
+#
+# SYNOPSIS
+#
+#   AX_BOOST_REGEX
+#
+# DESCRIPTION
+#
+#   Test for Regex library from the Boost C++ libraries. The macro requires
+#   a preceding call to AX_BOOST_BASE. Further documentation is available at
+#   <http://randspringer.de/boost/index.html>.
+#
+#   This macro calls:
+#
+#     AC_SUBST(BOOST_REGEX_LIB)
+#
+#   And sets:
+#
+#     HAVE_BOOST_REGEX
+#
+# LICENSE
+#
+#   Copyright (c) 2008 Thomas Porschberg <thomas@randspringer.de>
+#   Copyright (c) 2008 Michael Tindal
+#
+#   Copying and distribution of this file, with or without modification, are
+#   permitted in any medium without royalty provided the copyright notice
+#   and this notice are preserved. This file is offered as-is, without any
+#   warranty.
+
+#serial 20
+
+AC_DEFUN([AX_BOOST_REGEX],
+[
+        AC_ARG_WITH([boost-regex],
+        AS_HELP_STRING([--with-boost-regex@<:@=special-lib@:>@],
+                   [use the Regex library from boost - it is possible to specify a certain library for the linker
+                        e.g. --with-boost-regex=boost_regex-gcc-mt-d-1_33_1 ]),
+        [
+        if test "$withval" = "no"; then
+                want_boost="no"
+        elif test "$withval" = "yes"; then
+            want_boost="yes"
+            ax_boost_user_regex_lib=""
+        else
+                    want_boost="yes"
+                        ax_boost_user_regex_lib="$withval"
+                                fi
+        ],
+        [want_boost="yes"]
+        )
+
+        if test "x$want_boost" = "xyes"; then
+        AC_REQUIRE([AC_PROG_CC])
+                CPPFLAGS_SAVED="$CPPFLAGS"
+                        CPPFLAGS="$CPPFLAGS $BOOST_CPPFLAGS"
+                                            export CPPFLAGS
+
+                                                   LDFLAGS_SAVED="$LDFLAGS"
+                                                        LDFLAGS="$LDFLAGS $BOOST_LDFLAGS"
+                                                                          export LDFLAGS
+
+        AC_CACHE_CHECK(whether the Boost::Regex library is available,
+                                                           ax_cv_boost_regex,
+        [AC_LANG_PUSH([C++])
+                         AC_COMPILE_IFELSE([AC_LANG_PROGRAM([[@%:@include <boost/regex.hpp>
+                                                                                                                            ]],
+                                   [[boost::regex r(); return 0;]])],
+                   ax_cv_boost_regex=yes, ax_cv_boost_regex=no)
+         AC_LANG_POP([C++])
+                ])
+                        if test "x$ax_cv_boost_regex" = "xyes"; then
+                                AC_DEFINE(HAVE_BOOST_REGEX,,[define if the Boost::Regex library is available])
+            BOOSTLIBDIR=`echo $BOOST_LDFLAGS | sed -e 's/@<:@^\/@:>@*//'`
+            if test "x$ax_boost_user_regex_lib" = "x"; then
+                for libextension in `ls $BOOSTLIBDIR/libboost_regex*.so* $BOOSTLIBDIR/libboost_regex*.a* 2>/dev/null | sed 's,.*/,,' | sed -e 's;^lib\(boost_regex.*\)\.so.*$;\1;' -e 's;^lib\(boost_regex.*\)\.a*$;\1;'` ; do
+                     ax_lib=${libextension}
+                                            AC_CHECK_LIB($ax_lib, exit,
+                                 [BOOST_REGEX_LIB="-l$ax_lib"; AC_SUBST(BOOST_REGEX_LIB) link_regex="yes"; break],
+                                 [link_regex="no"])
+                                                        done
+                if test "x$link_regex" != "xyes"; then
+                for libextension in `ls $BOOSTLIBDIR/boost_regex*.{dll,a}* 2>/dev/null | sed 's,.*/,,' | sed -e 's;^\(boost_regex.*\)\.dll.*$;\1;' -e 's;^\(boost_regex.*\)\.a*$;\1;'` ; do
+                     ax_lib=${libextension}
+                                            AC_CHECK_LIB($ax_lib, exit,
+                                 [BOOST_REGEX_LIB="-l$ax_lib"; AC_SUBST(BOOST_REGEX_LIB) link_regex="yes"; break],
+                                 [link_regex="no"])
+                                                        done
+                fi
+
+            else
+               for ax_lib in $ax_boost_user_regex_lib boost_regex-$ax_boost_user_regex_lib; do
+                                   AC_CHECK_LIB($ax_lib, main,
+                                   [BOOST_REGEX_LIB="-l$ax_lib"; AC_SUBST(BOOST_REGEX_LIB) link_regex="yes"; break],
+                                   [link_regex="no"])
+               done
+            fi
+            if test "x$ax_lib" = "x"; then
+                AC_MSG_ERROR(Could not find a version of the Boost::Regex library!)
+            fi
+                        if test "x$link_regex" != "xyes"; then
+                                               AC_MSG_ERROR(Could not link against $ax_lib !)
+                                                                      fi
+                                                                        fi
+
+                                                                                CPPFLAGS="$CPPFLAGS_SAVED"
+                                                                                LDFLAGS="$LDFLAGS_SAVED"
+                                                                                fi
+])
\ No newline at end of file
--- /dev/null
+++ b/hphp/submodules/folly/folly/m4/ax_boost_system.m4
@@ -0,0 +1,120 @@
+# ===========================================================================
+#      http://www.gnu.org/software/autoconf-archive/ax_boost_system.html
+# ===========================================================================
+#
+# SYNOPSIS
+#
+#   AX_BOOST_SYSTEM
+#
+# DESCRIPTION
+#
+#   Test for System library from the Boost C++ libraries. The macro requires
+#   a preceding call to AX_BOOST_BASE. Further documentation is available at
+#   <http://randspringer.de/boost/index.html>.
+#
+#   This macro calls:
+#
+#     AC_SUBST(BOOST_SYSTEM_LIB)
+#
+#   And sets:
+#
+#     HAVE_BOOST_SYSTEM
+#
+# LICENSE
+#
+#   Copyright (c) 2008 Thomas Porschberg <thomas@randspringer.de>
+#   Copyright (c) 2008 Michael Tindal
+#   Copyright (c) 2008 Daniel Casimiro <dan.casimiro@gmail.com>
+#
+#   Copying and distribution of this file, with or without modification, are
+#   permitted in any medium without royalty provided the copyright notice
+#   and this notice are preserved. This file is offered as-is, without any
+#   warranty.
+
+#serial 17
+
+AC_DEFUN([AX_BOOST_SYSTEM],
+[
+	AC_ARG_WITH([boost-system],
+	AS_HELP_STRING([--with-boost-system@<:@=special-lib@:>@],
+                   [use the System library from boost - it is possible to specify a certain library for the linker
+                        e.g. --with-boost-system=boost_system-gcc-mt ]),
+        [
+        if test "$withval" = "no"; then
+			want_boost="no"
+        elif test "$withval" = "yes"; then
+            want_boost="yes"
+            ax_boost_user_system_lib=""
+        else
+		    want_boost="yes"
+		ax_boost_user_system_lib="$withval"
+		fi
+        ],
+        [want_boost="yes"]
+	)
+
+	if test "x$want_boost" = "xyes"; then
+        AC_REQUIRE([AC_PROG_CC])
+        AC_REQUIRE([AC_CANONICAL_BUILD])
+		CPPFLAGS_SAVED="$CPPFLAGS"
+		CPPFLAGS="$CPPFLAGS $BOOST_CPPFLAGS"
+		export CPPFLAGS
+
+		LDFLAGS_SAVED="$LDFLAGS"
+		LDFLAGS="$LDFLAGS $BOOST_LDFLAGS"
+		export LDFLAGS
+
+        AC_CACHE_CHECK(whether the Boost::System library is available,
+					   ax_cv_boost_system,
+        [AC_LANG_PUSH([C++])
+			 CXXFLAGS_SAVE=$CXXFLAGS
+
+			 AC_COMPILE_IFELSE([AC_LANG_PROGRAM([[@%:@include <boost/system/error_code.hpp>]],
+                                   [[boost::system::system_category]])],
+                   ax_cv_boost_system=yes, ax_cv_boost_system=no)
+			 CXXFLAGS=$CXXFLAGS_SAVE
+             AC_LANG_POP([C++])
+		])
+		if test "x$ax_cv_boost_system" = "xyes"; then
+			AC_SUBST(BOOST_CPPFLAGS)
+
+			AC_DEFINE(HAVE_BOOST_SYSTEM,,[define if the Boost::System library is available])
+            BOOSTLIBDIR=`echo $BOOST_LDFLAGS | sed -e 's/@<:@^\/@:>@*//'`
+
+			LDFLAGS_SAVE=$LDFLAGS
+            if test "x$ax_boost_user_system_lib" = "x"; then
+                for libextension in `ls -r $BOOSTLIBDIR/libboost_system* 2>/dev/null | sed 's,.*/lib,,' | sed 's,\..*,,'` ; do
+                     ax_lib=${libextension}
+				    AC_CHECK_LIB($ax_lib, exit,
+                                 [BOOST_SYSTEM_LIB="-l$ax_lib"; AC_SUBST(BOOST_SYSTEM_LIB) link_system="yes"; break],
+                                 [link_system="no"])
+				done
+                if test "x$link_system" != "xyes"; then
+                for libextension in `ls -r $BOOSTLIBDIR/boost_system* 2>/dev/null | sed 's,.*/,,' | sed -e 's,\..*,,'` ; do
+                     ax_lib=${libextension}
+				    AC_CHECK_LIB($ax_lib, exit,
+                                 [BOOST_SYSTEM_LIB="-l$ax_lib"; AC_SUBST(BOOST_SYSTEM_LIB) link_system="yes"; break],
+                                 [link_system="no"])
+				done
+                fi
+
+            else
+               for ax_lib in $ax_boost_user_system_lib boost_system-$ax_boost_user_system_lib; do
+				      AC_CHECK_LIB($ax_lib, exit,
+                                   [BOOST_SYSTEM_LIB="-l$ax_lib"; AC_SUBST(BOOST_SYSTEM_LIB) link_system="yes"; break],
+                                   [link_system="no"])
+                  done
+
+            fi
+            if test "x$ax_lib" = "x"; then
+                AC_MSG_ERROR(Could not find a version of the library!)
+            fi
+			if test "x$link_system" = "xno"; then
+				AC_MSG_ERROR(Could not link against $ax_lib !)
+			fi
+		fi
+
+		CPPFLAGS="$CPPFLAGS_SAVED"
+	LDFLAGS="$LDFLAGS_SAVED"
+	fi
+])
--- /dev/null
+++ b/hphp/submodules/folly/folly/m4/ax_boost_thread.m4
@@ -0,0 +1,149 @@
+# ===========================================================================
+#      http://www.gnu.org/software/autoconf-archive/ax_boost_thread.html
+# ===========================================================================
+#
+# SYNOPSIS
+#
+#   AX_BOOST_THREAD
+#
+# DESCRIPTION
+#
+#   Test for Thread library from the Boost C++ libraries. The macro requires
+#   a preceding call to AX_BOOST_BASE. Further documentation is available at
+#   <http://randspringer.de/boost/index.html>.
+#
+#   This macro calls:
+#
+#     AC_SUBST(BOOST_THREAD_LIB)
+#
+#   And sets:
+#
+#     HAVE_BOOST_THREAD
+#
+# LICENSE
+#
+#   Copyright (c) 2009 Thomas Porschberg <thomas@randspringer.de>
+#   Copyright (c) 2009 Michael Tindal
+#
+#   Copying and distribution of this file, with or without modification, are
+#   permitted in any medium without royalty provided the copyright notice
+#   and this notice are preserved. This file is offered as-is, without any
+#   warranty.
+
+#serial 23
+
+AC_DEFUN([AX_BOOST_THREAD],
+[
+        AC_ARG_WITH([boost-thread],
+        AS_HELP_STRING([--with-boost-thread@<:@=special-lib@:>@],
+                   [use the Thread library from boost - it is possible to specify a certain library for the linker
+                        e.g. --with-boost-thread=boost_thread-gcc-mt ]),
+        [
+        if test "$withval" = "no"; then
+                want_boost="no"
+        elif test "$withval" = "yes"; then
+            want_boost="yes"
+            ax_boost_user_thread_lib=""
+        else
+                    want_boost="yes"
+                        ax_boost_user_thread_lib="$withval"
+                                fi
+        ],
+        [want_boost="yes"]
+        )
+
+        if test "x$want_boost" = "xyes"; then
+        AC_REQUIRE([AC_PROG_CC])
+        AC_REQUIRE([AC_CANONICAL_BUILD])
+                CPPFLAGS_SAVED="$CPPFLAGS"
+                        CPPFLAGS="$CPPFLAGS $BOOST_CPPFLAGS"
+                                            export CPPFLAGS
+
+                                                   LDFLAGS_SAVED="$LDFLAGS"
+                                                        LDFLAGS="$LDFLAGS $BOOST_LDFLAGS"
+                                                                          export LDFLAGS
+
+        AC_CACHE_CHECK(whether the Boost::Thread library is available,
+                                                            ax_cv_boost_thread,
+        [AC_LANG_PUSH([C++])
+                         CXXFLAGS_SAVE=$CXXFLAGS
+
+                                         if test "x$host_os" = "xsolaris" ; then
+                                                              CXXFLAGS="-pthreads $CXXFLAGS"
+                                                                                         elif test "x$host_os" = "xmingw32" ; then
+                                                                                                                CXXFLAGS="-mthreads $CXXFLAGS"
+                                                                                                                                      else
+                                                                                                                                         CXXFLAGS="-pthread $CXXFLAGS"
+                                                                                                                                                              fi
+                                                                                                                                                                 AC_COMPILE_IFELSE([AC_LANG_PROGRAM([[@%:@include <boost/thread/thread.hpp>]],
+                                   [[boost::thread_group thrds;
+                                   return 0;]])],
+                   ax_cv_boost_thread=yes, ax_cv_boost_thread=no)
+                                                 CXXFLAGS=$CXXFLAGS_SAVE
+             AC_LANG_POP([C++])
+                ])
+                        if test "x$ax_cv_boost_thread" = "xyes"; then
+           if test "x$host_os" = "xsolaris" ; then
+                     BOOST_CPPFLAGS="-pthreads $BOOST_CPPFLAGS"
+                                                  elif test "x$host_os" = "xmingw32" ; then
+                                                              BOOST_CPPFLAGS="-mthreads $BOOST_CPPFLAGS"
+                                                                                           else
+                                                                                                          BOOST_CPPFLAGS="-pthread $BOOST_CPPFLAGS"
+                                                                                                                                      fi
+
+                                                                                                                                        AC_SUBST(BOOST_CPPFLAGS)
+
+                                                                                                                                          AC_DEFINE(HAVE_BOOST_THREAD,,[define if the Boost::Thread library is available])
+            BOOSTLIBDIR=`echo $BOOST_LDFLAGS | sed -e 's/@<:@^\/@:>@*//'`
+
+                                             LDFLAGS_SAVE=$LDFLAGS
+                        case "x$host_os" in
+                          *bsd* )
+                               LDFLAGS="-pthread $LDFLAGS"
+                          break;
+                          ;;
+                        esac
+            if test "x$ax_boost_user_thread_lib" = "x"; then
+                for libextension in `ls $BOOSTLIBDIR/libboost_thread*.so* 2>/dev/null | sed 's,.*/,,' | sed -e 's;^lib\(boost_thread.*\)\.so.*$;\1;'` `ls $BOOSTLIBDIR/libboost_thread*.a* 2>/dev/null | sed 's,.*/,,' | sed -e 's;^lib\(boost_thread.*\)\.a*$;\1;'`; do
+                     ax_lib=${libextension}
+                                            AC_CHECK_LIB($ax_lib, exit,
+                                 [BOOST_THREAD_LIB="-l$ax_lib"; AC_SUBST(BOOST_THREAD_LIB) link_thread="yes"; break],
+                                 [link_thread="no"])
+                                                        done
+                if test "x$link_thread" != "xyes"; then
+                for libextension in `ls $BOOSTLIBDIR/boost_thread*.dll* 2>/dev/null | sed 's,.*/,,' | sed -e 's;^\(boost_thread.*\)\.dll.*$;\1;'` `ls $BOOSTLIBDIR/boost_thread*.a* 2>/dev/null | sed 's,.*/,,' | sed -e 's;^\(boost_thread.*\)\.a*$;\1;'` ; do
+                     ax_lib=${libextension}
+                                            AC_CHECK_LIB($ax_lib, exit,
+                                 [BOOST_THREAD_LIB="-l$ax_lib"; AC_SUBST(BOOST_THREAD_LIB) link_thread="yes"; break],
+                                 [link_thread="no"])
+                                                        done
+                fi
+
+            else
+               for ax_lib in $ax_boost_user_thread_lib boost_thread-$ax_boost_user_thread_lib; do
+                                   AC_CHECK_LIB($ax_lib, exit,
+                                   [BOOST_THREAD_LIB="-l$ax_lib"; AC_SUBST(BOOST_THREAD_LIB) link_thread="yes"; break],
+                                   [link_thread="no"])
+                  done
+
+            fi
+            if test "x$ax_lib" = "x"; then
+                AC_MSG_ERROR(Could not find a version of the library!)
+            fi
+                        if test "x$link_thread" = "xno"; then
+                                                AC_MSG_ERROR(Could not link against $ax_lib !)
+                        else
+                           case "x$host_os" in
+                              *bsd* )
+                                                BOOST_LDFLAGS="-pthread $BOOST_LDFLAGS"
+                              break;
+                              ;;
+                           esac
+
+                                        fi
+                                                fi
+
+                                                        CPPFLAGS="$CPPFLAGS_SAVED"
+                                                        LDFLAGS="$LDFLAGS_SAVED"
+                                                        fi
+])
\ No newline at end of file
--- /dev/null
+++ b/hphp/submodules/folly/folly/m4/ax_prefix_config.m4
@@ -0,0 +1,209 @@
+# ===========================================================================
+#    http://www.gnu.org/software/autoconf-archive/ax_prefix_config_h.html
+# ===========================================================================
+#
+# SYNOPSIS
+#
+#   AX_PREFIX_CONFIG_H [(OUTPUT-HEADER [,PREFIX [,ORIG-HEADER]])]
+#
+# DESCRIPTION
+#
+#   This is a new variant from ac_prefix_config_ this one will use a
+#   lowercase-prefix if the config-define was starting with a
+#   lowercase-char, e.g. "#define const", "#define restrict", or "#define
+#   off_t", (and this one can live in another directory, e.g.
+#   testpkg/config.h therefore I decided to move the output-header to be the
+#   first arg)
+#
+#   takes the usual config.h generated header file; looks for each of the
+#   generated "#define SOMEDEF" lines, and prefixes the defined name (ie.
+#   makes it "#define PREFIX_SOMEDEF". The result is written to the output
+#   config.header file. The PREFIX is converted to uppercase for the
+#   conversions.
+#
+#   Defaults:
+#
+#     OUTPUT-HEADER = $PACKAGE-config.h
+#     PREFIX = $PACKAGE
+#     ORIG-HEADER, from AM_CONFIG_HEADER(config.h)
+#
+#   Your configure.ac script should contain both macros in this order, and
+#   unlike the earlier variations of this prefix-macro it is okay to place
+#   the AX_PREFIX_CONFIG_H call before the AC_OUTPUT invokation.
+#
+#   Example:
+#
+#     AC_INIT(config.h.in)        # config.h.in as created by "autoheader"
+#     AM_INIT_AUTOMAKE(testpkg, 0.1.1)    # makes #undef VERSION and PACKAGE
+#     AM_CONFIG_HEADER(config.h)          # prep config.h from config.h.in
+#     AX_PREFIX_CONFIG_H(mylib/_config.h) # prep mylib/_config.h from it..
+#     AC_MEMORY_H                         # makes "#undef NEED_MEMORY_H"
+#     AC_C_CONST_H                        # makes "#undef const"
+#     AC_OUTPUT(Makefile)                 # creates the "config.h" now
+#                                         # and also mylib/_config.h
+#
+#   if the argument to AX_PREFIX_CONFIG_H would have been omitted then the
+#   default outputfile would have been called simply "testpkg-config.h", but
+#   even under the name "mylib/_config.h" it contains prefix-defines like
+#
+#     #ifndef TESTPKG_VERSION
+#     #define TESTPKG_VERSION "0.1.1"
+#     #endif
+#     #ifndef TESTPKG_NEED_MEMORY_H
+#     #define TESTPKG_NEED_MEMORY_H 1
+#     #endif
+#     #ifndef _testpkg_const
+#     #define _testpkg_const _const
+#     #endif
+#
+#   and this "mylib/_config.h" can be installed along with other
+#   header-files, which is most convenient when creating a shared library
+#   (that has some headers) where some functionality is dependent on the
+#   OS-features detected at compile-time. No need to invent some
+#   "mylib-confdefs.h.in" manually. :-)
+#
+#   Note that some AC_DEFINEs that end up in the config.h file are actually
+#   self-referential - e.g. AC_C_INLINE, AC_C_CONST, and the AC_TYPE_OFF_T
+#   say that they "will define inline|const|off_t if the system does not do
+#   it by itself". You might want to clean up about these - consider an
+#   extra mylib/conf.h that reads something like:
+#
+#     #include <mylib/_config.h>
+#     #ifndef _testpkg_const
+#     #define _testpkg_const const
+#     #endif
+#
+#   and then start using _testpkg_const in the header files. That is also a
+#   good thing to differentiate whether some library-user has starting to
+#   take up with a different compiler, so perhaps it could read something
+#   like this:
+#
+#     #ifdef _MSC_VER
+#     #include <mylib/_msvc.h>
+#     #else
+#     #include <mylib/_config.h>
+#     #endif
+#     #ifndef _testpkg_const
+#     #define _testpkg_const const
+#     #endif
+#
+# LICENSE
+#
+#   Copyright (c) 2008 Guido U. Draheim <guidod@gmx.de>
+#   Copyright (c) 2008 Marten Svantesson
+#   Copyright (c) 2008 Gerald Point <Gerald.Point@labri.fr>
+#
+#   This program is free software; you can redistribute it and/or modify it
+#   under the terms of the GNU General Public License as published by the
+#   Free Software Foundation; either version 3 of the License, or (at your
+#   option) any later version.
+#
+#   This program is distributed in the hope that it will be useful, but
+#   WITHOUT ANY WARRANTY; without even the implied warranty of
+#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General
+#   Public License for more details.
+#
+#   You should have received a copy of the GNU General Public License along
+#   with this program. If not, see <http://www.gnu.org/licenses/>.
+#
+#   As a special exception, the respective Autoconf Macro's copyright owner
+#   gives unlimited permission to copy, distribute and modify the configure
+#   scripts that are the output of Autoconf when processing the Macro. You
+#   need not follow the terms of the GNU General Public License when using
+#   or distributing such scripts, even though portions of the text of the
+#   Macro appear in them. The GNU General Public License (GPL) does govern
+#   all other use of the material that constitutes the Autoconf Macro.
+#
+#   This special exception to the GPL applies to versions of the Autoconf
+#   Macro released by the Autoconf Archive. When you make and distribute a
+#   modified version of the Autoconf Macro, you may extend this special
+#   exception to the GPL to apply to your modified version as well.
+
+#serial 11
+
+AC_DEFUN([AX_PREFIX_CONFIG_H],[dnl
+AC_PREREQ([2.62])
+AC_BEFORE([AC_CONFIG_HEADERS],[$0])dnl
+AC_CONFIG_COMMANDS([ifelse($1,,$PACKAGE-config.h,$1)],[dnl
+AS_VAR_PUSHDEF([_OUT],[ac_prefix_conf_OUT])dnl
+AS_VAR_PUSHDEF([_DEF],[ac_prefix_conf_DEF])dnl
+AS_VAR_PUSHDEF([_PKG],[ac_prefix_conf_PKG])dnl
+AS_VAR_PUSHDEF([_LOW],[ac_prefix_conf_LOW])dnl
+AS_VAR_PUSHDEF([_UPP],[ac_prefix_conf_UPP])dnl
+AS_VAR_PUSHDEF([_INP],[ac_prefix_conf_INP])dnl
+m4_pushdef([_script],[conftest.prefix])dnl
+m4_pushdef([_symbol],[m4_cr_Letters[]m4_cr_digits[]_])dnl
+_OUT=`echo ifelse($1, , $PACKAGE-config.h, $1)`
+_DEF=`echo _$_OUT | sed -e "y:m4_cr_letters:m4_cr_LETTERS[]:" -e "s/@<:@^m4_cr_Letters@:>@/_/g"`
+_PKG=`echo ifelse($2, , $PACKAGE, $2)`
+_LOW=`echo _$_PKG | sed -e "y:m4_cr_LETTERS-:m4_cr_letters[]_:"`
+_UPP=`echo $_PKG | sed -e "y:m4_cr_letters-:m4_cr_LETTERS[]_:"  -e "/^@<:@m4_cr_digits@:>@/s/^/_/"`
+_INP=`echo "ifelse($3,,,$3)" | sed -e 's/ *//'`
+if test ".$_INP" = "."; then
+   for ac_file in : $CONFIG_HEADERS; do test "_$ac_file" = _: && continue
+     case "$ac_file" in
+        *.h) _INP=$ac_file ;;
+        *)
+     esac
+     test ".$_INP" != "." && break
+   done
+fi
+if test ".$_INP" = "."; then
+   case "$_OUT" in
+      */*) _INP=`basename "$_OUT"`
+      ;;
+      *-*) _INP=`echo "$_OUT" | sed -e "s/@<:@_symbol@:>@*-//"`
+      ;;
+      *) _INP=config.h
+      ;;
+   esac
+fi
+if test -z "$_PKG" ; then
+   AC_MSG_ERROR([no prefix for _PREFIX_PKG_CONFIG_H])
+else
+  if test ! -f "$_INP" ; then if test -f "$srcdir/$_INP" ; then
+     _INP="$srcdir/$_INP"
+  fi fi
+  AC_MSG_NOTICE(creating $_OUT - prefix $_UPP for $_INP defines)
+  if test -f $_INP ; then
+    AS_ECHO(["s/^@%:@undef  *\\(@<:@m4_cr_LETTERS[]_@:>@\\)/@%:@undef $_UPP""_\\1/"]) > _script
+    AS_ECHO(["s/^@%:@undef  *\\(@<:@m4_cr_letters@:>@\\)/@%:@undef $_LOW""_\\1/"]) >> _script
+    AS_ECHO(["s/^@%:@def[]ine  *\\(@<:@m4_cr_LETTERS[]_@:>@@<:@_symbol@:>@*\\)\\(.*\\)/@%:@ifndef $_UPP""_\\1\\"]) >> _script
+    AS_ECHO(["@%:@def[]ine $_UPP""_\\1\\2\\"]) >> _script
+    AS_ECHO(["@%:@endif/"]) >> _script
+    AS_ECHO(["s/^@%:@def[]ine  *\\(@<:@m4_cr_letters@:>@@<:@_symbol@:>@*\\)\\(.*\\)/@%:@ifndef $_LOW""_\\1\\"]) >> _script
+    AS_ECHO(["@%:@define $_LOW""_\\1\\2\\"]) >> _script
+    AS_ECHO(["@%:@endif/"]) >> _script
+    # now executing _script on _DEF input to create _OUT output file
+    echo "@%:@ifndef $_DEF"      >$tmp/pconfig.h
+    echo "@%:@def[]ine $_DEF 1" >>$tmp/pconfig.h
+    echo ' ' >>$tmp/pconfig.h
+    echo /'*' $_OUT. Generated automatically at end of configure. '*'/ >>$tmp/pconfig.h
+
+    sed -f _script $_INP >>$tmp/pconfig.h
+    echo ' ' >>$tmp/pconfig.h
+    echo '/* once:' $_DEF '*/' >>$tmp/pconfig.h
+    echo "@%:@endif" >>$tmp/pconfig.h
+    if cmp -s $_OUT $tmp/pconfig.h 2>/dev/null; then
+      AC_MSG_NOTICE([$_OUT is unchanged])
+    else
+      ac_dir=`AS_DIRNAME(["$_OUT"])`
+      AS_MKDIR_P(["$ac_dir"])
+      rm -f "$_OUT"
+      mv $tmp/pconfig.h "$_OUT"
+    fi
+    cp _script _configs.sed
+  else
+    AC_MSG_ERROR([input file $_INP does not exist - skip generating $_OUT])
+  fi
+  rm -f conftest.*
+fi
+m4_popdef([_symbol])dnl
+m4_popdef([_script])dnl
+AS_VAR_POPDEF([_INP])dnl
+AS_VAR_POPDEF([_UPP])dnl
+AS_VAR_POPDEF([_LOW])dnl
+AS_VAR_POPDEF([_PKG])dnl
+AS_VAR_POPDEF([_DEF])dnl
+AS_VAR_POPDEF([_OUT])dnl
+],[PACKAGE="$PACKAGE"])])
\ No newline at end of file
--- /dev/null
+++ b/hphp/submodules/folly/folly/Makefile.am
@@ -0,0 +1,233 @@
+SUBDIRS = . test
+
+ACLOCAL_AMFLAGS = -I m4
+
+CLEANFILES =
+
+noinst_PROGRAMS = generate_fingerprint_tables
+generate_fingerprint_tables_SOURCES = build/GenerateFingerprintTables.cpp
+generate_fingerprint_tables_LDADD = libfolly.la
+
+lib_LTLIBRARIES = \
+	libfolly.la \
+	libfollybenchmark.la \
+	libfollytimeout_queue.la \
+	libfollyfingerprint.la
+
+follyincludedir = $(includedir)/folly
+
+nobase_follyinclude_HEADERS = \
+	ApplyTuple.h \
+	Arena.h \
+	Arena-inl.h \
+	AtomicBitSet.h \
+	AtomicHashArray.h \
+	AtomicHashArray-inl.h \
+	AtomicHashMap.h \
+	AtomicHashMap-inl.h \
+	AtomicStruct.h \
+	Baton.h \
+	Benchmark.h \
+	Bits.h \
+	Chrono.h \
+	ConcurrentSkipList.h \
+	ConcurrentSkipList-inl.h \
+	Conv.h \
+	CpuId.h \
+	CPortability.h \
+	detail/AtomicHashUtils.h \
+	detail/BitIteratorDetail.h \
+	detail/BitsDetail.h \
+	detail/CacheLocality.h \
+	detail/ChecksumDetail.h \
+	detail/Clock.h \
+	detail/DiscriminatedPtrDetail.h \
+	detail/FileUtilDetail.h \
+	detail/FingerprintPolynomial.h \
+	detail/FunctionalExcept.h \
+	detail/Futex.h \
+	detail/GroupVarintDetail.h \
+	detail/Malloc.h \
+	detail/MemoryIdler.h \
+	detail/MPMCPipelineDetail.h \
+	detail/SlowFingerprint.h \
+	detail/Stats.h \
+	detail/ThreadLocalDetail.h \
+	detail/UncaughtExceptionCounter.h \
+	DiscriminatedPtr.h \
+	DynamicConverter.h \
+	dynamic.h \
+	dynamic-inl.h \
+	FBString.h \
+	FBVector.h \
+	File.h \
+	FileUtil.h \
+	Fingerprint.h \
+	folly-config.h \
+	Exception.h \
+	Foreach.h \
+	FormatArg.h \
+	Format.h \
+	Format-inl.h \
+	GroupVarint.h \
+	Hash.h \
+	IndexedMemPool.h \
+	IntrusiveList.h \
+	io/Cursor.h \
+	io/IOBuf.h \
+	io/IOBufQueue.h \
+	io/RecordIO.h \
+	io/RecordIO-inl.h \
+	io/TypedIOBuf.h \
+	io/async/AsyncTimeout.h \
+	io/async/EventBase.h \
+	io/async/EventFDWrapper.h \
+	io/async/EventHandler.h \
+	io/async/EventUtil.h \
+	io/async/NotificationQueue.h \
+	io/async/Request.h \
+	io/async/TimeoutManager.h \
+	json.h \
+	Lazy.h \
+	Likely.h \
+	Logging.h \
+	Malloc.h \
+	MapUtil.h \
+	Memory.h \
+	MemoryMapping.h \
+	MoveWrapper.h \
+	MPMCPipeline.h \
+	MPMCQueue.h \
+	Optional.h \
+	PackedSyncPtr.h \
+	Padded.h \
+	Portability.h \
+	Preprocessor.h \
+	ProducerConsumerQueue.h \
+	Random.h \
+	Range.h \
+	RWSpinLock.h \
+	ScopeGuard.h \
+	SmallLocks.h \
+	small_vector.h \
+	sorted_vector_types.h \
+	SpookyHashV1.h \
+	SpookyHashV2.h \
+	stats/BucketedTimeSeries-defs.h \
+	stats/BucketedTimeSeries.h \
+	stats/Histogram-defs.h \
+	stats/Histogram.h \
+	stats/MultiLevelTimeSeries-defs.h \
+	stats/MultiLevelTimeSeries.h \
+	String.h \
+	String-inl.h \
+	Subprocess.h \
+	Synchronized.h \
+	test/FBStringTestBenchmarks.cpp.h \
+	test/FBVectorTestBenchmarks.cpp.h \
+	test/function_benchmark/benchmark_impl.h \
+	test/function_benchmark/test_functions.h \
+	test/SynchronizedTestLib.h \
+	test/SynchronizedTestLib-inl.h \
+	ThreadCachedArena.h \
+	ThreadCachedInt.h \
+	ThreadLocal.h \
+	TimeoutQueue.h \
+	Traits.h \
+	Unicode.h \
+	Uri.h \
+	Uri-inl.h \
+	Varint.h \
+	wangle/Executor.h \
+	wangle/Future-inl.h \
+	wangle/Future.h \
+	wangle/GenericThreadGate.h \
+	wangle/InlineExecutor.h \
+	wangle/Later-inl.h \
+	wangle/Later.h \
+	wangle/ManualExecutor.h \
+	wangle/Promise-inl.h \
+	wangle/Promise.h \
+	wangle/ThreadGate.h \
+	wangle/Try-inl.h \
+	wangle/Try.h \
+	wangle/WangleException.h \
+	wangle/detail.h
+
+FormatTables.cpp: build/generate_format_tables.py
+	build/generate_format_tables.py
+CLEANFILES += FormatTables.cpp
+
+EscapeTables.cpp: build/generate_escape_tables.py
+	build/generate_escape_tables.py
+CLEANFILES += EscapeTables.cpp
+
+GroupVarintTables.cpp: build/generate_varint_tables.py
+	build/generate_varint_tables.py
+CLEANFILES += GroupVarintTables.cpp
+
+libfolly_la_SOURCES = \
+	Benchmark.cpp \
+	Bits.cpp \
+	Conv.cpp \
+	Demangle.cpp \
+	detail/CacheLocality.cpp \
+	dynamic.cpp \
+	EscapeTables.cpp \
+	File.cpp \
+	FileUtil.cpp \
+	Format.cpp \
+	FormatTables.cpp \
+	GroupVarint.cpp \
+	GroupVarintTables.cpp \
+	io/IOBuf.cpp \
+	io/IOBufQueue.cpp \
+	io/RecordIO.cpp \
+	io/async/AsyncTimeout.cpp \
+	io/async/EventBase.cpp \
+	io/async/EventHandler.cpp \
+	io/async/Request.cpp \
+	json.cpp \
+	detail/MemoryIdler.cpp \
+	MemoryMapping.cpp \
+	Random.cpp \
+	Range.cpp \
+	SpookyHashV1.cpp \
+	SpookyHashV2.cpp \
+	stats/Instantiations.cpp \
+	String.cpp \
+	Subprocess.cpp \
+	ThreadCachedArena.cpp \
+	TimeoutQueue.cpp \
+	Unicode.cpp \
+	Uri.cpp \
+	wangle/InlineExecutor.cpp \
+	wangle/ManualExecutor.cpp \
+	wangle/ThreadGate.cpp
+
+if !HAVE_LINUX
+nobase_follyinclude_HEADERS += detail/Clock.h
+libfolly_la_SOURCES += detail/Clock.cpp
+endif
+
+if !HAVE_WEAK_SYMBOLS
+libfolly_la_SOURCES += detail/Malloc.cpp
+endif
+
+if !HAVE_BITS_FUNCTEXCEPT
+libfolly_la_SOURCES += detail/FunctionalExcept.cpp
+endif
+
+FingerprintTables.cpp: generate_fingerprint_tables
+	./generate_fingerprint_tables
+CLEANFILES += FingerprintTables.cpp
+
+libfollyfingerprint_la_SOURCES = \
+	FingerprintTables.cpp
+libfollyfingerprint_la_LIBADD = libfolly.la
+
+libfollybenchmark_la_SOURCES = Benchmark.cpp
+libfollybenchmark_la_LIBADD = libfolly.la
+
+libfollytimeout_queue_la_SOURCES = TimeoutQueue.cpp
+libfollytimeout_queue_la_LIBADD = libfolly.la
--- /dev/null
+++ b/hphp/submodules/folly/folly/Malloc.h
@@ -0,0 +1,235 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// Functions to provide smarter use of jemalloc, if jemalloc is being used.
+// http://www.canonware.com/download/jemalloc/jemalloc-latest/doc/jemalloc.html
+
+#ifndef FOLLY_MALLOC_H_
+#define FOLLY_MALLOC_H_
+
+// If using fbstring from libstdc++, then just define stub code
+// here to typedef the fbstring type into the folly namespace.
+// This provides backwards compatibility for code that explicitly
+// includes and uses fbstring.
+#if defined(_GLIBCXX_USE_FB) && !defined(_LIBSTDCXX_FBSTRING)
+
+#include "folly/detail/Malloc.h"
+
+#include <string>
+namespace folly {
+  using std::goodMallocSize;
+  using std::jemallocMinInPlaceExpandable;
+  using std::usingJEMalloc;
+  using std::smartRealloc;
+  using std::checkedMalloc;
+  using std::checkedCalloc;
+  using std::checkedRealloc;
+}
+
+#else // !defined(_GLIBCXX_USE_FB) || defined(_LIBSTDCXX_FBSTRING)
+
+#ifdef _LIBSTDCXX_FBSTRING
+#pragma GCC system_header
+
+/**
+ * Declare rallocm() and allocm() as weak symbols. These will be provided by
+ * jemalloc if we are using jemalloc, or will be NULL if we are using another
+ * malloc implementation.
+ */
+extern "C" int rallocm(void**, size_t*, size_t, size_t, int)
+__attribute__((weak));
+extern "C" int allocm(void**, size_t*, size_t, int)
+__attribute__((weak));
+
+#include <bits/functexcept.h>
+#define FOLLY_HAVE_MALLOC_H 1
+#else
+#include "folly/detail/Malloc.h"
+#include "folly/Portability.h"
+#endif
+
+// for malloc_usable_size
+// NOTE: FreeBSD 9 doesn't have malloc.h.  It's defitions
+// are found in stdlib.h.
+#if FOLLY_HAVE_MALLOC_H
+#include <malloc.h>
+#else
+#include <stdlib.h>
+#endif
+
+#include <cassert>
+#include <cstddef>
+#include <cstdlib>
+#include <cstring>
+
+#include <new>
+
+/**
+ * Define various ALLOCM_* macros normally provided by jemalloc.  We define
+ * them so that we don't have to include jemalloc.h, in case the program is
+ * built without jemalloc support.
+ */
+#ifndef ALLOCM_SUCCESS
+
+#define ALLOCM_SUCCESS 0
+#define ALLOCM_ERR_OOM 1
+#define ALLOCM_ERR_NOT_MOVED 2
+
+#define ALLOCM_ZERO    64
+#define ALLOCM_NO_MOVE 128
+
+#define ALLOCM_LG_ALIGN(la) (la)
+
+#if defined(JEMALLOC_MANGLE) && defined(JEMALLOC_EXPERIMENTAL)
+#define rallocm je_rallocm
+#define allocm je_allocm
+#endif
+
+#endif /* ALLOCM_SUCCESS */
+
+#ifdef _LIBSTDCXX_FBSTRING
+namespace std _GLIBCXX_VISIBILITY(default) {
+_GLIBCXX_BEGIN_NAMESPACE_VERSION
+#else
+namespace folly {
+#endif
+
+
+/**
+ * Determine if we are using jemalloc or not.
+ */
+inline bool usingJEMalloc() {
+  return rallocm != NULL;
+}
+
+/**
+ * For jemalloc's size classes, see
+ * http://www.canonware.com/download/jemalloc/jemalloc-latest/doc/jemalloc.html
+ */
+inline size_t goodMallocSize(size_t minSize) {
+  if (!usingJEMalloc()) {
+    // Not using jemalloc - no smarts
+    return minSize;
+  }
+  if (minSize <= 64) {
+    // Choose smallest allocation to be 64 bytes - no tripping over
+    // cache line boundaries, and small string optimization takes care
+    // of short strings anyway.
+    return 64;
+  }
+  if (minSize <= 512) {
+    // Round up to the next multiple of 64; we don't want to trip over
+    // cache line boundaries.
+    return (minSize + 63) & ~size_t(63);
+  }
+  if (minSize <= 3840) {
+    // Round up to the next multiple of 256
+    return (minSize + 255) & ~size_t(255);
+  }
+  if (minSize <= 4072 * 1024) {
+    // Round up to the next multiple of 4KB
+    return (minSize + 4095) & ~size_t(4095);
+  }
+  // Holy Moly
+  // Round up to the next multiple of 4MB
+  return (minSize + 4194303) & ~size_t(4194303);
+}
+
+// We always request "good" sizes for allocation, so jemalloc can
+// never grow in place small blocks; they're already occupied to the
+// brim.  Blocks larger than or equal to 4096 bytes can in fact be
+// expanded in place, and this constant reflects that.
+static const size_t jemallocMinInPlaceExpandable = 4096;
+
+/**
+ * Trivial wrappers around malloc, calloc, realloc that check for allocation
+ * failure and throw std::bad_alloc in that case.
+ */
+inline void* checkedMalloc(size_t size) {
+  void* p = malloc(size);
+  if (!p) std::__throw_bad_alloc();
+  return p;
+}
+
+inline void* checkedCalloc(size_t n, size_t size) {
+  void* p = calloc(n, size);
+  if (!p) std::__throw_bad_alloc();
+  return p;
+}
+
+inline void* checkedRealloc(void* ptr, size_t size) {
+  void* p = realloc(ptr, size);
+  if (!p) std::__throw_bad_alloc();
+  return p;
+}
+
+/**
+ * This function tries to reallocate a buffer of which only the first
+ * currentSize bytes are used. The problem with using realloc is that
+ * if currentSize is relatively small _and_ if realloc decides it
+ * needs to move the memory chunk to a new buffer, then realloc ends
+ * up copying data that is not used. It's impossible to hook into
+ * GNU's malloc to figure whether expansion will occur in-place or as
+ * a malloc-copy-free troika. (If an expand_in_place primitive would
+ * be available, smartRealloc would use it.) As things stand, this
+ * routine just tries to call realloc() (thus benefitting of potential
+ * copy-free coalescing) unless there's too much slack memory.
+ */
+inline void* smartRealloc(void* p,
+                          const size_t currentSize,
+                          const size_t currentCapacity,
+                          const size_t newCapacity) {
+  assert(p);
+  assert(currentSize <= currentCapacity &&
+         currentCapacity < newCapacity);
+
+  if (usingJEMalloc()) {
+    // using jemalloc's API. Don't forget that jemalloc can never grow
+    // in place blocks smaller than 4096 bytes.
+    if (currentCapacity >= jemallocMinInPlaceExpandable &&
+        rallocm(&p, NULL, newCapacity, 0, ALLOCM_NO_MOVE) == ALLOCM_SUCCESS) {
+      // Managed to expand in place
+      return p;
+    }
+    // Cannot expand; must move
+    auto const result = checkedMalloc(newCapacity);
+    std::memcpy(result, p, currentSize);
+    free(p);
+    return result;
+  }
+
+  // No jemalloc no honey
+  auto const slack = currentCapacity - currentSize;
+  if (slack * 2 > currentSize) {
+    // Too much slack, malloc-copy-free cycle:
+    auto const result = checkedMalloc(newCapacity);
+    std::memcpy(result, p, currentSize);
+    free(p);
+    return result;
+  }
+  // If there's not too much slack, we realloc in hope of coalescing
+  return checkedRealloc(p, newCapacity);
+}
+
+#ifdef _LIBSTDCXX_FBSTRING
+_GLIBCXX_END_NAMESPACE_VERSION
+#endif
+
+} // folly
+
+#endif // !defined(_GLIBCXX_USE_FB) || defined(_LIBSTDCXX_FBSTRING)
+
+#endif // FOLLY_MALLOC_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/MapUtil.h
@@ -0,0 +1,72 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_MAPUTIL_H_
+#define FOLLY_MAPUTIL_H_
+
+namespace folly {
+
+/**
+ * Given a map and a key, return the value corresponding to the key in the map,
+ * or a given default value if the key doesn't exist in the map.
+ */
+template <class Map>
+typename Map::mapped_type get_default(
+    const Map& map, const typename Map::key_type& key,
+    const typename Map::mapped_type& dflt =
+    typename Map::mapped_type()) {
+  auto pos = map.find(key);
+  return (pos != map.end() ? pos->second : dflt);
+}
+
+/**
+ * Given a map and a key, return a reference to the value corresponding to the
+ * key in the map, or the given default reference if the key doesn't exist in
+ * the map.
+ */
+template <class Map>
+const typename Map::mapped_type& get_ref_default(
+    const Map& map, const typename Map::key_type& key,
+    const typename Map::mapped_type& dflt) {
+  auto pos = map.find(key);
+  return (pos != map.end() ? pos->second : dflt);
+}
+
+/**
+ * Given a map and a key, return a pointer to the value corresponding to the
+ * key in the map, or nullptr if the key doesn't exist in the map.
+ */
+template <class Map>
+const typename Map::mapped_type* get_ptr(
+    const Map& map, const typename Map::key_type& key) {
+  auto pos = map.find(key);
+  return (pos != map.end() ? &pos->second : nullptr);
+}
+
+/**
+ * Non-const overload of the above.
+ */
+template <class Map>
+typename Map::mapped_type* get_ptr(
+    Map& map, const typename Map::key_type& key) {
+  auto pos = map.find(key);
+  return (pos != map.end() ? &pos->second : nullptr);
+}
+
+}  // namespace folly
+
+#endif /* FOLLY_MAPUTIL_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/Memory.h
@@ -0,0 +1,346 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_MEMORY_H_
+#define FOLLY_MEMORY_H_
+
+#include "folly/Traits.h"
+
+#include <memory>
+#include <limits>
+#include <utility>
+#include <exception>
+#include <stdexcept>
+
+#include <cstddef>
+
+namespace folly {
+
+/**
+ * For exception safety and consistency with make_shared. Erase me when
+ * we have std::make_unique().
+ *
+ * @author Louis Brandy (ldbrandy@fb.com)
+ * @author Xu Ning (xning@fb.com)
+ */
+
+template<typename T, typename Dp = std::default_delete<T>, typename... Args>
+std::unique_ptr<T, Dp> make_unique(Args&&... args) {
+  return std::unique_ptr<T, Dp>(new T(std::forward<Args>(args)...));
+}
+
+/*
+ * StlAllocator wraps a SimpleAllocator into a STL-compliant
+ * allocator, maintaining an instance pointer to the simple allocator
+ * object.  The underlying SimpleAllocator object must outlive all
+ * instances of StlAllocator using it.
+ *
+ * A SimpleAllocator must provide two methods:
+ *
+ *    void* allocate(size_t size);
+ *    void deallocate(void* ptr);
+ *
+ * which, respectively, allocate a block of size bytes (aligned to the
+ * maximum alignment required on your system), throwing std::bad_alloc
+ * if the allocation can't be satisfied, and free a previously
+ * allocated block.
+ *
+ * Note that the following allocator resembles the standard allocator
+ * quite well:
+ *
+ * class MallocAllocator {
+ *  public:
+ *   void* allocate(size_t size) {
+ *     void* p = malloc(size);
+ *     if (!p) throw std::bad_alloc();
+ *     return p;
+ *   }
+ *   void deallocate(void* p) {
+ *     free(p);
+ *   }
+ * };
+ *
+ * But note that if you pass StlAllocator<MallocAllocator,...> to a
+ * standard container it will be larger due to the contained state
+ * pointer.
+ *
+ * author: Tudor Bosman <tudorb@fb.com>
+ */
+
+// This would be so much simpler with std::allocator_traits, but gcc 4.6.2
+// doesn't support it.
+template <class Alloc, class T> class StlAllocator;
+
+template <class Alloc> class StlAllocator<Alloc, void> {
+ public:
+  typedef void value_type;
+  typedef void* pointer;
+  typedef const void* const_pointer;
+
+  StlAllocator() : alloc_(nullptr) { }
+  explicit StlAllocator(Alloc* a) : alloc_(a) { }
+
+  Alloc* alloc() const {
+    return alloc_;
+  }
+
+  template <class U> struct rebind {
+    typedef StlAllocator<Alloc, U> other;
+  };
+
+  bool operator!=(const StlAllocator<Alloc, void>& other) const {
+    return alloc_ != other.alloc_;
+  }
+
+  bool operator==(const StlAllocator<Alloc, void>& other) const {
+    return alloc_ == other.alloc_;
+  }
+
+ private:
+  Alloc* alloc_;
+};
+
+template <class Alloc, class T>
+class StlAllocator {
+ public:
+  typedef T value_type;
+  typedef T* pointer;
+  typedef const T* const_pointer;
+  typedef T& reference;
+  typedef const T& const_reference;
+
+  typedef ptrdiff_t difference_type;
+  typedef size_t size_type;
+
+  StlAllocator() : alloc_(nullptr) { }
+  explicit StlAllocator(Alloc* a) : alloc_(a) { }
+
+  template <class U> StlAllocator(const StlAllocator<Alloc, U>& other)
+    : alloc_(other.alloc()) { }
+
+  T* allocate(size_t n, const void* hint = nullptr) {
+    return static_cast<T*>(alloc_->allocate(n * sizeof(T)));
+  }
+
+  void deallocate(T* p, size_t n) {
+    alloc_->deallocate(p);
+  }
+
+  size_t max_size() const {
+    return std::numeric_limits<size_t>::max();
+  }
+
+  T* address(T& x) const {
+    return std::addressof(x);
+  }
+
+  const T* address(const T& x) const {
+    return std::addressof(x);
+  }
+
+  template <class... Args>
+  void construct(T* p, Args&&... args) {
+    new (p) T(std::forward<Args>(args)...);
+  }
+
+  void destroy(T* p) {
+    p->~T();
+  }
+
+  Alloc* alloc() const {
+    return alloc_;
+  }
+
+  template <class U> struct rebind {
+    typedef StlAllocator<Alloc, U> other;
+  };
+
+  bool operator!=(const StlAllocator<Alloc, T>& other) const {
+    return alloc_ != other.alloc_;
+  }
+
+  bool operator==(const StlAllocator<Alloc, T>& other) const {
+    return alloc_ == other.alloc_;
+  }
+
+ private:
+  Alloc* alloc_;
+};
+
+/**
+ * Helper function to obtain rebound allocators
+ *
+ * @author: Marcelo Juchem <marcelo@fb.com>
+ */
+template <typename T, typename Allocator>
+typename Allocator::template rebind<T>::other rebind_allocator(
+  Allocator const& allocator
+) {
+  return typename Allocator::template rebind<T>::other(allocator);
+}
+
+/*
+ * Helper classes/functions for creating a unique_ptr using a custom
+ * allocator.
+ *
+ * @author: Marcelo Juchem <marcelo@fb.com>
+ */
+
+// Derives from the allocator to take advantage of the empty base
+// optimization when possible.
+template <typename Allocator>
+class allocator_delete
+  : private std::remove_reference<Allocator>::type
+{
+  typedef typename std::remove_reference<Allocator>::type allocator_type;
+
+public:
+  typedef typename Allocator::pointer pointer;
+
+  allocator_delete() = default;
+
+  explicit allocator_delete(const allocator_type& allocator)
+    : allocator_type(allocator)
+  {}
+
+  explicit allocator_delete(allocator_type&& allocator)
+    : allocator_type(std::move(allocator))
+  {}
+
+  template <typename U>
+  allocator_delete(const allocator_delete<U>& other)
+    : allocator_type(other.get_allocator())
+  {}
+
+  allocator_type& get_allocator() const {
+    return *const_cast<allocator_delete*>(this);
+  }
+
+  void operator()(pointer p) const {
+    if (!p) return;
+    const_cast<allocator_delete*>(this)->destroy(p);
+    const_cast<allocator_delete*>(this)->deallocate(p, 1);
+  }
+};
+
+template <typename T, typename Allocator>
+class is_simple_allocator {
+  FOLLY_CREATE_HAS_MEMBER_FN_TRAITS(has_destroy, destroy);
+
+  typedef typename std::remove_const<
+    typename std::remove_reference<Allocator>::type
+  >::type allocator;
+  typedef typename std::remove_reference<T>::type value_type;
+  typedef value_type* pointer;
+
+public:
+  constexpr static bool value = !has_destroy<allocator, void(pointer)>::value
+    && !has_destroy<allocator, void(void*)>::value;
+};
+
+template <typename T, typename Allocator>
+struct as_stl_allocator {
+  typedef typename std::conditional<
+    is_simple_allocator<T, Allocator>::value,
+    folly::StlAllocator<
+      typename std::remove_reference<Allocator>::type,
+      typename std::remove_reference<T>::type
+    >,
+    typename std::remove_reference<Allocator>::type
+  >::type type;
+};
+
+template <typename T, typename Allocator>
+typename std::enable_if<
+  is_simple_allocator<T, Allocator>::value,
+  folly::StlAllocator<
+    typename std::remove_reference<Allocator>::type,
+    typename std::remove_reference<T>::type
+  >
+>::type make_stl_allocator(Allocator&& allocator) {
+  return folly::StlAllocator<
+    typename std::remove_reference<Allocator>::type,
+    typename std::remove_reference<T>::type
+  >(&allocator);
+}
+
+template <typename T, typename Allocator>
+typename std::enable_if<
+  !is_simple_allocator<T, Allocator>::value,
+  typename std::remove_reference<Allocator>::type
+>::type make_stl_allocator(Allocator&& allocator) {
+  return std::move(allocator);
+}
+
+/**
+ * AllocatorUniquePtr: a unique_ptr that supports both STL-style
+ * allocators and SimpleAllocator
+ *
+ * @author: Marcelo Juchem <marcelo@fb.com>
+ */
+
+template <typename T, typename Allocator>
+struct AllocatorUniquePtr {
+  typedef std::unique_ptr<T,
+    folly::allocator_delete<
+      typename std::conditional<
+        is_simple_allocator<T, Allocator>::value,
+        folly::StlAllocator<typename std::remove_reference<Allocator>::type, T>,
+        typename std::remove_reference<Allocator>::type
+      >::type
+    >
+  > type;
+};
+
+/**
+ * Functions to allocate a unique_ptr / shared_ptr, supporting both
+ * STL-style allocators and SimpleAllocator, analog to std::allocate_shared
+ *
+ * @author: Marcelo Juchem <marcelo@fb.com>
+ */
+
+template <typename T, typename Allocator, typename ...Args>
+typename AllocatorUniquePtr<T, Allocator>::type allocate_unique(
+  Allocator&& allocator, Args&&... args
+) {
+  auto stlAllocator = folly::make_stl_allocator<T>(
+    std::forward<Allocator>(allocator)
+  );
+  auto p = stlAllocator.allocate(1);
+
+  try {
+    stlAllocator.construct(p, std::forward<Args>(args)...);
+
+    return {p,
+      folly::allocator_delete<decltype(stlAllocator)>(std::move(stlAllocator))
+    };
+  } catch (...) {
+    stlAllocator.deallocate(p, 1);
+    throw;
+  }
+}
+
+template <typename T, typename Allocator, typename ...Args>
+std::shared_ptr<T> allocate_shared(Allocator&& allocator, Args&&... args) {
+  return std::allocate_shared<T>(
+    folly::make_stl_allocator<T>(std::forward<Allocator>(allocator)),
+    std::forward<Args>(args)...
+  );
+}
+
+}  // namespace folly
+
+#endif /* FOLLY_MEMORY_H_ */
--- /dev/null
+++ b/hphp/submodules/folly/folly/MemoryMapping.cpp
@@ -0,0 +1,222 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/MemoryMapping.h"
+#include "folly/Format.h"
+
+#include <fcntl.h>
+#include <sys/mman.h>
+#include <sys/types.h>
+#include <system_error>
+#include <gflags/gflags.h>
+
+DEFINE_int64(mlock_chunk_size, 1 << 20,  // 1MB
+             "Maximum bytes to mlock/munlock/munmap at once "
+             "(will be rounded up to PAGESIZE)");
+
+namespace folly {
+
+/* protected constructor */
+MemoryMapping::MemoryMapping()
+  : mapStart_(nullptr)
+  , mapLength_(0)
+  , locked_(false) {
+}
+
+MemoryMapping::MemoryMapping(File file, off_t offset, off_t length)
+  : mapStart_(nullptr)
+  , mapLength_(0)
+  , locked_(false) {
+
+  init(std::move(file), offset, length, PROT_READ, false);
+}
+
+MemoryMapping::MemoryMapping(const char* name, off_t offset, off_t length)
+  : MemoryMapping(File(name), offset, length) { }
+
+MemoryMapping::MemoryMapping(int fd, off_t offset, off_t length)
+  : MemoryMapping(File(fd), offset, length) { }
+
+void MemoryMapping::init(File file,
+                         off_t offset, off_t length,
+                         int prot,
+                         bool grow) {
+  off_t pageSize = sysconf(_SC_PAGESIZE);
+  CHECK_GE(offset, 0);
+
+  // Round down the start of the mapped region
+  size_t skipStart = offset % pageSize;
+  offset -= skipStart;
+
+  file_ = std::move(file);
+  mapLength_ = length;
+  if (mapLength_ != -1) {
+    mapLength_ += skipStart;
+
+    // Round up the end of the mapped region
+    mapLength_ = (mapLength_ + pageSize - 1) / pageSize * pageSize;
+  }
+
+  // stat the file
+  struct stat st;
+  CHECK_ERR(fstat(file_.fd(), &st));
+  off_t remaining = st.st_size - offset;
+  if (mapLength_ == -1) {
+    length = mapLength_ = remaining;
+  } else {
+    if (length > remaining) {
+      if (grow) {
+        PCHECK(0 == ftruncate(file_.fd(), offset + length))
+          << "ftructate() failed, couldn't grow file";
+        remaining = length;
+      } else {
+        length = remaining;
+      }
+    }
+    if (mapLength_ > remaining) mapLength_ = remaining;
+  }
+
+  if (length == 0) {
+    mapLength_ = 0;
+    mapStart_ = nullptr;
+  } else {
+    unsigned char* start = static_cast<unsigned char*>(
+      mmap(nullptr, mapLength_, prot, MAP_SHARED, file_.fd(), offset));
+    PCHECK(start != MAP_FAILED)
+      << " offset=" << offset
+      << " length=" << mapLength_;
+    mapStart_ = start;
+    data_.reset(start + skipStart, length);
+  }
+}
+
+namespace {
+
+off_t memOpChunkSize(off_t length) {
+  off_t chunkSize = length;
+  if (FLAGS_mlock_chunk_size <= 0) {
+    return chunkSize;
+  }
+
+  chunkSize = FLAGS_mlock_chunk_size;
+  off_t pageSize = sysconf(_SC_PAGESIZE);
+  off_t r = chunkSize % pageSize;
+  if (r) {
+    chunkSize += (pageSize - r);
+  }
+  return chunkSize;
+}
+
+/**
+ * Run @op in chunks over the buffer @mem of @bufSize length.
+ *
+ * Return:
+ * - success: true + amountSucceeded == bufSize (op success on whole buffer)
+ * - failure: false + amountSucceeded == nr bytes on which op succeeded.
+ */
+bool memOpInChunks(std::function<int(void*, size_t)> op,
+                   void* mem, size_t bufSize,
+                   size_t& amountSucceeded) {
+  // unmap/mlock/munlock take a kernel semaphore and block other threads from
+  // doing other memory operations. If the size of the buffer is big the
+  // semaphore can be down for seconds (for benchmarks see
+  // http://kostja-osipov.livejournal.com/42963.html).  Doing the operations in
+  // chunks breaks the locking into intervals and lets other threads do memory
+  // operations of their own.
+
+  size_t chunkSize = memOpChunkSize(bufSize);
+
+  char* addr = static_cast<char*>(mem);
+  amountSucceeded = 0;
+
+  while (amountSucceeded < bufSize) {
+    size_t size = std::min(chunkSize, bufSize - amountSucceeded);
+    if (op(addr + amountSucceeded, size) != 0) {
+      return false;
+    }
+    amountSucceeded += size;
+  }
+
+  return true;
+}
+
+}  // anonymous namespace
+
+bool MemoryMapping::mlock(LockMode lock) {
+  size_t amountSucceeded = 0;
+  locked_ = memOpInChunks(::mlock, mapStart_, mapLength_, amountSucceeded);
+  if (locked_) {
+    return true;
+  }
+
+  auto msg(folly::format(
+    "mlock({}) failed at {}",
+    mapLength_, amountSucceeded).str());
+
+  if (lock == LockMode::TRY_LOCK && (errno == EPERM || errno == ENOMEM)) {
+    PLOG(WARNING) << msg;
+  } else {
+    PLOG(FATAL) << msg;
+  }
+
+  // only part of the buffer was mlocked, unlock it back
+  if (!memOpInChunks(::munlock, mapStart_, amountSucceeded, amountSucceeded)) {
+    PLOG(WARNING) << "munlock()";
+  }
+
+  return false;
+}
+
+void MemoryMapping::munlock(bool dontneed) {
+  if (!locked_) return;
+
+  size_t amountSucceeded = 0;
+  if (!memOpInChunks(::munlock, mapStart_, mapLength_, amountSucceeded)) {
+    PLOG(WARNING) << "munlock()";
+  }
+  if (mapLength_ && dontneed &&
+      ::madvise(mapStart_, mapLength_, MADV_DONTNEED)) {
+    PLOG(WARNING) << "madvise()";
+  }
+  locked_ = false;
+}
+
+void MemoryMapping::hintLinearScan() {
+  advise(MADV_SEQUENTIAL);
+}
+
+MemoryMapping::~MemoryMapping() {
+  if (mapLength_) {
+    size_t amountSucceeded = 0;
+    if (!memOpInChunks(::munmap, mapStart_, mapLength_, amountSucceeded)) {
+      PLOG(FATAL) << folly::format(
+        "munmap({}) failed at {}",
+        mapLength_, amountSucceeded).str();
+    }
+  }
+}
+
+void MemoryMapping::advise(int advice) const {
+  if (mapLength_ && ::madvise(mapStart_, mapLength_, advice)) {
+    PLOG(WARNING) << "madvise()";
+  }
+}
+
+WritableMemoryMapping::WritableMemoryMapping(File file, off_t offset, off_t length) {
+  init(std::move(file), offset, length, PROT_READ | PROT_WRITE, true);
+}
+
+}  // namespace folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/MemoryMapping.h
@@ -0,0 +1,171 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_MEMORYMAPPING_H_
+#define FOLLY_MEMORYMAPPING_H_
+
+#include "folly/FBString.h"
+#include "folly/File.h"
+#include "folly/Range.h"
+#include <glog/logging.h>
+#include <boost/noncopyable.hpp>
+
+namespace folly {
+
+/**
+ * Maps files in memory (read-only).
+ *
+ * @author Tudor Bosman (tudorb@fb.com)
+ */
+class MemoryMapping : boost::noncopyable {
+ public:
+  /**
+   * Lock the pages in memory?
+   * TRY_LOCK  = try to lock, log warning if permission denied
+   * MUST_LOCK = lock, fail assertion if permission denied.
+   */
+  enum class LockMode {
+    TRY_LOCK,
+    MUST_LOCK
+  };
+  /**
+   * Map a portion of the file indicated by filename in memory, causing a CHECK
+   * failure on error.
+   *
+   * By default, map the whole file.  length=-1: map from offset to EOF.
+   * Unlike the mmap() system call, offset and length don't need to be
+   * page-aligned.  length is clipped to the end of the file if it's too large.
+   *
+   * The mapping will be destroyed (and the memory pointed-to by data() will
+   * likely become inaccessible) when the MemoryMapping object is destroyed.
+   */
+  explicit MemoryMapping(File file,
+                         off_t offset=0,
+                         off_t length=-1);
+
+  explicit MemoryMapping(const char* name,
+                         off_t offset=0,
+                         off_t length=-1);
+
+  explicit MemoryMapping(int fd,
+                         off_t offset=0,
+                         off_t length=-1);
+
+  virtual ~MemoryMapping();
+
+  /**
+   * Lock the pages in memory
+   */
+  bool mlock(LockMode lock);
+
+  /**
+   * Unlock the pages.
+   * If dontneed is true, the kernel is instructed to release these pages
+   * (per madvise(MADV_DONTNEED)).
+   */
+  void munlock(bool dontneed=false);
+
+  /**
+   * Hint that these pages will be scanned linearly.
+   * madvise(MADV_SEQUENTIAL)
+   */
+  void hintLinearScan();
+
+  /**
+   * Advise the kernel about memory access.
+   */
+  void advise(int advice) const;
+
+  /**
+   * A bitwise cast of the mapped bytes as range of values. Only intended for
+   * use with POD or in-place usable types.
+   */
+  template<class T>
+  Range<const T*> asRange() const {
+    size_t count = data_.size() / sizeof(T);
+    return Range<const T*>(static_cast<const T*>(
+                             static_cast<const void*>(data_.data())),
+                           count);
+  }
+
+  /**
+   * A range of bytes mapped by this mapping.
+   */
+  Range<const uint8_t*> range() const {
+    return {data_.begin(), data_.end()};
+  }
+
+  /**
+   * Return the memory area where the file was mapped.
+   */
+  StringPiece data() const {
+    return asRange<const char>();
+  }
+
+  bool mlocked() const {
+    return locked_;
+  }
+
+  int fd() const { return file_.fd(); }
+
+ protected:
+  MemoryMapping();
+
+  void init(File file,
+            off_t offset, off_t length,
+            int prot,
+            bool grow);
+
+  File file_;
+  void* mapStart_;
+  off_t mapLength_;
+  bool locked_;
+  Range<uint8_t*> data_;
+};
+
+/**
+ * Maps files in memory for writing.
+ *
+ * @author Tom Jackson (tjackson@fb.com)
+ */
+class WritableMemoryMapping : public MemoryMapping {
+ public:
+  explicit WritableMemoryMapping(File file,
+                                 off_t offset = 0,
+                                 off_t length = -1);
+  /**
+   * A bitwise cast of the mapped bytes as range of mutable values. Only
+   * intended for use with POD or in-place usable types.
+   */
+  template<class T>
+  Range<T*> asWritableRange() const {
+    size_t count = data_.size() / sizeof(T);
+    return Range<T*>(static_cast<T*>(
+                       static_cast<void*>(data_.data())),
+                     count);
+  }
+
+  /**
+   * A range of mutable bytes mapped by this mapping.
+   */
+  Range<uint8_t*> writableRange() const {
+    return data_;
+  }
+};
+
+}  // namespace folly
+
+#endif /* FOLLY_MEMORYMAPPING_H_ */
--- /dev/null
+++ b/hphp/submodules/folly/folly/Merge.h
@@ -0,0 +1,86 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+ * folly::merge() is an implementation of std::merge with one additonal
+ * guarantee: if the input ranges overlap, the order that values *from the two
+ * different ranges* appear in the output is well defined (std::merge only
+ * guarantees relative ordering is maintained within a single input range).
+ * This semantic is very useful when the output container removes duplicates
+ * (such as std::map) to guarantee that elements from b override elements from
+ * a.
+ *
+ * ex. Let's say we have two vector<pair<int, int>> as input, and we are
+ * merging into a vector<pair<int, int>>. The comparator is returns true if the
+ * first argument has a lesser 'first' value in the pair.
+ *
+ * a = {{1, 1}, {2, 2}, {3, 3}};
+ * b = {{1, 2}, {2, 3}};
+ *
+ * folly::merge<...>(a.begin(), a.end(), b.begin(), b.end(), outputIter) is
+ * guaranteed to produce {{1, 1}, {1, 2}, {2, 2}, {2, 3}, {3, 3}}. That is,
+ * if comp(it_a, it_b) == comp(it_b, it_a) == false, we first insert the element
+ * from a.
+ */
+
+#ifndef FOLLY_MERGE_H_
+#define FOLLY_MERGE_H_
+
+#include <algorithm>
+
+namespace folly {
+
+template<class InputIt1, class InputIt2, class OutputIt, class Compare>
+OutputIt merge(InputIt1 first1, InputIt1 last1,
+               InputIt2 first2, InputIt2 last2,
+               OutputIt d_first, Compare comp) {
+  for (; first1 != last1; ++d_first) {
+    if (first2 == last2) {
+      return std::copy(first1, last1, d_first);
+    }
+    if (comp(*first2, *first1)) {
+      *d_first = *first2;
+      ++first2;
+    } else {
+      *d_first = *first1;
+      ++first1;
+    }
+  }
+  return std::copy(first2, last2, d_first);
+}
+
+template<class InputIt1, class InputIt2, class OutputIt>
+OutputIt merge(InputIt1 first1, InputIt1 last1,
+               InputIt2 first2, InputIt2 last2,
+               OutputIt d_first) {
+  for (; first1 != last1; ++d_first) {
+    if (first2 == last2) {
+      return std::copy(first1, last1, d_first);
+    }
+    if (*first2 < *first1) {
+      *d_first = *first2;
+      ++first2;
+    } else {
+      *d_first = *first1;
+      ++first1;
+    }
+  }
+  return std::copy(first2, last2, d_first);
+}
+
+}
+
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/MoveWrapper.h
@@ -0,0 +1,71 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <memory>
+
+namespace folly {
+
+/** C++11 closures don't support move-in capture. Nor does std::bind.
+    facepalm.
+
+    http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3610.html
+
+    "[...] a work-around that should make people's stomach crawl:
+    write a wrapper that performs move-on-copy, much like the deprecated
+    auto_ptr"
+
+    Unlike auto_ptr, this doesn't require a heap allocation.
+    */
+template <class T>
+class MoveWrapper {
+ public:
+  /** If value can be default-constructed, why not?
+      Then we don't have to move it in */
+  MoveWrapper() = default;
+
+  /// Move a value in.
+  explicit
+  MoveWrapper(T&& t) : value(std::move(t)) {}
+
+  /// copy is move
+  MoveWrapper(const MoveWrapper& other) : value(std::move(other.value)) {}
+
+  /// move is also move
+  MoveWrapper(MoveWrapper&& other) : value(std::move(other.value)) {}
+
+  const T& operator*() const { return value; }
+        T& operator*()       { return value; }
+
+  const T* operator->() const { return &value; }
+        T* operator->()       { return &value; }
+
+  // If you want these you're probably doing it wrong, though they'd be
+  // easy enough to implement
+  MoveWrapper& operator=(MoveWrapper const&) = delete;
+  MoveWrapper& operator=(MoveWrapper&&) = delete;
+
+ private:
+  mutable T value;
+};
+
+template <class T>
+MoveWrapper<T> makeMoveWrapper(T&& t) {
+    return MoveWrapper<T>(std::forward<T>(t));
+}
+
+} // namespace
--- /dev/null
+++ b/hphp/submodules/folly/folly/MPMCPipeline.h
@@ -0,0 +1,285 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <utility>
+
+#include <glog/logging.h>
+
+#include "folly/detail/MPMCPipelineDetail.h"
+
+namespace folly {
+
+/**
+ * Helper tag template to use amplification > 1
+ */
+template <class T, size_t Amp> class MPMCPipelineStage;
+
+/**
+ * Multi-Producer, Multi-Consumer pipeline.
+ *
+ * A N-stage pipeline is a combination of N+1 MPMC queues (see MPMCQueue.h).
+ *
+ * At each stage, you may dequeue the results from the previous stage (possibly
+ * from multiple threads) and enqueue results to the next stage. Regardless of
+ * the order of completion, data is delivered to the next stage in the original
+ * order.  Each input is matched with a "ticket" which must be produced
+ * when enqueueing to the next stage.
+ *
+ * A given stage must produce exactly K ("amplification factor", default K=1)
+ * results for every input. This is enforced by requiring that each ticket
+ * is used exactly K times.
+ *
+ * Usage:
+ *
+ * // arguments are queue sizes
+ * MPMCPipeline<int, std::string, int> pipeline(10, 10, 10);
+ *
+ * pipeline.blockingWrite(42);
+ *
+ * {
+ *   int val;
+ *   auto ticket = pipeline.blockingReadStage<0>(val);
+ *   pipeline.blockingWriteStage<0>(ticket, folly::to<std::string>(val));
+ * }
+ *
+ * {
+ *   std::string val;
+ *   auto ticket = pipeline.blockingReadStage<1>(val);
+ *   int ival = 0;
+ *   try {
+ *     ival = folly::to<int>(val);
+ *   } catch (...) {
+ *     // We must produce exactly 1 output even on exception!
+ *   }
+ *   pipeline.blockingWriteStage<1>(ticket, ival);
+ * }
+ *
+ * int result;
+ * pipeline.blockingRead(result);
+ * // result == 42
+ *
+ * To specify amplification factors greater than 1, use
+ * MPMCPipelineStage<T, amplification> instead of T in the declaration:
+ *
+ * MPMCPipeline<int,
+ *              MPMCPipelineStage<std::string, 2>,
+ *              MPMCPipelineStage<int, 4>>
+ *
+ * declares a two-stage pipeline: the first stage produces 2 strings
+ * for each input int, the second stage produces 4 ints for each input string,
+ * so, overall, the pipeline produces 2*4 = 8 ints for each input int.
+ *
+ * Implementation details: we use N+1 MPMCQueue objects; each intermediate
+ * queue connects two adjacent stages.  The MPMCQueue implementation is abused;
+ * instead of using it as a queue, we insert in the output queue at the
+ * position determined by the input queue's popTicket_.  We guarantee that
+ * all slots are filled (and therefore the queue doesn't freeze) because
+ * we require that each step produces exactly K outputs for every input.
+ */
+template <class In, class... Stages> class MPMCPipeline {
+  typedef std::tuple<detail::PipelineStageInfo<Stages>...> StageInfos;
+  typedef std::tuple<
+             detail::MPMCPipelineStageImpl<In>,
+             detail::MPMCPipelineStageImpl<
+                 typename detail::PipelineStageInfo<Stages>::value_type>...>
+    StageTuple;
+  static constexpr size_t kAmplification =
+    detail::AmplificationProduct<StageInfos>::value;
+
+ public:
+  /**
+   * Ticket, returned by blockingReadStage, must be given back to
+   * blockingWriteStage. Tickets are not thread-safe.
+   */
+  template <size_t Stage>
+  class Ticket {
+   public:
+    ~Ticket() noexcept {
+      CHECK_EQ(remainingUses_, 0) << "All tickets must be completely used!";
+    }
+
+#ifndef NDEBUG
+    Ticket() noexcept
+      : owner_(nullptr),
+        remainingUses_(0),
+        value_(0xdeadbeeffaceb00c) {
+    }
+#else
+    Ticket() noexcept : remainingUses_(0) { }
+#endif
+
+    Ticket(Ticket&& other) noexcept
+      :
+#ifndef NDEBUG
+        owner_(other.owner_),
+#endif
+        remainingUses_(other.remainingUses_),
+        value_(other.value_) {
+      other.remainingUses_ = 0;
+#ifndef NDEBUG
+      other.owner_ = nullptr;
+      other.value_ = 0xdeadbeeffaceb00c;
+#endif
+    }
+
+    Ticket& operator=(Ticket&& other) noexcept {
+      if (this != &other) {
+        this->~Ticket();
+        new (this) Ticket(std::move(other));
+      }
+      return *this;
+    }
+
+   private:
+    friend class MPMCPipeline;
+#ifndef NDEBUG
+    MPMCPipeline* owner_;
+#endif
+    size_t remainingUses_;
+    uint64_t value_;
+
+
+    Ticket(MPMCPipeline* owner, size_t amplification, uint64_t value) noexcept
+      :
+#ifndef NDEBUG
+        owner_(owner),
+#endif
+        remainingUses_(amplification),
+        value_(value * amplification) {
+    }
+
+    uint64_t use(MPMCPipeline* owner) {
+      CHECK_GT(remainingUses_--, 0);
+#ifndef NDEBUG
+      CHECK(owner == owner_);
+#endif
+      return value_++;
+    }
+  };
+
+  /**
+   * Default-construct pipeline. Useful to move-assign later,
+   * just like MPMCQueue, see MPMCQueue.h for more details.
+   */
+  MPMCPipeline() { }
+
+  /**
+   * Construct a pipeline with N+1 queue sizes.
+   */
+  template <class... Sizes>
+  explicit MPMCPipeline(Sizes... sizes) : stages_(sizes...) { }
+
+  /**
+   * Push an element into (the first stage of) the pipeline. Blocking.
+   */
+  template <class... Args>
+  void blockingWrite(Args&&... args) {
+    std::get<0>(stages_).blockingWrite(std::forward<Args>(args)...);
+  }
+
+  /**
+   * Try to push an element into (the first stage of) the pipeline.
+   * Non-blocking.
+   */
+  template <class... Args>
+  bool write(Args&&... args) {
+    return std::get<0>(stages_).write(std::forward<Args>(args)...);
+  }
+
+  /**
+   * Read an element for stage Stage and obtain a ticket. Blocking.
+   */
+  template <size_t Stage>
+  Ticket<Stage> blockingReadStage(
+      typename std::tuple_element<Stage, StageTuple>::type::value_type& elem) {
+    return Ticket<Stage>(
+        this,
+        std::tuple_element<Stage, StageInfos>::type::kAmplification,
+        std::get<Stage>(stages_).blockingRead(elem));
+  }
+
+  /**
+   * Try to read an element for stage Stage and obtain a ticket.
+   * Non-blocking.
+   */
+  template <size_t Stage>
+  bool readStage(
+      Ticket<Stage>& ticket,
+      typename std::tuple_element<Stage, StageTuple>::type::value_type& elem) {
+    uint64_t tval;
+    if (!std::get<Stage>(stages_).readAndGetTicket(tval, elem)) {
+      return false;
+    }
+    ticket = Ticket<Stage>(
+        this,
+        std::tuple_element<Stage, StageInfos>::type::kAmplification,
+        tval);
+    return true;
+  }
+
+  /**
+   * Complete an element in stage Stage (pushing it for stage Stage+1).
+   * Blocking.
+   */
+  template <size_t Stage, class... Args>
+  void blockingWriteStage(Ticket<Stage>& ticket, Args&&... args) {
+    std::get<Stage+1>(stages_).blockingWriteWithTicket(
+        ticket.use(this),
+        std::forward<Args>(args)...);
+  }
+
+  /**
+   * Pop an element from (the final stage of) the pipeline. Blocking.
+   */
+  void blockingRead(
+      typename std::tuple_element<
+          sizeof...(Stages),
+          StageTuple>::type::value_type& elem) {
+    std::get<sizeof...(Stages)>(stages_).blockingRead(elem);
+  }
+
+  /**
+   * Try to pop an element from (the final stage of) the pipeline.
+   * Non-blocking.
+   */
+  bool read(
+      typename std::tuple_element<
+          sizeof...(Stages),
+          StageTuple>::type::value_type& elem) {
+    return std::get<sizeof...(Stages)>(stages_).read(elem);
+  }
+
+  /**
+   * Estimate queue size, measured as values from the last stage.
+   * (so if the pipeline has an amplification factor > 1, pushing an element
+   * into the first stage will cause sizeGuess() to be == amplification factor)
+   * Elements "in flight" (currently processed as part of a stage, so not
+   * in any queue) are also counted.
+   */
+  ssize_t sizeGuess() const noexcept {
+    return (std::get<0>(stages_).writeCount() * kAmplification -
+            std::get<sizeof...(Stages)>(stages_).readCount());
+  }
+
+ private:
+  StageTuple stages_;
+};
+
+
+}  // namespaces
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/MPMCQueue.h
@@ -0,0 +1,856 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <algorithm>
+#include <atomic>
+#include <assert.h>
+#include <boost/noncopyable.hpp>
+#include <errno.h>
+#include <limits>
+#include <linux/futex.h>
+#include <string.h>
+#include <sys/syscall.h>
+#include <type_traits>
+#include <unistd.h>
+
+#include <folly/Traits.h>
+#include <folly/detail/CacheLocality.h>
+#include <folly/detail/Futex.h>
+
+namespace folly {
+
+namespace detail {
+
+template<typename T, template<typename> class Atom>
+class SingleElementQueue;
+
+template <typename T> class MPMCPipelineStageImpl;
+
+} // namespace detail
+
+/// MPMCQueue<T> is a high-performance bounded concurrent queue that
+/// supports multiple producers, multiple consumers, and optional blocking.
+/// The queue has a fixed capacity, for which all memory will be allocated
+/// up front.  The bulk of the work of enqueuing and dequeuing can be
+/// performed in parallel.
+///
+/// The underlying implementation uses a ticket dispenser for the head and
+/// the tail, spreading accesses across N single-element queues to produce
+/// a queue with capacity N.  The ticket dispensers use atomic increment,
+/// which is more robust to contention than a CAS loop.  Each of the
+/// single-element queues uses its own CAS to serialize access, with an
+/// adaptive spin cutoff.  When spinning fails on a single-element queue
+/// it uses futex()'s _BITSET operations to reduce unnecessary wakeups
+/// even if multiple waiters are present on an individual queue (such as
+/// when the MPMCQueue's capacity is smaller than the number of enqueuers
+/// or dequeuers).
+///
+/// NOEXCEPT INTERACTION: Ticket-based queues separate the assignment
+/// of In benchmarks (contained in tao/queues/ConcurrentQueueTests)
+/// it handles 1 to 1, 1 to N, N to 1, and N to M thread counts better
+/// than any of the alternatives present in fbcode, for both small (~10)
+/// and large capacities.  In these benchmarks it is also faster than
+/// tbb::concurrent_bounded_queue for all configurations.  When there are
+/// many more threads than cores, MPMCQueue is _much_ faster than the tbb
+/// queue because it uses futex() to block and unblock waiting threads,
+/// rather than spinning with sched_yield.
+///
+/// queue positions from the actual construction of the in-queue elements,
+/// which means that the T constructor used during enqueue must not throw
+/// an exception.  This is enforced at compile time using type traits,
+/// which requires that T be adorned with accurate noexcept information.
+/// If your type does not use noexcept, you will have to wrap it in
+/// something that provides the guarantee.  We provide an alternate
+/// safe implementation for types that don't use noexcept but that are
+/// marked folly::IsRelocatable and boost::has_nothrow_constructor,
+/// which is common for folly types.  In particular, if you can declare
+/// FOLLY_ASSUME_FBVECTOR_COMPATIBLE then your type can be put in
+/// MPMCQueue.
+template<typename T,
+         template<typename> class Atom = std::atomic>
+class MPMCQueue : boost::noncopyable {
+
+  static_assert(std::is_nothrow_constructible<T,T&&>::value ||
+                folly::IsRelocatable<T>::value,
+      "T must be relocatable or have a noexcept move constructor");
+
+  friend class detail::MPMCPipelineStageImpl<T>;
+ public:
+  typedef T value_type;
+
+  explicit MPMCQueue(size_t queueCapacity)
+    : capacity_(queueCapacity)
+    , slots_(new detail::SingleElementQueue<T,Atom>[queueCapacity +
+                                                    2 * kSlotPadding])
+    , stride_(computeStride(queueCapacity))
+    , pushTicket_(0)
+    , popTicket_(0)
+    , pushSpinCutoff_(0)
+    , popSpinCutoff_(0)
+  {
+    // ideally this would be a static assert, but g++ doesn't allow it
+    assert(alignof(MPMCQueue<T,Atom>)
+           >= detail::CacheLocality::kFalseSharingRange);
+    assert(static_cast<uint8_t*>(static_cast<void*>(&popTicket_))
+           - static_cast<uint8_t*>(static_cast<void*>(&pushTicket_))
+           >= detail::CacheLocality::kFalseSharingRange);
+  }
+
+  /// A default-constructed queue is useful because a usable (non-zero
+  /// capacity) queue can be moved onto it or swapped with it
+  MPMCQueue() noexcept
+    : capacity_(0)
+    , slots_(nullptr)
+    , stride_(0)
+    , pushTicket_(0)
+    , popTicket_(0)
+    , pushSpinCutoff_(0)
+    , popSpinCutoff_(0)
+  {}
+
+  /// IMPORTANT: The move constructor is here to make it easier to perform
+  /// the initialization phase, it is not safe to use when there are any
+  /// concurrent accesses (this is not checked).
+  MPMCQueue(MPMCQueue<T,Atom>&& rhs) noexcept
+    : capacity_(rhs.capacity_)
+    , slots_(rhs.slots_)
+    , stride_(rhs.stride_)
+    , pushTicket_(rhs.pushTicket_.load(std::memory_order_relaxed))
+    , popTicket_(rhs.popTicket_.load(std::memory_order_relaxed))
+    , pushSpinCutoff_(rhs.pushSpinCutoff_.load(std::memory_order_relaxed))
+    , popSpinCutoff_(rhs.popSpinCutoff_.load(std::memory_order_relaxed))
+  {
+    // relaxed ops are okay for the previous reads, since rhs queue can't
+    // be in concurrent use
+
+    // zero out rhs
+    rhs.capacity_ = 0;
+    rhs.slots_ = nullptr;
+    rhs.stride_ = 0;
+    rhs.pushTicket_.store(0, std::memory_order_relaxed);
+    rhs.popTicket_.store(0, std::memory_order_relaxed);
+    rhs.pushSpinCutoff_.store(0, std::memory_order_relaxed);
+    rhs.popSpinCutoff_.store(0, std::memory_order_relaxed);
+  }
+
+  /// IMPORTANT: The move operator is here to make it easier to perform
+  /// the initialization phase, it is not safe to use when there are any
+  /// concurrent accesses (this is not checked).
+  MPMCQueue<T,Atom> const& operator= (MPMCQueue<T,Atom>&& rhs) {
+    if (this != &rhs) {
+      this->~MPMCQueue();
+      new (this) MPMCQueue(std::move(rhs));
+    }
+    return *this;
+  }
+
+  /// MPMCQueue can only be safely destroyed when there are no
+  /// pending enqueuers or dequeuers (this is not checked).
+  ~MPMCQueue() {
+    delete[] slots_;
+  }
+
+  /// Returns the number of successful reads minus the number of successful
+  /// writes.  Waiting blockingRead and blockingWrite calls are included,
+  /// so this value can be negative.
+  ssize_t size() const noexcept {
+    // since both pushes and pops increase monotonically, we can get a
+    // consistent snapshot either by bracketing a read of popTicket_ with
+    // two reads of pushTicket_ that return the same value, or the other
+    // way around.  We maximize our chances by alternately attempting
+    // both bracketings.
+    uint64_t pushes = pushTicket_.load(std::memory_order_acquire); // A
+    uint64_t pops = popTicket_.load(std::memory_order_acquire); // B
+    while (true) {
+      uint64_t nextPushes = pushTicket_.load(std::memory_order_acquire); // C
+      if (pushes == nextPushes) {
+        // pushTicket_ didn't change from A (or the previous C) to C,
+        // so we can linearize at B (or D)
+        return pushes - pops;
+      }
+      pushes = nextPushes;
+      uint64_t nextPops = popTicket_.load(std::memory_order_acquire); // D
+      if (pops == nextPops) {
+        // popTicket_ didn't chance from B (or the previous D), so we
+        // can linearize at C
+        return pushes - pops;
+      }
+      pops = nextPops;
+    }
+  }
+
+  /// Returns true if there are no items available for dequeue
+  bool isEmpty() const noexcept {
+    return size() <= 0;
+  }
+
+  /// Returns true if there is currently no empty space to enqueue
+  bool isFull() const noexcept {
+    // careful with signed -> unsigned promotion, since size can be negative
+    return size() >= static_cast<ssize_t>(capacity_);
+  }
+
+  /// Returns is a guess at size() for contexts that don't need a precise
+  /// value, such as stats.
+  ssize_t sizeGuess() const noexcept {
+    return writeCount() - readCount();
+  }
+
+  /// Doesn't change
+  size_t capacity() const noexcept {
+    return capacity_;
+  }
+
+  /// Returns the total number of calls to blockingWrite or successful
+  /// calls to write, including those blockingWrite calls that are
+  /// currently blocking
+  uint64_t writeCount() const noexcept {
+    return pushTicket_.load(std::memory_order_acquire);
+  }
+
+  /// Returns the total number of calls to blockingRead or successful
+  /// calls to read, including those blockingRead calls that are currently
+  /// blocking
+  uint64_t readCount() const noexcept {
+    return popTicket_.load(std::memory_order_acquire);
+  }
+
+  /// Enqueues a T constructed from args, blocking until space is
+  /// available.  Note that this method signature allows enqueue via
+  /// move, if args is a T rvalue, via copy, if args is a T lvalue, or
+  /// via emplacement if args is an initializer list that can be passed
+  /// to a T constructor.
+  template <typename ...Args>
+  void blockingWrite(Args&&... args) noexcept {
+    enqueueWithTicket(pushTicket_++, std::forward<Args>(args)...);
+  }
+
+  /// If an item can be enqueued with no blocking, does so and returns
+  /// true, otherwise returns false.  This method is similar to
+  /// writeIfNotFull, but if you don't have a specific need for that
+  /// method you should use this one.
+  ///
+  /// One of the common usages of this method is to enqueue via the
+  /// move constructor, something like q.write(std::move(x)).  If write
+  /// returns false because the queue is full then x has not actually been
+  /// consumed, which looks strange.  To understand why it is actually okay
+  /// to use x afterward, remember that std::move is just a typecast that
+  /// provides an rvalue reference that enables use of a move constructor
+  /// or operator.  std::move doesn't actually move anything.  It could
+  /// more accurately be called std::rvalue_cast or std::move_permission.
+  template <typename ...Args>
+  bool write(Args&&... args) noexcept {
+    uint64_t ticket;
+    if (tryObtainReadyPushTicket(ticket)) {
+      // we have pre-validated that the ticket won't block
+      enqueueWithTicket(ticket, std::forward<Args>(args)...);
+      return true;
+    } else {
+      return false;
+    }
+  }
+
+  /// If the queue is not full, enqueues and returns true, otherwise
+  /// returns false.  Unlike write this method can be blocked by another
+  /// thread, specifically a read that has linearized (been assigned
+  /// a ticket) but not yet completed.  If you don't really need this
+  /// function you should probably use write.
+  ///
+  /// MPMCQueue isn't lock-free, so just because a read operation has
+  /// linearized (and isFull is false) doesn't mean that space has been
+  /// made available for another write.  In this situation write will
+  /// return false, but writeIfNotFull will wait for the dequeue to finish.
+  /// This method is required if you are composing queues and managing
+  /// your own wakeup, because it guarantees that after every successful
+  /// write a readIfNotFull will succeed.
+  template <typename ...Args>
+  bool writeIfNotFull(Args&&... args) noexcept {
+    uint64_t ticket;
+    if (tryObtainPromisedPushTicket(ticket)) {
+      // some other thread is already dequeuing the slot into which we
+      // are going to enqueue, but we might have to wait for them to finish
+      enqueueWithTicket(ticket, std::forward<Args>(args)...);
+      return true;
+    } else {
+      return false;
+    }
+  }
+
+  /// Moves a dequeued element onto elem, blocking until an element
+  /// is available
+  void blockingRead(T& elem) noexcept {
+    dequeueWithTicket(popTicket_++, elem);
+  }
+
+  /// If an item can be dequeued with no blocking, does so and returns
+  /// true, otherwise returns false.
+  bool read(T& elem) noexcept {
+    uint64_t ticket;
+    if (tryObtainReadyPopTicket(ticket)) {
+      // the ticket has been pre-validated to not block
+      dequeueWithTicket(ticket, elem);
+      return true;
+    } else {
+      return false;
+    }
+  }
+
+  /// If the queue is not empty, dequeues and returns true, otherwise
+  /// returns false.  If the matching write is still in progress then this
+  /// method may block waiting for it.  If you don't rely on being able
+  /// to dequeue (such as by counting completed write) then you should
+  /// prefer read.
+  bool readIfNotEmpty(T& elem) noexcept {
+    uint64_t ticket;
+    if (tryObtainPromisedPopTicket(ticket)) {
+      // the matching enqueue already has a ticket, but might not be done
+      dequeueWithTicket(ticket, elem);
+      return true;
+    } else {
+      return false;
+    }
+  }
+
+ private:
+  enum {
+    /// Once every kAdaptationFreq we will spin longer, to try to estimate
+    /// the proper spin backoff
+    kAdaptationFreq = 128,
+
+    /// To avoid false sharing in slots_ with neighboring memory
+    /// allocations, we pad it with this many SingleElementQueue-s at
+    /// each end
+    kSlotPadding = (detail::CacheLocality::kFalseSharingRange - 1)
+        / sizeof(detail::SingleElementQueue<T,Atom>) + 1
+  };
+
+  /// The maximum number of items in the queue at once
+  size_t FOLLY_ALIGN_TO_AVOID_FALSE_SHARING capacity_;
+
+  /// An array of capacity_ SingleElementQueue-s, each of which holds
+  /// either 0 or 1 item.  We over-allocate by 2 * kSlotPadding and don't
+  /// touch the slots at either end, to avoid false sharing
+  detail::SingleElementQueue<T,Atom>* slots_;
+
+  /// The number of slots_ indices that we advance for each ticket, to
+  /// avoid false sharing.  Ideally slots_[i] and slots_[i + stride_]
+  /// aren't on the same cache line
+  int stride_;
+
+  /// Enqueuers get tickets from here
+  Atom<uint64_t> FOLLY_ALIGN_TO_AVOID_FALSE_SHARING pushTicket_;
+
+  /// Dequeuers get tickets from here
+  Atom<uint64_t> FOLLY_ALIGN_TO_AVOID_FALSE_SHARING popTicket_;
+
+  /// This is how many times we will spin before using FUTEX_WAIT when
+  /// the queue is full on enqueue, adaptively computed by occasionally
+  /// spinning for longer and smoothing with an exponential moving average
+  Atom<int> FOLLY_ALIGN_TO_AVOID_FALSE_SHARING pushSpinCutoff_;
+
+  /// The adaptive spin cutoff when the queue is empty on dequeue
+  Atom<int> FOLLY_ALIGN_TO_AVOID_FALSE_SHARING popSpinCutoff_;
+
+  /// Alignment doesn't prevent false sharing at the end of the struct,
+  /// so fill out the last cache line
+  char padding_[detail::CacheLocality::kFalseSharingRange - sizeof(Atom<int>)];
+
+
+  /// We assign tickets in increasing order, but we don't want to
+  /// access neighboring elements of slots_ because that will lead to
+  /// false sharing (multiple cores accessing the same cache line even
+  /// though they aren't accessing the same bytes in that cache line).
+  /// To avoid this we advance by stride slots per ticket.
+  ///
+  /// We need gcd(capacity, stride) to be 1 so that we will use all
+  /// of the slots.  We ensure this by only considering prime strides,
+  /// which either have no common divisors with capacity or else have
+  /// a zero remainder after dividing by capacity.  That is sufficient
+  /// to guarantee correctness, but we also want to actually spread the
+  /// accesses away from each other to avoid false sharing (consider a
+  /// stride of 7 with a capacity of 8).  To that end we try a few taking
+  /// care to observe that advancing by -1 is as bad as advancing by 1
+  /// when in comes to false sharing.
+  ///
+  /// The simple way to avoid false sharing would be to pad each
+  /// SingleElementQueue, but since we have capacity_ of them that could
+  /// waste a lot of space.
+  static int computeStride(size_t capacity) noexcept {
+    static const int smallPrimes[] = { 2, 3, 5, 7, 11, 13, 17, 19, 23 };
+
+    int bestStride = 1;
+    size_t bestSep = 1;
+    for (int stride : smallPrimes) {
+      if ((stride % capacity) == 0 || (capacity % stride) == 0) {
+        continue;
+      }
+      size_t sep = stride % capacity;
+      sep = std::min(sep, capacity - sep);
+      if (sep > bestSep) {
+        bestStride = stride;
+        bestSep = sep;
+      }
+    }
+    return bestStride;
+  }
+
+  /// Returns the index into slots_ that should be used when enqueuing or
+  /// dequeuing with the specified ticket
+  size_t idx(uint64_t ticket) noexcept {
+    return ((ticket * stride_) % capacity_) + kSlotPadding;
+  }
+
+  /// Maps an enqueue or dequeue ticket to the turn should be used at the
+  /// corresponding SingleElementQueue
+  uint32_t turn(uint64_t ticket) noexcept {
+    return ticket / capacity_;
+  }
+
+  /// Tries to obtain a push ticket for which SingleElementQueue::enqueue
+  /// won't block.  Returns true on immediate success, false on immediate
+  /// failure.
+  bool tryObtainReadyPushTicket(uint64_t& rv) noexcept {
+    auto ticket = pushTicket_.load(std::memory_order_acquire); // A
+    while (true) {
+      if (!slots_[idx(ticket)].mayEnqueue(turn(ticket))) {
+        // if we call enqueue(ticket, ...) on the SingleElementQueue
+        // right now it would block, but this might no longer be the next
+        // ticket.  We can increase the chance of tryEnqueue success under
+        // contention (without blocking) by rechecking the ticket dispenser
+        auto prev = ticket;
+        ticket = pushTicket_.load(std::memory_order_acquire); // B
+        if (prev == ticket) {
+          // mayEnqueue was bracketed by two reads (A or prev B or prev
+          // failing CAS to B), so we are definitely unable to enqueue
+          return false;
+        }
+      } else {
+        // we will bracket the mayEnqueue check with a read (A or prev B
+        // or prev failing CAS) and the following CAS.  If the CAS fails
+        // it will effect a load of pushTicket_
+        if (pushTicket_.compare_exchange_strong(ticket, ticket + 1)) {
+          rv = ticket;
+          return true;
+        }
+      }
+    }
+  }
+
+  /// Tries to obtain a push ticket which can be satisfied if all
+  /// in-progress pops complete.  This function does not block, but
+  /// blocking may be required when using the returned ticket if some
+  /// other thread's pop is still in progress (ticket has been granted but
+  /// pop has not yet completed).
+  bool tryObtainPromisedPushTicket(uint64_t& rv) noexcept {
+    auto numPushes = pushTicket_.load(std::memory_order_acquire); // A
+    while (true) {
+      auto numPops = popTicket_.load(std::memory_order_acquire); // B
+      // n will be negative if pops are pending
+      int64_t n = numPushes - numPops;
+      if (n >= static_cast<ssize_t>(capacity_)) {
+        // Full, linearize at B.  We don't need to recheck the read we
+        // performed at A, because if numPushes was stale at B then the
+        // real numPushes value is even worse
+        return false;
+      }
+      if (pushTicket_.compare_exchange_strong(numPushes, numPushes + 1)) {
+        rv = numPushes;
+        return true;
+      }
+    }
+  }
+
+  /// Tries to obtain a pop ticket for which SingleElementQueue::dequeue
+  /// won't block.  Returns true on immediate success, false on immediate
+  /// failure.
+  bool tryObtainReadyPopTicket(uint64_t& rv) noexcept {
+    auto ticket = popTicket_.load(std::memory_order_acquire);
+    while (true) {
+      if (!slots_[idx(ticket)].mayDequeue(turn(ticket))) {
+        auto prev = ticket;
+        ticket = popTicket_.load(std::memory_order_acquire);
+        if (prev == ticket) {
+          return false;
+        }
+      } else {
+        if (popTicket_.compare_exchange_strong(ticket, ticket + 1)) {
+          rv = ticket;
+          return true;
+        }
+      }
+    }
+  }
+
+  /// Similar to tryObtainReadyPopTicket, but returns a pop ticket whose
+  /// corresponding push ticket has already been handed out, rather than
+  /// returning one whose corresponding push ticket has already been
+  /// completed.  This means that there is a possibility that the caller
+  /// will block when using the ticket, but it allows the user to rely on
+  /// the fact that if enqueue has succeeded, tryObtainPromisedPopTicket
+  /// will return true.  The "try" part of this is that we won't have
+  /// to block waiting for someone to call enqueue, although we might
+  /// have to block waiting for them to finish executing code inside the
+  /// MPMCQueue itself.
+  bool tryObtainPromisedPopTicket(uint64_t& rv) noexcept {
+    auto numPops = popTicket_.load(std::memory_order_acquire); // A
+    while (true) {
+      auto numPushes = pushTicket_.load(std::memory_order_acquire); // B
+      if (numPops >= numPushes) {
+        // Empty, or empty with pending pops.  Linearize at B.  We don't
+        // need to recheck the read we performed at A, because if numPops
+        // is stale then the fresh value is larger and the >= is still true
+        return false;
+      }
+      if (popTicket_.compare_exchange_strong(numPops, numPops + 1)) {
+        rv = numPops;
+        return true;
+      }
+    }
+  }
+
+  // Given a ticket, constructs an enqueued item using args
+  template <typename ...Args>
+  void enqueueWithTicket(uint64_t ticket, Args&&... args) noexcept {
+    slots_[idx(ticket)].enqueue(turn(ticket),
+                                pushSpinCutoff_,
+                                (ticket % kAdaptationFreq) == 0,
+                                std::forward<Args>(args)...);
+  }
+
+  // Given a ticket, dequeues the corresponding element
+  void dequeueWithTicket(uint64_t ticket, T& elem) noexcept {
+    slots_[idx(ticket)].dequeue(turn(ticket),
+                                popSpinCutoff_,
+                                (ticket % kAdaptationFreq) == 0,
+                                elem);
+  }
+};
+
+
+namespace detail {
+
+/// A TurnSequencer allows threads to order their execution according to
+/// a monotonically increasing (with wraparound) "turn" value.  The two
+/// operations provided are to wait for turn T, and to move to the next
+/// turn.  Every thread that is waiting for T must have arrived before
+/// that turn is marked completed (for MPMCQueue only one thread waits
+/// for any particular turn, so this is trivially true).
+///
+/// TurnSequencer's state_ holds 26 bits of the current turn (shifted
+/// left by 6), along with a 6 bit saturating value that records the
+/// maximum waiter minus the current turn.  Wraparound of the turn space
+/// is expected and handled.  This allows us to atomically adjust the
+/// number of outstanding waiters when we perform a FUTEX_WAKE operation.
+/// Compare this strategy to sem_t's separate num_waiters field, which
+/// isn't decremented until after the waiting thread gets scheduled,
+/// during which time more enqueues might have occurred and made pointless
+/// FUTEX_WAKE calls.
+///
+/// TurnSequencer uses futex() directly.  It is optimized for the
+/// case that the highest awaited turn is 32 or less higher than the
+/// current turn.  We use the FUTEX_WAIT_BITSET variant, which lets
+/// us embed 32 separate wakeup channels in a single futex.  See
+/// http://locklessinc.com/articles/futex_cheat_sheet for a description.
+///
+/// We only need to keep exact track of the delta between the current
+/// turn and the maximum waiter for the 32 turns that follow the current
+/// one, because waiters at turn t+32 will be awoken at turn t.  At that
+/// point they can then adjust the delta using the higher base.  Since we
+/// need to encode waiter deltas of 0 to 32 inclusive, we use 6 bits.
+/// We actually store waiter deltas up to 63, since that might reduce
+/// the number of CAS operations a tiny bit.
+///
+/// To avoid some futex() calls entirely, TurnSequencer uses an adaptive
+/// spin cutoff before waiting.  The overheads (and convergence rate)
+/// of separately tracking the spin cutoff for each TurnSequencer would
+/// be prohibitive, so the actual storage is passed in as a parameter and
+/// updated atomically.  This also lets the caller use different adaptive
+/// cutoffs for different operations (read versus write, for example).
+/// To avoid contention, the spin cutoff is only updated when requested
+/// by the caller.
+template <template<typename> class Atom>
+struct TurnSequencer {
+  explicit TurnSequencer(const uint32_t firstTurn = 0) noexcept
+      : state_(encode(firstTurn << kTurnShift, 0))
+  {}
+
+  /// Returns true iff a call to waitForTurn(turn, ...) won't block
+  bool isTurn(const uint32_t turn) const noexcept {
+    auto state = state_.load(std::memory_order_acquire);
+    return decodeCurrentSturn(state) == (turn << kTurnShift);
+  }
+
+  // Internally we always work with shifted turn values, which makes the
+  // truncation and wraparound work correctly.  This leaves us bits at
+  // the bottom to store the number of waiters.  We call shifted turns
+  // "sturns" inside this class.
+
+  /// Blocks the current thread until turn has arrived.  If
+  /// updateSpinCutoff is true then this will spin for up to kMaxSpins tries
+  /// before blocking and will adjust spinCutoff based on the results,
+  /// otherwise it will spin for at most spinCutoff spins.
+  void waitForTurn(const uint32_t turn,
+                   Atom<int>& spinCutoff,
+                   const bool updateSpinCutoff) noexcept {
+    int prevThresh = spinCutoff.load(std::memory_order_relaxed);
+    const int effectiveSpinCutoff =
+        updateSpinCutoff || prevThresh == 0 ? kMaxSpins : prevThresh;
+    int tries;
+
+    const uint32_t sturn = turn << kTurnShift;
+    for (tries = 0; ; ++tries) {
+      uint32_t state = state_.load(std::memory_order_acquire);
+      uint32_t current_sturn = decodeCurrentSturn(state);
+      if (current_sturn == sturn) {
+        break;
+      }
+
+      // wrap-safe version of assert(current_sturn < sturn)
+      assert(sturn - current_sturn < std::numeric_limits<uint32_t>::max() / 2);
+
+      // the first effectSpinCutoff tries are spins, after that we will
+      // record ourself as a waiter and block with futexWait
+      if (tries < effectiveSpinCutoff) {
+        asm volatile ("pause");
+        continue;
+      }
+
+      uint32_t current_max_waiter_delta = decodeMaxWaitersDelta(state);
+      uint32_t our_waiter_delta = (sturn - current_sturn) >> kTurnShift;
+      uint32_t new_state;
+      if (our_waiter_delta <= current_max_waiter_delta) {
+        // state already records us as waiters, probably because this
+        // isn't our first time around this loop
+        new_state = state;
+      } else {
+        new_state = encode(current_sturn, our_waiter_delta);
+        if (state != new_state &&
+            !state_.compare_exchange_strong(state, new_state)) {
+          continue;
+        }
+      }
+      state_.futexWait(new_state, futexChannel(turn));
+    }
+
+    if (updateSpinCutoff || prevThresh == 0) {
+      // if we hit kMaxSpins then spinning was pointless, so the right
+      // spinCutoff is kMinSpins
+      int target;
+      if (tries >= kMaxSpins) {
+        target = kMinSpins;
+      } else {
+        // to account for variations, we allow ourself to spin 2*N when
+        // we think that N is actually required in order to succeed
+        target = std::min(int{kMaxSpins}, std::max(int{kMinSpins}, tries * 2));
+      }
+
+      if (prevThresh == 0) {
+        // bootstrap
+        spinCutoff = target;
+      } else {
+        // try once, keep moving if CAS fails.  Exponential moving average
+        // with alpha of 7/8
+        spinCutoff.compare_exchange_weak(
+            prevThresh, prevThresh + (target - prevThresh) / 8);
+      }
+    }
+  }
+
+  /// Unblocks a thread running waitForTurn(turn + 1)
+  void completeTurn(const uint32_t turn) noexcept {
+    uint32_t state = state_.load(std::memory_order_acquire);
+    while (true) {
+      assert(state == encode(turn << kTurnShift, decodeMaxWaitersDelta(state)));
+      uint32_t max_waiter_delta = decodeMaxWaitersDelta(state);
+      uint32_t new_state = encode(
+              (turn + 1) << kTurnShift,
+              max_waiter_delta == 0 ? 0 : max_waiter_delta - 1);
+      if (state_.compare_exchange_strong(state, new_state)) {
+        if (max_waiter_delta != 0) {
+          state_.futexWake(std::numeric_limits<int>::max(),
+                           futexChannel(turn + 1));
+        }
+        break;
+      }
+      // failing compare_exchange_strong updates first arg to the value
+      // that caused the failure, so no need to reread state_
+    }
+  }
+
+  /// Returns the least-most significant byte of the current uncompleted
+  /// turn.  The full 32 bit turn cannot be recovered.
+  uint8_t uncompletedTurnLSB() const noexcept {
+    return state_.load(std::memory_order_acquire) >> kTurnShift;
+  }
+
+ private:
+  enum : uint32_t {
+    /// kTurnShift counts the bits that are stolen to record the delta
+    /// between the current turn and the maximum waiter. It needs to be big
+    /// enough to record wait deltas of 0 to 32 inclusive.  Waiters more
+    /// than 32 in the future will be woken up 32*n turns early (since
+    /// their BITSET will hit) and will adjust the waiter count again.
+    /// We go a bit beyond and let the waiter count go up to 63, which
+    /// is free and might save us a few CAS
+    kTurnShift = 6,
+    kWaitersMask = (1 << kTurnShift) - 1,
+
+    /// The minimum spin count that we will adaptively select
+    kMinSpins = 20,
+
+    /// The maximum spin count that we will adaptively select, and the
+    /// spin count that will be used when probing to get a new data point
+    /// for the adaptation
+    kMaxSpins = 2000,
+  };
+
+  /// This holds both the current turn, and the highest waiting turn,
+  /// stored as (current_turn << 6) | min(63, max(waited_turn - current_turn))
+  Futex<Atom> state_;
+
+  /// Returns the bitmask to pass futexWait or futexWake when communicating
+  /// about the specified turn
+  int futexChannel(uint32_t turn) const noexcept {
+    return 1 << (turn & 31);
+  }
+
+  uint32_t decodeCurrentSturn(uint32_t state) const noexcept {
+    return state & ~kWaitersMask;
+  }
+
+  uint32_t decodeMaxWaitersDelta(uint32_t state) const noexcept {
+    return state & kWaitersMask;
+  }
+
+  uint32_t encode(uint32_t currentSturn, uint32_t maxWaiterD) const noexcept {
+    return currentSturn | std::min(uint32_t{ kWaitersMask }, maxWaiterD);
+  }
+};
+
+
+/// SingleElementQueue implements a blocking queue that holds at most one
+/// item, and that requires its users to assign incrementing identifiers
+/// (turns) to each enqueue and dequeue operation.  Note that the turns
+/// used by SingleElementQueue are doubled inside the TurnSequencer
+template <typename T, template <typename> class Atom>
+struct SingleElementQueue {
+
+  ~SingleElementQueue() noexcept {
+    if ((sequencer_.uncompletedTurnLSB() & 1) == 1) {
+      // we are pending a dequeue, so we have a constructed item
+      destroyContents();
+    }
+  }
+
+  /// enqueue using in-place noexcept construction
+  template <typename ...Args,
+            typename = typename std::enable_if<
+                std::is_nothrow_constructible<T,Args...>::value>::type>
+  void enqueue(const uint32_t turn,
+               Atom<int>& spinCutoff,
+               const bool updateSpinCutoff,
+               Args&&... args) noexcept {
+    sequencer_.waitForTurn(turn * 2, spinCutoff, updateSpinCutoff);
+    new (&contents_) T(std::forward<Args>(args)...);
+    sequencer_.completeTurn(turn * 2);
+  }
+
+  /// enqueue using move construction, either real (if
+  /// is_nothrow_move_constructible) or simulated using relocation and
+  /// default construction (if IsRelocatable and has_nothrow_constructor)
+  template <typename = typename std::enable_if<
+                (folly::IsRelocatable<T>::value &&
+                 boost::has_nothrow_constructor<T>::value) ||
+                std::is_nothrow_constructible<T,T&&>::value>::type>
+  void enqueue(const uint32_t turn,
+               Atom<int>& spinCutoff,
+               const bool updateSpinCutoff,
+               T&& goner) noexcept {
+    if (std::is_nothrow_constructible<T,T&&>::value) {
+      // this is preferred
+      sequencer_.waitForTurn(turn * 2, spinCutoff, updateSpinCutoff);
+      new (&contents_) T(std::move(goner));
+      sequencer_.completeTurn(turn * 2);
+    } else {
+      // simulate nothrow move with relocation, followed by default
+      // construction to fill the gap we created
+      sequencer_.waitForTurn(turn * 2, spinCutoff, updateSpinCutoff);
+      memcpy(&contents_, &goner, sizeof(T));
+      sequencer_.completeTurn(turn * 2);
+      new (&goner) T();
+    }
+  }
+
+  bool mayEnqueue(const uint32_t turn) const noexcept {
+    return sequencer_.isTurn(turn * 2);
+  }
+
+  void dequeue(uint32_t turn,
+               Atom<int>& spinCutoff,
+               const bool updateSpinCutoff,
+               T& elem) noexcept {
+    if (folly::IsRelocatable<T>::value) {
+      // this version is preferred, because we do as much work as possible
+      // before waiting
+      try {
+        elem.~T();
+      } catch (...) {
+        // unlikely, but if we don't complete our turn the queue will die
+      }
+      sequencer_.waitForTurn(turn * 2 + 1, spinCutoff, updateSpinCutoff);
+      memcpy(&elem, &contents_, sizeof(T));
+      sequencer_.completeTurn(turn * 2 + 1);
+    } else {
+      // use nothrow move assignment
+      sequencer_.waitForTurn(turn * 2 + 1, spinCutoff, updateSpinCutoff);
+      elem = std::move(*ptr());
+      destroyContents();
+      sequencer_.completeTurn(turn * 2 + 1);
+    }
+  }
+
+  bool mayDequeue(const uint32_t turn) const noexcept {
+    return sequencer_.isTurn(turn * 2 + 1);
+  }
+
+ private:
+  /// Storage for a T constructed with placement new
+  typename std::aligned_storage<sizeof(T),alignof(T)>::type contents_;
+
+  /// Even turns are pushes, odd turns are pops
+  TurnSequencer<Atom> sequencer_;
+
+  T* ptr() noexcept {
+    return static_cast<T*>(static_cast<void*>(&contents_));
+  }
+
+  void destroyContents() noexcept {
+    try {
+      ptr()->~T();
+    } catch (...) {
+      // g++ doesn't seem to have std::is_nothrow_destructible yet
+    }
+#ifndef NDEBUG
+    memset(&contents_, 'Q', sizeof(T));
+#endif
+  }
+};
+
+} // namespace detail
+
+} // namespace folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/Optional.h
@@ -0,0 +1,318 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_OPTIONAL_H_
+#define FOLLY_OPTIONAL_H_
+
+/*
+ * Optional - For conditional initialization of values, like boost::optional,
+ * but with support for move semantics and emplacement.  Reference type support
+ * has not been included due to limited use cases and potential confusion with
+ * semantics of assignment: Assigning to an optional reference could quite
+ * reasonably copy its value or redirect the reference.
+ *
+ * Optional can be useful when a variable might or might not be needed:
+ *
+ *  Optional<Logger> maybeLogger = ...;
+ *  if (maybeLogger) {
+ *    maybeLogger->log("hello");
+ *  }
+ *
+ * Optional enables a 'null' value for types which do not otherwise have
+ * nullability, especially useful for parameter passing:
+ *
+ * void testIterator(const unique_ptr<Iterator>& it,
+ *                   initializer_list<int> idsExpected,
+ *                   Optional<initializer_list<int>> ranksExpected = none) {
+ *   for (int i = 0; it->next(); ++i) {
+ *     EXPECT_EQ(it->doc().id(), idsExpected[i]);
+ *     if (ranksExpected) {
+ *       EXPECT_EQ(it->doc().rank(), (*ranksExpected)[i]);
+ *     }
+ *   }
+ * }
+ *
+ * Optional models OptionalPointee, so calling 'get_pointer(opt)' will return a
+ * pointer to nullptr if the 'opt' is empty, and a pointer to the value if it is
+ * not:
+ *
+ *  Optional<int> maybeInt = ...;
+ *  if (int* v = get_pointer(maybeInt)) {
+ *    cout << *v << endl;
+ *  }
+ */
+#include <utility>
+#include <cassert>
+#include <cstddef>
+#include <type_traits>
+
+#include <boost/operators.hpp>
+
+
+namespace folly {
+
+namespace detail { struct NoneHelper {}; }
+
+typedef int detail::NoneHelper::*None;
+
+const None none = nullptr;
+
+/**
+ * gcc-4.7 warns about use of uninitialized memory around the use of storage_
+ * even though this is explicitly initialized at each point.
+ */
+#if defined(__GNUC__) && !defined(__clang__)
+# pragma GCC diagnostic push
+# pragma GCC diagnostic ignored "-Wuninitialized"
+# pragma GCC diagnostic ignored "-Wpragmas"
+# pragma GCC diagnostic ignored "-Wmaybe-uninitialized"
+#endif // __GNUC__
+
+template<class Value>
+class Optional {
+ public:
+  static_assert(!std::is_reference<Value>::value,
+                "Optional may not be used with reference types");
+
+  Optional()
+    : hasValue_(false) {
+  }
+
+  Optional(const Optional& src)
+    noexcept(std::is_nothrow_copy_constructible<Value>::value) {
+
+    if (src.hasValue()) {
+      construct(src.value());
+    } else {
+      hasValue_ = false;
+    }
+  }
+
+  Optional(Optional&& src)
+    noexcept(std::is_nothrow_move_constructible<Value>::value) {
+
+    if (src.hasValue()) {
+      construct(std::move(src.value()));
+      src.clear();
+    } else {
+      hasValue_ = false;
+    }
+  }
+
+  /* implicit */ Optional(const None&)
+    : hasValue_(false) {
+  }
+
+  /* implicit */ Optional(Value&& newValue) {
+    construct(std::move(newValue));
+  }
+
+  /* implicit */ Optional(const Value& newValue) {
+    construct(newValue);
+  }
+
+  ~Optional() noexcept {
+    clear();
+  }
+
+  void assign(const None&) {
+    clear();
+  }
+
+  void assign(Optional&& src) {
+    if (src.hasValue()) {
+      assign(std::move(src.value()));
+      src.clear();
+    } else {
+      clear();
+    }
+  }
+
+  void assign(const Optional& src) {
+    if (src.hasValue()) {
+      assign(src.value());
+    } else {
+      clear();
+    }
+  }
+
+  void assign(Value&& newValue) {
+    if (hasValue()) {
+      value_ = std::move(newValue);
+    } else {
+      construct(std::move(newValue));
+    }
+  }
+
+  void assign(const Value& newValue) {
+    if (hasValue()) {
+      value_ = newValue;
+    } else {
+      construct(newValue);
+    }
+  }
+
+  template<class Arg>
+  Optional& operator=(Arg&& arg) {
+    assign(std::forward<Arg>(arg));
+    return *this;
+  }
+
+  Optional& operator=(Optional &&other)
+    noexcept (std::is_nothrow_move_assignable<Value>::value) {
+
+    assign(std::move(other));
+    return *this;
+  }
+
+  Optional& operator=(const Optional &other)
+    noexcept (std::is_nothrow_copy_assignable<Value>::value) {
+
+    assign(other);
+    return *this;
+  }
+
+  template<class... Args>
+  void emplace(Args&&... args) {
+    clear();
+    construct(std::forward<Args>(args)...);
+  }
+
+  void clear() {
+    if (hasValue()) {
+      hasValue_ = false;
+      value_.~Value();
+    }
+  }
+
+  const Value& value() const {
+    assert(hasValue());
+    return value_;
+  }
+
+  Value& value() {
+    assert(hasValue());
+    return value_;
+  }
+
+  bool hasValue() const { return hasValue_; }
+
+  explicit operator bool() const {
+    return hasValue();
+  }
+
+  const Value& operator*() const { return value(); }
+        Value& operator*()       { return value(); }
+
+  const Value* operator->() const { return &value(); }
+        Value* operator->()       { return &value(); }
+
+ private:
+  template<class... Args>
+  void construct(Args&&... args) {
+    const void* ptr = &value_;
+    // for supporting const types
+    new(const_cast<void*>(ptr)) Value(std::forward<Args>(args)...);
+    hasValue_ = true;
+  }
+
+  // uninitialized
+  union { Value value_; };
+  bool hasValue_;
+};
+
+#if defined(__GNUC__) && !defined(__clang__)
+#pragma GCC diagnostic pop
+#endif
+
+template<class T>
+const T* get_pointer(const Optional<T>& opt) {
+  return opt ? &opt.value() : nullptr;
+}
+
+template<class T>
+T* get_pointer(Optional<T>& opt) {
+  return opt ? &opt.value() : nullptr;
+}
+
+template<class T>
+void swap(Optional<T>& a, Optional<T>& b) {
+  if (a.hasValue() && b.hasValue()) {
+    // both full
+    using std::swap;
+    swap(a.value(), b.value());
+  } else if (a.hasValue() || b.hasValue()) {
+    std::swap(a, b); // fall back to default implementation if they're mixed.
+  }
+}
+
+template<class T,
+         class Opt = Optional<typename std::decay<T>::type>>
+Opt make_optional(T&& v) {
+  return Opt(std::forward<T>(v));
+}
+
+template<class V>
+bool operator< (const Optional<V>& a, const Optional<V>& b) {
+  if (a.hasValue() != b.hasValue()) { return a.hasValue() < b.hasValue(); }
+  if (a.hasValue())                 { return a.value()    < b.value(); }
+  return false;
+}
+
+template<class V>
+bool operator==(const Optional<V>& a, const Optional<V>& b) {
+  if (a.hasValue() != b.hasValue()) { return false; }
+  if (a.hasValue())                 { return a.value() == b.value(); }
+  return true;
+}
+
+template<class V>
+bool operator<=(const Optional<V>& a, const Optional<V>& b) {
+  return !(b < a);
+}
+
+template<class V>
+bool operator!=(const Optional<V>& a, const Optional<V>& b) {
+  return !(b == a);
+}
+
+template<class V>
+bool operator>=(const Optional<V>& a, const Optional<V>& b) {
+  return !(a < b);
+}
+
+template<class V>
+bool operator> (const Optional<V>& a, const Optional<V>& b) {
+  return b < a;
+}
+
+// To supress comparability of Optional<T> with T, despite implicit conversion.
+template<class V> bool operator< (const Optional<V>&, const V& other) = delete;
+template<class V> bool operator<=(const Optional<V>&, const V& other) = delete;
+template<class V> bool operator==(const Optional<V>&, const V& other) = delete;
+template<class V> bool operator!=(const Optional<V>&, const V& other) = delete;
+template<class V> bool operator>=(const Optional<V>&, const V& other) = delete;
+template<class V> bool operator> (const Optional<V>&, const V& other) = delete;
+template<class V> bool operator< (const V& other, const Optional<V>&) = delete;
+template<class V> bool operator<=(const V& other, const Optional<V>&) = delete;
+template<class V> bool operator==(const V& other, const Optional<V>&) = delete;
+template<class V> bool operator!=(const V& other, const Optional<V>&) = delete;
+template<class V> bool operator>=(const V& other, const Optional<V>&) = delete;
+template<class V> bool operator> (const V& other, const Optional<V>&) = delete;
+
+} // namespace folly
+
+#endif//FOLLY_OPTIONAL_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/PackedSyncPtr.h
@@ -0,0 +1,150 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_PACKEDSYNCPTR_H_
+#define FOLLY_PACKEDSYNCPTR_H_
+
+#ifndef __x86_64__
+# error "PackedSyncPtr is x64-specific code."
+#endif
+
+/*
+ * An 8-byte pointer with an integrated spin lock and 15-bit integer
+ * (you can use this for a size of the allocation, if you want, or
+ * something else, or nothing).
+ *
+ * This is using an x64-specific detail about the effective virtual
+ * address space.  Long story short: the upper two bytes of all our
+ * pointers will be zero in reality---and if you have a couple billion
+ * such pointers in core, it makes pretty good sense to try to make
+ * use of that memory.  The exact details can be perused here:
+ *
+ *    http://en.wikipedia.org/wiki/X86-64#Canonical_form_addresses
+ *
+ * This is not a "smart" pointer: nothing automagical is going on
+ * here.  Locking is up to the user.  Resource deallocation is up to
+ * the user.  Locks are never acquired or released outside explicit
+ * calls to lock() and unlock().
+ *
+ * Change the value of the raw pointer with set(), but you must hold
+ * the lock when calling this function if multiple threads could be
+ * using this class.
+ *
+ * TODO(jdelong): should we use the low order bit for the lock, so we
+ * get a whole 16-bits for our integer?  (There's also 2 more bits
+ * down there if the pointer comes from malloc.)
+ *
+ * @author Spencer Ahrens <sahrens@fb.com>
+ * @author Jordan DeLong <delong.j@fb.com>
+ */
+
+#include "folly/SmallLocks.h"
+#include <type_traits>
+#include <glog/logging.h>
+
+namespace folly {
+
+template<class T>
+class PackedSyncPtr {
+  // This just allows using this class even with T=void.  Attempting
+  // to use the operator* or operator[] on a PackedSyncPtr<void> will
+  // still properly result in a compile error.
+  typedef typename std::add_lvalue_reference<T>::type reference;
+
+public:
+  /*
+   * If you default construct one of these, you must call this init()
+   * function before using it.
+   *
+   * (We are avoiding a constructor to ensure gcc allows us to put
+   * this class in packed structures.)
+   */
+  void init(T* initialPtr = 0, uint16_t initialExtra = 0) {
+    auto intPtr = reinterpret_cast<uintptr_t>(initialPtr);
+    CHECK(!(intPtr >> 48));
+    data_.init(intPtr);
+    setExtra(initialExtra);
+  }
+
+  /*
+   * Sets a new pointer.  You must hold the lock when calling this
+   * function, or else be able to guarantee no other threads could be
+   * using this PackedSyncPtr<>.
+   */
+  void set(T* t) {
+    auto intPtr = reinterpret_cast<uintptr_t>(t);
+    auto shiftedExtra = uintptr_t(extra()) << 48;
+    CHECK(!(intPtr >> 48));
+    data_.setData(intPtr | shiftedExtra);
+  }
+
+  /*
+   * Get the pointer.
+   *
+   * You can call any of these without holding the lock, with the
+   * normal types of behavior you'll get on x64 from reading a pointer
+   * without locking.
+   */
+  T* get() const {
+    return reinterpret_cast<T*>(data_.getData() & (-1ull >> 16));
+  }
+  T* operator->() const { return get(); }
+  reference operator*() const { return *get(); }
+  reference operator[](std::ptrdiff_t i) const { return get()[i]; }
+
+  // Synchronization (logically const, even though this mutates our
+  // locked state: you can lock a const PackedSyncPtr<T> to read it).
+  void lock() const { data_.lock(); }
+  void unlock() const { data_.unlock(); }
+  bool try_lock() const { return data_.try_lock(); }
+
+  /*
+   * Access extra data stored in unused bytes of the pointer.
+   *
+   * It is ok to call this without holding the lock.
+   */
+  uint16_t extra() const {
+    return data_.getData() >> 48;
+  }
+
+  /*
+   * Don't try to put anything into this that has the high bit set:
+   * that's what we're using for the mutex.
+   *
+   * Don't call this without holding the lock.
+   */
+  void setExtra(uint16_t extra) {
+    CHECK(!(extra & 0x8000));
+    auto ptr = data_.getData() & (-1ull >> 16);
+    data_.setData((uintptr_t(extra) << 48) | ptr);
+  }
+
+  // Logically private, but we can't have private data members and
+  // still be considered a POD.  (In C++11 we are still a standard
+  // layout struct if this is private, but it doesn't matter, since
+  // gcc (4.6) won't let us use this with attribute packed still in
+  // that case.)
+  PicoSpinLock<uintptr_t> data_;
+};
+
+static_assert(sizeof(PackedSyncPtr<void>) == 8,
+              "PackedSyncPtr should be only 8 bytes---something is "
+              "messed up");
+
+}
+
+#endif
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/Padded.h
@@ -0,0 +1,506 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_PADDED_H_
+#define FOLLY_PADDED_H_
+
+#include <cassert>
+#include <cstdint>
+#include <cstring>
+#include <functional>
+#include <iterator>
+#include <limits>
+#include <type_traits>
+
+#include <boost/iterator/iterator_adaptor.hpp>
+
+#include "folly/Portability.h"
+
+/**
+ * Code that aids in storing data aligned on block (possibly cache-line)
+ * boundaries, perhaps with padding.
+ *
+ * Class Node represents one block.  Given an iterator to a container of
+ * Node, class Iterator encapsulates an iterator to the underlying elements.
+ * Adaptor converts a sequence of Node into a sequence of underlying elements
+ * (not fully compatible with STL container requirements, see comments
+ * near the Node class declaration).
+ */
+
+namespace folly {
+namespace padded {
+
+/**
+ * A Node is a fixed-size container of as many objects of type T as would
+ * fit in a region of memory of size NS.  The last NS % sizeof(T)
+ * bytes are ignored and uninitialized.
+ *
+ * Node only works for trivial types, which is usually not a concern.  This
+ * is intentional: Node itself is trivial, which means that it can be
+ * serialized / deserialized using a simple memcpy.
+ */
+template <class T, size_t NS, class Enable=void>
+class Node;
+
+namespace detail {
+// Shortcut to avoid writing the long enable_if expression every time
+template <class T, size_t NS, class Enable=void> struct NodeValid;
+template <class T, size_t NS>
+struct NodeValid<T, NS,
+                 typename std::enable_if<(
+                     std::is_trivial<T>::value &&
+                     sizeof(T) <= NS &&
+                     NS % alignof(T) == 0)>::type> {
+  typedef void type;
+};
+}  // namespace detail
+
+template <class T, size_t NS>
+class Node<T, NS, typename detail::NodeValid<T,NS>::type> {
+ public:
+  typedef T value_type;
+  static constexpr size_t kNodeSize = NS;
+  static constexpr size_t kElementCount = NS / sizeof(T);
+  static constexpr size_t kPaddingBytes = NS % sizeof(T);
+
+  T* data() { return storage_.data; }
+  const T* data() const { return storage_.data; }
+
+  bool operator==(const Node& other) const {
+    return memcmp(data(), other.data(), sizeof(T) * kElementCount) == 0;
+  }
+  bool operator!=(const Node& other) const {
+    return !(*this == other);
+  }
+
+  /**
+   * Return the number of nodes needed to represent n values.  Rounds up.
+   */
+  static constexpr size_t nodeCount(size_t n) {
+    return (n + kElementCount - 1) / kElementCount;
+  }
+
+  /**
+   * Return the total byte size needed to represent n values, rounded up
+   * to the nearest full node.
+   */
+  static constexpr size_t paddedByteSize(size_t n) {
+    return nodeCount(n) * NS;
+  }
+
+  /**
+   * Return the number of bytes used for padding n values.
+   * Note that, even if n is a multiple of kElementCount, this may
+   * return non-zero if kPaddingBytes != 0, as the padding at the end of
+   * the last node is not included in the result.
+   */
+  static constexpr size_t paddingBytes(size_t n) {
+    return (n ? (kPaddingBytes +
+                 (kElementCount - 1 - (n-1) % kElementCount) * sizeof(T)) :
+            0);
+  }
+
+  /**
+   * Return the minimum byte size needed to represent n values.
+   * Does not round up.  Even if n is a multiple of kElementCount, this
+   * may be different from paddedByteSize() if kPaddingBytes != 0, as
+   * the padding at the end of the last node is not included in the result.
+   * Note that the calculation below works for n=0 correctly (returns 0).
+   */
+  static constexpr size_t unpaddedByteSize(size_t n) {
+    return paddedByteSize(n) - paddingBytes(n);
+  }
+
+ private:
+  union Storage {
+    unsigned char bytes[NS];
+    T data[kElementCount];
+  } storage_;
+};
+
+// We must define kElementCount and kPaddingBytes to work around a bug
+// in gtest that odr-uses them.
+template <class T, size_t NS> constexpr size_t
+Node<T, NS, typename detail::NodeValid<T,NS>::type>::kNodeSize;
+template <class T, size_t NS> constexpr size_t
+Node<T, NS, typename detail::NodeValid<T,NS>::type>::kElementCount;
+template <class T, size_t NS> constexpr size_t
+Node<T, NS, typename detail::NodeValid<T,NS>::type>::kPaddingBytes;
+
+template <class Iter> class Iterator;
+
+namespace detail {
+
+// Helper class to transfer the constness from From (a lvalue reference)
+// and create a lvalue reference to To.
+//
+// TransferReferenceConstness<const string&, int> -> const int&
+// TransferReferenceConstness<string&, int> -> int&
+// TransferReferenceConstness<string&, const int> -> const int&
+template <class From, class To, class Enable=void>
+struct TransferReferenceConstness;
+
+template <class From, class To>
+struct TransferReferenceConstness<
+  From, To, typename std::enable_if<std::is_const<
+    typename std::remove_reference<From>::type>::value>::type> {
+  typedef typename std::add_lvalue_reference<
+    typename std::add_const<To>::type>::type type;
+};
+
+template <class From, class To>
+struct TransferReferenceConstness<
+  From, To, typename std::enable_if<!std::is_const<
+    typename std::remove_reference<From>::type>::value>::type> {
+  typedef typename std::add_lvalue_reference<To>::type type;
+};
+
+// Helper class template to define a base class for Iterator (below) and save
+// typing.
+template <class Iter>
+struct IteratorBase {
+  typedef boost::iterator_adaptor<
+    // CRTC
+    Iterator<Iter>,
+    // Base iterator type
+    Iter,
+    // Value type
+    typename std::iterator_traits<Iter>::value_type::value_type,
+    // Category or traversal
+    boost::use_default,
+    // Reference type
+    typename detail::TransferReferenceConstness<
+      typename std::iterator_traits<Iter>::reference,
+      typename std::iterator_traits<Iter>::value_type::value_type
+    >::type
+  > type;
+};
+
+}  // namespace detail
+
+/**
+ * Wrapper around iterators to Node to return iterators to the underlying
+ * node elements.
+ */
+template <class Iter>
+class Iterator : public detail::IteratorBase<Iter>::type {
+  typedef typename detail::IteratorBase<Iter>::type Super;
+ public:
+  typedef typename std::iterator_traits<Iter>::value_type Node;
+
+  Iterator() : pos_(0) { }
+
+  explicit Iterator(Iter base)
+    : Super(base),
+      pos_(0) {
+  }
+
+  // Return the current node and the position inside the node
+  const Node& node() const { return *this->base_reference(); }
+  size_t pos() const { return pos_; }
+
+ private:
+  typename Super::reference dereference() const {
+    return (*this->base_reference()).data()[pos_];
+  }
+
+  bool equal(const Iterator& other) const {
+    return (this->base_reference() == other.base_reference() &&
+            pos_ == other.pos_);
+  }
+
+  void advance(typename Super::difference_type n) {
+    constexpr ssize_t elementCount = Node::kElementCount;  // signed!
+    ssize_t newPos = pos_ + n;
+    if (newPos >= 0 && newPos < elementCount) {
+      pos_ = newPos;
+      return;
+    }
+    ssize_t nblocks = newPos / elementCount;
+    newPos %= elementCount;
+    if (newPos < 0) {
+      --nblocks;  // negative
+      newPos += elementCount;
+    }
+    this->base_reference() += nblocks;
+    pos_ = newPos;
+  }
+
+  void increment() {
+    if (++pos_ == Node::kElementCount) {
+      ++this->base_reference();
+      pos_ = 0;
+    }
+  }
+
+  void decrement() {
+    if (--pos_ == -1) {
+      --this->base_reference();
+      pos_ = Node::kElementCount - 1;
+    }
+  }
+
+  typename Super::difference_type distance_to(const Iterator& other) const {
+    constexpr ssize_t elementCount = Node::kElementCount;  // signed!
+    ssize_t nblocks =
+      std::distance(this->base_reference(), other.base_reference());
+    return nblocks * elementCount + (other.pos_ - pos_);
+  }
+
+  friend class boost::iterator_core_access;
+  ssize_t pos_;  // signed for easier advance() implementation
+};
+
+/**
+ * Given a container to Node, return iterators to the first element in
+ * the first Node / one past the last element in the last Node.
+ * Note that the last node is assumed to be full; if that's not the case,
+ * subtract from end() as appropriate.
+ */
+
+template <class Container>
+Iterator<typename Container::const_iterator> cbegin(const Container& c) {
+  return Iterator<typename Container::const_iterator>(std::begin(c));
+}
+
+template <class Container>
+Iterator<typename Container::const_iterator> cend(const Container& c) {
+  return Iterator<typename Container::const_iterator>(std::end(c));
+}
+
+template <class Container>
+Iterator<typename Container::const_iterator> begin(const Container& c) {
+  return cbegin(c);
+}
+
+template <class Container>
+Iterator<typename Container::const_iterator> end(const Container& c) {
+  return cend(c);
+}
+
+template <class Container>
+Iterator<typename Container::iterator> begin(Container& c) {
+  return Iterator<typename Container::iterator>(std::begin(c));
+}
+
+template <class Container>
+Iterator<typename Container::iterator> end(Container& c) {
+  return Iterator<typename Container::iterator>(std::end(c));
+}
+
+/**
+ * Adaptor around a STL sequence container.
+ *
+ * Converts a sequence of Node into a sequence of its underlying elements
+ * (with enough functionality to make it useful, although it's not fully
+ * compatible with the STL containre requiremenets, see below).
+ *
+ * Provides iterators (of the same category as those of the underlying
+ * container), size(), front(), back(), push_back(), pop_back(), and const /
+ * non-const versions of operator[] (if the underlying container supports
+ * them).  Does not provide push_front() / pop_front() or arbitrary insert /
+ * emplace / erase.  Also provides reserve() / capacity() if supported by the
+ * underlying container.
+ *
+ * Yes, it's called Adaptor, not Adapter, as that's the name used by the STL
+ * and by boost.  Deal with it.
+ *
+ * Internally, we hold a container of Node and the number of elements in
+ * the last block.  We don't keep empty blocks, so the number of elements in
+ * the last block is always between 1 and Node::kElementCount (inclusive).
+ * (this is true if the container is empty as well to make push_back() simpler,
+ * see the implementation of the size() method for details).
+ */
+template <class Container>
+class Adaptor {
+ public:
+  typedef typename Container::value_type Node;
+  typedef typename Node::value_type value_type;
+  typedef value_type& reference;
+  typedef const value_type& const_reference;
+  typedef Iterator<typename Container::iterator> iterator;
+  typedef Iterator<typename Container::const_iterator> const_iterator;
+  typedef typename const_iterator::difference_type difference_type;
+  typedef typename Container::size_type size_type;
+
+  static constexpr size_t kElementsPerNode = Node::kElementCount;
+  // Constructors
+  Adaptor() : lastCount_(Node::kElementCount) { }
+  explicit Adaptor(Container c, size_t lastCount=Node::kElementCount)
+    : c_(std::move(c)),
+      lastCount_(lastCount) {
+  }
+  explicit Adaptor(size_t n, const value_type& value = value_type())
+    : c_(Node::nodeCount(n), fullNode(value)),
+      lastCount_(n % Node::kElementCount ?: Node::kElementCount) {
+  }
+
+  Adaptor(const Adaptor&) = default;
+  Adaptor& operator=(const Adaptor&) = default;
+  Adaptor(Adaptor&& other)
+    : c_(std::move(other.c_)),
+      lastCount_(other.lastCount_) {
+    other.lastCount_ = Node::kElementCount;
+  }
+  Adaptor& operator=(Adaptor&& other) {
+    if (this != &other) {
+      c_ = std::move(other.c_);
+      lastCount_ = other.lastCount_;
+      other.lastCount_ = Node::kElementCount;
+    }
+    return *this;
+  }
+
+  // Iterators
+  const_iterator cbegin() const {
+    return const_iterator(c_.begin());
+  }
+  const_iterator cend() const {
+    auto it = const_iterator(c_.end());
+    if (lastCount_ != Node::kElementCount) {
+      it -= (Node::kElementCount - lastCount_);
+    }
+    return it;
+  }
+  const_iterator begin() const { return cbegin(); }
+  const_iterator end() const { return cend(); }
+  iterator begin() {
+    return iterator(c_.begin());
+  }
+  iterator end() {
+    auto it = iterator(c_.end());
+    if (lastCount_ != Node::kElementCount) {
+      it -= (Node::kElementCount - lastCount_);
+    }
+    return it;
+  }
+  void swap(Adaptor& other) {
+    using std::swap;
+    swap(c_, other.c_);
+    swap(lastCount_, other.lastCount_);
+  }
+  bool empty() const {
+    return c_.empty();
+  }
+  size_type size() const {
+    return (c_.empty() ? 0 :
+            (c_.size() - 1) * Node::kElementCount + lastCount_);
+  }
+  size_type max_size() const {
+    return ((c_.max_size() <= std::numeric_limits<size_type>::max() /
+             Node::kElementCount) ?
+            c_.max_size() * Node::kElementCount :
+            std::numeric_limits<size_type>::max());
+  }
+
+  const value_type& front() const {
+    assert(!empty());
+    return c_.front().data()[0];
+  }
+  value_type& front() {
+    assert(!empty());
+    return c_.front().data()[0];
+  }
+
+  const value_type& back() const {
+    assert(!empty());
+    return c_.back().data()[lastCount_ - 1];
+  }
+  value_type& back() {
+    assert(!empty());
+    return c_.back().data()[lastCount_ - 1];
+  }
+
+  void push_back(value_type x) {
+    if (lastCount_ == Node::kElementCount) {
+      c_.push_back(Node());
+      lastCount_ = 0;
+    }
+    c_.back().data()[lastCount_++] = std::move(x);
+  }
+
+  void pop_back() {
+    assert(!empty());
+    if (--lastCount_ == 0) {
+      c_.pop_back();
+      lastCount_ = Node::kElementCount;
+    }
+  }
+
+  void clear() {
+    c_.clear();
+    lastCount_ = Node::kElementCount;
+  }
+
+  void reserve(size_type n) {
+    assert(n >= 0);
+    c_.reserve(Node::nodeCount(n));
+  }
+
+  size_type capacity() const {
+    return c_.capacity() * Node::kElementCount;
+  }
+
+  const value_type& operator[](size_type idx) const {
+    return c_[idx / Node::kElementCount].data()[idx % Node::kElementCount];
+  }
+  value_type& operator[](size_type idx) {
+    return c_[idx / Node::kElementCount].data()[idx % Node::kElementCount];
+  }
+
+  /**
+   * Return the underlying container and number of elements in the last block,
+   * and clear *this.  Useful when you want to process the data as Nodes
+   * (again) and want to avoid copies.
+   */
+  std::pair<Container, size_t> move() {
+    std::pair<Container, size_t> p(std::move(c_), lastCount_);
+    lastCount_ = Node::kElementCount;
+    return std::move(p);
+  }
+
+  /**
+   * Return a const reference to the underlying container and the current
+   * number of elements in the last block.
+   */
+  std::pair<const Container&, size_t> peek() const {
+    return std::make_pair(std::cref(c_), lastCount_);
+  }
+
+  void padToFullNode(const value_type& padValue) {
+    // the if is necessary because c_ may be empty so we can't call c_.back()
+    if (lastCount_ != Node::kElementCount) {
+      auto last = c_.back().data();
+      std::fill(last + lastCount_, last + Node::kElementCount, padValue);
+      lastCount_ = Node::kElementCount;
+    }
+  }
+
+ private:
+  static Node fullNode(const value_type& value) {
+    Node n;
+    std::fill(n.data(), n.data() + kElementsPerNode, value);
+    return n;
+  }
+  Container c_;  // container of Nodes
+  size_t lastCount_;  // number of elements in last Node
+};
+
+}  // namespace padded
+}  // namespace folly
+
+#endif /* FOLLY_PADDED_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/Portability.h
@@ -0,0 +1,136 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_PORTABILITY_H_
+#define FOLLY_PORTABILITY_H_
+
+#ifndef FOLLY_NO_CONFIG
+#include "folly-config.h"
+#endif
+
+#if FOLLY_HAVE_FEATURES_H
+#include <features.h>
+#endif
+
+#include "CPortability.h"
+
+#if FOLLY_HAVE_SCHED_H
+ #include <sched.h>
+ #ifndef FOLLY_HAVE_PTHREAD_YIELD
+  #define pthread_yield sched_yield
+ #endif
+#endif
+
+
+// MaxAlign: max_align_t isn't supported by gcc
+#ifdef __GNUC__
+struct MaxAlign { char c; } __attribute__((aligned));
+#else /* !__GNUC__ */
+# error Cannot define MaxAlign on this platform
+#endif
+
+
+// noreturn
+#if defined(__clang__) || defined(__GNUC__)
+# define FOLLY_NORETURN __attribute__((noreturn))
+#else
+# define FOLLY_NORETURN
+#endif
+
+
+// portable version check
+#ifndef __GNUC_PREREQ
+# if defined __GNUC__ && defined __GNUC_MINOR__
+#  define __GNUC_PREREQ(maj, min) ((__GNUC__ << 16) + __GNUC_MINOR__ >= \
+                                   ((maj) << 16) + (min))
+# else
+#  define __GNUC_PREREQ(maj, min) 0
+# endif
+#endif
+
+
+/* Define macro wrappers for C++11's "final" and "override" keywords, which
+ * are supported in gcc 4.7 but not gcc 4.6. */
+#if !defined(FOLLY_FINAL) && !defined(FOLLY_OVERRIDE)
+# if defined(__clang__) || __GNUC_PREREQ(4, 7)
+#  define FOLLY_FINAL final
+#  define FOLLY_OVERRIDE override
+# else
+#  define FOLLY_FINAL /**/
+#  define FOLLY_OVERRIDE /**/
+# endif
+#endif
+
+
+// Define to 1 if you have the `preadv' and `pwritev' functions, respectively
+#if !defined(FOLLY_HAVE_PREADV) && !defined(FOLLY_HAVE_PWRITEV)
+# if defined(__GLIBC_PREREQ)
+#  if __GLIBC_PREREQ(2, 10)
+#   define FOLLY_HAVE_PREADV 1
+#   define FOLLY_HAVE_PWRITEV 1
+#  endif
+# endif
+#endif
+
+// It turns out that GNU libstdc++ and LLVM libc++ differ on how they implement
+// the 'std' namespace; the latter uses inline namepsaces. Wrap this decision
+// up in a macro to make forward-declarations easier.
+#if FOLLY_USE_LIBCPP
+#define FOLLY_NAMESPACE_STD_BEGIN     _LIBCPP_BEGIN_NAMESPACE_STD
+#define FOLLY_NAMESPACE_STD_END       _LIBCPP_END_NAMESPACE_STD
+#else
+#define FOLLY_NAMESPACE_STD_BEGIN     namespace std {
+#define FOLLY_NAMESPACE_STD_END       }
+#endif
+
+// Some platforms lack clock_gettime(2) and clock_getres(2). Inject our own
+// versions of these into the global namespace.
+#if FOLLY_HAVE_CLOCK_GETTIME
+#include <time.h>
+#else
+#include "folly/detail/Clock.h"
+#endif
+
+// Provide our own std::__throw_* wrappers for platforms that don't have them
+#if FOLLY_HAVE_BITS_FUNCTEXCEPT_H
+#include <bits/functexcept.h>
+#else
+#include "folly/detail/FunctionalExcept.h"
+#endif
+
+#if defined(__cplusplus)
+// Unfortunately, boost::has_trivial_copy<T> is broken in libc++ due to its
+// usage of __has_trivial_copy(), so we can't use it as a
+// least-common-denominator for C++11 implementations that don't support
+// std::is_trivially_copyable<T>.
+//
+//      http://stackoverflow.com/questions/12754886/has-trivial-copy-behaves-differently-in-clang-and-gcc-whos-right
+//
+// As a result, use std::is_trivially_copyable() where it exists, and fall back
+// to Boost otherwise.
+#if FOLLY_HAVE_STD__IS_TRIVIALLY_COPYABLE
+#include <type_traits>
+#define FOLLY_IS_TRIVIALLY_COPYABLE(T)                   \
+  (std::is_trivially_copyable<T>::value)
+#else
+#include <boost/type_traits.hpp>
+#define FOLLY_IS_TRIVIALLY_COPYABLE(T)                   \
+  (boost::has_trivial_copy<T>::value &&                  \
+   boost::has_trivial_destructor<T>::value)
+#endif
+#endif // __cplusplus
+
+#endif // FOLLY_PORTABILITY_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/Preprocessor.h
@@ -0,0 +1,80 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// @author: Andrei Alexandrescu
+
+#ifndef FOLLY_PREPROCESSOR_
+#define FOLLY_PREPROCESSOR_
+
+/**
+ * Necessarily evil preprocessor-related amenities.
+ */
+
+/**
+ * FB_ONE_OR_NONE(hello, world) expands to hello and
+ * FB_ONE_OR_NONE(hello) expands to nothing. This macro is used to
+ * insert or eliminate text based on the presence of another argument.
+ */
+#define FB_ONE_OR_NONE(a, ...) FB_THIRD(a, ## __VA_ARGS__, a)
+#define FB_THIRD(a, b, ...) __VA_ARGS__
+
+/**
+ * Helper macro that extracts the first argument out of a list of any
+ * number of arguments.
+ */
+#define FB_ARG_1(a, ...) a
+
+/**
+ * Helper macro that extracts the second argument out of a list of any
+ * number of arguments. If only one argument is given, it returns
+ * that.
+ */
+#define FB_ARG_2_OR_1(...) FB_ARG_2_OR_1_IMPL(__VA_ARGS__, __VA_ARGS__)
+// Support macro for the above
+#define FB_ARG_2_OR_1_IMPL(a, b, ...) b
+
+/**
+ * Helper macro that provides a way to pass argument with commas in it to
+ * some other macro whose syntax doesn't allow using extra parentheses.
+ * Example:
+ *
+ *   #define MACRO(type, name) type name
+ *   MACRO(FB_SINGLE_ARG(std::pair<size_t, size_t>), x);
+ *
+ */
+#define FB_SINGLE_ARG(...) __VA_ARGS__
+
+/**
+ * FB_ANONYMOUS_VARIABLE(str) introduces an identifier starting with
+ * str and ending with a number that varies with the line.
+ */
+#ifndef FB_ANONYMOUS_VARIABLE
+#define FB_CONCATENATE_IMPL(s1, s2) s1##s2
+#define FB_CONCATENATE(s1, s2) FB_CONCATENATE_IMPL(s1, s2)
+#ifdef __COUNTER__
+#define FB_ANONYMOUS_VARIABLE(str) FB_CONCATENATE(str, __COUNTER__)
+#else
+#define FB_ANONYMOUS_VARIABLE(str) FB_CONCATENATE(str, __LINE__)
+#endif
+#endif
+
+/**
+ * Use FB_STRINGIZE(x) when you'd want to do what #x does inside
+ * another macro expansion.
+ */
+#define FB_STRINGIZE(x) #x
+
+#endif // FOLLY_PREPROCESSOR_
--- /dev/null
+++ b/hphp/submodules/folly/folly/ProducerConsumerQueue.h
@@ -0,0 +1,177 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// @author Bo Hu (bhu@fb.com)
+// @author Jordan DeLong (delong.j@fb.com)
+
+#ifndef PRODUCER_CONSUMER_QUEUE_H_
+#define PRODUCER_CONSUMER_QUEUE_H_
+
+#include <atomic>
+#include <cassert>
+#include <cstdlib>
+#include <stdexcept>
+#include <type_traits>
+#include <utility>
+#include <boost/noncopyable.hpp>
+#include <boost/type_traits.hpp>
+
+namespace folly {
+
+/*
+ * ProducerConsumerQueue is a one producer and one consumer queue
+ * without locks.
+ */
+template<class T>
+struct ProducerConsumerQueue : private boost::noncopyable {
+  typedef T value_type;
+
+  // size must be >= 2.
+  //
+  // Also, note that the number of usable slots in the queue at any
+  // given time is actually (size-1), so if you start with an empty queue,
+  // isFull() will return true after size-1 insertions.
+  explicit ProducerConsumerQueue(uint32_t size)
+    : size_(size)
+    , records_(static_cast<T*>(std::malloc(sizeof(T) * size)))
+    , readIndex_(0)
+    , writeIndex_(0)
+  {
+    assert(size >= 2);
+    if (!records_) {
+      throw std::bad_alloc();
+    }
+  }
+
+  ~ProducerConsumerQueue() {
+    // We need to destruct anything that may still exist in our queue.
+    // (No real synchronization needed at destructor time: only one
+    // thread can be doing this.)
+    if (!boost::has_trivial_destructor<T>::value) {
+      int read = readIndex_;
+      int end = writeIndex_;
+      while (read != end) {
+        records_[read].~T();
+        if (++read == size_) {
+          read = 0;
+        }
+      }
+    }
+
+    std::free(records_);
+  }
+
+  template<class ...Args>
+  bool write(Args&&... recordArgs) {
+    auto const currentWrite = writeIndex_.load(std::memory_order_relaxed);
+    auto nextRecord = currentWrite + 1;
+    if (nextRecord == size_) {
+      nextRecord = 0;
+    }
+    if (nextRecord != readIndex_.load(std::memory_order_acquire)) {
+      new (&records_[currentWrite]) T(std::forward<Args>(recordArgs)...);
+      writeIndex_.store(nextRecord, std::memory_order_release);
+      return true;
+    }
+
+    // queue is full
+    return false;
+  }
+
+  // move (or copy) the value at the front of the queue to given variable
+  bool read(T& record) {
+    auto const currentRead = readIndex_.load(std::memory_order_relaxed);
+    if (currentRead == writeIndex_.load(std::memory_order_acquire)) {
+      // queue is empty
+      return false;
+    }
+
+    auto nextRecord = currentRead + 1;
+    if (nextRecord == size_) {
+      nextRecord = 0;
+    }
+    record = std::move(records_[currentRead]);
+    records_[currentRead].~T();
+    readIndex_.store(nextRecord, std::memory_order_release);
+    return true;
+  }
+
+  // pointer to the value at the front of the queue (for use in-place) or
+  // nullptr if empty.
+  T* frontPtr() {
+    auto const currentRead = readIndex_.load(std::memory_order_relaxed);
+    if (currentRead == writeIndex_.load(std::memory_order_acquire)) {
+      // queue is empty
+      return nullptr;
+    }
+    return &records_[currentRead];
+  }
+
+  // queue must not be empty
+  void popFront() {
+    auto const currentRead = readIndex_.load(std::memory_order_relaxed);
+    assert(currentRead != writeIndex_.load(std::memory_order_acquire));
+
+    auto nextRecord = currentRead + 1;
+    if (nextRecord == size_) {
+      nextRecord = 0;
+    }
+    records_[currentRead].~T();
+    readIndex_.store(nextRecord, std::memory_order_release);
+  }
+
+  bool isEmpty() const {
+   return readIndex_.load(std::memory_order_consume) ==
+         writeIndex_.load(std::memory_order_consume);
+  }
+
+  bool isFull() const {
+    auto nextRecord = writeIndex_.load(std::memory_order_consume) + 1;
+    if (nextRecord == size_) {
+      nextRecord = 0;
+    }
+    if (nextRecord != readIndex_.load(std::memory_order_consume)) {
+      return false;
+    }
+    // queue is full
+    return true;
+  }
+
+  // * If called by consumer, then true size may be more (because producer may
+  //   be adding items concurrently).
+  // * If called by producer, then true size may be less (because consumer may
+  //   be removing items concurrently).
+  // * It is undefined to call this from any other thread.
+  size_t sizeGuess() const {
+    int ret = writeIndex_.load(std::memory_order_consume) -
+              readIndex_.load(std::memory_order_consume);
+    if (ret < 0) {
+      ret += size_;
+    }
+    return ret;
+  }
+
+private:
+  const uint32_t size_;
+  T* const records_;
+
+  std::atomic<int> readIndex_;
+  std::atomic<int> writeIndex_;
+};
+
+}
+
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/Random.cpp
@@ -0,0 +1,84 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Random.h"
+
+#include <atomic>
+#include <unistd.h>
+#include <sys/time.h>
+#include <random>
+#include <array>
+
+#if __GNUC_PREREQ(4, 8)
+#include <ext/random>
+#define USE_SIMD_PRNG
+#endif
+
+namespace folly {
+
+namespace {
+std::atomic<uint32_t> seedInput(0);
+}
+
+uint32_t randomNumberSeed() {
+  struct timeval tv;
+  gettimeofday(&tv, NULL);
+  const uint32_t kPrime0 = 51551;
+  const uint32_t kPrime1 = 61631;
+  const uint32_t kPrime2 = 64997;
+  const uint32_t kPrime3 = 111857;
+  return kPrime0 * (seedInput++)
+       + kPrime1 * static_cast<uint32_t>(getpid())
+       + kPrime2 * static_cast<uint32_t>(tv.tv_sec)
+       + kPrime3 * static_cast<uint32_t>(tv.tv_usec);
+}
+
+
+folly::ThreadLocalPtr<ThreadLocalPRNG::LocalInstancePRNG>
+ThreadLocalPRNG::localInstance;
+
+class ThreadLocalPRNG::LocalInstancePRNG {
+#ifdef USE_SIMD_PRNG
+  typedef  __gnu_cxx::sfmt19937 RNG;
+#else
+  typedef std::mt19937 RNG;
+#endif
+
+  static RNG makeRng() {
+    std::array<int, RNG::state_size> seed_data;
+    std::random_device r;
+    std::generate_n(seed_data.data(), seed_data.size(), std::ref(r));
+    std::seed_seq seq(std::begin(seed_data), std::end(seed_data));
+    return RNG(seq);
+  }
+
+ public:
+  LocalInstancePRNG() : rng(std::move(makeRng())) {}
+
+  RNG rng;
+};
+
+ThreadLocalPRNG::LocalInstancePRNG* ThreadLocalPRNG::initLocal() {
+  auto ret = new LocalInstancePRNG;
+  localInstance.reset(ret);
+  return ret;
+}
+
+uint32_t ThreadLocalPRNG::getImpl(LocalInstancePRNG* local) {
+  return local->rng();
+}
+
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/Random.h
@@ -0,0 +1,201 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_BASE_RANDOM_H_
+#define FOLLY_BASE_RANDOM_H_
+
+#include <stdint.h>
+#include "folly/ThreadLocal.h"
+
+namespace folly {
+
+/*
+ * Return a good seed for a random number generator.
+ */
+uint32_t randomNumberSeed();
+
+class Random;
+
+/**
+ * A PRNG with one instance per thread. This PRNG uses a mersenne twister random
+ * number generator and is seeded from /dev/urandom. It should not be used for
+ * anything which requires security, only for statistical randomness.
+ *
+ * An instance of this class represents the current threads PRNG. This means
+ * copying an instance of this class across threads will result in corruption
+ *
+ * Most users will use the Random class which implicitly creates this class.
+ * However, if you are worried about performance, you can memoize the TLS
+ * lookups that get the per thread state by manually using this class:
+ *
+ * ThreadLocalPRNG rng = Random::threadLocalPRNG()
+ * for (...) {
+ *   Random::rand32(rng);
+ * }
+ */
+class ThreadLocalPRNG {
+ public:
+  typedef uint32_t result_type;
+
+  uint32_t operator()() {
+    // Using a static method allows the compiler to avoid allocating stack space
+    // for this class.
+    return getImpl(local_);
+  }
+
+  static constexpr result_type min() {
+    return std::numeric_limits<result_type>::min();
+  }
+  static constexpr result_type max() {
+    return std::numeric_limits<result_type>::max();
+  }
+  friend class Random;
+
+  ThreadLocalPRNG() {
+    local_ = localInstance.get();
+    if (!local_) {
+      local_ = initLocal();
+    }
+  }
+
+ private:
+  class LocalInstancePRNG;
+  static LocalInstancePRNG* initLocal();
+  static folly::ThreadLocalPtr<ThreadLocalPRNG::LocalInstancePRNG>
+    localInstance;
+
+  static result_type getImpl(LocalInstancePRNG* local);
+  LocalInstancePRNG* local_;
+};
+
+
+
+class Random {
+
+ private:
+  template<class RNG>
+  using ValidRNG = typename std::enable_if<
+   std::is_unsigned<typename std::result_of<RNG&()>::type>::value,
+   RNG>::type;
+
+ public:
+
+  /**
+   * Returns a random uint32_t
+   */
+  template<class RNG = ThreadLocalPRNG>
+  static uint32_t rand32(ValidRNG<RNG>  rrng = RNG()) {
+    uint32_t r = rrng.operator()();
+    return r;
+  }
+
+  /**
+   * Returns a random uint32_t in [0, max). If max == 0, returns 0.
+   */
+  template<class RNG = ThreadLocalPRNG>
+  static uint32_t rand32(uint32_t max, ValidRNG<RNG> rng = RNG()) {
+    if (max == 0) {
+      return 0;
+    }
+
+    return std::uniform_int_distribution<uint32_t>(0, max - 1)(rng);
+  }
+
+  /**
+   * Returns a random uint32_t in [min, max). If min == max, returns 0.
+   */
+  template<class RNG = ThreadLocalPRNG>
+  static uint32_t rand32(uint32_t min,
+                         uint32_t max,
+                         ValidRNG<RNG> rng = RNG()) {
+    if (min == max) {
+      return 0;
+    }
+
+    return std::uniform_int_distribution<uint32_t>(min, max - 1)(rng);
+  }
+
+  /**
+   * Returns a random uint64_t
+   */
+  template<class RNG = ThreadLocalPRNG>
+  static uint64_t rand64(ValidRNG<RNG> rng = RNG()) {
+    return ((uint64_t) rng() << 32) | rng();
+  }
+
+  /**
+   * Returns a random uint64_t in [0, max). If max == 0, returns 0.
+   */
+  template<class RNG = ThreadLocalPRNG>
+  static uint64_t rand64(uint64_t max, ValidRNG<RNG> rng = RNG()) {
+    if (max == 0) {
+      return 0;
+    }
+
+    return std::uniform_int_distribution<uint64_t>(0, max - 1)(rng);
+  }
+
+  /**
+   * Returns a random uint64_t in [min, max). If min == max, returns 0.
+   */
+  template<class RNG = ThreadLocalPRNG>
+  static uint64_t rand64(uint64_t min,
+                         uint64_t max,
+                         ValidRNG<RNG> rng = RNG()) {
+    if (min == max) {
+      return 0;
+    }
+
+    return std::uniform_int_distribution<uint64_t>(min, max - 1)(rng);
+  }
+
+  /**
+   * Returns true 1/n of the time. If n == 0, always returns false
+   */
+  template<class RNG = ThreadLocalPRNG>
+  static bool oneIn(uint32_t n, ValidRNG<RNG> rng = RNG()) {
+    if (n == 0) {
+      return false;
+    }
+
+    return rand32(n, rng) == 0;
+  }
+
+  /**
+   * Returns a double in [0, 1)
+   */
+  template<class RNG = ThreadLocalPRNG>
+  static double randDouble01(ValidRNG<RNG> rng = RNG()) {
+    return std::generate_canonical<double, std::numeric_limits<double>::digits>
+      (rng);
+  }
+
+  /**
+    * Returns a double in [min, max), if min == max, returns 0.
+    */
+  template<class RNG = ThreadLocalPRNG>
+  static double randDouble(double min, double max, ValidRNG<RNG> rng = RNG()) {
+    if (std::fabs(max - min) < std::numeric_limits<double>::epsilon()) {
+      return 0;
+    }
+    return std::uniform_real_distribution<double>(min, max)(rng);
+  }
+
+};
+
+}
+
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/Range.cpp
@@ -0,0 +1,289 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// @author Mark Rabkin (mrabkin@fb.com)
+// @author Andrei Alexandrescu (andrei.alexandrescu@fb.com)
+
+#include "folly/Range.h"
+
+#if FOLLY_HAVE_EMMINTRIN_H
+#include <emmintrin.h>  // __v16qi
+#endif
+#include <iostream>
+
+namespace folly {
+
+/**
+ * Predicates that can be used with qfind and startsWith
+ */
+const AsciiCaseSensitive asciiCaseSensitive = AsciiCaseSensitive();
+const AsciiCaseInsensitive asciiCaseInsensitive = AsciiCaseInsensitive();
+
+std::ostream& operator<<(std::ostream& os, const StringPiece& piece) {
+  os.write(piece.start(), piece.size());
+  return os;
+}
+
+namespace detail {
+
+size_t qfind_first_byte_of_memchr(const StringPiece& haystack,
+                                  const StringPiece& needles) {
+  size_t best = haystack.size();
+  for (char needle: needles) {
+    const void* ptr = memchr(haystack.data(), needle, best);
+    if (ptr) {
+      auto found = static_cast<const char*>(ptr) - haystack.data();
+      best = std::min<size_t>(best, found);
+    }
+  }
+  if (best == haystack.size()) {
+    return StringPiece::npos;
+  }
+  return best;
+}
+
+}  // namespace detail
+
+namespace {
+
+// It's okay if pages are bigger than this (as powers of two), but they should
+// not be smaller.
+constexpr size_t kMinPageSize = 4096;
+static_assert(kMinPageSize >= 16,
+              "kMinPageSize must be at least SSE register size");
+#define PAGE_FOR(addr) \
+  (reinterpret_cast<uintptr_t>(addr) / kMinPageSize)
+
+
+// Earlier versions of GCC (for example, Clang on Mac OS X, which is based on
+// GCC 4.2) do not have a full compliment of SSE builtins.
+#if FOLLY_HAVE_EMMINTRIN_H && __GNUC_PREREQ(4, 6)
+inline size_t nextAlignedIndex(const char* arr) {
+   auto firstPossible = reinterpret_cast<uintptr_t>(arr) + 1;
+   return 1 +                       // add 1 because the index starts at 'arr'
+     ((firstPossible + 15) & ~0xF)  // round up to next multiple of 16
+     - firstPossible;
+}
+
+// build sse4.2-optimized version even if -msse4.2 is not passed to GCC
+size_t qfind_first_byte_of_needles16(const StringPiece& haystack,
+                                     const StringPiece& needles)
+  __attribute__ ((__target__("sse4.2"), noinline))
+  FOLLY_DISABLE_ADDRESS_SANITIZER;
+
+// helper method for case where needles.size() <= 16
+size_t qfind_first_byte_of_needles16(const StringPiece& haystack,
+                                     const StringPiece& needles) {
+  DCHECK(!haystack.empty());
+  DCHECK(!needles.empty());
+  DCHECK_LE(needles.size(), 16);
+  // benchmarking shows that memchr beats out SSE for small needle-sets
+  // with large haystacks.
+  if ((needles.size() <= 2 && haystack.size() >= 256) ||
+      // must bail if we can't even SSE-load a single segment of haystack
+      (haystack.size() < 16 &&
+       PAGE_FOR(haystack.end() - 1) != PAGE_FOR(haystack.data() + 15)) ||
+      // can't load needles into SSE register if it could cross page boundary
+      PAGE_FOR(needles.end() - 1) != PAGE_FOR(needles.data() + 15)) {
+    return detail::qfind_first_byte_of_memchr(haystack, needles);
+  }
+
+  auto arr2 = __builtin_ia32_loaddqu(needles.data());
+  // do an unaligned load for first block of haystack
+  auto arr1 = __builtin_ia32_loaddqu(haystack.data());
+  auto index = __builtin_ia32_pcmpestri128(arr2, needles.size(),
+                                           arr1, haystack.size(), 0);
+  if (index < 16) {
+    return index;
+  }
+
+  // Now, we can do aligned loads hereafter...
+  size_t i = nextAlignedIndex(haystack.data());
+  for (; i < haystack.size(); i+= 16) {
+    void* ptr1 = __builtin_assume_aligned(haystack.data() + i, 16);
+    auto arr1 = *reinterpret_cast<const __v16qi*>(ptr1);
+    auto index = __builtin_ia32_pcmpestri128(arr2, needles.size(),
+                                             arr1, haystack.size() - i, 0);
+    if (index < 16) {
+      return i + index;
+    }
+  }
+  return StringPiece::npos;
+}
+#endif // FOLLY_HAVE_EMMINTRIN_H && GCC 4.6+
+
+// Aho, Hopcroft, and Ullman refer to this trick in "The Design and Analysis
+// of Computer Algorithms" (1974), but the best description is here:
+// http://research.swtch.com/sparse
+class FastByteSet {
+ public:
+  FastByteSet() : size_(0) { }  // no init of arrays required!
+
+  inline void add(uint8_t i) {
+    if (!contains(i)) {
+      dense_[size_] = i;
+      sparse_[i] = size_;
+      size_++;
+    }
+  }
+  inline bool contains(uint8_t i) const {
+    DCHECK_LE(size_, 256);
+    return sparse_[i] < size_ && dense_[sparse_[i]] == i;
+  }
+
+ private:
+  uint16_t size_;  // can't use uint8_t because it would overflow if all
+                   // possible values were inserted.
+  uint8_t sparse_[256];
+  uint8_t dense_[256];
+};
+
+}  // namespace
+
+namespace detail {
+
+size_t qfind_first_byte_of_byteset(const StringPiece& haystack,
+                                   const StringPiece& needles) {
+  FastByteSet s;
+  for (auto needle: needles) {
+    s.add(needle);
+  }
+  for (size_t index = 0; index < haystack.size(); ++index) {
+    if (s.contains(haystack[index])) {
+      return index;
+    }
+  }
+  return StringPiece::npos;
+}
+
+#if FOLLY_HAVE_EMMINTRIN_H && __GNUC_PREREQ(4, 6)
+
+template <bool HAYSTACK_ALIGNED>
+size_t scanHaystackBlock(const StringPiece& haystack,
+                         const StringPiece& needles,
+                         int64_t idx)
+// inline is okay because it's only called from other sse4.2 functions
+  __attribute__ ((__target__("sse4.2")))
+// Turn off ASAN because the "arr2 = ..." assignment in the loop below reads
+// up to 15 bytes beyond end of the buffer in #needles#.  That is ok because
+// ptr2 is always 16-byte aligned, so the read can never span a page boundary.
+// Also, the extra data that may be read is never actually used.
+  FOLLY_DISABLE_ADDRESS_SANITIZER;
+
+// Scans a 16-byte block of haystack (starting at blockStartIdx) to find first
+// needle. If HAYSTACK_ALIGNED, then haystack must be 16byte aligned.
+// If !HAYSTACK_ALIGNED, then caller must ensure that it is safe to load the
+// block.
+template <bool HAYSTACK_ALIGNED>
+size_t scanHaystackBlock(const StringPiece& haystack,
+                         const StringPiece& needles,
+                         int64_t blockStartIdx) {
+  DCHECK_GT(needles.size(), 16);  // should handled by *needles16() method
+  DCHECK(blockStartIdx + 16 <= haystack.size() ||
+         (PAGE_FOR(haystack.data() + blockStartIdx) ==
+          PAGE_FOR(haystack.data() + blockStartIdx + 15)));
+
+  __v16qi arr1;
+  if (HAYSTACK_ALIGNED) {
+    void* ptr1 = __builtin_assume_aligned(haystack.data() + blockStartIdx, 16);
+    arr1 = *reinterpret_cast<const __v16qi*>(ptr1);
+  } else {
+    arr1 = __builtin_ia32_loaddqu(haystack.data() + blockStartIdx);
+  }
+
+  // This load is safe because needles.size() >= 16
+  auto arr2 = __builtin_ia32_loaddqu(needles.data());
+  size_t b = __builtin_ia32_pcmpestri128(
+    arr2, 16, arr1, haystack.size() - blockStartIdx, 0);
+
+  size_t j = nextAlignedIndex(needles.data());
+  for (; j < needles.size(); j += 16) {
+    void* ptr2 = __builtin_assume_aligned(needles.data() + j, 16);
+    arr2 = *reinterpret_cast<const __v16qi*>(ptr2);
+
+    auto index = __builtin_ia32_pcmpestri128(
+      arr2, needles.size() - j, arr1, haystack.size() - blockStartIdx, 0);
+    b = std::min<size_t>(index, b);
+  }
+
+  if (b < 16) {
+    return blockStartIdx + b;
+  }
+  return StringPiece::npos;
+}
+
+size_t qfind_first_byte_of_sse42(const StringPiece& haystack,
+                                 const StringPiece& needles)
+  __attribute__ ((__target__("sse4.2"), noinline));
+
+size_t qfind_first_byte_of_sse42(const StringPiece& haystack,
+                                 const StringPiece& needles) {
+  if (UNLIKELY(needles.empty() || haystack.empty())) {
+    return StringPiece::npos;
+  } else if (needles.size() <= 16) {
+    // we can save some unnecessary load instructions by optimizing for
+    // the common case of needles.size() <= 16
+    return qfind_first_byte_of_needles16(haystack, needles);
+  }
+
+  if (haystack.size() < 16 &&
+      PAGE_FOR(haystack.end() - 1) != PAGE_FOR(haystack.data() + 16)) {
+    // We can't safely SSE-load haystack. Use a different approach.
+    if (haystack.size() <= 2) {
+      return qfind_first_of(haystack, needles, asciiCaseSensitive);
+    }
+    return qfind_first_byte_of_byteset(haystack, needles);
+  }
+
+  auto ret = scanHaystackBlock<false>(haystack, needles, 0);
+  if (ret != StringPiece::npos) {
+    return ret;
+  }
+
+  size_t i = nextAlignedIndex(haystack.data());
+  for (; i < haystack.size(); i += 16) {
+    auto ret = scanHaystackBlock<true>(haystack, needles, i);
+    if (ret != StringPiece::npos) {
+      return ret;
+    }
+  }
+
+  return StringPiece::npos;
+}
+#endif // FOLLY_HAVE_EMMINTRIN_H && GCC 4.6+
+
+size_t qfind_first_byte_of_nosse(const StringPiece& haystack,
+                                 const StringPiece& needles) {
+  if (UNLIKELY(needles.empty() || haystack.empty())) {
+    return StringPiece::npos;
+  }
+  // The thresholds below were empirically determined by benchmarking.
+  // This is not an exact science since it depends on the CPU, the size of
+  // needles, and the size of haystack.
+  if (haystack.size() == 1 ||
+      (haystack.size() < 4 && needles.size() <= 16)) {
+    return qfind_first_of(haystack, needles, asciiCaseSensitive);
+  } else if ((needles.size() >= 4 && haystack.size() <= 10) ||
+             (needles.size() >= 16 && haystack.size() <= 64) ||
+             needles.size() >= 32) {
+    return qfind_first_byte_of_byteset(haystack, needles);
+  }
+
+  return qfind_first_byte_of_memchr(haystack, needles);
+}
+
+}  // namespace detail
+}  // namespace folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/Range.h
@@ -0,0 +1,961 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// @author Mark Rabkin (mrabkin@fb.com)
+// @author Andrei Alexandrescu (andrei.alexandrescu@fb.com)
+
+#ifndef FOLLY_RANGE_H_
+#define FOLLY_RANGE_H_
+
+#include "folly/Portability.h"
+#include "folly/FBString.h"
+#include <algorithm>
+#include <boost/operators.hpp>
+#include <cstring>
+#include <glog/logging.h>
+#include <iosfwd>
+#include <stdexcept>
+#include <string>
+#include <type_traits>
+
+// libc++ doesn't provide this header
+#if !FOLLY_USE_LIBCPP
+// This file appears in two locations: inside fbcode and in the
+// libstdc++ source code (when embedding fbstring as std::string).
+// To aid in this schizophrenic use, two macros are defined in
+// c++config.h:
+//   _LIBSTDCXX_FBSTRING - Set inside libstdc++.  This is useful to
+//      gate use inside fbcode v. libstdc++
+#include <bits/c++config.h>
+#endif
+
+#include "folly/CpuId.h"
+#include "folly/Traits.h"
+#include "folly/Likely.h"
+
+// Ignore shadowing warnings within this file, so includers can use -Wshadow.
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wshadow"
+
+namespace folly {
+
+template <class T> class Range;
+
+/**
+ * Finds the first occurrence of needle in haystack. The algorithm is on
+ * average faster than O(haystack.size() * needle.size()) but not as fast
+ * as Boyer-Moore. On the upside, it does not do any upfront
+ * preprocessing and does not allocate memory.
+ */
+template <class T, class Comp = std::equal_to<typename Range<T>::value_type>>
+inline size_t qfind(const Range<T> & haystack,
+                    const Range<T> & needle,
+                    Comp eq = Comp());
+
+/**
+ * Finds the first occurrence of needle in haystack. The result is the
+ * offset reported to the beginning of haystack, or string::npos if
+ * needle wasn't found.
+ */
+template <class T>
+size_t qfind(const Range<T> & haystack,
+             const typename Range<T>::value_type& needle);
+
+/**
+ * Finds the last occurrence of needle in haystack. The result is the
+ * offset reported to the beginning of haystack, or string::npos if
+ * needle wasn't found.
+ */
+template <class T>
+size_t rfind(const Range<T> & haystack,
+             const typename Range<T>::value_type& needle);
+
+
+/**
+ * Finds the first occurrence of any element of needle in
+ * haystack. The algorithm is O(haystack.size() * needle.size()).
+ */
+template <class T>
+inline size_t qfind_first_of(const Range<T> & haystack,
+                             const Range<T> & needle);
+
+/**
+ * Small internal helper - returns the value just before an iterator.
+ */
+namespace detail {
+
+/**
+ * For random-access iterators, the value before is simply i[-1].
+ */
+template <class Iter>
+typename std::enable_if<
+  std::is_same<typename std::iterator_traits<Iter>::iterator_category,
+               std::random_access_iterator_tag>::value,
+  typename std::iterator_traits<Iter>::reference>::type
+value_before(Iter i) {
+  return i[-1];
+}
+
+/**
+ * For all other iterators, we need to use the decrement operator.
+ */
+template <class Iter>
+typename std::enable_if<
+  !std::is_same<typename std::iterator_traits<Iter>::iterator_category,
+                std::random_access_iterator_tag>::value,
+  typename std::iterator_traits<Iter>::reference>::type
+value_before(Iter i) {
+  return *--i;
+}
+
+} // namespace detail
+
+/**
+ * Range abstraction keeping a pair of iterators. We couldn't use
+ * boost's similar range abstraction because we need an API identical
+ * with the former StringPiece class, which is used by a lot of other
+ * code. This abstraction does fulfill the needs of boost's
+ * range-oriented algorithms though.
+ *
+ * (Keep memory lifetime in mind when using this class, since it
+ * doesn't manage the data it refers to - just like an iterator
+ * wouldn't.)
+ */
+template <class Iter>
+class Range : private boost::totally_ordered<Range<Iter> > {
+public:
+  typedef std::size_t size_type;
+  typedef Iter iterator;
+  typedef Iter const_iterator;
+  typedef typename std::remove_reference<
+    typename std::iterator_traits<Iter>::reference>::type
+  value_type;
+  typedef typename std::iterator_traits<Iter>::reference reference;
+  typedef std::char_traits<typename std::remove_const<value_type>::type>
+    traits_type;
+
+  static const size_type npos;
+
+  // Works for all iterators
+  Range() : b_(), e_() {
+  }
+
+public:
+  // Works for all iterators
+  Range(Iter start, Iter end) : b_(start), e_(end) {
+  }
+
+  // Works only for random-access iterators
+  Range(Iter start, size_t size)
+      : b_(start), e_(start + size) { }
+
+#if FOLLY_HAVE_CONSTEXPR_STRLEN
+  // Works only for Range<const char*>
+  /* implicit */ constexpr Range(Iter str)
+      : b_(str), e_(str + strlen(str)) {}
+#else
+  // Works only for Range<const char*>
+  /* implicit */ Range(Iter str)
+      : b_(str), e_(str + strlen(str)) {}
+#endif
+  // Works only for Range<const char*>
+  /* implicit */ Range(const std::string& str)
+      : b_(str.data()), e_(b_ + str.size()) {}
+
+  // Works only for Range<const char*>
+  Range(const std::string& str, std::string::size_type startFrom) {
+    if (UNLIKELY(startFrom > str.size())) {
+      throw std::out_of_range("index out of range");
+    }
+    b_ = str.data() + startFrom;
+    e_ = str.data() + str.size();
+  }
+  // Works only for Range<const char*>
+  Range(const std::string& str,
+        std::string::size_type startFrom,
+        std::string::size_type size) {
+    if (UNLIKELY(startFrom > str.size())) {
+      throw std::out_of_range("index out of range");
+    }
+    b_ = str.data() + startFrom;
+    if (str.size() - startFrom < size) {
+      e_ = str.data() + str.size();
+    } else {
+      e_ = b_ + size;
+    }
+  }
+  Range(const Range<Iter>& str,
+        size_t startFrom,
+        size_t size) {
+    if (UNLIKELY(startFrom > str.size())) {
+      throw std::out_of_range("index out of range");
+    }
+    b_ = str.b_ + startFrom;
+    if (str.size() - startFrom < size) {
+      e_ = str.e_;
+    } else {
+      e_ = b_ + size;
+    }
+  }
+  // Works only for Range<const char*>
+  /* implicit */ Range(const fbstring& str)
+    : b_(str.data()), e_(b_ + str.size()) { }
+  // Works only for Range<const char*>
+  Range(const fbstring& str, fbstring::size_type startFrom) {
+    if (UNLIKELY(startFrom > str.size())) {
+      throw std::out_of_range("index out of range");
+    }
+    b_ = str.data() + startFrom;
+    e_ = str.data() + str.size();
+  }
+  // Works only for Range<const char*>
+  Range(const fbstring& str, fbstring::size_type startFrom,
+        fbstring::size_type size) {
+    if (UNLIKELY(startFrom > str.size())) {
+      throw std::out_of_range("index out of range");
+    }
+    b_ = str.data() + startFrom;
+    if (str.size() - startFrom < size) {
+      e_ = str.data() + str.size();
+    } else {
+      e_ = b_ + size;
+    }
+  }
+
+  // Allow implicit conversion from Range<const char*> (aka StringPiece) to
+  // Range<const unsigned char*> (aka ByteRange), as they're both frequently
+  // used to represent ranges of bytes.  Allow explicit conversion in the other
+  // direction.
+  template <class OtherIter, typename std::enable_if<
+      (std::is_same<Iter, const unsigned char*>::value &&
+       (std::is_same<OtherIter, const char*>::value ||
+        std::is_same<OtherIter, char*>::value)), int>::type = 0>
+  /* implicit */ Range(const Range<OtherIter>& other)
+    : b_(reinterpret_cast<const unsigned char*>(other.begin())),
+      e_(reinterpret_cast<const unsigned char*>(other.end())) {
+  }
+
+  template <class OtherIter, typename std::enable_if<
+      (std::is_same<Iter, unsigned char*>::value &&
+       std::is_same<OtherIter, char*>::value), int>::type = 0>
+  /* implicit */ Range(const Range<OtherIter>& other)
+    : b_(reinterpret_cast<unsigned char*>(other.begin())),
+      e_(reinterpret_cast<unsigned char*>(other.end())) {
+  }
+
+  template <class OtherIter, typename std::enable_if<
+      (std::is_same<Iter, const char*>::value &&
+       (std::is_same<OtherIter, const unsigned char*>::value ||
+        std::is_same<OtherIter, unsigned char*>::value)), int>::type = 0>
+  explicit Range(const Range<OtherIter>& other)
+    : b_(reinterpret_cast<const char*>(other.begin())),
+      e_(reinterpret_cast<const char*>(other.end())) {
+  }
+
+  template <class OtherIter, typename std::enable_if<
+      (std::is_same<Iter, char*>::value &&
+       std::is_same<OtherIter, unsigned char*>::value), int>::type = 0>
+  explicit Range(const Range<OtherIter>& other)
+    : b_(reinterpret_cast<char*>(other.begin())),
+      e_(reinterpret_cast<char*>(other.end())) {
+  }
+
+  // Allow implicit conversion from Range<From> to Range<To> if From is
+  // implicitly convertible to To.
+  template <class OtherIter, typename std::enable_if<
+     (!std::is_same<Iter, OtherIter>::value &&
+      std::is_convertible<OtherIter, Iter>::value), int>::type = 0>
+  /* implicit */ Range(const Range<OtherIter>& other)
+    : b_(other.begin()),
+      e_(other.end()) {
+  }
+
+  // Allow explicit conversion from Range<From> to Range<To> if From is
+  // explicitly convertible to To.
+  template <class OtherIter, typename std::enable_if<
+    (!std::is_same<Iter, OtherIter>::value &&
+     !std::is_convertible<OtherIter, Iter>::value &&
+     std::is_constructible<Iter, const OtherIter&>::value), int>::type = 0>
+  explicit Range(const Range<OtherIter>& other)
+    : b_(other.begin()),
+      e_(other.end()) {
+  }
+
+  void clear() {
+    b_ = Iter();
+    e_ = Iter();
+  }
+
+  void assign(Iter start, Iter end) {
+    b_ = start;
+    e_ = end;
+  }
+
+  void reset(Iter start, size_type size) {
+    b_ = start;
+    e_ = start + size;
+  }
+
+  // Works only for Range<const char*>
+  void reset(const std::string& str) {
+    reset(str.data(), str.size());
+  }
+
+  size_type size() const {
+    assert(b_ <= e_);
+    return e_ - b_;
+  }
+  size_type walk_size() const {
+    assert(b_ <= e_);
+    return std::distance(b_, e_);
+  }
+  bool empty() const { return b_ == e_; }
+  Iter data() const { return b_; }
+  Iter start() const { return b_; }
+  Iter begin() const { return b_; }
+  Iter end() const { return e_; }
+  Iter cbegin() const { return b_; }
+  Iter cend() const { return e_; }
+  value_type& front() {
+    assert(b_ < e_);
+    return *b_;
+  }
+  value_type& back() {
+    assert(b_ < e_);
+    return detail::value_before(e_);
+  }
+  const value_type& front() const {
+    assert(b_ < e_);
+    return *b_;
+  }
+  const value_type& back() const {
+    assert(b_ < e_);
+    return detail::value_before(e_);
+  }
+  // Works only for Range<const char*>
+  std::string str() const { return std::string(b_, size()); }
+  std::string toString() const { return str(); }
+  // Works only for Range<const char*>
+  fbstring fbstr() const { return fbstring(b_, size()); }
+  fbstring toFbstring() const { return fbstr(); }
+
+  // Works only for Range<const char*>
+  int compare(const Range& o) const {
+    const size_type tsize = this->size();
+    const size_type osize = o.size();
+    const size_type msize = std::min(tsize, osize);
+    int r = traits_type::compare(data(), o.data(), msize);
+    if (r == 0) r = tsize - osize;
+    return r;
+  }
+
+  value_type& operator[](size_t i) {
+    DCHECK_GT(size(), i);
+    return b_[i];
+  }
+
+  const value_type& operator[](size_t i) const {
+    DCHECK_GT(size(), i);
+    return b_[i];
+  }
+
+  value_type& at(size_t i) {
+    if (i >= size()) throw std::out_of_range("index out of range");
+    return b_[i];
+  }
+
+  const value_type& at(size_t i) const {
+    if (i >= size()) throw std::out_of_range("index out of range");
+    return b_[i];
+  }
+
+  // Works only for Range<const char*>
+  uint32_t hash() const {
+    // Taken from fbi/nstring.h:
+    //    Quick and dirty bernstein hash...fine for short ascii strings
+    uint32_t hash = 5381;
+    for (size_t ix = 0; ix < size(); ix++) {
+      hash = ((hash << 5) + hash) + b_[ix];
+    }
+    return hash;
+  }
+
+  void advance(size_type n) {
+    if (UNLIKELY(n > size())) {
+      throw std::out_of_range("index out of range");
+    }
+    b_ += n;
+  }
+
+  void subtract(size_type n) {
+    if (UNLIKELY(n > size())) {
+      throw std::out_of_range("index out of range");
+    }
+    e_ -= n;
+  }
+
+  void pop_front() {
+    assert(b_ < e_);
+    ++b_;
+  }
+
+  void pop_back() {
+    assert(b_ < e_);
+    --e_;
+  }
+
+  Range subpiece(size_type first,
+                 size_type length = std::string::npos) const {
+    if (UNLIKELY(first > size())) {
+      throw std::out_of_range("index out of range");
+    }
+    return Range(b_ + first,
+                 std::min<std::string::size_type>(length, size() - first));
+  }
+
+  // string work-alike functions
+  size_type find(Range str) const {
+    return qfind(*this, str);
+  }
+
+  size_type find(Range str, size_t pos) const {
+    if (pos > size()) return std::string::npos;
+    size_t ret = qfind(subpiece(pos), str);
+    return ret == npos ? ret : ret + pos;
+  }
+
+  size_type find(Iter s, size_t pos, size_t n) const {
+    if (pos > size()) return std::string::npos;
+    size_t ret = qfind(pos ? subpiece(pos) : *this, Range(s, n));
+    return ret == npos ? ret : ret + pos;
+  }
+
+  // Works only for Range<const (unsigned) char*> which have Range(Iter) ctor
+  size_type find(const Iter s) const {
+    return qfind(*this, Range(s));
+  }
+
+  // Works only for Range<const (unsigned) char*> which have Range(Iter) ctor
+  size_type find(const Iter s, size_t pos) const {
+    if (pos > size()) return std::string::npos;
+    size_type ret = qfind(subpiece(pos), Range(s));
+    return ret == npos ? ret : ret + pos;
+  }
+
+  size_type find(value_type c) const {
+    return qfind(*this, c);
+  }
+
+  size_type rfind(value_type c) const {
+    return folly::rfind(*this, c);
+  }
+
+  size_type find(value_type c, size_t pos) const {
+    if (pos > size()) return std::string::npos;
+    size_type ret = qfind(subpiece(pos), c);
+    return ret == npos ? ret : ret + pos;
+  }
+
+  size_type find_first_of(Range needles) const {
+    return qfind_first_of(*this, needles);
+  }
+
+  size_type find_first_of(Range needles, size_t pos) const {
+    if (pos > size()) return std::string::npos;
+    size_type ret = qfind_first_of(subpiece(pos), needles);
+    return ret == npos ? ret : ret + pos;
+  }
+
+  // Works only for Range<const (unsigned) char*> which have Range(Iter) ctor
+  size_type find_first_of(Iter needles) const {
+    return find_first_of(Range(needles));
+  }
+
+  // Works only for Range<const (unsigned) char*> which have Range(Iter) ctor
+  size_type find_first_of(Iter needles, size_t pos) const {
+    return find_first_of(Range(needles), pos);
+  }
+
+  size_type find_first_of(Iter needles, size_t pos, size_t n) const {
+    return find_first_of(Range(needles, n), pos);
+  }
+
+  size_type find_first_of(value_type c) const {
+    return find(c);
+  }
+
+  size_type find_first_of(value_type c, size_t pos) const {
+    return find(c, pos);
+  }
+
+  void swap(Range& rhs) {
+    std::swap(b_, rhs.b_);
+    std::swap(e_, rhs.e_);
+  }
+
+  /**
+   * Does this Range start with another range?
+   */
+  bool startsWith(const Range& other) const {
+    return size() >= other.size() && subpiece(0, other.size()) == other;
+  }
+  bool startsWith(value_type c) const {
+    return !empty() && front() == c;
+  }
+
+  /**
+   * Does this Range end with another range?
+   */
+  bool endsWith(const Range& other) const {
+    return size() >= other.size() && subpiece(size() - other.size()) == other;
+  }
+  bool endsWith(value_type c) const {
+    return !empty() && back() == c;
+  }
+
+  /**
+   * Remove the given prefix and return true if the range starts with the given
+   * prefix; return false otherwise.
+   */
+  bool removePrefix(const Range& prefix) {
+    return startsWith(prefix) && (b_ += prefix.size(), true);
+  }
+  bool removePrefix(value_type prefix) {
+    return startsWith(prefix) && (++b_, true);
+  }
+
+  /**
+   * Remove the given suffix and return true if the range ends with the given
+   * suffix; return false otherwise.
+   */
+  bool removeSuffix(const Range& suffix) {
+    return endsWith(suffix) && (e_ -= suffix.size(), true);
+  }
+  bool removeSuffix(value_type suffix) {
+    return endsWith(suffix) && (--e_, true);
+  }
+
+  /**
+   * Splits this `Range` `[b, e)` in the position `i` dictated by the next
+   * occurence of `delimiter`.
+   *
+   * Returns a new `Range` `[b, i)` and adjusts this range to start right after
+   * the delimiter's position. This range will be empty if the delimiter is not
+   * found. If called on an empty `Range`, both this and the returned `Range`
+   * will be empty.
+   *
+   * Example:
+   *
+   *  folly::StringPiece s("sample string for split_next");
+   *  auto p = s.split_step(' ');
+   *
+   *  // prints "sample"
+   *  cout << s << endl;
+   *
+   *  // prints "string for split_next"
+   *  cout << p << endl;
+   *
+   * Example 2:
+   *
+   *  void tokenize(StringPiece s, char delimiter) {
+   *    while (!s.empty()) {
+   *      cout << s.split_step(delimiter);
+   *    }
+   *  }
+   *
+   * @author: Marcelo Juchem <marcelo@fb.com>
+   */
+  Range split_step(value_type delimiter) {
+    auto i = std::find(b_, e_, delimiter);
+    Range result(b_, i);
+
+    b_ = i == e_ ? e_ : std::next(i);
+
+    return result;
+  }
+
+  Range split_step(Range delimiter) {
+    auto i = find(delimiter);
+    Range result(b_, i == std::string::npos ? size() : i);
+
+    b_ = result.end() == e_ ? e_ : std::next(result.end(), delimiter.size());
+
+    return result;
+  }
+
+  /**
+   * Convenience method that calls `split_step()` and passes the result to a
+   * functor, returning whatever the functor does.
+   *
+   * Say you have a functor with this signature:
+   *
+   *  Foo fn(Range r) { }
+   *
+   * `split_step()`'s return type will be `Foo`. It works just like:
+   *
+   *  auto result = fn(myRange.split_step(' '));
+   *
+   * A functor returning `void` is also supported.
+   *
+   * Example:
+   *
+   *  void do_some_parsing(folly::StringPiece s) {
+   *    auto version = s.split_step(' ', [&](folly::StringPiece x) {
+   *      if (x.empty()) {
+   *        throw std::invalid_argument("empty string");
+   *      }
+   *      return std::strtoull(x.begin(), x.end(), 16);
+   *    });
+   *
+   *    // ...
+   *  }
+   *
+   * @author: Marcelo Juchem <marcelo@fb.com>
+   */
+  template <typename TProcess>
+  auto split_step(value_type delimiter, TProcess &&process)
+    -> decltype(process(std::declval<Range>()))
+  { return process(split_step(delimiter)); }
+
+  template <typename TProcess>
+  auto split_step(Range delimiter, TProcess &&process)
+    -> decltype(process(std::declval<Range>()))
+  { return process(split_step(delimiter)); }
+
+private:
+  Iter b_, e_;
+};
+
+template <class Iter>
+const typename Range<Iter>::size_type Range<Iter>::npos = std::string::npos;
+
+template <class T>
+void swap(Range<T>& lhs, Range<T>& rhs) {
+  lhs.swap(rhs);
+}
+
+/**
+ * Create a range from two iterators, with type deduction.
+ */
+template <class Iter>
+Range<Iter> range(Iter first, Iter last) {
+  return Range<Iter>(first, last);
+}
+
+/*
+ * Creates a range to reference the contents of a contiguous-storage container.
+ */
+// Use pointers for types with '.data()' member
+template <class Collection,
+          class T = typename std::remove_pointer<
+              decltype(std::declval<Collection>().data())>::type>
+Range<T*> range(Collection&& v) {
+  return Range<T*>(v.data(), v.data() + v.size());
+}
+
+template <class T, size_t n>
+Range<T*> range(T (&array)[n]) {
+  return Range<T*>(array, array + n);
+}
+
+typedef Range<const char*> StringPiece;
+typedef Range<char*> MutableStringPiece;
+typedef Range<const unsigned char*> ByteRange;
+typedef Range<unsigned char*> MutableByteRange;
+
+std::ostream& operator<<(std::ostream& os, const StringPiece& piece);
+
+/**
+ * Templated comparison operators
+ */
+
+template <class T>
+inline bool operator==(const Range<T>& lhs, const Range<T>& rhs) {
+  return lhs.size() == rhs.size() && lhs.compare(rhs) == 0;
+}
+
+template <class T>
+inline bool operator<(const Range<T>& lhs, const Range<T>& rhs) {
+  return lhs.compare(rhs) < 0;
+}
+
+/**
+ * Specializations of comparison operators for StringPiece
+ */
+
+namespace detail {
+
+template <class A, class B>
+struct ComparableAsStringPiece {
+  enum {
+    value =
+    (std::is_convertible<A, StringPiece>::value
+     && std::is_same<B, StringPiece>::value)
+    ||
+    (std::is_convertible<B, StringPiece>::value
+     && std::is_same<A, StringPiece>::value)
+  };
+};
+
+} // namespace detail
+
+/**
+ * operator== through conversion for Range<const char*>
+ */
+template <class T, class U>
+typename
+std::enable_if<detail::ComparableAsStringPiece<T, U>::value, bool>::type
+operator==(const T& lhs, const U& rhs) {
+  return StringPiece(lhs) == StringPiece(rhs);
+}
+
+/**
+ * operator< through conversion for Range<const char*>
+ */
+template <class T, class U>
+typename
+std::enable_if<detail::ComparableAsStringPiece<T, U>::value, bool>::type
+operator<(const T& lhs, const U& rhs) {
+  return StringPiece(lhs) < StringPiece(rhs);
+}
+
+/**
+ * operator> through conversion for Range<const char*>
+ */
+template <class T, class U>
+typename
+std::enable_if<detail::ComparableAsStringPiece<T, U>::value, bool>::type
+operator>(const T& lhs, const U& rhs) {
+  return StringPiece(lhs) > StringPiece(rhs);
+}
+
+/**
+ * operator< through conversion for Range<const char*>
+ */
+template <class T, class U>
+typename
+std::enable_if<detail::ComparableAsStringPiece<T, U>::value, bool>::type
+operator<=(const T& lhs, const U& rhs) {
+  return StringPiece(lhs) <= StringPiece(rhs);
+}
+
+/**
+ * operator> through conversion for Range<const char*>
+ */
+template <class T, class U>
+typename
+std::enable_if<detail::ComparableAsStringPiece<T, U>::value, bool>::type
+operator>=(const T& lhs, const U& rhs) {
+  return StringPiece(lhs) >= StringPiece(rhs);
+}
+
+struct StringPieceHash {
+  std::size_t operator()(const StringPiece& str) const {
+    return static_cast<std::size_t>(str.hash());
+  }
+};
+
+/**
+ * Finds substrings faster than brute force by borrowing from Boyer-Moore
+ */
+template <class T, class Comp>
+size_t qfind(const Range<T>& haystack,
+             const Range<T>& needle,
+             Comp eq) {
+  // Don't use std::search, use a Boyer-Moore-like trick by comparing
+  // the last characters first
+  auto const nsize = needle.size();
+  if (haystack.size() < nsize) {
+    return std::string::npos;
+  }
+  if (!nsize) return 0;
+  auto const nsize_1 = nsize - 1;
+  auto const lastNeedle = needle[nsize_1];
+
+  // Boyer-Moore skip value for the last char in the needle. Zero is
+  // not a valid value; skip will be computed the first time it's
+  // needed.
+  std::string::size_type skip = 0;
+
+  auto i = haystack.begin();
+  auto iEnd = haystack.end() - nsize_1;
+
+  while (i < iEnd) {
+    // Boyer-Moore: match the last element in the needle
+    while (!eq(i[nsize_1], lastNeedle)) {
+      if (++i == iEnd) {
+        // not found
+        return std::string::npos;
+      }
+    }
+    // Here we know that the last char matches
+    // Continue in pedestrian mode
+    for (size_t j = 0; ; ) {
+      assert(j < nsize);
+      if (!eq(i[j], needle[j])) {
+        // Not found, we can skip
+        // Compute the skip value lazily
+        if (skip == 0) {
+          skip = 1;
+          while (skip <= nsize_1 && !eq(needle[nsize_1 - skip], lastNeedle)) {
+            ++skip;
+          }
+        }
+        i += skip;
+        break;
+      }
+      // Check if done searching
+      if (++j == nsize) {
+        // Yay
+        return i - haystack.begin();
+      }
+    }
+  }
+  return std::string::npos;
+}
+
+namespace detail {
+
+size_t qfind_first_byte_of_nosse(const StringPiece& haystack,
+                                 const StringPiece& needles);
+
+#if FOLLY_HAVE_EMMINTRIN_H && __GNUC_PREREQ(4, 6)
+size_t qfind_first_byte_of_sse42(const StringPiece& haystack,
+                                 const StringPiece& needles);
+
+inline size_t qfind_first_byte_of(const StringPiece& haystack,
+                                  const StringPiece& needles) {
+  static auto const qfind_first_byte_of_fn =
+    folly::CpuId().sse42() ? qfind_first_byte_of_sse42
+                           : qfind_first_byte_of_nosse;
+  return qfind_first_byte_of_fn(haystack, needles);
+}
+
+#else
+inline size_t qfind_first_byte_of(const StringPiece& haystack,
+                                  const StringPiece& needles) {
+  return qfind_first_byte_of_nosse(haystack, needles);
+}
+#endif // FOLLY_HAVE_EMMINTRIN_H
+
+} // namespace detail
+
+template <class T, class Comp>
+size_t qfind_first_of(const Range<T> & haystack,
+                      const Range<T> & needles,
+                      Comp eq) {
+  auto ret = std::find_first_of(haystack.begin(), haystack.end(),
+                                needles.begin(), needles.end(),
+                                eq);
+  return ret == haystack.end() ? std::string::npos : ret - haystack.begin();
+}
+
+struct AsciiCaseSensitive {
+  bool operator()(char lhs, char rhs) const {
+    return lhs == rhs;
+  }
+};
+
+struct AsciiCaseInsensitive {
+  bool operator()(char lhs, char rhs) const {
+    return toupper(lhs) == toupper(rhs);
+  }
+};
+
+extern const AsciiCaseSensitive asciiCaseSensitive;
+extern const AsciiCaseInsensitive asciiCaseInsensitive;
+
+template <class T>
+size_t qfind(const Range<T>& haystack,
+             const typename Range<T>::value_type& needle) {
+  auto pos = std::find(haystack.begin(), haystack.end(), needle);
+  return pos == haystack.end() ? std::string::npos : pos - haystack.data();
+}
+
+template <class T>
+size_t rfind(const Range<T>& haystack,
+             const typename Range<T>::value_type& needle) {
+  for (auto i = haystack.size(); i-- > 0; ) {
+    if (haystack[i] == needle) {
+      return i;
+    }
+  }
+  return std::string::npos;
+}
+
+// specialization for StringPiece
+template <>
+inline size_t qfind(const Range<const char*>& haystack, const char& needle) {
+  auto pos = static_cast<const char*>(
+    ::memchr(haystack.data(), needle, haystack.size()));
+  return pos == nullptr ? std::string::npos : pos - haystack.data();
+}
+
+#if FOLLY_HAVE_MEMRCHR
+template <>
+inline size_t rfind(const Range<const char*>& haystack, const char& needle) {
+  auto pos = static_cast<const char*>(
+    ::memrchr(haystack.data(), needle, haystack.size()));
+  return pos == nullptr ? std::string::npos : pos - haystack.data();
+}
+#endif
+
+// specialization for ByteRange
+template <>
+inline size_t qfind(const Range<const unsigned char*>& haystack,
+                    const unsigned char& needle) {
+  auto pos = static_cast<const unsigned char*>(
+    ::memchr(haystack.data(), needle, haystack.size()));
+  return pos == nullptr ? std::string::npos : pos - haystack.data();
+}
+
+#if FOLLY_HAVE_MEMRCHR
+template <>
+inline size_t rfind(const Range<const unsigned char*>& haystack,
+                    const unsigned char& needle) {
+  auto pos = static_cast<const unsigned char*>(
+    ::memrchr(haystack.data(), needle, haystack.size()));
+  return pos == nullptr ? std::string::npos : pos - haystack.data();
+}
+#endif
+
+template <class T>
+size_t qfind_first_of(const Range<T>& haystack,
+                      const Range<T>& needles) {
+  return qfind_first_of(haystack, needles, asciiCaseSensitive);
+}
+
+// specialization for StringPiece
+template <>
+inline size_t qfind_first_of(const Range<const char*>& haystack,
+                             const Range<const char*>& needles) {
+  return detail::qfind_first_byte_of(haystack, needles);
+}
+
+// specialization for ByteRange
+template <>
+inline size_t qfind_first_of(const Range<const unsigned char*>& haystack,
+                             const Range<const unsigned char*>& needles) {
+  return detail::qfind_first_byte_of(StringPiece(haystack),
+                                     StringPiece(needles));
+}
+}  // !namespace folly
+
+#pragma GCC diagnostic pop
+
+FOLLY_ASSUME_FBVECTOR_COMPATIBLE_1(folly::Range);
+
+#endif // FOLLY_RANGE_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/RWSpinLock.h
@@ -0,0 +1,744 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+ * Two Read-Write spin lock implementations.
+ *
+ *  Ref: http://locklessinc.com/articles/locks
+ *
+ *  Both locks here are faster than pthread_rwlock and have very low
+ *  overhead (usually 20-30ns).  They don't use any system mutexes and
+ *  are very compact (4/8 bytes), so are suitable for per-instance
+ *  based locking, particularly when contention is not expected.
+ *
+ *  In most cases, RWSpinLock is a reasonable choice.  It has minimal
+ *  overhead, and comparable contention performance when the number of
+ *  competing threads is less than or equal to the number of logical
+ *  CPUs.  Even as the number of threads gets larger, RWSpinLock can
+ *  still be very competitive in READ, although it is slower on WRITE,
+ *  and also inherently unfair to writers.
+ *
+ *  RWTicketSpinLock shows more balanced READ/WRITE performance.  If
+ *  your application really needs a lot more threads, and a
+ *  higher-priority writer, prefer one of the RWTicketSpinLock locks.
+ *
+ *  Caveats:
+ *
+ *    RWTicketSpinLock locks can only be used with GCC on x86/x86-64
+ *    based systems.
+ *
+ *    RWTicketSpinLock<32> only allows up to 2^8 - 1 concurrent
+ *    readers and writers.
+ *
+ *    RWTicketSpinLock<64> only allows up to 2^16 - 1 concurrent
+ *    readers and writers.
+ *
+ *    RWSpinLock handles 2^30 - 1 concurrent readers.
+ *
+ * @author Xin Liu <xliux@fb.com>
+ */
+
+#ifndef FOLLY_RWSPINLOCK_H_
+#define FOLLY_RWSPINLOCK_H_
+
+/*
+========================================================================
+Benchmark on (Intel(R) Xeon(R) CPU  L5630  @ 2.13GHz)  8 cores(16 HTs)
+========================================================================
+
+------------------------------------------------------------------------------
+1. Single thread benchmark (read/write lock + unlock overhead)
+Benchmark                                    Iters   Total t    t/iter iter/sec
+-------------------------------------------------------------------------------
+*      BM_RWSpinLockRead                     100000  1.786 ms  17.86 ns   53.4M
++30.5% BM_RWSpinLockWrite                    100000  2.331 ms  23.31 ns  40.91M
++85.7% BM_RWTicketSpinLock32Read             100000  3.317 ms  33.17 ns  28.75M
++96.0% BM_RWTicketSpinLock32Write            100000    3.5 ms     35 ns  27.25M
++85.6% BM_RWTicketSpinLock64Read             100000  3.315 ms  33.15 ns  28.77M
++96.0% BM_RWTicketSpinLock64Write            100000    3.5 ms     35 ns  27.25M
++85.7% BM_RWTicketSpinLock32FavorWriterRead  100000  3.317 ms  33.17 ns  28.75M
++29.7% BM_RWTicketSpinLock32FavorWriterWrite 100000  2.316 ms  23.16 ns  41.18M
++85.3% BM_RWTicketSpinLock64FavorWriterRead  100000  3.309 ms  33.09 ns  28.82M
++30.2% BM_RWTicketSpinLock64FavorWriterWrite 100000  2.325 ms  23.25 ns  41.02M
++ 175% BM_PThreadRWMutexRead                 100000  4.917 ms  49.17 ns   19.4M
++ 166% BM_PThreadRWMutexWrite                100000  4.757 ms  47.57 ns  20.05M
+
+------------------------------------------------------------------------------
+2. Contention Benchmark      90% read  10% write
+Benchmark                    hits       average    min       max        sigma
+------------------------------------------------------------------------------
+---------- 8  threads ------------
+RWSpinLock       Write       142666     220ns      78ns      40.8us     269ns
+RWSpinLock       Read        1282297    222ns      80ns      37.7us     248ns
+RWTicketSpinLock Write       85692      209ns      71ns      17.9us     252ns
+RWTicketSpinLock Read        769571     215ns      78ns      33.4us     251ns
+pthread_rwlock_t Write       84248      2.48us     99ns      269us      8.19us
+pthread_rwlock_t Read        761646     933ns      101ns     374us      3.25us
+
+---------- 16 threads ------------
+RWSpinLock       Write       124236     237ns      78ns      261us      801ns
+RWSpinLock       Read        1115807    236ns      78ns      2.27ms     2.17us
+RWTicketSpinLock Write       81781      231ns      71ns      31.4us     351ns
+RWTicketSpinLock Read        734518     238ns      78ns      73.6us     379ns
+pthread_rwlock_t Write       83363      7.12us     99ns      785us      28.1us
+pthread_rwlock_t Read        754978     2.18us     101ns     1.02ms     14.3us
+
+---------- 50 threads ------------
+RWSpinLock       Write       131142     1.37us     82ns      7.53ms     68.2us
+RWSpinLock       Read        1181240    262ns      78ns      6.62ms     12.7us
+RWTicketSpinLock Write       83045      397ns      73ns      7.01ms     31.5us
+RWTicketSpinLock Read        744133     386ns      78ns        11ms     31.4us
+pthread_rwlock_t Write       80849      112us      103ns     4.52ms     263us
+pthread_rwlock_t Read        728698     24us       101ns     7.28ms     194us
+
+*/
+
+#if defined(__GNUC__) && !defined(__clang__) && \
+  (defined(__i386) || defined(__x86_64__) || \
+   defined(ARCH_K8))
+#define RW_SPINLOCK_USE_X86_INTRINSIC_
+#include <x86intrin.h>
+#else
+#undef RW_SPINLOCK_USE_X86_INTRINSIC_
+#endif
+
+#include <atomic>
+#include <string>
+#include <algorithm>
+#include <boost/noncopyable.hpp>
+
+#include <sched.h>
+#include <glog/logging.h>
+
+#include "folly/Likely.h"
+
+namespace folly {
+
+/*
+ * A simple, small (4-bytes), but unfair rwlock.  Use it when you want
+ * a nice writer and don't expect a lot of write/read contention, or
+ * when you need small rwlocks since you are creating a large number
+ * of them.
+ *
+ * Note that the unfairness here is extreme: if the lock is
+ * continually accessed for read, writers will never get a chance.  If
+ * the lock can be that highly contended this class is probably not an
+ * ideal choice anyway.
+ *
+ * It currently implements most of the Lockable, SharedLockable and
+ * UpgradeLockable concepts except the TimedLockable related locking/unlocking
+ * interfaces.
+ */
+class RWSpinLock : boost::noncopyable {
+  enum : int32_t { READER = 4, UPGRADED = 2, WRITER = 1 };
+ public:
+  RWSpinLock() : bits_(0) {}
+
+  // Lockable Concept
+  void lock() {
+    int count = 0;
+    while (!LIKELY(try_lock())) {
+      if (++count > 1000) sched_yield();
+    }
+  }
+
+  // Writer is responsible for clearing up both the UPGRADED and WRITER bits.
+  void unlock() {
+    static_assert(READER > WRITER + UPGRADED, "wrong bits!");
+    bits_.fetch_and(~(WRITER | UPGRADED), std::memory_order_release);
+  }
+
+  // SharedLockable Concept
+  void lock_shared() {
+    int count = 0;
+    while (!LIKELY(try_lock_shared())) {
+      if (++count > 1000) sched_yield();
+    }
+  }
+
+  void unlock_shared() {
+    bits_.fetch_add(-READER, std::memory_order_release);
+  }
+
+  // Downgrade the lock from writer status to reader status.
+  void unlock_and_lock_shared() {
+    bits_.fetch_add(READER, std::memory_order_acquire);
+    unlock();
+  }
+
+  // UpgradeLockable Concept
+  void lock_upgrade() {
+    int count = 0;
+    while (!try_lock_upgrade()) {
+      if (++count > 1000) sched_yield();
+    }
+  }
+
+  void unlock_upgrade() {
+    bits_.fetch_add(-UPGRADED, std::memory_order_acq_rel);
+  }
+
+  // unlock upgrade and try to acquire write lock
+  void unlock_upgrade_and_lock() {
+    int64_t count = 0;
+    while (!try_unlock_upgrade_and_lock()) {
+      if (++count > 1000) sched_yield();
+    }
+  }
+
+  // unlock upgrade and read lock atomically
+  void unlock_upgrade_and_lock_shared() {
+    bits_.fetch_add(READER - UPGRADED, std::memory_order_acq_rel);
+  }
+
+  // write unlock and upgrade lock atomically
+  void unlock_and_lock_upgrade() {
+    // need to do it in two steps here -- as the UPGRADED bit might be OR-ed at
+    // the same time when other threads are trying do try_lock_upgrade().
+    bits_.fetch_or(UPGRADED, std::memory_order_acquire);
+    bits_.fetch_add(-WRITER, std::memory_order_release);
+  }
+
+
+  // Attempt to acquire writer permission. Return false if we didn't get it.
+  bool try_lock() {
+    int32_t expect = 0;
+    return bits_.compare_exchange_strong(expect, WRITER,
+      std::memory_order_acq_rel);
+  }
+
+  // Try to get reader permission on the lock. This can fail if we
+  // find out someone is a writer or upgrader.
+  // Setting the UPGRADED bit would allow a writer-to-be to indicate
+  // its intention to write and block any new readers while waiting
+  // for existing readers to finish and release their read locks. This
+  // helps avoid starving writers (promoted from upgraders).
+  bool try_lock_shared() {
+    // fetch_add is considerably (100%) faster than compare_exchange,
+    // so here we are optimizing for the common (lock success) case.
+    int32_t value = bits_.fetch_add(READER, std::memory_order_acquire);
+    if (UNLIKELY(value & (WRITER|UPGRADED))) {
+      bits_.fetch_add(-READER, std::memory_order_release);
+      return false;
+    }
+    return true;
+  }
+
+  // try to unlock upgrade and write lock atomically
+  bool try_unlock_upgrade_and_lock() {
+    int32_t expect = UPGRADED;
+    return bits_.compare_exchange_strong(expect, WRITER,
+        std::memory_order_acq_rel);
+  }
+
+  // try to acquire an upgradable lock.
+  bool try_lock_upgrade() {
+    int32_t value = bits_.fetch_or(UPGRADED, std::memory_order_acquire);
+
+    // Note: when failed, we cannot flip the UPGRADED bit back,
+    // as in this case there is either another upgrade lock or a write lock.
+    // If it's a write lock, the bit will get cleared up when that lock's done
+    // with unlock().
+    return ((value & (UPGRADED | WRITER)) == 0);
+  }
+
+  // mainly for debugging purposes.
+  int32_t bits() const { return bits_.load(std::memory_order_acquire); }
+
+  class ReadHolder;
+  class UpgradedHolder;
+  class WriteHolder;
+
+  class ReadHolder {
+   public:
+    explicit ReadHolder(RWSpinLock* lock = nullptr) : lock_(lock) {
+      if (lock_) lock_->lock_shared();
+    }
+
+    explicit ReadHolder(RWSpinLock& lock) : lock_(&lock) {
+      lock_->lock_shared();
+    }
+
+    ReadHolder(ReadHolder&& other) : lock_(other.lock_) {
+      other.lock_ = nullptr;
+    }
+
+    // down-grade
+    explicit ReadHolder(UpgradedHolder&& upgraded) : lock_(upgraded.lock_) {
+      upgraded.lock_ = nullptr;
+      if (lock_) lock_->unlock_upgrade_and_lock_shared();
+    }
+
+    explicit ReadHolder(WriteHolder&& writer) : lock_(writer.lock_) {
+      writer.lock_ = nullptr;
+      if (lock_) lock_->unlock_and_lock_shared();
+    }
+
+    ReadHolder& operator=(ReadHolder&& other) {
+      using std::swap;
+      swap(lock_, other.lock_);
+      return *this;
+    }
+
+    ReadHolder(const ReadHolder& other) = delete;
+    ReadHolder& operator=(const ReadHolder& other) = delete;
+
+    ~ReadHolder() { if (lock_) lock_->unlock_shared(); }
+
+    void reset(RWSpinLock* lock = nullptr) {
+      if (lock == lock_) return;
+      if (lock_) lock_->unlock_shared();
+      lock_ = lock;
+      if (lock_) lock_->lock_shared();
+    }
+
+    void swap(ReadHolder* other) {
+      std::swap(lock_, other->lock_);
+    }
+
+   private:
+    friend class UpgradedHolder;
+    friend class WriteHolder;
+    RWSpinLock* lock_;
+  };
+
+  class UpgradedHolder {
+   public:
+    explicit UpgradedHolder(RWSpinLock* lock = nullptr) : lock_(lock) {
+      if (lock_) lock_->lock_upgrade();
+    }
+
+    explicit UpgradedHolder(RWSpinLock& lock) : lock_(&lock) {
+      lock_->lock_upgrade();
+    }
+
+    explicit UpgradedHolder(WriteHolder&& writer) {
+      lock_ = writer.lock_;
+      writer.lock_ = nullptr;
+      if (lock_) lock_->unlock_and_lock_upgrade();
+    }
+
+    UpgradedHolder(UpgradedHolder&& other) : lock_(other.lock_) {
+      other.lock_ = nullptr;
+    }
+
+    UpgradedHolder& operator =(UpgradedHolder&& other) {
+      using std::swap;
+      swap(lock_, other.lock_);
+      return *this;
+    }
+
+    UpgradedHolder(const UpgradedHolder& other) = delete;
+    UpgradedHolder& operator =(const UpgradedHolder& other) = delete;
+
+    ~UpgradedHolder() { if (lock_) lock_->unlock_upgrade(); }
+
+    void reset(RWSpinLock* lock = nullptr) {
+      if (lock == lock_) return;
+      if (lock_) lock_->unlock_upgrade();
+      lock_ = lock;
+      if (lock_) lock_->lock_upgrade();
+    }
+
+    void swap(UpgradedHolder* other) {
+      using std::swap;
+      swap(lock_, other->lock_);
+    }
+
+   private:
+    friend class WriteHolder;
+    friend class ReadHolder;
+    RWSpinLock* lock_;
+  };
+
+  class WriteHolder {
+   public:
+    explicit WriteHolder(RWSpinLock* lock = nullptr) : lock_(lock) {
+      if (lock_) lock_->lock();
+    }
+
+    explicit WriteHolder(RWSpinLock& lock) : lock_(&lock) {
+      lock_->lock();
+    }
+
+    // promoted from an upgrade lock holder
+    explicit WriteHolder(UpgradedHolder&& upgraded) {
+      lock_ = upgraded.lock_;
+      upgraded.lock_ = nullptr;
+      if (lock_) lock_->unlock_upgrade_and_lock();
+    }
+
+    WriteHolder(WriteHolder&& other) : lock_(other.lock_) {
+      other.lock_ = nullptr;
+    }
+
+    WriteHolder& operator =(WriteHolder&& other) {
+      using std::swap;
+      swap(lock_, other.lock_);
+      return *this;
+    }
+
+    WriteHolder(const WriteHolder& other) = delete;
+    WriteHolder& operator =(const WriteHolder& other) = delete;
+
+    ~WriteHolder () { if (lock_) lock_->unlock(); }
+
+    void reset(RWSpinLock* lock = nullptr) {
+      if (lock == lock_) return;
+      if (lock_) lock_->unlock();
+      lock_ = lock;
+      if (lock_) lock_->lock();
+    }
+
+    void swap(WriteHolder* other) {
+      using std::swap;
+      swap(lock_, other->lock_);
+    }
+
+   private:
+    friend class ReadHolder;
+    friend class UpgradedHolder;
+    RWSpinLock* lock_;
+  };
+
+  // Synchronized<> adaptors
+  friend void acquireRead(RWSpinLock& l) { return l.lock_shared(); }
+  friend void acquireReadWrite(RWSpinLock& l) { return l.lock(); }
+  friend void releaseRead(RWSpinLock& l) { return l.unlock_shared(); }
+  friend void releaseReadWrite(RWSpinLock& l) { return l.unlock(); }
+
+ private:
+  std::atomic<int32_t> bits_;
+};
+
+
+#ifdef RW_SPINLOCK_USE_X86_INTRINSIC_
+// A more balanced Read-Write spin lock implemented based on GCC intrinsics.
+
+namespace detail {
+template <size_t kBitWidth> struct RWTicketIntTrait {
+  static_assert(kBitWidth == 32 || kBitWidth == 64,
+      "bit width has to be either 32 or 64 ");
+};
+
+template <>
+struct RWTicketIntTrait<64> {
+  typedef uint64_t FullInt;
+  typedef uint32_t HalfInt;
+  typedef uint16_t QuarterInt;
+
+#ifdef __SSE2__
+  static __m128i make128(const uint16_t v[4]) {
+    return _mm_set_epi16(0, 0, 0, 0, v[3], v[2], v[1], v[0]);
+  }
+  static inline __m128i fromInteger(uint64_t from) {
+    return _mm_cvtsi64_si128(from);
+  }
+  static inline uint64_t toInteger(__m128i in) {
+    return _mm_cvtsi128_si64(in);
+  }
+  static inline uint64_t addParallel(__m128i in, __m128i kDelta) {
+    return toInteger(_mm_add_epi16(in, kDelta));
+  }
+#endif
+};
+
+template <>
+struct RWTicketIntTrait<32> {
+  typedef uint32_t FullInt;
+  typedef uint16_t HalfInt;
+  typedef uint8_t QuarterInt;
+
+#ifdef __SSE2__
+  static __m128i make128(const uint8_t v[4]) {
+    return _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0,
+        0, 0, 0, 0, v[3], v[2], v[1], v[0]);
+  }
+  static inline __m128i fromInteger(uint32_t from) {
+    return _mm_cvtsi32_si128(from);
+  }
+  static inline uint32_t toInteger(__m128i in) {
+    return _mm_cvtsi128_si32(in);
+  }
+  static inline uint32_t addParallel(__m128i in, __m128i kDelta) {
+    return toInteger(_mm_add_epi8(in, kDelta));
+  }
+#endif
+};
+}  // detail
+
+
+template<size_t kBitWidth, bool kFavorWriter=false>
+class RWTicketSpinLockT : boost::noncopyable {
+  typedef detail::RWTicketIntTrait<kBitWidth> IntTraitType;
+  typedef typename detail::RWTicketIntTrait<kBitWidth>::FullInt FullInt;
+  typedef typename detail::RWTicketIntTrait<kBitWidth>::HalfInt HalfInt;
+  typedef typename detail::RWTicketIntTrait<kBitWidth>::QuarterInt
+    QuarterInt;
+
+  union RWTicket {
+    FullInt whole;
+    HalfInt readWrite;
+    __extension__ struct {
+      QuarterInt write;
+      QuarterInt read;
+      QuarterInt users;
+    };
+  } ticket;
+
+ private: // Some x64-specific utilities for atomic access to ticket.
+  template<class T> static T load_acquire(T* addr) {
+    T t = *addr; // acquire barrier
+    asm volatile("" : : : "memory");
+    return t;
+  }
+
+  template<class T>
+  static void store_release(T* addr, T v) {
+    asm volatile("" : : : "memory");
+    *addr = v; // release barrier
+  }
+
+ public:
+
+  RWTicketSpinLockT() {
+    store_release(&ticket.whole, FullInt(0));
+  }
+
+  void lock() {
+    if (kFavorWriter) {
+      writeLockAggressive();
+    } else {
+      writeLockNice();
+    }
+  }
+
+  /*
+   * Both try_lock and try_lock_shared diverge in our implementation from the
+   * lock algorithm described in the link above.
+   *
+   * In the read case, it is undesirable that the readers could wait
+   * for another reader (before increasing ticket.read in the other
+   * implementation).  Our approach gives up on
+   * first-come-first-serve, but our benchmarks showed improve
+   * performance for both readers and writers under heavily contended
+   * cases, particularly when the number of threads exceeds the number
+   * of logical CPUs.
+   *
+   * We have writeLockAggressive() using the original implementation
+   * for a writer, which gives some advantage to the writer over the
+   * readers---for that path it is guaranteed that the writer will
+   * acquire the lock after all the existing readers exit.
+   */
+  bool try_lock() {
+    RWTicket t;
+    FullInt old = t.whole = load_acquire(&ticket.whole);
+    if (t.users != t.write) return false;
+    ++t.users;
+    return __sync_bool_compare_and_swap(&ticket.whole, old, t.whole);
+  }
+
+  /*
+   * Call this if you want to prioritize writer to avoid starvation.
+   * Unlike writeLockNice, immediately acquires the write lock when
+   * the existing readers (arriving before the writer) finish their
+   * turns.
+   */
+  void writeLockAggressive() {
+    // sched_yield() is needed here to avoid a pathology if the number
+    // of threads attempting concurrent writes is >= the number of real
+    // cores allocated to this process. This is less likely than the
+    // corresponding situation in lock_shared(), but we still want to
+    // avoid it
+    int count = 0;
+    QuarterInt val = __sync_fetch_and_add(&ticket.users, 1);
+    while (val != load_acquire(&ticket.write)) {
+      asm volatile("pause");
+      if (UNLIKELY(++count > 1000)) sched_yield();
+    }
+  }
+
+  // Call this when the writer should be nicer to the readers.
+  void writeLockNice() {
+    // Here it doesn't cpu-relax the writer.
+    //
+    // This is because usually we have many more readers than the
+    // writers, so the writer has less chance to get the lock when
+    // there are a lot of competing readers.  The aggressive spinning
+    // can help to avoid starving writers.
+    //
+    // We don't worry about sched_yield() here because the caller
+    // has already explicitly abandoned fairness.
+    while (!try_lock()) {}
+  }
+
+  // Atomically unlock the write-lock from writer and acquire the read-lock.
+  void unlock_and_lock_shared() {
+    QuarterInt val = __sync_fetch_and_add(&ticket.read, 1);
+  }
+
+  // Release writer permission on the lock.
+  void unlock() {
+    RWTicket t;
+    t.whole = load_acquire(&ticket.whole);
+    FullInt old = t.whole;
+
+#ifdef __SSE2__
+    // SSE2 can reduce the lock and unlock overhead by 10%
+    static const QuarterInt kDeltaBuf[4] = { 1, 1, 0, 0 };   // write/read/user
+    static const __m128i kDelta = IntTraitType::make128(kDeltaBuf);
+    __m128i m = IntTraitType::fromInteger(old);
+    t.whole = IntTraitType::addParallel(m, kDelta);
+#else
+    ++t.read;
+    ++t.write;
+#endif
+    store_release(&ticket.readWrite, t.readWrite);
+  }
+
+  void lock_shared() {
+    // sched_yield() is important here because we can't grab the
+    // shared lock if there is a pending writeLockAggressive, so we
+    // need to let threads that already have a shared lock complete
+    int count = 0;
+    while (!LIKELY(try_lock_shared())) {
+      asm volatile("pause");
+      if (UNLIKELY((++count & 1023) == 0)) sched_yield();
+    }
+  }
+
+  bool try_lock_shared() {
+    RWTicket t, old;
+    old.whole = t.whole = load_acquire(&ticket.whole);
+    old.users = old.read;
+#ifdef  __SSE2__
+    // SSE2 may reduce the total lock and unlock overhead by 10%
+    static const QuarterInt kDeltaBuf[4] = { 0, 1, 1, 0 };   // write/read/user
+    static const __m128i kDelta = IntTraitType::make128(kDeltaBuf);
+    __m128i m = IntTraitType::fromInteger(old.whole);
+    t.whole = IntTraitType::addParallel(m, kDelta);
+#else
+    ++t.read;
+    ++t.users;
+#endif
+    return __sync_bool_compare_and_swap(&ticket.whole, old.whole, t.whole);
+  }
+
+  void unlock_shared() {
+    QuarterInt val = __sync_fetch_and_add(&ticket.write, 1);
+  }
+
+  class WriteHolder;
+
+  typedef RWTicketSpinLockT<kBitWidth, kFavorWriter> RWSpinLock;
+  class ReadHolder : boost::noncopyable {
+   public:
+    explicit ReadHolder(RWSpinLock *lock = nullptr) :
+      lock_(lock) {
+      if (lock_) lock_->lock_shared();
+    }
+
+    explicit ReadHolder(RWSpinLock &lock) : lock_ (&lock) {
+      if (lock_) lock_->lock_shared();
+    }
+
+    // atomically unlock the write-lock from writer and acquire the read-lock
+    explicit ReadHolder(WriteHolder *writer) : lock_(nullptr) {
+      std::swap(this->lock_, writer->lock_);
+      if (lock_) {
+        lock_->unlock_and_lock_shared();
+      }
+    }
+
+    ~ReadHolder() {
+      if (lock_) lock_->unlock_shared();
+    }
+
+    void reset(RWSpinLock *lock = nullptr) {
+      if (lock_) lock_->unlock_shared();
+      lock_ = lock;
+      if (lock_) lock_->lock_shared();
+    }
+
+    void swap(ReadHolder *other) {
+      std::swap(this->lock_, other->lock_);
+    }
+
+   private:
+    RWSpinLock *lock_;
+  };
+
+  class WriteHolder : boost::noncopyable {
+   public:
+    explicit WriteHolder(RWSpinLock *lock = nullptr) : lock_(lock) {
+      if (lock_) lock_->lock();
+    }
+    explicit WriteHolder(RWSpinLock &lock) : lock_ (&lock) {
+      if (lock_) lock_->lock();
+    }
+
+    ~WriteHolder() {
+      if (lock_) lock_->unlock();
+    }
+
+    void reset(RWSpinLock *lock = nullptr) {
+      if (lock == lock_) return;
+      if (lock_) lock_->unlock();
+      lock_ = lock;
+      if (lock_) lock_->lock();
+    }
+
+    void swap(WriteHolder *other) {
+      std::swap(this->lock_, other->lock_);
+    }
+
+   private:
+    friend class ReadHolder;
+    RWSpinLock *lock_;
+  };
+
+  // Synchronized<> adaptors.
+  friend void acquireRead(RWTicketSpinLockT& mutex) {
+    mutex.lock_shared();
+  }
+  friend void acquireReadWrite(RWTicketSpinLockT& mutex) {
+    mutex.lock();
+  }
+  friend bool acquireReadWrite(RWTicketSpinLockT& mutex,
+                               unsigned int milliseconds) {
+    mutex.lock();
+    return true;
+  }
+  friend void releaseRead(RWTicketSpinLockT& mutex) {
+    mutex.unlock_shared();
+  }
+  friend void releaseReadWrite(RWTicketSpinLockT& mutex) {
+    mutex.unlock();
+  }
+};
+
+typedef RWTicketSpinLockT<32> RWTicketSpinLock32;
+typedef RWTicketSpinLockT<64> RWTicketSpinLock64;
+
+#endif  // RW_SPINLOCK_USE_X86_INTRINSIC_
+
+}  // namespace folly
+
+#ifdef RW_SPINLOCK_USE_X86_INTRINSIC_
+#undef RW_SPINLOCK_USE_X86_INTRINSIC_
+#endif
+
+#endif  // FOLLY_RWSPINLOCK_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/SafeAssert.cpp
@@ -0,0 +1,50 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/SafeAssert.h"
+
+#include "folly/Conv.h"
+#include "folly/FileUtil.h"
+
+namespace folly { namespace detail {
+
+namespace {
+void writeStderr(const char* s) {
+  writeFull(STDERR_FILENO, s, strlen(s));
+}
+}  // namespace
+
+void assertionFailure(const char* expr, const char* msg, const char* file,
+                      unsigned int line, const char* function) {
+  writeStderr("\n\nAssertion failure: ");
+  writeStderr(expr);
+  writeStderr("\nMessage: ");
+  writeStderr(msg);
+  writeStderr("\nFile: ");
+  writeStderr(file);
+  writeStderr("\nLine: ");
+  char buf[20];
+  uint32_t n = uint64ToBufferUnsafe(line, buf);
+  writeFull(STDERR_FILENO, buf, n);
+  writeStderr("\nFunction: ");
+  writeStderr(function);
+  writeStderr("\n");
+  fsyncNoInt(STDERR_FILENO);
+  abort();
+}
+
+}}  // namespaces
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/SafeAssert.h
@@ -0,0 +1,53 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_SAFEASSERT_H_
+#define FOLLY_SAFEASSERT_H_
+
+#include "folly/Portability.h"
+#include "folly/Preprocessor.h"
+
+/**
+ * Verify that the expression is true. If not, prints an error message
+ * (containing msg) to stderr and abort()s. Just like CHECK(), but only
+ * logs to stderr and only does async-signal-safe calls.
+ */
+#define FOLLY_SAFE_CHECK(expr, msg) \
+  ((expr) ? static_cast<void>(0) : \
+   ::folly::detail::assertionFailure( \
+       FB_STRINGIZE(expr), (msg), __FILE__, __LINE__, __PRETTY_FUNCTION__))
+
+/**
+ * In debug mode, verify that the expression is true. Otherwise, do nothing
+ * (do not even evaluate expr). Just like assert() or DCHECK(), but only
+ * logs to stderr and only does async-signal-safe calls.
+ */
+#ifdef NDEBUG
+#define FOLLY_SAFE_DCHECK(expr, msg) (static_cast<void>(0))
+#else
+#define FOLLY_SAFE_DCHECK FOLLY_SAFE_CHECK
+#endif
+
+namespace folly { namespace detail {
+
+void assertionFailure(const char* expr, const char* msg, const char* file,
+                      unsigned int line, const char* function)
+  FOLLY_NORETURN;
+
+}}  // namespace folly
+
+#endif /* FOLLY_SAFEASSERT_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/SConstruct.double-conversion
@@ -0,0 +1,21 @@
+double_conversion_sources = ['src/' + x for x in SConscript('src/SConscript')]
+double_conversion_test_sources = ['test/cctest/' + x for x in SConscript('test/cctest/SConscript')]
+test = double_conversion_sources + double_conversion_test_sources
+print(test)
+env = Environment(CPPPATH='#/src')
+debug = ARGUMENTS.get('debug', 0)
+optimize = ARGUMENTS.get('optimize', 0)
+if int(debug):
+  env.Append(CCFLAGS = '-g -Wall -Werror')
+if int(optimize):
+  env.Append(CCFLAGS = '-O3')
+print double_conversion_sources
+print double_conversion_test_sources
+double_conversion_shared_objects = [
+    env.SharedObject(src) for src in double_conversion_sources]
+double_conversion_static_objects = [
+    env.StaticObject(src) for src in double_conversion_sources]
+library_name = 'double_conversion'
+static_lib = env.StaticLibrary(library_name, double_conversion_static_objects)
+env.StaticLibrary(library_name + '_pic', double_conversion_shared_objects)
+env.Program('run_tests', double_conversion_test_sources, LIBS=[static_lib])
--- /dev/null
+++ b/hphp/submodules/folly/folly/ScopeGuard.h
@@ -0,0 +1,233 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_SCOPEGUARD_H_
+#define FOLLY_SCOPEGUARD_H_
+
+#include <cstddef>
+#include <functional>
+#include <new>
+
+#include "folly/Preprocessor.h"
+#include "folly/detail/UncaughtExceptionCounter.h"
+
+namespace folly {
+
+/**
+ * ScopeGuard is a general implementation of the "Initialization is
+ * Resource Acquisition" idiom.  Basically, it guarantees that a function
+ * is executed upon leaving the currrent scope unless otherwise told.
+ *
+ * The makeGuard() function is used to create a new ScopeGuard object.
+ * It can be instantiated with a lambda function, a std::function<void()>,
+ * a functor, or a void(*)() function pointer.
+ *
+ *
+ * Usage example: Add a friend to memory iff it is also added to the db.
+ *
+ * void User::addFriend(User& newFriend) {
+ *   // add the friend to memory
+ *   friends_.push_back(&newFriend);
+ *
+ *   // If the db insertion that follows fails, we should
+ *   // remove it from memory.
+ *   // (You could also declare this as "auto guard = makeGuard(...)")
+ *   ScopeGuard guard = makeGuard([&] { friends_.pop_back(); });
+ *
+ *   // this will throw an exception upon error, which
+ *   // makes the ScopeGuard execute UserCont::pop_back()
+ *   // once the Guard's destructor is called.
+ *   db_->addFriend(GetName(), newFriend.GetName());
+ *
+ *   // an exception was not thrown, so don't execute
+ *   // the Guard.
+ *   guard.dismiss();
+ * }
+ *
+ * Examine ScopeGuardTest.cpp for some more sample usage.
+ *
+ * Stolen from:
+ *   Andrei's and Petru Marginean's CUJ article:
+ *     http://drdobbs.com/184403758
+ *   and the loki library:
+ *     http://loki-lib.sourceforge.net/index.php?n=Idioms.ScopeGuardPointer
+ *   and triendl.kj article:
+ *     http://www.codeproject.com/KB/cpp/scope_guard.aspx
+ */
+class ScopeGuardImplBase {
+ public:
+  void dismiss() noexcept {
+    dismissed_ = true;
+  }
+
+ protected:
+  ScopeGuardImplBase()
+    : dismissed_(false) {}
+
+  ScopeGuardImplBase(ScopeGuardImplBase&& other)
+    : dismissed_(other.dismissed_) {
+    other.dismissed_ = true;
+  }
+
+  bool dismissed_;
+};
+
+template <typename FunctionType>
+class ScopeGuardImpl : public ScopeGuardImplBase {
+ public:
+  explicit ScopeGuardImpl(const FunctionType& fn)
+    : function_(fn) {}
+
+  explicit ScopeGuardImpl(FunctionType&& fn)
+    : function_(std::move(fn)) {}
+
+  ScopeGuardImpl(ScopeGuardImpl&& other)
+    : ScopeGuardImplBase(std::move(other))
+    , function_(std::move(other.function_)) {
+  }
+
+  ~ScopeGuardImpl() noexcept {
+    if (!dismissed_) {
+      execute();
+    }
+  }
+
+ private:
+  void* operator new(size_t) = delete;
+
+  void execute() noexcept { function_(); }
+
+  FunctionType function_;
+};
+
+template <typename FunctionType>
+ScopeGuardImpl<typename std::decay<FunctionType>::type>
+makeGuard(FunctionType&& fn) {
+  return ScopeGuardImpl<typename std::decay<FunctionType>::type>(
+      std::forward<FunctionType>(fn));
+}
+
+/**
+ * This is largely unneeded if you just use auto for your guards.
+ */
+typedef ScopeGuardImplBase&& ScopeGuard;
+
+namespace detail {
+
+#if defined(FOLLY_EXCEPTION_COUNT_USE_CXA_GET_GLOBALS) || \
+    defined(FOLLY_EXCEPTION_COUNT_USE_GETPTD)
+
+/**
+ * ScopeGuard used for executing a function when leaving the current scope
+ * depending on the presence of a new uncaught exception.
+ *
+ * If the executeOnException template parameter is true, the function is
+ * executed if a new uncaught exception is present at the end of the scope.
+ * If the parameter is false, then the function is executed if no new uncaught
+ * exceptions are present at the end of the scope.
+ *
+ * Used to implement SCOPE_FAIL and SCOPE_SUCCES below.
+ */
+template <typename FunctionType, bool executeOnException>
+class ScopeGuardForNewException {
+ public:
+  explicit ScopeGuardForNewException(const FunctionType& fn)
+      : function_(fn) {
+  }
+
+  explicit ScopeGuardForNewException(FunctionType&& fn)
+      : function_(std::move(fn)) {
+  }
+
+  ScopeGuardForNewException(ScopeGuardForNewException&& other)
+      : function_(std::move(other.function_))
+      , exceptionCounter_(std::move(other.exceptionCounter_)) {
+  }
+
+  ~ScopeGuardForNewException() noexcept(executeOnException) {
+    if (executeOnException == exceptionCounter_.isNewUncaughtException()) {
+      function_();
+    }
+  }
+
+ private:
+  ScopeGuardForNewException(const ScopeGuardForNewException& other) = delete;
+
+  void* operator new(size_t) = delete;
+
+  FunctionType function_;
+  UncaughtExceptionCounter exceptionCounter_;
+};
+
+/**
+ * Internal use for the macro SCOPE_FAIL below
+ */
+enum class ScopeGuardOnFail {};
+
+template <typename FunctionType>
+ScopeGuardForNewException<typename std::decay<FunctionType>::type, true>
+operator+(detail::ScopeGuardOnFail, FunctionType&& fn) {
+  return
+      ScopeGuardForNewException<typename std::decay<FunctionType>::type, true>(
+      std::forward<FunctionType>(fn));
+}
+
+/**
+ * Internal use for the macro SCOPE_SUCCESS below
+ */
+enum class ScopeGuardOnSuccess {};
+
+template <typename FunctionType>
+ScopeGuardForNewException<typename std::decay<FunctionType>::type, false>
+operator+(ScopeGuardOnSuccess, FunctionType&& fn) {
+  return
+      ScopeGuardForNewException<typename std::decay<FunctionType>::type, false>(
+      std::forward<FunctionType>(fn));
+}
+
+#endif // native uncaught_exception() supported
+
+/**
+ * Internal use for the macro SCOPE_EXIT below
+ */
+enum class ScopeGuardOnExit {};
+
+template <typename FunctionType>
+ScopeGuardImpl<typename std::decay<FunctionType>::type>
+operator+(detail::ScopeGuardOnExit, FunctionType&& fn) {
+  return ScopeGuardImpl<typename std::decay<FunctionType>::type>(
+      std::forward<FunctionType>(fn));
+}
+} // namespace detail
+
+} // folly
+
+#define SCOPE_EXIT \
+  auto FB_ANONYMOUS_VARIABLE(SCOPE_EXIT_STATE) \
+  = ::folly::detail::ScopeGuardOnExit() + [&]() noexcept
+
+#if defined(FOLLY_EXCEPTION_COUNT_USE_CXA_GET_GLOBALS) || \
+    defined(FOLLY_EXCEPTION_COUNT_USE_GETPTD)
+#define SCOPE_FAIL \
+  auto FB_ANONYMOUS_VARIABLE(SCOPE_FAIL_STATE) \
+  = ::folly::detail::ScopeGuardOnFail() + [&]() noexcept
+
+#define SCOPE_SUCCESS \
+  auto FB_ANONYMOUS_VARIABLE(SCOPE_SUCCESS_STATE) \
+  = ::folly::detail::ScopeGuardOnSuccess() + [&]()
+#endif // native uncaught_exception() supported
+
+#endif // FOLLY_SCOPEGUARD_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/SmallLocks.h
@@ -0,0 +1,332 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_SMALLLOCKS_H_
+#define FOLLY_SMALLLOCKS_H_
+
+/*
+ * This header defines a few very small mutex types.  These are useful
+ * in highly memory-constrained environments where contention is
+ * unlikely.
+ *
+ * Note: these locks are for use when you aren't likely to contend on
+ * the critical section, or when the critical section is incredibly
+ * small.  Given that, both of the locks defined in this header are
+ * inherently unfair: that is, the longer a thread is waiting, the
+ * longer it waits between attempts to acquire, so newer waiters are
+ * more likely to get the mutex.  For the intended use-case this is
+ * fine.
+ *
+ * @author Keith Adams <kma@fb.com>
+ * @author Jordan DeLong <delong.j@fb.com>
+ */
+
+#include <array>
+#include <cinttypes>
+#include <type_traits>
+#include <ctime>
+#include <boost/noncopyable.hpp>
+#include <cstdlib>
+#include <pthread.h>
+#include <mutex>
+
+#include <glog/logging.h>
+
+#ifndef __x86_64__
+# error "SmallLocks.h is currently x64-only."
+#endif
+
+#include "folly/Portability.h"
+
+namespace folly {
+
+//////////////////////////////////////////////////////////////////////
+
+namespace detail {
+
+  /*
+   * A helper object for the condended case. Starts off with eager
+   * spinning, and falls back to sleeping for small quantums.
+   */
+  class Sleeper {
+    static const uint32_t kMaxActiveSpin = 4000;
+
+    uint32_t spinCount;
+
+  public:
+    Sleeper() : spinCount(0) {}
+
+    void wait() {
+      if (spinCount < kMaxActiveSpin) {
+        ++spinCount;
+        asm volatile("pause");
+      } else {
+        /*
+         * Always sleep 0.5ms, assuming this will make the kernel put
+         * us down for whatever its minimum timer resolution is (in
+         * linux this varies by kernel version from 1ms to 10ms).
+         */
+        struct timespec ts = { 0, 500000 };
+        nanosleep(&ts, NULL);
+      }
+    }
+  };
+
+}
+
+//////////////////////////////////////////////////////////////////////
+
+/*
+ * A really, *really* small spinlock for fine-grained locking of lots
+ * of teeny-tiny data.
+ *
+ * Zero initializing these is guaranteed to be as good as calling
+ * init(), since the free state is guaranteed to be all-bits zero.
+ *
+ * This class should be kept a POD, so we can used it in other packed
+ * structs (gcc does not allow __attribute__((packed)) on structs that
+ * contain non-POD data).  This means avoid adding a constructor, or
+ * making some members private, etc.
+ */
+struct MicroSpinLock {
+  enum { FREE = 0, LOCKED = 1 };
+  uint8_t lock_;
+
+  /*
+   * Atomically move lock_ from "compare" to "newval". Return boolean
+   * success. Do not play on or around.
+   */
+  bool cas(uint8_t compare, uint8_t newVal) {
+    bool out;
+    bool memVal; // only set if the cmpxchg fails
+    asm volatile("lock; cmpxchgb %[newVal], (%[lockPtr]);"
+                 "setz %[output];"
+                 : [output] "=r" (out), "=a" (memVal)
+                 : "a" (compare), // cmpxchgb constrains this to be in %al
+                   [newVal] "q" (newVal),  // Needs to be byte-accessible
+                   [lockPtr] "r" (&lock_)
+                 : "memory", "flags");
+    return out;
+  }
+
+  // Initialize this MSL.  It is unnecessary to call this if you
+  // zero-initialize the MicroSpinLock.
+  void init() {
+    lock_ = FREE;
+  }
+
+  bool try_lock() {
+    return cas(FREE, LOCKED);
+  }
+
+  void lock() {
+    detail::Sleeper sleeper;
+    do {
+      while (lock_ != FREE) {
+        asm volatile("" : : : "memory");
+        sleeper.wait();
+      }
+    } while (!try_lock());
+    DCHECK(lock_ == LOCKED);
+  }
+
+  void unlock() {
+    CHECK(lock_ == LOCKED);
+    asm volatile("" : : : "memory");
+    lock_ = FREE; // release barrier on x86
+  }
+};
+
+//////////////////////////////////////////////////////////////////////
+
+/*
+ * Spin lock on a single bit in an integral type.  You can use this
+ * with 16, 32, or 64-bit integral types.
+ *
+ * This is useful if you want a small lock and already have an int
+ * with a bit in it that you aren't using.  But note that it can't be
+ * as small as MicroSpinLock (1 byte), if you don't already have a
+ * convenient int with an unused bit lying around to put it on.
+ *
+ * To construct these, either use init() or zero initialize.  We don't
+ * have a real constructor because we want this to be a POD type so we
+ * can put it into packed structs.
+ */
+template<class IntType, int Bit = sizeof(IntType) * 8 - 1>
+struct PicoSpinLock {
+  // Internally we deal with the unsigned version of the type.
+  typedef typename std::make_unsigned<IntType>::type UIntType;
+
+  static_assert(std::is_integral<IntType>::value,
+                "PicoSpinLock needs an integral type");
+  static_assert(sizeof(IntType) == 2 || sizeof(IntType) == 4 ||
+                  sizeof(IntType) == 8,
+                "PicoSpinLock can't work on integers smaller than 2 bytes");
+
+ public:
+  static const UIntType kLockBitMask_ = UIntType(1) << Bit;
+  UIntType lock_;
+
+  /*
+   * You must call this function before using this class, if you
+   * default constructed it.  If you zero-initialized it you can
+   * assume the PicoSpinLock is in a valid unlocked state with
+   * getData() == 0.
+   *
+   * (This doesn't use a constructor because we want to be a POD.)
+   */
+  void init(IntType initialValue = 0) {
+    CHECK(!(initialValue & kLockBitMask_));
+    lock_ = initialValue;
+  }
+
+  /*
+   * Returns the value of the integer we using for our lock, except
+   * with the bit we are using as a lock cleared, regardless of
+   * whether the lock is held.
+   *
+   * It is 'safe' to call this without holding the lock.  (As in: you
+   * get the same guarantees for simultaneous accesses to an integer
+   * as you normally get.)
+   */
+  IntType getData() const {
+    return static_cast<IntType>(lock_ & ~kLockBitMask_);
+  }
+
+  /*
+   * Set the value of the other bits in our integer.
+   *
+   * Don't use this when you aren't holding the lock, unless it can be
+   * guaranteed that no other threads may be trying to use this.
+   */
+  void setData(IntType w) {
+    CHECK(!(w & kLockBitMask_));
+    lock_ = (lock_ & kLockBitMask_) | w;
+  }
+
+  /*
+   * Try to get the lock without blocking: returns whether or not we
+   * got it.
+   */
+  bool try_lock() const {
+    bool ret = false;
+
+#define FB_DOBTS(size)                                  \
+  asm volatile("lock; bts" #size " %1, (%2); setnc %0"  \
+               : "=r" (ret)                             \
+               : "i" (Bit),                             \
+                 "r" (&lock_)                           \
+               : "memory", "flags")
+
+    switch (sizeof(IntType)) {
+    case 2: FB_DOBTS(w); break;
+    case 4: FB_DOBTS(l); break;
+    case 8: FB_DOBTS(q); break;
+    }
+
+#undef FB_DOBTS
+
+    return ret;
+  }
+
+  /*
+   * Block until we can acquire the lock.  Uses Sleeper to wait.
+   */
+  void lock() const {
+    detail::Sleeper sleeper;
+    while (!try_lock()) {
+      sleeper.wait();
+    }
+  }
+
+  /*
+   * Release the lock, without changing the value of the rest of the
+   * integer.
+   */
+  void unlock() const {
+#define FB_DOBTR(size)                          \
+  asm volatile("lock; btr" #size " %0, (%1)"    \
+               :                                \
+               : "i" (Bit),                     \
+                 "r" (&lock_)                   \
+               : "memory", "flags")
+
+
+    // Reads and writes can not be reordered wrt locked instructions,
+    // so we don't need a memory fence here.
+    switch (sizeof(IntType)) {
+    case 2: FB_DOBTR(w); break;
+    case 4: FB_DOBTR(l); break;
+    case 8: FB_DOBTR(q); break;
+    }
+
+#undef FB_DOBTR
+  }
+};
+
+//////////////////////////////////////////////////////////////////////
+
+/**
+ * Array of spinlocks where each one is padded to prevent false sharing.
+ * Useful for shard-based locking implementations in environments where
+ * contention is unlikely.
+ */
+
+// TODO: generate it from configure (`getconf LEVEL1_DCACHE_LINESIZE`)
+#define FOLLY_CACHE_LINE_SIZE 64
+
+template <class T, size_t N>
+struct SpinLockArray {
+  T& operator[](size_t i) {
+    return data_[i].lock;
+  }
+
+  const T& operator[](size_t i) const {
+    return data_[i].lock;
+  }
+
+  constexpr size_t size() const { return N; }
+
+ private:
+  struct PaddedSpinLock {
+    PaddedSpinLock() : lock() { }
+    T lock;
+    char padding[FOLLY_CACHE_LINE_SIZE - sizeof(T)];
+  };
+  static_assert(sizeof(PaddedSpinLock) == FOLLY_CACHE_LINE_SIZE,
+                "Invalid size of PaddedSpinLock");
+
+  // Check if T can theoretically cross a cache line.
+  // NOTE: It should be alignof(std::max_align_t), but max_align_t
+  // isn't supported by gcc 4.6.2.
+  static_assert(alignof(MaxAlign) > 0 &&
+                FOLLY_CACHE_LINE_SIZE % alignof(MaxAlign) == 0 &&
+                sizeof(T) <= alignof(MaxAlign),
+                "T can cross cache line boundaries");
+
+  char padding_[FOLLY_CACHE_LINE_SIZE];
+  std::array<PaddedSpinLock, N> data_;
+} __attribute__((aligned));
+
+//////////////////////////////////////////////////////////////////////
+
+typedef std::lock_guard<MicroSpinLock> MSLGuard;
+
+//////////////////////////////////////////////////////////////////////
+
+}
+
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/small_vector.h
@@ -0,0 +1,1193 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+ * For high-level documentation and usage examples see
+ * folly/docs/small_vector.md
+ *
+ * @author Jordan DeLong <delong.j@fb.com>
+ */
+#ifndef FOLLY_SMALL_VECTOR_H_
+#define FOLLY_SMALL_VECTOR_H_
+
+#include "Portability.h"
+
+#include <stdexcept>
+#include <cstdlib>
+#include <type_traits>
+#include <algorithm>
+#include <iterator>
+#include <cassert>
+
+#include <boost/operators.hpp>
+#include <boost/type_traits.hpp>
+#include <boost/mpl/if.hpp>
+#include <boost/mpl/eval_if.hpp>
+#include <boost/mpl/vector.hpp>
+#include <boost/mpl/front.hpp>
+#include <boost/mpl/filter_view.hpp>
+#include <boost/mpl/identity.hpp>
+#include <boost/mpl/placeholders.hpp>
+#include <boost/mpl/empty.hpp>
+#include <boost/mpl/size.hpp>
+#include <boost/mpl/count.hpp>
+#include <boost/mpl/max.hpp>
+
+#include "folly/Malloc.h"
+
+#if defined(__GNUC__) && defined(__x86_64__)
+# include "folly/SmallLocks.h"
+# define FB_PACKED __attribute__((packed))
+#else
+# define FB_PACKED
+#endif
+
+#if FOLLY_HAVE_MALLOC_SIZE
+  extern "C" std::size_t malloc_size(const void*);
+# if !FOLLY_HAVE_MALLOC_USABLE_SIZE
+#  define malloc_usable_size malloc_size
+# endif
+# ifndef malloc_usable_size
+#  define malloc_usable_size malloc_size
+# endif
+#endif
+
+// Ignore shadowing warnings within this file, so includers can use -Wshadow.
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wshadow"
+
+namespace folly {
+
+//////////////////////////////////////////////////////////////////////
+
+namespace small_vector_policy {
+
+//////////////////////////////////////////////////////////////////////
+
+/*
+ * A flag which makes us refuse to use the heap at all.  If we
+ * overflow the in situ capacity we throw an exception.
+ */
+struct NoHeap;
+
+/*
+ * Passing this policy will cause small_vector to provide lock() and
+ * unlock() functions using a 1-bit spin lock in the size value.
+ *
+ * Note that this is intended for a fairly specialized (although
+ * strangely common at facebook) use case, where you have billions of
+ * vectors in memory where none of them are "hot" and most of them are
+ * small.  This allows you to get fine-grained locks without spending
+ * a lot of memory on mutexes (the alternative of a large hashtable of
+ * locks leads to extra cache misses in the lookup path).
+ *
+ * __x86_64__ only.
+ */
+struct OneBitMutex;
+
+//////////////////////////////////////////////////////////////////////
+
+} // small_vector_policy
+
+//////////////////////////////////////////////////////////////////////
+
+template<class T, std::size_t M, class A, class B, class C>
+class small_vector;
+
+//////////////////////////////////////////////////////////////////////
+
+namespace detail {
+
+  /*
+   * Move a range to a range of uninitialized memory.  Assumes the
+   * ranges don't overlap.
+   */
+  template<class T>
+  typename std::enable_if<
+    !FOLLY_IS_TRIVIALLY_COPYABLE(T)
+  >::type
+  moveToUninitialized(T* first, T* last, T* out) {
+    auto const count = last - first;
+    std::size_t idx = 0;
+    try {
+      for (; idx < count; ++first, ++idx) {
+        new (&out[idx]) T(std::move(*first));
+      }
+    } catch (...) {
+      // Even for callers trying to give the strong guarantee
+      // (e.g. push_back) it's ok to assume here that we don't have to
+      // move things back and that it was a copy constructor that
+      // threw: if someone throws from a move constructor the effects
+      // are unspecified.
+      for (std::size_t i = 0; i < idx; ++i) {
+        out[i].~T();
+      }
+      throw;
+    }
+  }
+
+  // Specialization for trivially copyable types.
+  template<class T>
+  typename std::enable_if<
+    FOLLY_IS_TRIVIALLY_COPYABLE(T)
+  >::type
+  moveToUninitialized(T* first, T* last, T* out) {
+    std::memmove(out, first, (last - first) * sizeof *first);
+  }
+
+  /*
+   * Move objects in memory to the right into some uninitialized
+   * memory, where the region overlaps.  This doesn't just use
+   * std::move_backward because move_backward only works if all the
+   * memory is initialized to type T already.
+   */
+  template<class T>
+  typename std::enable_if<
+    !FOLLY_IS_TRIVIALLY_COPYABLE(T)
+  >::type
+  moveObjectsRight(T* first, T* lastConstructed, T* realLast) {
+    if (lastConstructed == realLast) {
+      return;
+    }
+
+    T* end = first - 1; // Past the end going backwards.
+    T* out = realLast - 1;
+    T* in = lastConstructed - 1;
+    try {
+      for (; in != end && out >= lastConstructed; --in, --out) {
+        new (out) T(std::move(*in));
+      }
+      for (; in != end; --in, --out) {
+        *out = std::move(*in);
+      }
+      for (; out >= lastConstructed; --out) {
+        new (out) T();
+      }
+    } catch (...) {
+      // We want to make sure the same stuff is uninitialized memory
+      // if we exit via an exception (this is to make sure we provide
+      // the basic exception safety guarantee for insert functions).
+      if (out < lastConstructed) {
+        out = lastConstructed - 1;
+      }
+      for (auto it = out + 1; it != realLast; ++it) {
+        it->~T();
+      }
+      throw;
+    }
+  }
+
+  // Specialization for trivially copyable types.  The call to
+  // std::move_backward here will just turn into a memmove.  (TODO:
+  // change to std::is_trivially_copyable when that works.)
+  template<class T>
+  typename std::enable_if<
+    FOLLY_IS_TRIVIALLY_COPYABLE(T)
+  >::type
+  moveObjectsRight(T* first, T* lastConstructed, T* realLast) {
+    std::move_backward(first, lastConstructed, realLast);
+  }
+
+  /*
+   * Populate a region of memory using `op' to construct elements.  If
+   * anything throws, undo what we did.
+   */
+  template<class T, class Function>
+  void populateMemForward(T* mem, std::size_t n, Function const& op) {
+    std::size_t idx = 0;
+    try {
+      for (size_t i = 0; i < n; ++i) {
+        op(&mem[idx]);
+        ++idx;
+      }
+    } catch (...) {
+      for (std::size_t i = 0; i < idx; ++i) {
+        mem[i].~T();
+      }
+      throw;
+    }
+  }
+
+  template<class SizeType, bool ShouldUseHeap>
+  struct IntegralSizePolicy {
+    typedef SizeType InternalSizeType;
+
+    IntegralSizePolicy() : size_(0) {}
+
+  protected:
+    static constexpr std::size_t policyMaxSize() {
+      return SizeType(~kExternMask);
+    }
+
+    std::size_t doSize() const {
+      return size_ & ~kExternMask;
+    }
+
+    std::size_t isExtern() const {
+      return kExternMask & size_;
+    }
+
+    void setExtern(bool b) {
+      if (b) {
+        size_ |= kExternMask;
+      } else {
+        size_ &= ~kExternMask;
+      }
+    }
+
+    void setSize(std::size_t sz) {
+      assert(sz <= policyMaxSize());
+      size_ = (kExternMask & size_) | SizeType(sz);
+    }
+
+    void swapSizePolicy(IntegralSizePolicy& o) {
+      std::swap(size_, o.size_);
+    }
+
+  protected:
+    static bool const kShouldUseHeap = ShouldUseHeap;
+
+  private:
+    static SizeType const kExternMask =
+      kShouldUseHeap ? SizeType(1) << (sizeof(SizeType) * 8 - 1)
+                     : 0;
+
+    SizeType size_;
+  };
+
+#ifdef __x86_64__
+  template<class SizeType, bool ShouldUseHeap>
+  struct OneBitMutexImpl {
+    typedef SizeType InternalSizeType;
+
+    OneBitMutexImpl() { psl_.init(); }
+
+    void lock()     const { psl_.lock(); }
+    void unlock()   const { psl_.unlock(); }
+    bool try_lock() const { return psl_.try_lock(); }
+
+  protected:
+    static bool const kShouldUseHeap = ShouldUseHeap;
+
+    static constexpr std::size_t policyMaxSize() {
+      return SizeType(~(SizeType(1) << kLockBit | kExternMask));
+    }
+
+    std::size_t doSize() const {
+      return psl_.getData() & ~kExternMask;
+    }
+
+    std::size_t isExtern() const {
+      return psl_.getData() & kExternMask;
+    }
+
+    void setExtern(bool b) {
+      if (b) {
+        setSize(SizeType(doSize()) | kExternMask);
+      } else {
+        setSize(SizeType(doSize()) & ~kExternMask);
+      }
+    }
+
+    void setSize(std::size_t sz) {
+      assert(sz < (std::size_t(1) << kLockBit));
+      psl_.setData((kExternMask & psl_.getData()) | SizeType(sz));
+    }
+
+    void swapSizePolicy(OneBitMutexImpl& o) {
+      std::swap(psl_, o.psl_);
+    }
+
+  private:
+    static SizeType const kLockBit = sizeof(SizeType) * 8 - 1;
+    static SizeType const kExternMask =
+      kShouldUseHeap ? SizeType(1) << (sizeof(SizeType) * 8 - 2)
+                     : 0;
+
+    PicoSpinLock<SizeType,kLockBit> psl_;
+  };
+#else
+  template<class SizeType, bool ShouldUseHeap>
+  struct OneBitMutexImpl {
+    static_assert(std::is_same<SizeType,void>::value,
+                  "OneBitMutex only works on x86-64");
+  };
+#endif
+
+  /*
+   * If you're just trying to use this class, ignore everything about
+   * this next small_vector_base class thing.
+   *
+   * The purpose of this junk is to minimize sizeof(small_vector<>)
+   * and allow specifying the template parameters in whatever order is
+   * convenient for the user.  There's a few extra steps here to try
+   * to keep the error messages at least semi-reasonable.
+   *
+   * Apologies for all the black magic.
+   */
+  namespace mpl = boost::mpl;
+  template<class Value,
+           std::size_t RequestedMaxInline,
+           class InPolicyA,
+           class InPolicyB,
+           class InPolicyC>
+  struct small_vector_base {
+    typedef mpl::vector<InPolicyA,InPolicyB,InPolicyC> PolicyList;
+
+    /*
+     * Determine the size type
+     */
+    typedef typename mpl::filter_view<
+      PolicyList,
+      boost::is_integral<mpl::placeholders::_1>
+    >::type Integrals;
+    typedef typename mpl::eval_if<
+      mpl::empty<Integrals>,
+      mpl::identity<std::size_t>,
+      mpl::front<Integrals>
+    >::type SizeType;
+
+    static_assert(std::is_unsigned<SizeType>::value,
+                  "Size type should be an unsigned integral type");
+    static_assert(mpl::size<Integrals>::value == 0 ||
+                    mpl::size<Integrals>::value == 1,
+                  "Multiple size types specified in small_vector<>");
+
+    /*
+     * Figure out if we're supposed to supply a one-bit mutex. :)
+     */
+    typedef typename mpl::count<
+      PolicyList,small_vector_policy::OneBitMutex
+    >::type HasMutex;
+
+    static_assert(HasMutex::value == 0 || HasMutex::value == 1,
+                  "Multiple copies of small_vector_policy::OneBitMutex "
+                  "supplied; this is probably a mistake");
+
+    /*
+     * Determine whether we should allow spilling to the heap or not.
+     */
+    typedef typename mpl::count<
+      PolicyList,small_vector_policy::NoHeap
+    >::type HasNoHeap;
+
+    static_assert(HasNoHeap::value == 0 || HasNoHeap::value == 1,
+                  "Multiple copies of small_vector_policy::NoHeap "
+                  "supplied; this is probably a mistake");
+
+    /*
+     * Make the real policy base classes.
+     */
+    typedef typename mpl::if_<
+      HasMutex,
+      OneBitMutexImpl<SizeType,!HasNoHeap::value>,
+      IntegralSizePolicy<SizeType,!HasNoHeap::value>
+    >::type ActualSizePolicy;
+
+    /*
+     * Now inherit from them all.  This is done in such a convoluted
+     * way to make sure we get the empty base optimizaton on all these
+     * types to keep sizeof(small_vector<>) minimal.
+     */
+    typedef boost::totally_ordered1<
+      small_vector<Value,RequestedMaxInline,InPolicyA,InPolicyB,InPolicyC>,
+      ActualSizePolicy
+    > type;
+  };
+
+  template <class T>
+  T* pointerFlagSet(T* p) {
+    return reinterpret_cast<T*>(reinterpret_cast<uintptr_t>(p) | 1);
+  }
+  template <class T>
+  bool pointerFlagGet(T* p) {
+    return reinterpret_cast<uintptr_t>(p) & 1;
+  }
+  template <class T>
+  T* pointerFlagClear(T* p) {
+    return reinterpret_cast<T*>(
+      reinterpret_cast<uintptr_t>(p) & ~uintptr_t(1));
+  }
+  inline void* shiftPointer(void* p, size_t sizeBytes) {
+    return static_cast<char*>(p) + sizeBytes;
+  }
+}
+
+//////////////////////////////////////////////////////////////////////
+
+template<class Value,
+         std::size_t RequestedMaxInline    = 1,
+         class PolicyA                     = void,
+         class PolicyB                     = void,
+         class PolicyC                     = void>
+class small_vector
+  : public detail::small_vector_base<
+      Value,RequestedMaxInline,PolicyA,PolicyB,PolicyC
+    >::type
+{
+  typedef typename detail::small_vector_base<
+    Value,RequestedMaxInline,PolicyA,PolicyB,PolicyC
+  >::type BaseType;
+  typedef typename BaseType::InternalSizeType InternalSizeType;
+
+  /*
+   * Figure out the max number of elements we should inline.  (If
+   * the user asks for less inlined elements than we can fit unioned
+   * into our value_type*, we will inline more than they asked.)
+   */
+  enum {
+    MaxInline = boost::mpl::max<
+                  boost::mpl::int_<sizeof(Value*) / sizeof(Value)>,
+                  boost::mpl::int_<RequestedMaxInline>
+                >::type::value
+  };
+
+public:
+  typedef std::size_t size_type;
+  typedef Value              value_type;
+  typedef value_type&        reference;
+  typedef value_type const&  const_reference;
+  typedef value_type*        iterator;
+  typedef value_type const*  const_iterator;
+  typedef std::ptrdiff_t     difference_type;
+
+  typedef std::reverse_iterator<iterator>       reverse_iterator;
+  typedef std::reverse_iterator<const_iterator> const_reverse_iterator;
+
+  explicit small_vector() {}
+
+  small_vector(small_vector const& o) {
+    assign(o.begin(), o.end());
+  }
+
+  small_vector(small_vector&& o) {
+    *this = std::move(o);
+  }
+
+  small_vector(std::initializer_list<value_type> il) {
+    constructImpl(il.begin(), il.end(), std::false_type());
+  }
+
+  explicit small_vector(size_type n, value_type const& t = value_type()) {
+    doConstruct(n, t);
+  }
+
+  template<class Arg>
+  explicit small_vector(Arg arg1, Arg arg2)  {
+    // Forward using std::is_arithmetic to get to the proper
+    // implementation; this disambiguates between the iterators and
+    // (size_t, value_type) meaning for this constructor.
+    constructImpl(arg1, arg2, std::is_arithmetic<Arg>());
+  }
+
+  ~small_vector() {
+    for (auto& t : *this) {
+      (&t)->~value_type();
+    }
+    if (this->isExtern()) {
+      u.freeHeap();
+    }
+  }
+
+  small_vector& operator=(small_vector const& o) {
+    assign(o.begin(), o.end());
+    return *this;
+  }
+
+  small_vector& operator=(small_vector&& o) {
+    clear();
+    if (!o.isExtern()) {
+      makeSize(o.size());
+      for (std::size_t i = 0; i < o.size(); ++i) {
+        new (data() + i) value_type(std::move(o[i]));
+      }
+      this->setSize(o.size());
+    } else {
+      swap(o);
+    }
+    return *this;
+  }
+
+  bool operator==(small_vector const& o) const {
+    return size() == o.size() && std::equal(begin(), end(), o.begin());
+  }
+
+  bool operator<(small_vector const& o) const {
+    return std::lexicographical_compare(begin(), end(), o.begin(), o.end());
+  }
+
+  static constexpr size_type max_size() {
+    return !BaseType::kShouldUseHeap ? MaxInline
+                                     : BaseType::policyMaxSize();
+  }
+
+  size_type size()         const { return this->doSize(); }
+  bool      empty()        const { return !size(); }
+
+  iterator       begin()         { return data(); }
+  iterator       end()           { return data() + size(); }
+  const_iterator begin()   const { return data(); }
+  const_iterator end()     const { return data() + size(); }
+  const_iterator cbegin()  const { return begin(); }
+  const_iterator cend()    const { return end(); }
+
+  reverse_iterator       rbegin()        { return reverse_iterator(end()); }
+  reverse_iterator       rend()          { return reverse_iterator(begin()); }
+
+  const_reverse_iterator rbegin() const {
+    return const_reverse_iterator(end());
+  }
+
+  const_reverse_iterator rend() const {
+    return const_reverse_iterator(begin());
+  }
+
+  const_reverse_iterator crbegin() const { return rbegin(); }
+  const_reverse_iterator crend()   const { return rend(); }
+
+  /*
+   * Usually one of the simplest functions in a Container-like class
+   * but a bit more complex here.  We have to handle all combinations
+   * of in-place vs. heap between this and o.
+   *
+   * Basic guarantee only.  Provides the nothrow guarantee iff our
+   * value_type has a nothrow move or copy constructor.
+   */
+  void swap(small_vector& o) {
+    using std::swap; // Allow ADL on swap for our value_type.
+
+    if (this->isExtern() && o.isExtern()) {
+      this->swapSizePolicy(o);
+
+      auto thisCapacity = this->capacity();
+      auto oCapacity = o.capacity();
+
+      std::swap(unpackHack(&u.pdata_.heap_), unpackHack(&o.u.pdata_.heap_));
+
+      this->setCapacity(oCapacity);
+      o.setCapacity(thisCapacity);
+
+      return;
+    }
+
+    if (!this->isExtern() && !o.isExtern()) {
+      auto& oldSmall = size() < o.size() ? *this : o;
+      auto& oldLarge = size() < o.size() ? o : *this;
+
+      for (size_type i = 0; i < oldSmall.size(); ++i) {
+        swap(oldSmall[i], oldLarge[i]);
+      }
+
+      size_type i = oldSmall.size();
+      try {
+        for (; i < oldLarge.size(); ++i) {
+          new (&oldSmall[i]) value_type(std::move(oldLarge[i]));
+          oldLarge[i].~value_type();
+        }
+      } catch (...) {
+        for (; i < oldLarge.size(); ++i) {
+          oldLarge[i].~value_type();
+        }
+        oldLarge.setSize(oldSmall.size());
+        throw;
+      }
+      this->swapSizePolicy(o);
+      return;
+    }
+
+    // isExtern != o.isExtern()
+    auto& oldExtern = o.isExtern() ? o : *this;
+    auto& oldIntern = o.isExtern() ? *this : o;
+
+    auto oldExternCapacity = oldExtern.capacity();
+    auto oldExternHeap     = oldExtern.u.pdata_.heap_;
+
+    auto buff = oldExtern.u.buffer();
+    size_type i = 0;
+    try {
+      for (; i < oldIntern.size(); ++i) {
+        new (&buff[i]) value_type(std::move(oldIntern[i]));
+        oldIntern[i].~value_type();
+      }
+    } catch (...) {
+      for (size_type kill = 0; kill < i; ++kill) {
+        buff[kill].~value_type();
+      }
+      for (; i < oldIntern.size(); ++i) {
+        oldIntern[i].~value_type();
+      }
+      oldIntern.setSize(0);
+      oldExtern.u.pdata_.heap_ = oldExternHeap;
+      oldExtern.setCapacity(oldExternCapacity);
+      throw;
+    }
+    oldIntern.u.pdata_.heap_ = oldExternHeap;
+    this->swapSizePolicy(o);
+    oldIntern.setCapacity(oldExternCapacity);
+  }
+
+  void resize(size_type sz) {
+    if (sz < size()) {
+      erase(begin() + sz, end());
+      return;
+    }
+    makeSize(sz);
+    detail::populateMemForward(begin() + size(), sz - size(),
+      [&] (void* p) { new (p) value_type(); }
+    );
+    this->setSize(sz);
+  }
+
+  void resize(size_type sz, value_type const& v) {
+    if (sz < size()) {
+      erase(begin() + sz, end());
+      return;
+    }
+    makeSize(sz);
+    detail::populateMemForward(begin() + size(), sz - size(),
+      [&] (void* p) { new (p) value_type(v); }
+    );
+    this->setSize(sz);
+  }
+
+  value_type* data() noexcept {
+    return this->isExtern() ? u.heap() : u.buffer();
+  }
+
+  value_type const* data() const noexcept {
+    return this->isExtern() ? u.heap() : u.buffer();
+  }
+
+  template<class ...Args>
+  iterator emplace(const_iterator p, Args&&... args) {
+    if (p == cend()) {
+      emplace_back(std::forward<Args>(args)...);
+      return end() - 1;
+    }
+
+    /*
+     * We implement emplace at places other than at the back with a
+     * temporary for exception safety reasons.  It is possible to
+     * avoid having to do this, but it becomes hard to maintain the
+     * basic exception safety guarantee (unless you respond to a copy
+     * constructor throwing by clearing the whole vector).
+     *
+     * The reason for this is that otherwise you have to destruct an
+     * element before constructing this one in its place---if the
+     * constructor throws, you either need a nothrow default
+     * constructor or a nothrow copy/move to get something back in the
+     * "gap", and the vector requirements don't guarantee we have any
+     * of these.  Clearing the whole vector is a legal response in
+     * this situation, but it seems like this implementation is easy
+     * enough and probably better.
+     */
+    return insert(p, value_type(std::forward<Args>(args)...));
+  }
+
+  void reserve(size_type sz) {
+    makeSize(sz);
+  }
+
+  size_type capacity() const {
+    if (this->isExtern()) {
+      if (u.hasCapacity()) {
+        return *u.getCapacity();
+      }
+      return malloc_usable_size(u.pdata_.heap_) / sizeof(value_type);
+    }
+    return MaxInline;
+  }
+
+  void shrink_to_fit() {
+    if (!this->isExtern()) {
+      return;
+    }
+
+    small_vector tmp(begin(), end());
+    tmp.swap(*this);
+  }
+
+  template<class ...Args>
+  void emplace_back(Args&&... args) {
+    // call helper function for static dispatch of special cases
+    emplaceBack(std::forward<Args>(args)...);
+  }
+
+  void push_back(value_type&& t) {
+    if (capacity() == size()) {
+      makeSize(std::max(size_type(2), 3 * size() / 2), &t, size());
+    } else {
+      new (end()) value_type(std::move(t));
+    }
+    this->setSize(size() + 1);
+  }
+
+  void push_back(value_type const& t) {
+    // Make a copy and forward to the rvalue value_type&& overload
+    // above.
+    push_back(value_type(t));
+  }
+
+  void pop_back() {
+    erase(end() - 1);
+  }
+
+  iterator insert(const_iterator constp, value_type&& t) {
+    iterator p = unconst(constp);
+
+    if (p == end()) {
+      push_back(std::move(t));
+      return end() - 1;
+    }
+
+    auto offset = p - begin();
+
+    if (capacity() == size()) {
+      makeSize(size() + 1, &t, offset);
+      this->setSize(this->size() + 1);
+    } else {
+      makeSize(size() + 1);
+      detail::moveObjectsRight(data() + offset,
+                               data() + size(),
+                               data() + size() + 1);
+      this->setSize(size() + 1);
+      data()[offset] = std::move(t);
+    }
+    return begin() + offset;
+
+  }
+
+  iterator insert(const_iterator p, value_type const& t) {
+    // Make a copy and forward to the rvalue value_type&& overload
+    // above.
+    return insert(p, value_type(t));
+  }
+
+  iterator insert(const_iterator pos, size_type n, value_type const& val) {
+    auto offset = pos - begin();
+    makeSize(size() + n);
+    detail::moveObjectsRight(data() + offset,
+                             data() + size(),
+                             data() + size() + n);
+    this->setSize(size() + n);
+    std::generate_n(begin() + offset, n, [&] { return val; });
+    return begin() + offset;
+  }
+
+  template<class Arg>
+  iterator insert(const_iterator p, Arg arg1, Arg arg2) {
+    // Forward using std::is_arithmetic to get to the proper
+    // implementation; this disambiguates between the iterators and
+    // (size_t, value_type) meaning for this function.
+    return insertImpl(unconst(p), arg1, arg2, std::is_arithmetic<Arg>());
+  }
+
+  iterator insert(const_iterator p, std::initializer_list<value_type> il) {
+    return insert(p, il.begin(), il.end());
+  }
+
+  iterator erase(const_iterator q) {
+    std::move(unconst(q) + 1, end(), unconst(q));
+    (data() + size() - 1)->~value_type();
+    this->setSize(size() - 1);
+    return unconst(q);
+  }
+
+  iterator erase(const_iterator q1, const_iterator q2) {
+    std::move(unconst(q2), end(), unconst(q1));
+    for (auto it = q1; it != end(); ++it) {
+      it->~value_type();
+    }
+    this->setSize(size() - (q2 - q1));
+    return unconst(q1);
+  }
+
+  void clear() {
+    erase(begin(), end());
+  }
+
+  template<class Arg>
+  void assign(Arg first, Arg last) {
+    clear();
+    insert(end(), first, last);
+  }
+
+  void assign(std::initializer_list<value_type> il) {
+    assign(il.begin(), il.end());
+  }
+
+  void assign(size_type n, const value_type& t) {
+    clear();
+    insert(end(), n, t);
+  }
+
+  reference front()             { assert(!empty()); return *begin(); }
+  reference back()              { assert(!empty()); return *(end() - 1); }
+  const_reference front() const { assert(!empty()); return *begin(); }
+  const_reference back() const  { assert(!empty()); return *(end() - 1); }
+
+  reference operator[](size_type i) {
+    assert(i < size());
+    return *(begin() + i);
+  }
+
+  const_reference operator[](size_type i) const {
+    assert(i < size());
+    return *(begin() + i);
+  }
+
+  reference at(size_type i) {
+    if (i >= size()) {
+      throw std::out_of_range("index out of range");
+    }
+    return (*this)[i];
+  }
+
+  const_reference at(size_type i) const {
+    if (i >= size()) {
+      throw std::out_of_range("index out of range");
+    }
+    return (*this)[i];
+  }
+
+private:
+
+  /*
+   * This is doing the same like emplace_back, but we need this helper
+   * to catch the special case - see the next overload function..
+   */
+  template<class ...Args>
+  void emplaceBack(Args&&... args) {
+    makeSize(size() + 1);
+    new (end()) value_type(std::forward<Args>(args)...);
+    this->setSize(size() + 1);
+  }
+
+  /*
+   * Special case of emplaceBack for rvalue
+   */
+  void emplaceBack(value_type&& t) {
+    push_back(std::move(t));
+  }
+
+  static iterator unconst(const_iterator it) {
+    return const_cast<iterator>(it);
+  }
+
+  /*
+   * g++ doesn't allow you to bind a non-const reference to a member
+   * of a packed structure, presumably because it would make it too
+   * easy to accidentally make an unaligned memory access?
+   */
+  template<class T> static T& unpackHack(T* p) {
+    return *p;
+  }
+
+  // The std::false_type argument is part of disambiguating the
+  // iterator insert functions from integral types (see insert().)
+  template<class It>
+  iterator insertImpl(iterator pos, It first, It last, std::false_type) {
+    typedef typename std::iterator_traits<It>::iterator_category categ;
+    if (std::is_same<categ,std::input_iterator_tag>::value) {
+      auto offset = pos - begin();
+      while (first != last) {
+        pos = insert(pos, *first++);
+        ++pos;
+      }
+      return begin() + offset;
+    }
+
+    auto distance = std::distance(first, last);
+    auto offset = pos - begin();
+    makeSize(size() + distance);
+    detail::moveObjectsRight(data() + offset,
+                             data() + size(),
+                             data() + size() + distance);
+    this->setSize(size() + distance);
+    std::copy_n(first, distance, begin() + offset);
+    return begin() + offset;
+  }
+
+  iterator insertImpl(iterator pos, size_type n, const value_type& val,
+      std::true_type) {
+    // The true_type means this should call the size_t,value_type
+    // overload.  (See insert().)
+    return insert(pos, n, val);
+  }
+
+  // The std::false_type argument came from std::is_arithmetic as part
+  // of disambiguating an overload (see the comment in the
+  // constructor).
+  template<class It>
+  void constructImpl(It first, It last, std::false_type) {
+    typedef typename std::iterator_traits<It>::iterator_category categ;
+    if (std::is_same<categ,std::input_iterator_tag>::value) {
+      // With iterators that only allow a single pass, we can't really
+      // do anything sane here.
+      while (first != last) {
+        push_back(*first++);
+      }
+      return;
+    }
+
+    auto distance = std::distance(first, last);
+    makeSize(distance);
+    this->setSize(distance);
+
+    detail::populateMemForward(data(), distance,
+      [&] (void* p) { new (p) value_type(*first++); }
+    );
+  }
+
+  void doConstruct(size_type n, value_type const& val) {
+    makeSize(n);
+    this->setSize(n);
+    detail::populateMemForward(data(), n,
+      [&] (void* p) { new (p) value_type(val); }
+    );
+  }
+
+  // The true_type means we should forward to the size_t,value_type
+  // overload.
+  void constructImpl(size_type n, value_type const& val, std::true_type) {
+    doConstruct(n, val);
+  }
+
+  void makeSize(size_type size, value_type* v = NULL) {
+    makeSize(size, v, size - 1);
+  }
+
+  /*
+   * Ensure we have a large enough memory region to be size `size'.
+   * Will move/copy elements if we are spilling to heap_ or needed to
+   * allocate a new region, but if resized in place doesn't initialize
+   * anything in the new region.  In any case doesn't change size().
+   * Supports insertion of new element during reallocation by given
+   * pointer to new element and position of new element.
+   * NOTE: If reallocation is not needed, and new element should be
+   * inserted in the middle of vector (not at the end), do the move
+   * objects and insertion outside the function, otherwise exception is thrown.
+   */
+  void makeSize(size_type size, value_type* v, size_type pos) {
+    if (size > this->max_size()) {
+      throw std::length_error("max_size exceeded in small_vector");
+    }
+    if (size <= this->capacity()) {
+      return;
+    }
+
+    auto needBytes = size * sizeof(value_type);
+    // If the capacity isn't explicitly stored inline, but the heap
+    // allocation is grown to over some threshold, we should store
+    // a capacity at the front of the heap allocation.
+    bool heapifyCapacity =
+      !kHasInlineCapacity && needBytes > kHeapifyCapacityThreshold;
+    if (heapifyCapacity) {
+      needBytes += kHeapifyCapacitySize;
+    }
+    auto const sizeBytes = goodMallocSize(needBytes);
+    void* newh = checkedMalloc(sizeBytes);
+    // We expect newh to be at least 2-aligned, because we want to
+    // use its least significant bit as a flag.
+    assert(!detail::pointerFlagGet(newh));
+
+    value_type* newp = static_cast<value_type*>(
+      heapifyCapacity ?
+        detail::shiftPointer(newh, kHeapifyCapacitySize) :
+        newh);
+
+    if (v != NULL) {
+      // move new element
+      try {
+        new (&newp[pos]) value_type(std::move(*v));
+      } catch (...) {
+        free(newh);
+        throw;
+      }
+
+      // move old elements to the left of the new one
+      try {
+        detail::moveToUninitialized(begin(), begin() + pos, newp);
+      } catch (...) {
+        newp[pos].~value_type();
+        free(newh);
+        throw;
+      }
+
+      // move old elements to the right of the new one
+      try {
+        if (pos < size-1) {
+          detail::moveToUninitialized(begin() + pos, end(), newp + pos + 1);
+        }
+      } catch (...) {
+        for (size_type i = 0; i <= pos; ++i) {
+          newp[i].~value_type();
+        }
+        free(newh);
+        throw;
+      }
+    } else {
+      // move without inserting new element
+      try {
+        detail::moveToUninitialized(begin(), end(), newp);
+      } catch (...) {
+        free(newh);
+        throw;
+      }
+    }
+    for (auto& val : *this) {
+      val.~value_type();
+    }
+
+    if (this->isExtern()) {
+      u.freeHeap();
+    }
+    auto availableSizeBytes = sizeBytes;
+    if (heapifyCapacity) {
+      u.pdata_.heap_ = detail::pointerFlagSet(newh);
+      availableSizeBytes -= kHeapifyCapacitySize;
+    } else {
+      u.pdata_.heap_ = newh;
+    }
+    this->setExtern(true);
+    this->setCapacity(availableSizeBytes / sizeof(value_type));
+  }
+
+  /*
+   * This will set the capacity field, stored inline in the storage_ field
+   * if there is sufficient room to store it.
+   */
+  void setCapacity(size_type newCapacity) {
+    assert(this->isExtern());
+    if (u.hasCapacity()) {
+      assert(newCapacity < std::numeric_limits<InternalSizeType>::max());
+      *u.getCapacity() = InternalSizeType(newCapacity);
+    }
+  }
+
+private:
+  struct HeapPtrWithCapacity {
+    void* heap_;
+    InternalSizeType capacity_;
+
+    InternalSizeType* getCapacity() {
+      return &capacity_;
+    }
+  } FB_PACKED;
+
+  struct HeapPtr {
+    // Lower order bit of heap_ is used as flag to indicate whether capacity is
+    // stored at the front of the heap allocation.
+    void* heap_;
+
+    InternalSizeType* getCapacity() {
+      assert(detail::pointerFlagGet(heap_));
+      return static_cast<InternalSizeType*>(
+        detail::pointerFlagClear(heap_));
+    }
+  } FB_PACKED;
+
+#if defined(__x86_64_)
+  typedef unsigned char InlineStorageType[sizeof(value_type) * MaxInline];
+#else
+  typedef typename std::aligned_storage<
+    sizeof(value_type) * MaxInline,
+    alignof(value_type)
+  >::type InlineStorageType;
+#endif
+
+  static bool const kHasInlineCapacity =
+    sizeof(HeapPtrWithCapacity) < sizeof(InlineStorageType);
+
+  // This value should we multiple of word size.
+  static size_t const kHeapifyCapacitySize = sizeof(
+    typename std::aligned_storage<
+      sizeof(InternalSizeType),
+      alignof(value_type)
+    >::type);
+  // Threshold to control capacity heapifying.
+  static size_t const kHeapifyCapacityThreshold =
+    100 * kHeapifyCapacitySize;
+
+  typedef typename std::conditional<
+    kHasInlineCapacity,
+    HeapPtrWithCapacity,
+    HeapPtr
+  >::type PointerType;
+
+  union Data {
+    explicit Data() { pdata_.heap_ = 0; }
+
+    PointerType pdata_;
+    InlineStorageType storage_;
+
+    value_type* buffer() noexcept {
+      void* vp = &storage_;
+      return static_cast<value_type*>(vp);
+    }
+    value_type const* buffer() const noexcept {
+      return const_cast<Data*>(this)->buffer();
+    }
+    value_type* heap() noexcept {
+      if (kHasInlineCapacity || !detail::pointerFlagGet(pdata_.heap_)) {
+        return static_cast<value_type*>(pdata_.heap_);
+      }
+      return static_cast<value_type*>(
+        detail::shiftPointer(
+          detail::pointerFlagClear(pdata_.heap_), kHeapifyCapacitySize));
+    }
+    value_type const* heap() const noexcept {
+      return const_cast<Data*>(this)->heap();
+    }
+
+    bool hasCapacity() const {
+      return kHasInlineCapacity || detail::pointerFlagGet(pdata_.heap_);
+    }
+    InternalSizeType* getCapacity() {
+      return pdata_.getCapacity();
+    }
+    InternalSizeType* getCapacity() const {
+      return const_cast<Data*>(this)->getCapacity();
+    }
+
+    void freeHeap() {
+      auto vp = detail::pointerFlagClear(pdata_.heap_);
+      free(vp);
+    }
+  } FB_PACKED u;
+} FB_PACKED;
+
+//////////////////////////////////////////////////////////////////////
+
+// Basic guarantee only, or provides the nothrow guarantee iff T has a
+// nothrow move or copy constructor.
+template<class T, std::size_t MaxInline, class A, class B, class C>
+void swap(small_vector<T,MaxInline,A,B,C>& a,
+          small_vector<T,MaxInline,A,B,C>& b) {
+  a.swap(b);
+}
+
+//////////////////////////////////////////////////////////////////////
+
+}
+
+#pragma GCC diagnostic pop
+
+#ifdef FB_PACKED
+# undef FB_PACKED
+#endif
+
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/sorted_vector_types.h
@@ -0,0 +1,641 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+ * This header defines two classes that very nearly model
+ * AssociativeContainer (but not quite).  These implement set-like and
+ * map-like behavior on top of a sorted vector, instead of using
+ * rb-trees like std::set and std::map.
+ *
+ * This is potentially useful in cases where the number of elements in
+ * the set or map is small, or when you want to avoid using more
+ * memory than necessary and insertions/deletions are much more rare
+ * than lookups (these classes have O(N) insertions/deletions).
+ *
+ * In the interest of using these in conditions where the goal is to
+ * minimize memory usage, they support a GrowthPolicy parameter, which
+ * is a class defining a single function called increase_capacity,
+ * which will be called whenever we are about to insert something: you
+ * can then decide to call reserve() based on the current capacity()
+ * and size() of the passed in vector-esque Container type.  An
+ * example growth policy that grows one element at a time:
+ *
+ *    struct OneAtATimePolicy {
+ *      template<class Container>
+ *      void increase_capacity(Container& c) {
+ *        if (c.size() == c.capacity()) {
+ *          c.reserve(c.size() + 1);
+ *        }
+ *      }
+ *    };
+ *
+ *    typedef sorted_vector_set<int,
+ *                              std::less<int>,
+ *                              std::allocator<int>,
+ *                              OneAtATimePolicy>
+ *            OneAtATimeIntSet;
+ *
+ * Important differences from std::set and std::map:
+ *   - insert() and erase() invalidate iterators and references
+ *   - insert() and erase() are O(N)
+ *   - our iterators model RandomAccessIterator
+ *   - sorted_vector_map::value_type is pair<K,V>, not pair<const K,V>.
+ *     (This is basically because we want to store the value_type in
+ *     std::vector<>, which requires it to be Assignable.)
+ */
+
+#ifndef FOLLY_SORTED_VECTOR_TYPES_H_
+#define FOLLY_SORTED_VECTOR_TYPES_H_
+
+#include <algorithm>
+#include <initializer_list>
+#include <iterator>
+#include <utility>
+#include <vector>
+#include <boost/operators.hpp>
+#include <boost/bind.hpp>
+#include <boost/type_traits/is_same.hpp>
+
+namespace folly {
+
+//////////////////////////////////////////////////////////////////////
+
+namespace detail {
+
+  // This wrapper goes around a GrowthPolicy and provides iterator
+  // preservation semantics, but only if the growth policy is not the
+  // default (i.e. nothing).
+  template<class Policy>
+  struct growth_policy_wrapper : private Policy {
+    template<class Container, class Iterator>
+    Iterator increase_capacity(Container& c, Iterator desired_insertion)
+    {
+      typedef typename Container::difference_type diff_t;
+      diff_t d = desired_insertion - c.begin();
+      Policy::increase_capacity(c);
+      return c.begin() + d;
+    }
+  };
+  template<>
+  struct growth_policy_wrapper<void> {
+    template<class Container, class Iterator>
+    Iterator increase_capacity(Container&, Iterator it) {
+      return it;
+    }
+  };
+
+  /*
+   * This helper returns the distance between two iterators if it is
+   * possible to figure it out without messing up the range
+   * (i.e. unless they are InputIterators).  Otherwise this returns
+   * -1.
+   */
+  template<class Iterator>
+  int distance_if_multipass(Iterator first, Iterator last) {
+    typedef typename std::iterator_traits<Iterator>::iterator_category categ;
+    if (boost::is_same<categ,std::input_iterator_tag>::value)
+      return -1;
+    return std::distance(first, last);
+  }
+
+  template<class OurContainer, class Vector, class GrowthPolicy>
+  typename OurContainer::iterator
+  insert_with_hint(OurContainer& sorted,
+                   Vector& cont,
+                   typename OurContainer::iterator hint,
+                   typename OurContainer::value_type&& value,
+                   GrowthPolicy& po)
+  {
+    const typename OurContainer::value_compare& cmp(sorted.value_comp());
+    if (hint == cont.end() || cmp(value, *hint)) {
+      if (hint == cont.begin()) {
+        po.increase_capacity(cont, cont.begin());
+        return cont.insert(cont.begin(), std::move(value));
+      }
+      if (cmp(*(hint - 1), value)) {
+        hint = po.increase_capacity(cont, hint);
+        return cont.insert(hint, std::move(value));
+      }
+      return sorted.insert(std::move(value)).first;
+    }
+
+    if (cmp(*hint, value)) {
+      if (hint + 1 == cont.end() || cmp(value, *(hint + 1))) {
+        typename OurContainer::iterator it =
+          po.increase_capacity(cont, hint + 1);
+        return cont.insert(it, std::move(value));
+      }
+    }
+
+    // Value and *hint did not compare, so they are equal keys.
+    return hint;
+  }
+
+}
+
+//////////////////////////////////////////////////////////////////////
+
+/**
+ * A sorted_vector_set is a container similar to std::set<>, but
+ * implemented as as a sorted array with std::vector<>.
+ *
+ * @param class T               Data type to store
+ * @param class Compare         Comparison function that imposes a
+ *                              strict weak ordering over instances of T
+ * @param class Allocator       allocation policy
+ * @param class GrowthPolicy    policy object to control growth
+ *
+ * @author Aditya Agarwal <aditya@fb.com>
+ * @author Akhil Wable    <akhil@fb.com>
+ * @author Jordan DeLong  <delong.j@fb.com>
+ */
+template<class T,
+         class Compare      = std::less<T>,
+         class Allocator    = std::allocator<T>,
+         class GrowthPolicy = void>
+class sorted_vector_set
+  : boost::totally_ordered1<
+      sorted_vector_set<T,Compare,Allocator,GrowthPolicy>
+    , detail::growth_policy_wrapper<GrowthPolicy> >
+{
+  typedef std::vector<T,Allocator> ContainerT;
+
+  detail::growth_policy_wrapper<GrowthPolicy>&
+  get_growth_policy() { return *this; }
+
+public:
+  typedef T       value_type;
+  typedef T       key_type;
+  typedef Compare key_compare;
+  typedef Compare value_compare;
+
+  typedef typename ContainerT::pointer                pointer;
+  typedef typename ContainerT::reference              reference;
+  typedef typename ContainerT::const_reference        const_reference;
+  /*
+   * XXX: Our normal iterator ought to also be a constant iterator
+   * (cf. Defect Report 103 for std::set), but this is a bit more of a
+   * pain.
+   */
+  typedef typename ContainerT::iterator               iterator;
+  typedef typename ContainerT::const_iterator         const_iterator;
+  typedef typename ContainerT::difference_type        difference_type;
+  typedef typename ContainerT::size_type              size_type;
+  typedef typename ContainerT::reverse_iterator       reverse_iterator;
+  typedef typename ContainerT::const_reverse_iterator const_reverse_iterator;
+
+  explicit sorted_vector_set(const Compare& comp = Compare(),
+                             const Allocator& alloc = Allocator())
+    : m_(comp, alloc)
+  {}
+
+  template<class InputIterator>
+  explicit sorted_vector_set(
+      InputIterator first,
+      InputIterator last,
+      const Compare& comp = Compare(),
+      const Allocator& alloc = Allocator())
+    : m_(comp, alloc)
+  {
+    // This is linear if [first, last) is already sorted (and if we
+    // can figure out the distance between the two iterators).
+    insert(first, last);
+  }
+
+  explicit sorted_vector_set(
+      std::initializer_list<value_type> list,
+      const Compare& comp = Compare(),
+      const Allocator& alloc = Allocator())
+    : m_(comp, alloc)
+  {
+    insert(list.begin(), list.end());
+  }
+
+  key_compare key_comp() const { return m_; }
+  value_compare value_comp() const { return m_; }
+
+  iterator begin()                      { return m_.cont_.begin();  }
+  iterator end()                        { return m_.cont_.end();    }
+  const_iterator begin() const          { return m_.cont_.begin();  }
+  const_iterator end() const            { return m_.cont_.end();    }
+  reverse_iterator rbegin()             { return m_.cont_.rbegin(); }
+  reverse_iterator rend()               { return m_.cont_.rend();   }
+  const_reverse_iterator rbegin() const { return m_.cont_.rbegin(); }
+  const_reverse_iterator rend() const   { return m_.cont_.rend();   }
+
+  void clear()                  { return m_.cont_.clear();    }
+  size_type size() const        { return m_.cont_.size();     }
+  size_type max_size() const    { return m_.cont_.max_size(); }
+  bool empty() const            { return m_.cont_.empty();    }
+  void reserve(size_type s)     { return m_.cont_.reserve(s); }
+  size_type capacity() const    { return m_.cont_.capacity(); }
+
+  std::pair<iterator,bool> insert(const value_type& value) {
+    return insert(value_type(value));
+  }
+
+  std::pair<iterator,bool> insert(value_type&& value) {
+    iterator it = lower_bound(value);
+    if (it == end() || value_comp()(value, *it)) {
+      it = get_growth_policy().increase_capacity(m_.cont_, it);
+      return std::make_pair(m_.cont_.insert(it, std::move(value)), true);
+    }
+    return std::make_pair(it, false);
+  }
+
+  iterator insert(iterator hint, const value_type& value) {
+    return insert(hint, value_type(value));
+  }
+
+  iterator insert(iterator hint, value_type&& value) {
+    return detail::insert_with_hint(*this, m_.cont_, hint, std::move(value),
+      get_growth_policy());
+  }
+
+  template<class InputIterator>
+  void insert(InputIterator first, InputIterator last) {
+    int d = detail::distance_if_multipass(first, last);
+    if (d != -1) {
+      m_.cont_.reserve(m_.cont_.size() + d);
+    }
+    for (; first != last; ++first) {
+      insert(end(), *first);
+    }
+  }
+
+  size_type erase(const key_type& key) {
+    iterator it = lower_bound(key);
+    if (it == end()) {
+      return 0;
+    }
+    m_.cont_.erase(it);
+    return 1;
+  }
+
+  void erase(iterator it) {
+    m_.cont_.erase(it);
+  }
+
+  void erase(iterator first, iterator last) {
+    m_.cont_.erase(first, last);
+  }
+
+  iterator find(const key_type& key) {
+    iterator it = lower_bound(key);
+    if (it == end() || !key_comp()(key, *it))
+      return it;
+    return end();
+  }
+
+  const_iterator find(const key_type& key) const {
+    const_iterator it = lower_bound(key);
+    if (it == end() || !key_comp()(key, *it))
+      return it;
+    return end();
+  }
+
+  size_type count(const key_type& key) const {
+    return find(key) == end() ? 0 : 1;
+  }
+
+  iterator lower_bound(const key_type& key) {
+    return std::lower_bound(begin(), end(), key, key_comp());
+  }
+
+  const_iterator lower_bound(const key_type& key) const {
+    return std::lower_bound(begin(), end(), key, key_comp());
+  }
+
+  iterator upper_bound(const key_type& key) {
+    return std::upper_bound(begin(), end(), key, key_comp());
+  }
+
+  const_iterator upper_bound(const key_type& key) const {
+    return std::upper_bound(begin(), end(), key, key_comp());
+  }
+
+  std::pair<iterator,iterator> equal_range(const key_type& key) {
+    return std::equal_range(begin(), end(), key, key_comp());
+  }
+
+  std::pair<const_iterator,const_iterator>
+  equal_range(const key_type& key) const {
+    return std::equal_range(begin(), end(), key, key_comp());
+  }
+
+  // Nothrow as long as swap() on the Compare type is nothrow.
+  void swap(sorted_vector_set& o) {
+    using std::swap;  // Allow ADL for swap(); fall back to std::swap().
+    Compare& a = m_;
+    Compare& b = o.m_;
+    swap(a, b);
+    m_.cont_.swap(o.m_.cont_);
+  }
+
+  bool operator==(const sorted_vector_set& other) const {
+    return other.m_.cont_ == m_.cont_;
+  }
+
+  bool operator<(const sorted_vector_set& other) const {
+    return m_.cont_ < other.m_.cont_;
+  }
+
+private:
+  /*
+   * This structure derives from the comparison object in order to
+   * make use of the empty base class optimization if our comparison
+   * functor is an empty class (usual case).
+   *
+   * Wrapping up this member like this is better than deriving from
+   * the Compare object ourselves (there are some perverse edge cases
+   * involving virtual functions).
+   *
+   * More info:  http://www.cantrip.org/emptyopt.html
+   */
+  struct EBO : Compare {
+    explicit EBO(const Compare& c, const Allocator& alloc)
+      : Compare(c)
+      , cont_(alloc)
+    {}
+    ContainerT cont_;
+  } m_;
+};
+
+// Swap function that can be found using ADL.
+template<class T, class C, class A, class G>
+inline void swap(sorted_vector_set<T,C,A,G>& a,
+                 sorted_vector_set<T,C,A,G>& b) {
+  return a.swap(b);
+}
+
+//////////////////////////////////////////////////////////////////////
+
+/**
+ * A sorted_vector_map is similar to a sorted_vector_set but stores
+ * <key,value> pairs instead of single elements.
+ *
+ * @param class Key           Key type
+ * @param class Value         Value type
+ * @param class Compare       Function that can compare key types and impose
+ *                            a strict weak ordering over them.
+ * @param class Allocator     allocation policy
+ * @param class GrowthPolicy  policy object to control growth
+ *
+ * @author Aditya Agarwal <aditya@fb.com>
+ * @author Akhil Wable    <akhil@fb.com>
+ * @author Jordan DeLong  <delong.j@fb.com>
+ */
+template<class Key,
+         class Value,
+         class Compare        = std::less<Key>,
+         class Allocator      = std::allocator<std::pair<Key,Value> >,
+         class GrowthPolicy   = void>
+class sorted_vector_map
+  : boost::totally_ordered1<
+      sorted_vector_map<Key,Value,Compare,Allocator,GrowthPolicy>
+    , detail::growth_policy_wrapper<GrowthPolicy> >
+{
+  typedef std::vector<std::pair<Key,Value>,Allocator> ContainerT;
+
+  detail::growth_policy_wrapper<GrowthPolicy>&
+  get_growth_policy() { return *this; }
+
+public:
+  typedef Key                                       key_type;
+  typedef Value                                     mapped_type;
+  typedef std::pair<key_type,mapped_type>           value_type;
+  typedef Compare                                   key_compare;
+
+  struct value_compare
+    : std::binary_function<value_type,value_type,bool>
+    , private Compare
+  {
+    bool operator()(const value_type& a, const value_type& b) const {
+      return Compare::operator()(a.first, b.first);
+    }
+
+  protected:
+    friend class sorted_vector_map;
+    explicit value_compare(const Compare& c) : Compare(c) {}
+  };
+
+  typedef typename ContainerT::pointer                pointer;
+  typedef typename ContainerT::reference              reference;
+  typedef typename ContainerT::const_reference        const_reference;
+  typedef typename ContainerT::iterator               iterator;
+  typedef typename ContainerT::const_iterator         const_iterator;
+  typedef typename ContainerT::difference_type        difference_type;
+  typedef typename ContainerT::size_type              size_type;
+  typedef typename ContainerT::reverse_iterator       reverse_iterator;
+  typedef typename ContainerT::const_reverse_iterator const_reverse_iterator;
+
+  explicit sorted_vector_map(const Compare& comp = Compare(),
+                             const Allocator& alloc = Allocator())
+    : m_(value_compare(comp), alloc)
+  {}
+
+  template<class InputIterator>
+  explicit sorted_vector_map(
+      InputIterator first,
+      InputIterator last,
+      const Compare& comp = Compare(),
+      const Allocator& alloc = Allocator())
+    : m_(value_compare(comp), alloc)
+  {
+    insert(first, last);
+  }
+
+  explicit sorted_vector_map(
+      std::initializer_list<value_type> list,
+      const Compare& comp = Compare(),
+      const Allocator& alloc = Allocator())
+    : m_(value_compare(comp), alloc)
+  {
+    insert(list.begin(), list.end());
+  }
+
+  key_compare key_comp() const { return m_; }
+  value_compare value_comp() const { return m_; }
+
+  iterator begin()                      { return m_.cont_.begin();  }
+  iterator end()                        { return m_.cont_.end();    }
+  const_iterator begin() const          { return m_.cont_.begin();  }
+  const_iterator end() const            { return m_.cont_.end();    }
+  reverse_iterator rbegin()             { return m_.cont_.rbegin(); }
+  reverse_iterator rend()               { return m_.cont_.rend();   }
+  const_reverse_iterator rbegin() const { return m_.cont_.rbegin(); }
+  const_reverse_iterator rend() const   { return m_.cont_.rend();   }
+
+  void clear()                  { return m_.cont_.clear();    }
+  size_type size() const        { return m_.cont_.size();     }
+  size_type max_size() const    { return m_.cont_.max_size(); }
+  bool empty() const            { return m_.cont_.empty();    }
+  void reserve(size_type s)     { return m_.cont_.reserve(s); }
+  size_type capacity() const    { return m_.cont_.capacity(); }
+
+  std::pair<iterator,bool> insert(const value_type& value) {
+    return insert(value_type(value));
+  }
+
+  std::pair<iterator,bool> insert(value_type&& value) {
+    iterator it = lower_bound(value.first);
+    if (it == end() || value_comp()(value, *it)) {
+      it = get_growth_policy().increase_capacity(m_.cont_, it);
+      return std::make_pair(m_.cont_.insert(it, std::move(value)), true);
+    }
+    return std::make_pair(it, false);
+  }
+
+  iterator insert(iterator hint, const value_type& value) {
+    return insert(hint, value_type(value));
+  }
+
+  iterator insert(iterator hint, value_type&& value) {
+    return detail::insert_with_hint(*this, m_.cont_, hint, std::move(value),
+      get_growth_policy());
+  }
+
+  template<class InputIterator>
+  void insert(InputIterator first, InputIterator last) {
+    int d = detail::distance_if_multipass(first, last);
+    if (d != -1) {
+      m_.cont_.reserve(m_.cont_.size() + d);
+    }
+    for (; first != last; ++first) {
+      insert(end(), *first);
+    }
+  }
+
+  size_type erase(const key_type& key) {
+    iterator it = find(key);
+    if (it == end()) {
+      return 0;
+    }
+    m_.cont_.erase(it);
+    return 1;
+  }
+
+  void erase(iterator it) {
+    m_.cont_.erase(it);
+  }
+
+  void erase(iterator first, iterator last) {
+    m_.cont_.erase(first, last);
+  }
+
+  iterator find(const key_type& key) {
+    iterator it = lower_bound(key);
+    if (it == end() || !key_comp()(key, it->first))
+      return it;
+    return end();
+  }
+
+  const_iterator find(const key_type& key) const {
+    const_iterator it = lower_bound(key);
+    if (it == end() || !key_comp()(key, it->first))
+      return it;
+    return end();
+  }
+
+  size_type count(const key_type& key) const {
+    return find(key) == end() ? 0 : 1;
+  }
+
+  iterator lower_bound(const key_type& key) {
+    return std::lower_bound(begin(), end(), key,
+      boost::bind(key_comp(), boost::bind(&value_type::first, _1), _2));
+  }
+
+  const_iterator lower_bound(const key_type& key) const {
+    return std::lower_bound(begin(), end(), key,
+      boost::bind(key_comp(), boost::bind(&value_type::first, _1), _2));
+  }
+
+  iterator upper_bound(const key_type& key) {
+    return std::upper_bound(begin(), end(), key,
+      boost::bind(key_comp(), _1, boost::bind(&value_type::first, _2)));
+  }
+
+  const_iterator upper_bound(const key_type& key) const {
+    return std::upper_bound(begin(), end(), key,
+      boost::bind(key_comp(), _1, boost::bind(&value_type::first, _2)));
+  }
+
+  std::pair<iterator,iterator> equal_range(const key_type& key) {
+    // Note: std::equal_range can't be passed a functor that takes
+    // argument types different from the iterator value_type, so we
+    // have to do this.
+    iterator low = lower_bound(key);
+    iterator high = std::upper_bound(low, end(), key,
+      boost::bind(key_comp(), _1, boost::bind(&value_type::first, _2)));
+    return std::make_pair(low, high);
+  }
+
+  std::pair<const_iterator,const_iterator>
+  equal_range(const key_type& key) const {
+    return const_cast<sorted_vector_map*>(this)->equal_range(key);
+  }
+
+  // Nothrow as long as swap() on the Compare type is nothrow.
+  void swap(sorted_vector_map& o) {
+    using std::swap; // Allow ADL for swap(); fall back to std::swap().
+    Compare& a = m_;
+    Compare& b = o.m_;
+    swap(a, b);
+    m_.cont_.swap(o.m_.cont_);
+  }
+
+  mapped_type& operator[](const key_type& key) {
+    iterator it = lower_bound(key);
+    if (it == end() || key_comp()(key, it->first)) {
+      return insert(it, value_type(key, mapped_type()))->second;
+    }
+    return it->second;
+  }
+
+  bool operator==(const sorted_vector_map& other) const {
+    return m_.cont_ == other.m_.cont_;
+  }
+
+  bool operator<(const sorted_vector_map& other) const {
+    return m_.cont_ < other.m_.cont_;
+  }
+
+private:
+  // This is to get the empty base optimization; see the comment in
+  // sorted_vector_set.
+  struct EBO : value_compare {
+    explicit EBO(const value_compare& c, const Allocator& alloc)
+      : value_compare(c)
+      , cont_(alloc)
+    {}
+    ContainerT cont_;
+  } m_;
+};
+
+// Swap function that can be found using ADL.
+template<class K, class V, class C, class A, class G>
+inline void swap(sorted_vector_map<K,V,C,A,G>& a,
+                 sorted_vector_map<K,V,C,A,G>& b) {
+  return a.swap(b);
+}
+
+//////////////////////////////////////////////////////////////////////
+
+}
+
+#endif
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/SpookyHashV1.cpp
@@ -0,0 +1,374 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// Spooky Hash
+// A 128-bit noncryptographic hash, for checksums and table lookup
+// By Bob Jenkins.  Public domain.
+//   Oct 31 2010: published framework, disclaimer ShortHash isn't right
+//   Nov 7 2010: disabled ShortHash
+//   Oct 31 2011: replace End, ShortMix, ShortEnd, enable ShortHash again
+//   April 10 2012: buffer overflow on platforms without unaligned reads
+//   July 12 2012: was passing out variables in final to in/out in short
+//   July 30 2012: I reintroduced the buffer overflow
+
+#include "folly/SpookyHashV1.h"
+
+#include <cstring>
+
+#define ALLOW_UNALIGNED_READS 1
+
+namespace folly {
+namespace hash {
+
+//
+// short hash ... it could be used on any message,
+// but it's used by Spooky just for short messages.
+//
+void SpookyHashV1::Short(
+    const void *message,
+    size_t length,
+    uint64_t *hash1,
+    uint64_t *hash2)
+{
+    uint64_t buf[2*sc_numVars];
+    union
+    {
+        const uint8_t *p8;
+        uint32_t *p32;
+        uint64_t *p64;
+        size_t i;
+    } u;
+
+    u.p8 = (const uint8_t *)message;
+
+    if (!ALLOW_UNALIGNED_READS && (u.i & 0x7))
+    {
+        memcpy(buf, message, length);
+        u.p64 = buf;
+    }
+
+    size_t remainder = length%32;
+    uint64_t a=*hash1;
+    uint64_t b=*hash2;
+    uint64_t c=sc_const;
+    uint64_t d=sc_const;
+
+    if (length > 15)
+    {
+        const uint64_t *end = u.p64 + (length/32)*4;
+
+        // handle all complete sets of 32 bytes
+        for (; u.p64 < end; u.p64 += 4)
+        {
+            c += u.p64[0];
+            d += u.p64[1];
+            ShortMix(a,b,c,d);
+            a += u.p64[2];
+            b += u.p64[3];
+        }
+
+        //Handle the case of 16+ remaining bytes.
+        if (remainder >= 16)
+        {
+            c += u.p64[0];
+            d += u.p64[1];
+            ShortMix(a,b,c,d);
+            u.p64 += 2;
+            remainder -= 16;
+        }
+    }
+
+    // Handle the last 0..15 bytes, and its length
+    d = ((uint64_t)length) << 56;
+    switch (remainder)
+    {
+    case 15:
+    d += ((uint64_t)u.p8[14]) << 48;
+    case 14:
+        d += ((uint64_t)u.p8[13]) << 40;
+    case 13:
+        d += ((uint64_t)u.p8[12]) << 32;
+    case 12:
+        d += u.p32[2];
+        c += u.p64[0];
+        break;
+    case 11:
+        d += ((uint64_t)u.p8[10]) << 16;
+    case 10:
+        d += ((uint64_t)u.p8[9]) << 8;
+    case 9:
+        d += (uint64_t)u.p8[8];
+    case 8:
+        c += u.p64[0];
+        break;
+    case 7:
+        c += ((uint64_t)u.p8[6]) << 48;
+    case 6:
+        c += ((uint64_t)u.p8[5]) << 40;
+    case 5:
+        c += ((uint64_t)u.p8[4]) << 32;
+    case 4:
+        c += u.p32[0];
+        break;
+    case 3:
+        c += ((uint64_t)u.p8[2]) << 16;
+    case 2:
+        c += ((uint64_t)u.p8[1]) << 8;
+    case 1:
+        c += (uint64_t)u.p8[0];
+        break;
+    case 0:
+        c += sc_const;
+        d += sc_const;
+    }
+    ShortEnd(a,b,c,d);
+    *hash1 = a;
+    *hash2 = b;
+}
+
+
+
+
+// do the whole hash in one call
+void SpookyHashV1::Hash128(
+    const void *message,
+    size_t length,
+    uint64_t *hash1,
+    uint64_t *hash2)
+{
+    if (length < sc_bufSize)
+    {
+        Short(message, length, hash1, hash2);
+        return;
+    }
+
+    uint64_t h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11;
+    uint64_t buf[sc_numVars];
+    uint64_t *end;
+    union
+    {
+        const uint8_t *p8;
+        uint64_t *p64;
+        size_t i;
+    } u;
+    size_t remainder;
+
+    h0=h3=h6=h9  = *hash1;
+    h1=h4=h7=h10 = *hash2;
+    h2=h5=h8=h11 = sc_const;
+
+    u.p8 = (const uint8_t *)message;
+    end = u.p64 + (length/sc_blockSize)*sc_numVars;
+
+    // handle all whole sc_blockSize blocks of bytes
+    if (ALLOW_UNALIGNED_READS || ((u.i & 0x7) == 0))
+    {
+        while (u.p64 < end)
+        {
+            Mix(u.p64, h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11);
+            u.p64 += sc_numVars;
+        }
+    }
+    else
+    {
+        while (u.p64 < end)
+        {
+            memcpy(buf, u.p64, sc_blockSize);
+            Mix(buf, h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11);
+            u.p64 += sc_numVars;
+        }
+    }
+
+    // handle the last partial block of sc_blockSize bytes
+    remainder = (length - ((const uint8_t *)end-(const uint8_t *)message));
+    memcpy(buf, end, remainder);
+    memset(((uint8_t *)buf)+remainder, 0, sc_blockSize-remainder);
+    ((uint8_t *)buf)[sc_blockSize-1] = remainder;
+    Mix(buf, h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11);
+
+    // do some final mixing
+    End(h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11);
+    *hash1 = h0;
+    *hash2 = h1;
+}
+
+
+
+// init spooky state
+void SpookyHashV1::Init(uint64_t seed1, uint64_t seed2)
+{
+    m_length = 0;
+    m_remainder = 0;
+    m_state[0] = seed1;
+    m_state[1] = seed2;
+}
+
+
+// add a message fragment to the state
+void SpookyHashV1::Update(const void *message, size_t length)
+{
+    uint64_t h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11;
+    size_t newLength = length + m_remainder;
+    uint8_t  remainder;
+    union
+    {
+        const uint8_t *p8;
+        uint64_t *p64;
+        size_t i;
+    } u;
+    const uint64_t *end;
+
+    // Is this message fragment too short?  If it is, stuff it away.
+    if (newLength < sc_bufSize)
+    {
+        memcpy(&((uint8_t *)m_data)[m_remainder], message, length);
+        m_length = length + m_length;
+        m_remainder = (uint8_t)newLength;
+        return;
+    }
+
+    // init the variables
+    if (m_length < sc_bufSize)
+    {
+        h0=h3=h6=h9  = m_state[0];
+        h1=h4=h7=h10 = m_state[1];
+        h2=h5=h8=h11 = sc_const;
+    }
+    else
+    {
+        h0 = m_state[0];
+        h1 = m_state[1];
+        h2 = m_state[2];
+        h3 = m_state[3];
+        h4 = m_state[4];
+        h5 = m_state[5];
+        h6 = m_state[6];
+        h7 = m_state[7];
+        h8 = m_state[8];
+        h9 = m_state[9];
+        h10 = m_state[10];
+        h11 = m_state[11];
+    }
+    m_length = length + m_length;
+
+    // if we've got anything stuffed away, use it now
+    if (m_remainder)
+    {
+        uint8_t prefix = sc_bufSize-m_remainder;
+        memcpy(&(((uint8_t *)m_data)[m_remainder]), message, prefix);
+        u.p64 = m_data;
+        Mix(u.p64, h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11);
+        Mix(&u.p64[sc_numVars], h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11);
+        u.p8 = ((const uint8_t *)message) + prefix;
+        length -= prefix;
+    }
+    else
+    {
+        u.p8 = (const uint8_t *)message;
+    }
+
+    // handle all whole blocks of sc_blockSize bytes
+    end = u.p64 + (length/sc_blockSize)*sc_numVars;
+    remainder = (uint8_t)(length-((const uint8_t *)end-u.p8));
+    if (ALLOW_UNALIGNED_READS || (u.i & 0x7) == 0)
+    {
+        while (u.p64 < end)
+        {
+            Mix(u.p64, h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11);
+            u.p64 += sc_numVars;
+        }
+    }
+    else
+    {
+        while (u.p64 < end)
+        {
+            memcpy(m_data, u.p8, sc_blockSize);
+            Mix(m_data, h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11);
+            u.p64 += sc_numVars;
+        }
+    }
+
+    // stuff away the last few bytes
+    m_remainder = remainder;
+    memcpy(m_data, end, remainder);
+
+    // stuff away the variables
+    m_state[0] = h0;
+    m_state[1] = h1;
+    m_state[2] = h2;
+    m_state[3] = h3;
+    m_state[4] = h4;
+    m_state[5] = h5;
+    m_state[6] = h6;
+    m_state[7] = h7;
+    m_state[8] = h8;
+    m_state[9] = h9;
+    m_state[10] = h10;
+    m_state[11] = h11;
+}
+
+
+// report the hash for the concatenation of all message fragments so far
+void SpookyHashV1::Final(uint64_t *hash1, uint64_t *hash2)
+{
+    // init the variables
+    if (m_length < sc_bufSize)
+    {
+        *hash1 = m_state[0];
+        *hash2 = m_state[1];
+        Short( m_data, m_length, hash1, hash2);
+        return;
+    }
+
+    const uint64_t *data = (const uint64_t *)m_data;
+    uint8_t remainder = m_remainder;
+
+    uint64_t h0 = m_state[0];
+    uint64_t h1 = m_state[1];
+    uint64_t h2 = m_state[2];
+    uint64_t h3 = m_state[3];
+    uint64_t h4 = m_state[4];
+    uint64_t h5 = m_state[5];
+    uint64_t h6 = m_state[6];
+    uint64_t h7 = m_state[7];
+    uint64_t h8 = m_state[8];
+    uint64_t h9 = m_state[9];
+    uint64_t h10 = m_state[10];
+    uint64_t h11 = m_state[11];
+
+    if (remainder >= sc_blockSize)
+    {
+        // m_data can contain two blocks; handle any whole first block
+        Mix(data, h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11);
+        data += sc_numVars;
+        remainder -= sc_blockSize;
+    }
+
+    // mix in the last partial block, and the length mod sc_blockSize
+    memset(&((uint8_t *)data)[remainder], 0, (sc_blockSize-remainder));
+
+    ((uint8_t *)data)[sc_blockSize-1] = remainder;
+    Mix(data, h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11);
+
+    // do some final mixing
+    End(h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11);
+
+    *hash1 = h0;
+    *hash2 = h1;
+}
+
+}  // namespace hash
+}  // namespace folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/SpookyHashV1.h
@@ -0,0 +1,304 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// This is version 1 of SpookyHash, incompatible with version 2.
+//
+// SpookyHash: a 128-bit noncryptographic hash function
+// By Bob Jenkins, public domain
+//   Oct 31 2010: alpha, framework + SpookyHash::Mix appears right
+//   Oct 31 2011: alpha again, Mix only good to 2^^69 but rest appears right
+//   Dec 31 2011: beta, improved Mix, tested it for 2-bit deltas
+//   Feb  2 2012: production, same bits as beta
+//   Feb  5 2012: adjusted definitions of uint* to be more portable
+//   Mar 30 2012: 3 bytes/cycle, not 4.  Alpha was 4 but wasn't thorough enough.
+//
+// Up to 3 bytes/cycle for long messages.  Reasonably fast for short messages.
+// All 1 or 2 bit deltas achieve avalanche within 1% bias per output bit.
+//
+// This was developed for and tested on 64-bit x86-compatible processors.
+// It assumes the processor is little-endian.  There is a macro
+// controlling whether unaligned reads are allowed (by default they are).
+// This should be an equally good hash on big-endian machines, but it will
+// compute different results on them than on little-endian machines.
+//
+// Google's CityHash has similar specs to SpookyHash, and CityHash is faster
+// on some platforms.  MD4 and MD5 also have similar specs, but they are orders
+// of magnitude slower.  CRCs are two or more times slower, but unlike
+// SpookyHash, they have nice math for combining the CRCs of pieces to form
+// the CRCs of wholes.  There are also cryptographic hashes, but those are even
+// slower than MD5.
+//
+
+#ifndef FOLLY_SPOOKYHASHV1_H_
+#define FOLLY_SPOOKYHASHV1_H_
+
+#include <cstddef>
+#include <cstdint>
+
+namespace folly {
+namespace hash {
+
+class SpookyHashV1
+{
+public:
+    //
+    // SpookyHash: hash a single message in one call, produce 128-bit output
+    //
+    static void Hash128(
+        const void *message,  // message to hash
+        size_t length,        // length of message in bytes
+        uint64_t *hash1,      // in/out: in seed 1, out hash value 1
+        uint64_t *hash2);     // in/out: in seed 2, out hash value 2
+
+    //
+    // Hash64: hash a single message in one call, return 64-bit output
+    //
+    static uint64_t Hash64(
+        const void *message,  // message to hash
+        size_t length,        // length of message in bytes
+        uint64_t seed)        // seed
+    {
+        uint64_t hash1 = seed;
+        Hash128(message, length, &hash1, &seed);
+        return hash1;
+    }
+
+    //
+    // Hash32: hash a single message in one call, produce 32-bit output
+    //
+    static uint32_t Hash32(
+        const void *message,  // message to hash
+        size_t length,        // length of message in bytes
+        uint32_t seed)        // seed
+    {
+        uint64_t hash1 = seed, hash2 = seed;
+        Hash128(message, length, &hash1, &hash2);
+        return (uint32_t)hash1;
+    }
+
+    //
+    // Init: initialize the context of a SpookyHash
+    //
+    void Init(
+        uint64_t seed1,     // any 64-bit value will do, including 0
+        uint64_t seed2);    // different seeds produce independent hashes
+
+    //
+    // Update: add a piece of a message to a SpookyHash state
+    //
+    void Update(
+        const void *message,  // message fragment
+        size_t length);       // length of message fragment in bytes
+
+
+    //
+    // Final: compute the hash for the current SpookyHash state
+    //
+    // This does not modify the state; you can keep updating it afterward
+    //
+    // The result is the same as if SpookyHash() had been called with
+    // all the pieces concatenated into one message.
+    //
+    void Final(
+        uint64_t *hash1,  // out only: first 64 bits of hash value.
+        uint64_t *hash2); // out only: second 64 bits of hash value.
+
+    //
+    // left rotate a 64-bit value by k bytes
+    //
+    static inline uint64_t Rot64(uint64_t x, int k)
+    {
+        return (x << k) | (x >> (64 - k));
+    }
+
+    //
+    // This is used if the input is 96 bytes long or longer.
+    //
+    // The internal state is fully overwritten every 96 bytes.
+    // Every input bit appears to cause at least 128 bits of entropy
+    // before 96 other bytes are combined, when run forward or backward
+    //   For every input bit,
+    //   Two inputs differing in just that input bit
+    //   Where "differ" means xor or subtraction
+    //   And the base value is random
+    //   When run forward or backwards one Mix
+    // I tried 3 pairs of each; they all differed by at least 212 bits.
+    //
+    static inline void Mix(
+        const uint64_t *data,
+        uint64_t &s0, uint64_t &s1, uint64_t &s2, uint64_t &s3,
+        uint64_t &s4, uint64_t &s5, uint64_t &s6, uint64_t &s7,
+        uint64_t &s8, uint64_t &s9, uint64_t &s10,uint64_t &s11)
+    {
+      s0 += data[0];    s2 ^= s10;    s11 ^= s0;    s0 = Rot64(s0,11);    s11 += s1;
+      s1 += data[1];    s3 ^= s11;    s0 ^= s1;    s1 = Rot64(s1,32);    s0 += s2;
+      s2 += data[2];    s4 ^= s0;    s1 ^= s2;    s2 = Rot64(s2,43);    s1 += s3;
+      s3 += data[3];    s5 ^= s1;    s2 ^= s3;    s3 = Rot64(s3,31);    s2 += s4;
+      s4 += data[4];    s6 ^= s2;    s3 ^= s4;    s4 = Rot64(s4,17);    s3 += s5;
+      s5 += data[5];    s7 ^= s3;    s4 ^= s5;    s5 = Rot64(s5,28);    s4 += s6;
+      s6 += data[6];    s8 ^= s4;    s5 ^= s6;    s6 = Rot64(s6,39);    s5 += s7;
+      s7 += data[7];    s9 ^= s5;    s6 ^= s7;    s7 = Rot64(s7,57);    s6 += s8;
+      s8 += data[8];    s10 ^= s6;    s7 ^= s8;    s8 = Rot64(s8,55);    s7 += s9;
+      s9 += data[9];    s11 ^= s7;    s8 ^= s9;    s9 = Rot64(s9,54);    s8 += s10;
+      s10 += data[10];    s0 ^= s8;    s9 ^= s10;    s10 = Rot64(s10,22);    s9 += s11;
+      s11 += data[11];    s1 ^= s9;    s10 ^= s11;    s11 = Rot64(s11,46);    s10 += s0;
+    }
+
+    //
+    // Mix all 12 inputs together so that h0, h1 are a hash of them all.
+    //
+    // For two inputs differing in just the input bits
+    // Where "differ" means xor or subtraction
+    // And the base value is random, or a counting value starting at that bit
+    // The final result will have each bit of h0, h1 flip
+    // For every input bit,
+    // with probability 50 +- .3%
+    // For every pair of input bits,
+    // with probability 50 +- 3%
+    //
+    // This does not rely on the last Mix() call having already mixed some.
+    // Two iterations was almost good enough for a 64-bit result, but a
+    // 128-bit result is reported, so End() does three iterations.
+    //
+    static inline void EndPartial(
+        uint64_t &h0, uint64_t &h1, uint64_t &h2, uint64_t &h3,
+        uint64_t &h4, uint64_t &h5, uint64_t &h6, uint64_t &h7,
+        uint64_t &h8, uint64_t &h9, uint64_t &h10,uint64_t &h11)
+    {
+        h11+= h1;    h2 ^= h11;   h1 = Rot64(h1,44);
+        h0 += h2;    h3 ^= h0;    h2 = Rot64(h2,15);
+        h1 += h3;    h4 ^= h1;    h3 = Rot64(h3,34);
+        h2 += h4;    h5 ^= h2;    h4 = Rot64(h4,21);
+        h3 += h5;    h6 ^= h3;    h5 = Rot64(h5,38);
+        h4 += h6;    h7 ^= h4;    h6 = Rot64(h6,33);
+        h5 += h7;    h8 ^= h5;    h7 = Rot64(h7,10);
+        h6 += h8;    h9 ^= h6;    h8 = Rot64(h8,13);
+        h7 += h9;    h10^= h7;    h9 = Rot64(h9,38);
+        h8 += h10;   h11^= h8;    h10= Rot64(h10,53);
+        h9 += h11;   h0 ^= h9;    h11= Rot64(h11,42);
+        h10+= h0;    h1 ^= h10;   h0 = Rot64(h0,54);
+    }
+
+    static inline void End(
+        uint64_t &h0, uint64_t &h1, uint64_t &h2, uint64_t &h3,
+        uint64_t &h4, uint64_t &h5, uint64_t &h6, uint64_t &h7,
+        uint64_t &h8, uint64_t &h9, uint64_t &h10,uint64_t &h11)
+    {
+        EndPartial(h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11);
+        EndPartial(h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11);
+        EndPartial(h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11);
+    }
+
+    //
+    // The goal is for each bit of the input to expand into 128 bits of
+    //   apparent entropy before it is fully overwritten.
+    // n trials both set and cleared at least m bits of h0 h1 h2 h3
+    //   n: 2   m: 29
+    //   n: 3   m: 46
+    //   n: 4   m: 57
+    //   n: 5   m: 107
+    //   n: 6   m: 146
+    //   n: 7   m: 152
+    // when run forwards or backwards
+    // for all 1-bit and 2-bit diffs
+    // with diffs defined by either xor or subtraction
+    // with a base of all zeros plus a counter, or plus another bit, or random
+    //
+    static inline void ShortMix(uint64_t &h0, uint64_t &h1, uint64_t &h2, uint64_t &h3)
+    {
+        h2 = Rot64(h2,50);  h2 += h3;  h0 ^= h2;
+        h3 = Rot64(h3,52);  h3 += h0;  h1 ^= h3;
+        h0 = Rot64(h0,30);  h0 += h1;  h2 ^= h0;
+        h1 = Rot64(h1,41);  h1 += h2;  h3 ^= h1;
+        h2 = Rot64(h2,54);  h2 += h3;  h0 ^= h2;
+        h3 = Rot64(h3,48);  h3 += h0;  h1 ^= h3;
+        h0 = Rot64(h0,38);  h0 += h1;  h2 ^= h0;
+        h1 = Rot64(h1,37);  h1 += h2;  h3 ^= h1;
+        h2 = Rot64(h2,62);  h2 += h3;  h0 ^= h2;
+        h3 = Rot64(h3,34);  h3 += h0;  h1 ^= h3;
+        h0 = Rot64(h0,5);   h0 += h1;  h2 ^= h0;
+        h1 = Rot64(h1,36);  h1 += h2;  h3 ^= h1;
+    }
+
+    //
+    // Mix all 4 inputs together so that h0, h1 are a hash of them all.
+    //
+    // For two inputs differing in just the input bits
+    // Where "differ" means xor or subtraction
+    // And the base value is random, or a counting value starting at that bit
+    // The final result will have each bit of h0, h1 flip
+    // For every input bit,
+    // with probability 50 +- .3% (it is probably better than that)
+    // For every pair of input bits,
+    // with probability 50 +- .75% (the worst case is approximately that)
+    //
+    static inline void ShortEnd(uint64_t &h0, uint64_t &h1, uint64_t &h2, uint64_t &h3)
+    {
+        h3 ^= h2;  h2 = Rot64(h2,15);  h3 += h2;
+        h0 ^= h3;  h3 = Rot64(h3,52);  h0 += h3;
+        h1 ^= h0;  h0 = Rot64(h0,26);  h1 += h0;
+        h2 ^= h1;  h1 = Rot64(h1,51);  h2 += h1;
+        h3 ^= h2;  h2 = Rot64(h2,28);  h3 += h2;
+        h0 ^= h3;  h3 = Rot64(h3,9);   h0 += h3;
+        h1 ^= h0;  h0 = Rot64(h0,47);  h1 += h0;
+        h2 ^= h1;  h1 = Rot64(h1,54);  h2 += h1;
+        h3 ^= h2;  h2 = Rot64(h2,32);  h3 += h2;
+        h0 ^= h3;  h3 = Rot64(h3,25);  h0 += h3;
+        h1 ^= h0;  h0 = Rot64(h0,63);  h1 += h0;
+    }
+
+private:
+
+    //
+    // Short is used for messages under 192 bytes in length
+    // Short has a low startup cost, the normal mode is good for long
+    // keys, the cost crossover is at about 192 bytes.  The two modes were
+    // held to the same quality bar.
+    //
+    static void Short(
+        const void *message,  // message (array of bytes, not necessarily aligned)
+        size_t length,        // length of message (in bytes)
+        uint64_t *hash1,      // in/out: in the seed, out the hash value
+        uint64_t *hash2);     // in/out: in the seed, out the hash value
+
+    // number of uint64_t's in internal state
+    static const size_t sc_numVars = 12;
+
+    // size of the internal state
+    static const size_t sc_blockSize = sc_numVars*8;
+
+    // size of buffer of unhashed data, in bytes
+    static const size_t sc_bufSize = 2*sc_blockSize;
+
+    //
+    // sc_const: a constant which:
+    //  * is not zero
+    //  * is odd
+    //  * is a not-very-regular mix of 1's and 0's
+    //  * does not need any other special mathematical properties
+    //
+    static const uint64_t sc_const = 0xdeadbeefdeadbeefLL;
+
+    uint64_t m_data[2*sc_numVars];  // unhashed data, for partial messages
+    uint64_t m_state[sc_numVars];   // internal state of the hash
+    size_t m_length;                // total length of the input so far
+    uint8_t  m_remainder;           // length of unhashed data stashed in m_data
+};
+
+}  // namespace hash
+}  // namespace folly
+
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/SpookyHashV2.cpp
@@ -0,0 +1,373 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// Spooky Hash
+// A 128-bit noncryptographic hash, for checksums and table lookup
+// By Bob Jenkins.  Public domain.
+//   Oct 31 2010: published framework, disclaimer ShortHash isn't right
+//   Nov 7 2010: disabled ShortHash
+//   Oct 31 2011: replace End, ShortMix, ShortEnd, enable ShortHash again
+//   April 10 2012: buffer overflow on platforms without unaligned reads
+//   July 12 2012: was passing out variables in final to in/out in short
+//   July 30 2012: I reintroduced the buffer overflow
+//   August 5 2012: SpookyV2: d = should be d += in short hash, and remove extra mix from long hash
+
+#include "folly/SpookyHashV2.h"
+
+#include <cstring>
+
+#define ALLOW_UNALIGNED_READS 1
+
+namespace folly {
+namespace hash {
+
+//
+// short hash ... it could be used on any message,
+// but it's used by Spooky just for short messages.
+//
+void SpookyHashV2::Short(
+    const void *message,
+    size_t length,
+    uint64_t *hash1,
+    uint64_t *hash2)
+{
+    uint64_t buf[2*sc_numVars];
+    union
+    {
+        const uint8_t *p8;
+        uint32_t *p32;
+        uint64_t *p64;
+        size_t i;
+    } u;
+
+    u.p8 = (const uint8_t *)message;
+
+    if (!ALLOW_UNALIGNED_READS && (u.i & 0x7))
+    {
+        memcpy(buf, message, length);
+        u.p64 = buf;
+    }
+
+    size_t remainder = length%32;
+    uint64_t a=*hash1;
+    uint64_t b=*hash2;
+    uint64_t c=sc_const;
+    uint64_t d=sc_const;
+
+    if (length > 15)
+    {
+        const uint64_t *end = u.p64 + (length/32)*4;
+
+        // handle all complete sets of 32 bytes
+        for (; u.p64 < end; u.p64 += 4)
+        {
+            c += u.p64[0];
+            d += u.p64[1];
+            ShortMix(a,b,c,d);
+            a += u.p64[2];
+            b += u.p64[3];
+        }
+
+        //Handle the case of 16+ remaining bytes.
+        if (remainder >= 16)
+        {
+            c += u.p64[0];
+            d += u.p64[1];
+            ShortMix(a,b,c,d);
+            u.p64 += 2;
+            remainder -= 16;
+        }
+    }
+
+    // Handle the last 0..15 bytes, and its length
+    d += ((uint64_t)length) << 56;
+    switch (remainder)
+    {
+    case 15:
+    d += ((uint64_t)u.p8[14]) << 48;
+    case 14:
+        d += ((uint64_t)u.p8[13]) << 40;
+    case 13:
+        d += ((uint64_t)u.p8[12]) << 32;
+    case 12:
+        d += u.p32[2];
+        c += u.p64[0];
+        break;
+    case 11:
+        d += ((uint64_t)u.p8[10]) << 16;
+    case 10:
+        d += ((uint64_t)u.p8[9]) << 8;
+    case 9:
+        d += (uint64_t)u.p8[8];
+    case 8:
+        c += u.p64[0];
+        break;
+    case 7:
+        c += ((uint64_t)u.p8[6]) << 48;
+    case 6:
+        c += ((uint64_t)u.p8[5]) << 40;
+    case 5:
+        c += ((uint64_t)u.p8[4]) << 32;
+    case 4:
+        c += u.p32[0];
+        break;
+    case 3:
+        c += ((uint64_t)u.p8[2]) << 16;
+    case 2:
+        c += ((uint64_t)u.p8[1]) << 8;
+    case 1:
+        c += (uint64_t)u.p8[0];
+        break;
+    case 0:
+        c += sc_const;
+        d += sc_const;
+    }
+    ShortEnd(a,b,c,d);
+    *hash1 = a;
+    *hash2 = b;
+}
+
+
+
+
+// do the whole hash in one call
+void SpookyHashV2::Hash128(
+    const void *message,
+    size_t length,
+    uint64_t *hash1,
+    uint64_t *hash2)
+{
+    if (length < sc_bufSize)
+    {
+        Short(message, length, hash1, hash2);
+        return;
+    }
+
+    uint64_t h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11;
+    uint64_t buf[sc_numVars];
+    uint64_t *end;
+    union
+    {
+        const uint8_t *p8;
+        uint64_t *p64;
+        size_t i;
+    } u;
+    size_t remainder;
+
+    h0=h3=h6=h9  = *hash1;
+    h1=h4=h7=h10 = *hash2;
+    h2=h5=h8=h11 = sc_const;
+
+    u.p8 = (const uint8_t *)message;
+    end = u.p64 + (length/sc_blockSize)*sc_numVars;
+
+    // handle all whole sc_blockSize blocks of bytes
+    if (ALLOW_UNALIGNED_READS || ((u.i & 0x7) == 0))
+    {
+        while (u.p64 < end)
+        {
+            Mix(u.p64, h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11);
+            u.p64 += sc_numVars;
+        }
+    }
+    else
+    {
+        while (u.p64 < end)
+        {
+            memcpy(buf, u.p64, sc_blockSize);
+            Mix(buf, h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11);
+            u.p64 += sc_numVars;
+        }
+    }
+
+    // handle the last partial block of sc_blockSize bytes
+    remainder = (length - ((const uint8_t *)end-(const uint8_t *)message));
+    memcpy(buf, end, remainder);
+    memset(((uint8_t *)buf)+remainder, 0, sc_blockSize-remainder);
+    ((uint8_t *)buf)[sc_blockSize-1] = remainder;
+
+    // do some final mixing
+    End(buf, h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11);
+    *hash1 = h0;
+    *hash2 = h1;
+}
+
+
+
+// init spooky state
+void SpookyHashV2::Init(uint64_t seed1, uint64_t seed2)
+{
+    m_length = 0;
+    m_remainder = 0;
+    m_state[0] = seed1;
+    m_state[1] = seed2;
+}
+
+
+// add a message fragment to the state
+void SpookyHashV2::Update(const void *message, size_t length)
+{
+    uint64_t h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11;
+    size_t newLength = length + m_remainder;
+    uint8_t  remainder;
+    union
+    {
+        const uint8_t *p8;
+        uint64_t *p64;
+        size_t i;
+    } u;
+    const uint64_t *end;
+
+    // Is this message fragment too short?  If it is, stuff it away.
+    if (newLength < sc_bufSize)
+    {
+        memcpy(&((uint8_t *)m_data)[m_remainder], message, length);
+        m_length = length + m_length;
+        m_remainder = (uint8_t)newLength;
+        return;
+    }
+
+    // init the variables
+    if (m_length < sc_bufSize)
+    {
+        h0=h3=h6=h9  = m_state[0];
+        h1=h4=h7=h10 = m_state[1];
+        h2=h5=h8=h11 = sc_const;
+    }
+    else
+    {
+        h0 = m_state[0];
+        h1 = m_state[1];
+        h2 = m_state[2];
+        h3 = m_state[3];
+        h4 = m_state[4];
+        h5 = m_state[5];
+        h6 = m_state[6];
+        h7 = m_state[7];
+        h8 = m_state[8];
+        h9 = m_state[9];
+        h10 = m_state[10];
+        h11 = m_state[11];
+    }
+    m_length = length + m_length;
+
+    // if we've got anything stuffed away, use it now
+    if (m_remainder)
+    {
+        uint8_t prefix = sc_bufSize-m_remainder;
+        memcpy(&(((uint8_t *)m_data)[m_remainder]), message, prefix);
+        u.p64 = m_data;
+        Mix(u.p64, h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11);
+        Mix(&u.p64[sc_numVars], h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11);
+        u.p8 = ((const uint8_t *)message) + prefix;
+        length -= prefix;
+    }
+    else
+    {
+        u.p8 = (const uint8_t *)message;
+    }
+
+    // handle all whole blocks of sc_blockSize bytes
+    end = u.p64 + (length/sc_blockSize)*sc_numVars;
+    remainder = (uint8_t)(length-((const uint8_t *)end-u.p8));
+    if (ALLOW_UNALIGNED_READS || (u.i & 0x7) == 0)
+    {
+        while (u.p64 < end)
+        {
+            Mix(u.p64, h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11);
+            u.p64 += sc_numVars;
+        }
+    }
+    else
+    {
+        while (u.p64 < end)
+        {
+            memcpy(m_data, u.p8, sc_blockSize);
+            Mix(m_data, h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11);
+            u.p64 += sc_numVars;
+        }
+    }
+
+    // stuff away the last few bytes
+    m_remainder = remainder;
+    memcpy(m_data, end, remainder);
+
+    // stuff away the variables
+    m_state[0] = h0;
+    m_state[1] = h1;
+    m_state[2] = h2;
+    m_state[3] = h3;
+    m_state[4] = h4;
+    m_state[5] = h5;
+    m_state[6] = h6;
+    m_state[7] = h7;
+    m_state[8] = h8;
+    m_state[9] = h9;
+    m_state[10] = h10;
+    m_state[11] = h11;
+}
+
+
+// report the hash for the concatenation of all message fragments so far
+void SpookyHashV2::Final(uint64_t *hash1, uint64_t *hash2)
+{
+    // init the variables
+    if (m_length < sc_bufSize)
+    {
+        *hash1 = m_state[0];
+        *hash2 = m_state[1];
+        Short( m_data, m_length, hash1, hash2);
+        return;
+    }
+
+    const uint64_t *data = (const uint64_t *)m_data;
+    uint8_t remainder = m_remainder;
+
+    uint64_t h0 = m_state[0];
+    uint64_t h1 = m_state[1];
+    uint64_t h2 = m_state[2];
+    uint64_t h3 = m_state[3];
+    uint64_t h4 = m_state[4];
+    uint64_t h5 = m_state[5];
+    uint64_t h6 = m_state[6];
+    uint64_t h7 = m_state[7];
+    uint64_t h8 = m_state[8];
+    uint64_t h9 = m_state[9];
+    uint64_t h10 = m_state[10];
+    uint64_t h11 = m_state[11];
+
+    if (remainder >= sc_blockSize)
+    {
+        // m_data can contain two blocks; handle any whole first block
+        Mix(data, h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11);
+        data += sc_numVars;
+        remainder -= sc_blockSize;
+    }
+
+    // mix in the last partial block, and the length mod sc_blockSize
+    memset(&((uint8_t *)data)[remainder], 0, (sc_blockSize-remainder));
+
+    ((uint8_t *)data)[sc_blockSize-1] = remainder;
+
+    // do some final mixing
+    End(data, h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11);
+
+    *hash1 = h0;
+    *hash2 = h1;
+}
+
+}  // namespace hash
+}  // namespace folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/SpookyHashV2.h
@@ -0,0 +1,309 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// This is version 2 of SpookyHash, incompatible with version 1.
+//
+// SpookyHash: a 128-bit noncryptographic hash function
+// By Bob Jenkins, public domain
+//   Oct 31 2010: alpha, framework + SpookyHash::Mix appears right
+//   Oct 31 2011: alpha again, Mix only good to 2^^69 but rest appears right
+//   Dec 31 2011: beta, improved Mix, tested it for 2-bit deltas
+//   Feb  2 2012: production, same bits as beta
+//   Feb  5 2012: adjusted definitions of uint* to be more portable
+//   Mar 30 2012: 3 bytes/cycle, not 4.  Alpha was 4 but wasn't thorough enough.
+//   August 5 2012: SpookyV2 (different results)
+//
+// Up to 3 bytes/cycle for long messages.  Reasonably fast for short messages.
+// All 1 or 2 bit deltas achieve avalanche within 1% bias per output bit.
+//
+// This was developed for and tested on 64-bit x86-compatible processors.
+// It assumes the processor is little-endian.  There is a macro
+// controlling whether unaligned reads are allowed (by default they are).
+// This should be an equally good hash on big-endian machines, but it will
+// compute different results on them than on little-endian machines.
+//
+// Google's CityHash has similar specs to SpookyHash, and CityHash is faster
+// on new Intel boxes.  MD4 and MD5 also have similar specs, but they are orders
+// of magnitude slower.  CRCs are two or more times slower, but unlike
+// SpookyHash, they have nice math for combining the CRCs of pieces to form
+// the CRCs of wholes.  There are also cryptographic hashes, but those are even
+// slower than MD5.
+//
+
+#ifndef FOLLY_SPOOKYHASHV2_H_
+#define FOLLY_SPOOKYHASHV2_H_
+
+#include <cstddef>
+#include <cstdint>
+
+namespace folly {
+namespace hash {
+
+class SpookyHashV2
+{
+public:
+    //
+    // SpookyHash: hash a single message in one call, produce 128-bit output
+    //
+    static void Hash128(
+        const void *message,  // message to hash
+        size_t length,        // length of message in bytes
+        uint64_t *hash1,        // in/out: in seed 1, out hash value 1
+        uint64_t *hash2);       // in/out: in seed 2, out hash value 2
+
+    //
+    // Hash64: hash a single message in one call, return 64-bit output
+    //
+    static uint64_t Hash64(
+        const void *message,  // message to hash
+        size_t length,        // length of message in bytes
+        uint64_t seed)          // seed
+    {
+        uint64_t hash1 = seed;
+        Hash128(message, length, &hash1, &seed);
+        return hash1;
+    }
+
+    //
+    // Hash32: hash a single message in one call, produce 32-bit output
+    //
+    static uint32_t Hash32(
+        const void *message,  // message to hash
+        size_t length,        // length of message in bytes
+        uint32_t seed)          // seed
+    {
+        uint64_t hash1 = seed, hash2 = seed;
+        Hash128(message, length, &hash1, &hash2);
+        return (uint32_t)hash1;
+    }
+
+    //
+    // Init: initialize the context of a SpookyHash
+    //
+    void Init(
+        uint64_t seed1,       // any 64-bit value will do, including 0
+        uint64_t seed2);      // different seeds produce independent hashes
+
+    //
+    // Update: add a piece of a message to a SpookyHash state
+    //
+    void Update(
+        const void *message,  // message fragment
+        size_t length);       // length of message fragment in bytes
+
+
+    //
+    // Final: compute the hash for the current SpookyHash state
+    //
+    // This does not modify the state; you can keep updating it afterward
+    //
+    // The result is the same as if SpookyHash() had been called with
+    // all the pieces concatenated into one message.
+    //
+    void Final(
+        uint64_t *hash1,    // out only: first 64 bits of hash value.
+        uint64_t *hash2);   // out only: second 64 bits of hash value.
+
+    //
+    // left rotate a 64-bit value by k bytes
+    //
+    static inline uint64_t Rot64(uint64_t x, int k)
+    {
+        return (x << k) | (x >> (64 - k));
+    }
+
+    //
+    // This is used if the input is 96 bytes long or longer.
+    //
+    // The internal state is fully overwritten every 96 bytes.
+    // Every input bit appears to cause at least 128 bits of entropy
+    // before 96 other bytes are combined, when run forward or backward
+    //   For every input bit,
+    //   Two inputs differing in just that input bit
+    //   Where "differ" means xor or subtraction
+    //   And the base value is random
+    //   When run forward or backwards one Mix
+    // I tried 3 pairs of each; they all differed by at least 212 bits.
+    //
+    static inline void Mix(
+        const uint64_t *data,
+        uint64_t &s0, uint64_t &s1, uint64_t &s2, uint64_t &s3,
+        uint64_t &s4, uint64_t &s5, uint64_t &s6, uint64_t &s7,
+        uint64_t &s8, uint64_t &s9, uint64_t &s10,uint64_t &s11)
+    {
+      s0 += data[0];    s2 ^= s10;    s11 ^= s0;    s0 = Rot64(s0,11);    s11 += s1;
+      s1 += data[1];    s3 ^= s11;    s0 ^= s1;    s1 = Rot64(s1,32);    s0 += s2;
+      s2 += data[2];    s4 ^= s0;    s1 ^= s2;    s2 = Rot64(s2,43);    s1 += s3;
+      s3 += data[3];    s5 ^= s1;    s2 ^= s3;    s3 = Rot64(s3,31);    s2 += s4;
+      s4 += data[4];    s6 ^= s2;    s3 ^= s4;    s4 = Rot64(s4,17);    s3 += s5;
+      s5 += data[5];    s7 ^= s3;    s4 ^= s5;    s5 = Rot64(s5,28);    s4 += s6;
+      s6 += data[6];    s8 ^= s4;    s5 ^= s6;    s6 = Rot64(s6,39);    s5 += s7;
+      s7 += data[7];    s9 ^= s5;    s6 ^= s7;    s7 = Rot64(s7,57);    s6 += s8;
+      s8 += data[8];    s10 ^= s6;    s7 ^= s8;    s8 = Rot64(s8,55);    s7 += s9;
+      s9 += data[9];    s11 ^= s7;    s8 ^= s9;    s9 = Rot64(s9,54);    s8 += s10;
+      s10 += data[10];    s0 ^= s8;    s9 ^= s10;    s10 = Rot64(s10,22);    s9 += s11;
+      s11 += data[11];    s1 ^= s9;    s10 ^= s11;    s11 = Rot64(s11,46);    s10 += s0;
+    }
+
+    //
+    // Mix all 12 inputs together so that h0, h1 are a hash of them all.
+    //
+    // For two inputs differing in just the input bits
+    // Where "differ" means xor or subtraction
+    // And the base value is random, or a counting value starting at that bit
+    // The final result will have each bit of h0, h1 flip
+    // For every input bit,
+    // with probability 50 +- .3%
+    // For every pair of input bits,
+    // with probability 50 +- 3%
+    //
+    // This does not rely on the last Mix() call having already mixed some.
+    // Two iterations was almost good enough for a 64-bit result, but a
+    // 128-bit result is reported, so End() does three iterations.
+    //
+    static inline void EndPartial(
+        uint64_t &h0, uint64_t &h1, uint64_t &h2, uint64_t &h3,
+        uint64_t &h4, uint64_t &h5, uint64_t &h6, uint64_t &h7,
+        uint64_t &h8, uint64_t &h9, uint64_t &h10,uint64_t &h11)
+    {
+        h11+= h1;    h2 ^= h11;   h1 = Rot64(h1,44);
+        h0 += h2;    h3 ^= h0;    h2 = Rot64(h2,15);
+        h1 += h3;    h4 ^= h1;    h3 = Rot64(h3,34);
+        h2 += h4;    h5 ^= h2;    h4 = Rot64(h4,21);
+        h3 += h5;    h6 ^= h3;    h5 = Rot64(h5,38);
+        h4 += h6;    h7 ^= h4;    h6 = Rot64(h6,33);
+        h5 += h7;    h8 ^= h5;    h7 = Rot64(h7,10);
+        h6 += h8;    h9 ^= h6;    h8 = Rot64(h8,13);
+        h7 += h9;    h10^= h7;    h9 = Rot64(h9,38);
+        h8 += h10;   h11^= h8;    h10= Rot64(h10,53);
+        h9 += h11;   h0 ^= h9;    h11= Rot64(h11,42);
+        h10+= h0;    h1 ^= h10;   h0 = Rot64(h0,54);
+    }
+
+    static inline void End(
+        const uint64_t *data,
+        uint64_t &h0, uint64_t &h1, uint64_t &h2, uint64_t &h3,
+        uint64_t &h4, uint64_t &h5, uint64_t &h6, uint64_t &h7,
+        uint64_t &h8, uint64_t &h9, uint64_t &h10,uint64_t &h11)
+    {
+        h0 += data[0];   h1 += data[1];   h2 += data[2];   h3 += data[3];
+        h4 += data[4];   h5 += data[5];   h6 += data[6];   h7 += data[7];
+        h8 += data[8];   h9 += data[9];   h10 += data[10]; h11 += data[11];
+        EndPartial(h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11);
+        EndPartial(h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11);
+        EndPartial(h0,h1,h2,h3,h4,h5,h6,h7,h8,h9,h10,h11);
+    }
+
+    //
+    // The goal is for each bit of the input to expand into 128 bits of
+    //   apparent entropy before it is fully overwritten.
+    // n trials both set and cleared at least m bits of h0 h1 h2 h3
+    //   n: 2   m: 29
+    //   n: 3   m: 46
+    //   n: 4   m: 57
+    //   n: 5   m: 107
+    //   n: 6   m: 146
+    //   n: 7   m: 152
+    // when run forwards or backwards
+    // for all 1-bit and 2-bit diffs
+    // with diffs defined by either xor or subtraction
+    // with a base of all zeros plus a counter, or plus another bit, or random
+    //
+    static inline void ShortMix(uint64_t &h0, uint64_t &h1, uint64_t &h2, uint64_t &h3)
+    {
+        h2 = Rot64(h2,50);  h2 += h3;  h0 ^= h2;
+        h3 = Rot64(h3,52);  h3 += h0;  h1 ^= h3;
+        h0 = Rot64(h0,30);  h0 += h1;  h2 ^= h0;
+        h1 = Rot64(h1,41);  h1 += h2;  h3 ^= h1;
+        h2 = Rot64(h2,54);  h2 += h3;  h0 ^= h2;
+        h3 = Rot64(h3,48);  h3 += h0;  h1 ^= h3;
+        h0 = Rot64(h0,38);  h0 += h1;  h2 ^= h0;
+        h1 = Rot64(h1,37);  h1 += h2;  h3 ^= h1;
+        h2 = Rot64(h2,62);  h2 += h3;  h0 ^= h2;
+        h3 = Rot64(h3,34);  h3 += h0;  h1 ^= h3;
+        h0 = Rot64(h0,5);   h0 += h1;  h2 ^= h0;
+        h1 = Rot64(h1,36);  h1 += h2;  h3 ^= h1;
+    }
+
+    //
+    // Mix all 4 inputs together so that h0, h1 are a hash of them all.
+    //
+    // For two inputs differing in just the input bits
+    // Where "differ" means xor or subtraction
+    // And the base value is random, or a counting value starting at that bit
+    // The final result will have each bit of h0, h1 flip
+    // For every input bit,
+    // with probability 50 +- .3% (it is probably better than that)
+    // For every pair of input bits,
+    // with probability 50 +- .75% (the worst case is approximately that)
+    //
+    static inline void ShortEnd(uint64_t &h0, uint64_t &h1, uint64_t &h2, uint64_t &h3)
+    {
+        h3 ^= h2;  h2 = Rot64(h2,15);  h3 += h2;
+        h0 ^= h3;  h3 = Rot64(h3,52);  h0 += h3;
+        h1 ^= h0;  h0 = Rot64(h0,26);  h1 += h0;
+        h2 ^= h1;  h1 = Rot64(h1,51);  h2 += h1;
+        h3 ^= h2;  h2 = Rot64(h2,28);  h3 += h2;
+        h0 ^= h3;  h3 = Rot64(h3,9);   h0 += h3;
+        h1 ^= h0;  h0 = Rot64(h0,47);  h1 += h0;
+        h2 ^= h1;  h1 = Rot64(h1,54);  h2 += h1;
+        h3 ^= h2;  h2 = Rot64(h2,32);  h3 += h2;
+        h0 ^= h3;  h3 = Rot64(h3,25);  h0 += h3;
+        h1 ^= h0;  h0 = Rot64(h0,63);  h1 += h0;
+    }
+
+private:
+
+    //
+    // Short is used for messages under 192 bytes in length
+    // Short has a low startup cost, the normal mode is good for long
+    // keys, the cost crossover is at about 192 bytes.  The two modes were
+    // held to the same quality bar.
+    //
+    static void Short(
+        const void *message,  // message (array of bytes, not necessarily aligned)
+        size_t length,        // length of message (in bytes)
+        uint64_t *hash1,        // in/out: in the seed, out the hash value
+        uint64_t *hash2);       // in/out: in the seed, out the hash value
+
+    // number of uint64_t's in internal state
+    static const size_t sc_numVars = 12;
+
+    // size of the internal state
+    static const size_t sc_blockSize = sc_numVars*8;
+
+    // size of buffer of unhashed data, in bytes
+    static const size_t sc_bufSize = 2*sc_blockSize;
+
+    //
+    // sc_const: a constant which:
+    //  * is not zero
+    //  * is odd
+    //  * is a not-very-regular mix of 1's and 0's
+    //  * does not need any other special mathematical properties
+    //
+    static const uint64_t sc_const = 0xdeadbeefdeadbeefLL;
+
+    uint64_t m_data[2*sc_numVars];   // unhashed data, for partial messages
+    uint64_t m_state[sc_numVars];  // internal state of the hash
+    size_t m_length;             // total length of the input so far
+    uint8_t  m_remainder;          // length of unhashed data stashed in m_data
+};
+
+}  // namespace hash
+}  // namespace folly
+
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/stats/BucketedTimeSeries-defs.h
@@ -0,0 +1,433 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_STATS_BUCKETEDTIMESERIES_INL_H_
+#define FOLLY_STATS_BUCKETEDTIMESERIES_INL_H_
+
+#include <glog/logging.h>
+
+namespace folly {
+
+template <typename VT, typename TT>
+BucketedTimeSeries<VT, TT>::BucketedTimeSeries(size_t numBuckets,
+                                               TimeType duration)
+  : firstTime_(1),
+    latestTime_(0),
+    duration_(duration) {
+  // For tracking all-time data we only use total_, and don't need to bother
+  // with buckets_
+  if (!isAllTime()) {
+    // Round numBuckets down to duration_.count().
+    //
+    // There is no point in having more buckets than our timestamp
+    // granularity: otherwise we would have buckets that could never be used.
+    if (numBuckets > duration_.count()) {
+      numBuckets = duration_.count();
+    }
+
+    buckets_.resize(numBuckets, Bucket());
+  }
+}
+
+template <typename VT, typename TT>
+void BucketedTimeSeries<VT, TT>::addValue(TimeType now, const ValueType& val) {
+  addValueAggregated(now, val, 1);
+}
+
+template <typename VT, typename TT>
+void BucketedTimeSeries<VT, TT>::addValue(TimeType now,
+                                          const ValueType& val,
+                                          int64_t times) {
+  addValueAggregated(now, val * times, times);
+}
+
+template <typename VT, typename TT>
+void BucketedTimeSeries<VT, TT>::addValueAggregated(TimeType now,
+                                                    const ValueType& sum,
+                                                    int64_t nsamples) {
+  // Make sure time doesn't go backwards
+  now = std::max(now, latestTime_);
+
+  if (isAllTime()) {
+    if (empty()) {
+      firstTime_ = now;
+    }
+    latestTime_ = now;
+    total_.add(sum, nsamples);
+    return;
+  }
+
+  // Update the buckets
+  size_t curBucket = update(now);
+  buckets_[curBucket].add(sum, nsamples);
+
+  // Update the aggregate sum/count
+  total_.add(sum, nsamples);
+}
+
+template <typename VT, typename TT>
+size_t BucketedTimeSeries<VT, TT>::update(TimeType now) {
+  if (empty()) {
+    // This is the first data point.
+    firstTime_ = now;
+  }
+
+  // For all-time data, all we need to do is update latestTime_
+  if (isAllTime()) {
+    latestTime_ = std::max(latestTime_, now);
+    return 0;
+  }
+
+  // Make sure time doesn't go backwards.
+  // If the time is less than or equal to the latest time we have already seen,
+  // we don't need to do anything.
+  if (now <= latestTime_) {
+    return getBucketIdx(latestTime_);
+  }
+
+  // We could cache nextBucketStart as a member variable, so we don't have to
+  // recompute it each time update() is called with a new timestamp value.
+  // This makes things faster when update() (or addValue()) is called once
+  // per second, but slightly slower when update() is called multiple times a
+  // second.  We care more about optimizing the cases where addValue() is being
+  // called frequently.  If addValue() is only being called once every few
+  // seconds, it doesn't matter as much if it is fast.
+
+  // Get info about the bucket that latestTime_ points at
+  size_t currentBucket;
+  TimeType currentBucketStart;
+  TimeType nextBucketStart;
+  getBucketInfo(latestTime_, &currentBucket,
+                &currentBucketStart, &nextBucketStart);
+
+  // Update latestTime_
+  latestTime_ = now;
+
+  if (now < nextBucketStart) {
+    // We're still in the same bucket.
+    // We're done after updating latestTime_.
+    return currentBucket;
+  } else if (now >= currentBucketStart + duration_) {
+    // It's been a while.  We have wrapped, and all of the buckets need to be
+    // cleared.
+    for (Bucket& bucket : buckets_) {
+      bucket.clear();
+    }
+    total_.clear();
+    return getBucketIdx(latestTime_);
+  } else {
+    // clear all the buckets between the last time and current time, meaning
+    // buckets in the range [(currentBucket+1), newBucket]. Note that
+    // the bucket (currentBucket+1) is always the oldest bucket we have. Since
+    // our array is circular, loop when we reach the end.
+    size_t newBucket = getBucketIdx(now);
+    size_t idx = currentBucket;
+    while (idx != newBucket) {
+      ++idx;
+      if (idx >= buckets_.size()) {
+        idx = 0;
+      }
+      total_ -= buckets_[idx];
+      buckets_[idx].clear();
+    }
+    return newBucket;
+  }
+}
+
+template <typename VT, typename TT>
+void BucketedTimeSeries<VT, TT>::clear() {
+  for (Bucket& bucket : buckets_) {
+    bucket.clear();
+  }
+  total_.clear();
+  // Set firstTime_ larger than latestTime_,
+  // to indicate that the timeseries is empty
+  firstTime_ = TimeType(1);
+  latestTime_ = TimeType(0);
+}
+
+
+template <typename VT, typename TT>
+TT BucketedTimeSeries<VT, TT>::getEarliestTime() const {
+  if (empty()) {
+    return TimeType(0);
+  }
+  if (isAllTime()) {
+    return firstTime_;
+  }
+
+  size_t currentBucket;
+  TimeType currentBucketStart;
+  TimeType nextBucketStart;
+  getBucketInfo(latestTime_, &currentBucket,
+                &currentBucketStart, &nextBucketStart);
+
+  // Subtract 1 duration from the start of the next bucket to find the
+  // earliest possible data point we could be tracking.
+  TimeType earliestTime = nextBucketStart - duration_;
+
+  // We're never tracking data before firstTime_
+  earliestTime = std::max(earliestTime, firstTime_);
+
+  return earliestTime;
+}
+
+template <typename VT, typename TT>
+TT BucketedTimeSeries<VT, TT>::elapsed() const {
+  if (empty()) {
+    return TimeType(0);
+  }
+
+  // Add 1 since [latestTime_, earliestTime] is an inclusive interval.
+  return latestTime_ - getEarliestTime() + TimeType(1);
+}
+
+template <typename VT, typename TT>
+TT BucketedTimeSeries<VT, TT>::elapsed(TimeType start, TimeType end) const {
+  if (empty()) {
+    return TimeType(0);
+  }
+  start = std::max(start, getEarliestTime());
+  end = std::min(end, latestTime_ + TimeType(1));
+  end = std::max(start, end);
+  return end - start;
+}
+
+template <typename VT, typename TT>
+VT BucketedTimeSeries<VT, TT>::sum(TimeType start, TimeType end) const {
+  ValueType sum = ValueType();
+  forEachBucket(start, end, [&](const Bucket& bucket,
+                                TimeType bucketStart,
+                                TimeType nextBucketStart) -> bool {
+    sum += this->rangeAdjust(bucketStart, nextBucketStart, start, end,
+                             bucket.sum);
+    return true;
+  });
+
+  return sum;
+}
+
+template <typename VT, typename TT>
+uint64_t BucketedTimeSeries<VT, TT>::count(TimeType start, TimeType end) const {
+  uint64_t count = 0;
+  forEachBucket(start, end, [&](const Bucket& bucket,
+                                TimeType bucketStart,
+                                TimeType nextBucketStart) -> bool {
+    count += this->rangeAdjust(bucketStart, nextBucketStart, start, end,
+                               bucket.count);
+    return true;
+  });
+
+  return count;
+}
+
+template <typename VT, typename TT>
+template <typename ReturnType>
+ReturnType BucketedTimeSeries<VT, TT>::avg(TimeType start, TimeType end) const {
+  ValueType sum = ValueType();
+  uint64_t count = 0;
+  forEachBucket(start, end, [&](const Bucket& bucket,
+                                TimeType bucketStart,
+                                TimeType nextBucketStart) -> bool {
+    sum += this->rangeAdjust(bucketStart, nextBucketStart, start, end,
+                             bucket.sum);
+    count += this->rangeAdjust(bucketStart, nextBucketStart, start, end,
+                               bucket.count);
+    return true;
+  });
+
+  if (count == 0) {
+    return ReturnType(0);
+  }
+
+  return detail::avgHelper<ReturnType>(sum, count);
+}
+
+/*
+ * A note about some of the bucket index calculations below:
+ *
+ * buckets_.size() may not divide evenly into duration_.  When this happens,
+ * some buckets will be wider than others.  We still want to spread the data
+ * out as evenly as possible among the buckets (as opposed to just making the
+ * last bucket be significantly wider than all of the others).
+ *
+ * To make the division work out, we pretend that the buckets are each
+ * duration_ wide, so that the overall duration becomes
+ * buckets.size() * duration_.
+ *
+ * To transform a real timestamp into the scale used by our buckets,
+ * we have to multiply by buckets_.size().  To figure out which bucket it goes
+ * into, we then divide by duration_.
+ */
+
+template <typename VT, typename TT>
+size_t BucketedTimeSeries<VT, TT>::getBucketIdx(TimeType time) const {
+  // For all-time data we don't use buckets_.  Everything is tracked in total_.
+  DCHECK(!isAllTime());
+
+  time %= duration_;
+  return time.count() * buckets_.size() / duration_.count();
+}
+
+/*
+ * Compute the bucket index for the specified time, as well as the earliest
+ * time that falls into this bucket.
+ */
+template <typename VT, typename TT>
+void BucketedTimeSeries<VT, TT>::getBucketInfo(
+    TimeType time, size_t *bucketIdx,
+    TimeType* bucketStart, TimeType* nextBucketStart) const {
+  typedef typename TimeType::rep TimeInt;
+  DCHECK(!isAllTime());
+
+  // Keep these two lines together.  The compiler should be able to compute
+  // both the division and modulus with a single operation.
+  TimeType timeMod = time % duration_;
+  TimeInt numFullDurations = time / duration_;
+
+  TimeInt scaledTime = timeMod.count() * buckets_.size();
+
+  // Keep these two lines together.  The compiler should be able to compute
+  // both the division and modulus with a single operation.
+  *bucketIdx = scaledTime / duration_.count();
+  TimeInt scaledOffsetInBucket = scaledTime % duration_.count();
+
+  TimeInt scaledBucketStart = scaledTime - scaledOffsetInBucket;
+  TimeInt scaledNextBucketStart = scaledBucketStart + duration_.count();
+
+  TimeType bucketStartMod((scaledBucketStart + buckets_.size() - 1) /
+                          buckets_.size());
+  TimeType nextBucketStartMod((scaledNextBucketStart + buckets_.size() - 1) /
+                              buckets_.size());
+
+  TimeType durationStart(numFullDurations * duration_.count());
+  *bucketStart = bucketStartMod + durationStart;
+  *nextBucketStart = nextBucketStartMod + durationStart;
+}
+
+template <typename VT, typename TT>
+template <typename Function>
+void BucketedTimeSeries<VT, TT>::forEachBucket(Function fn) const {
+  if (isAllTime()) {
+    fn(total_, firstTime_, latestTime_ + TimeType(1));
+    return;
+  }
+
+  typedef typename TimeType::rep TimeInt;
+
+  // Compute durationStart, latestBucketIdx, and scaledNextBucketStart,
+  // the same way as in getBucketInfo().
+  TimeType timeMod = latestTime_ % duration_;
+  TimeInt numFullDurations = latestTime_ / duration_;
+  TimeType durationStart(numFullDurations * duration_.count());
+  TimeInt scaledTime = timeMod.count() * buckets_.size();
+  size_t latestBucketIdx = scaledTime / duration_.count();
+  TimeInt scaledOffsetInBucket = scaledTime % duration_.count();
+  TimeInt scaledBucketStart = scaledTime - scaledOffsetInBucket;
+  TimeInt scaledNextBucketStart = scaledBucketStart + duration_.count();
+
+  // Walk through the buckets, starting one past the current bucket.
+  // The next bucket is from the previous cycle, so subtract 1 duration
+  // from durationStart.
+  size_t idx = latestBucketIdx;
+  durationStart -= duration_;
+
+  TimeType nextBucketStart =
+    TimeType((scaledNextBucketStart + buckets_.size() - 1) / buckets_.size()) +
+    durationStart;
+  while (true) {
+    ++idx;
+    if (idx >= buckets_.size()) {
+      idx = 0;
+      durationStart += duration_;
+      scaledNextBucketStart = duration_.count();
+    } else {
+      scaledNextBucketStart += duration_.count();
+    }
+
+    TimeType bucketStart = nextBucketStart;
+    nextBucketStart = TimeType((scaledNextBucketStart + buckets_.size() - 1) /
+                               buckets_.size()) + durationStart;
+
+    // Should we bother skipping buckets where firstTime_ >= nextBucketStart?
+    // For now we go ahead and invoke the function with these buckets.
+    // sum and count should always be 0 in these buckets.
+
+    DCHECK_LE(bucketStart.count(), latestTime_.count());
+    bool ret = fn(buckets_[idx], bucketStart, nextBucketStart);
+    if (!ret) {
+      break;
+    }
+
+    if (idx == latestBucketIdx) {
+      // all done
+      break;
+    }
+  }
+}
+
+/*
+ * Adjust the input value from the specified bucket to only account
+ * for the desired range.
+ *
+ * For example, if the bucket spans time [10, 20), but we only care about the
+ * range [10, 16), this will return 60% of the input value.
+ */
+template<typename VT, typename TT>
+VT BucketedTimeSeries<VT, TT>::rangeAdjust(
+    TimeType bucketStart, TimeType nextBucketStart,
+    TimeType start, TimeType end, ValueType input) const {
+  // If nextBucketStart is greater than latestTime_, treat nextBucketStart as
+  // if it were latestTime_.  This makes us more accurate when someone is
+  // querying for all of the data up to latestTime_.  Even though latestTime_
+  // may only be partially through the bucket, we don't want to adjust
+  // downwards in this case, because the bucket really only has data up to
+  // latestTime_.
+  if (bucketStart <= latestTime_ && nextBucketStart > latestTime_) {
+    nextBucketStart = latestTime_ + TimeType(1);
+  }
+
+  if (start <= bucketStart && end >= nextBucketStart) {
+    // The bucket is wholly contained in the [start, end) interval
+    return input;
+  }
+
+  TimeType intervalStart = std::max(start, bucketStart);
+  TimeType intervalEnd = std::min(end, nextBucketStart);
+  return input * (intervalEnd - intervalStart) /
+    (nextBucketStart - bucketStart);
+}
+
+template <typename VT, typename TT>
+template <typename Function>
+void BucketedTimeSeries<VT, TT>::forEachBucket(TimeType start, TimeType end,
+                                               Function fn) const {
+  forEachBucket([&start, &end, &fn] (const Bucket& bucket, TimeType bucketStart,
+                                     TimeType nextBucketStart) -> bool {
+    if (start >= nextBucketStart) {
+      return true;
+    }
+    if (end <= bucketStart) {
+      return false;
+    }
+    bool ret = fn(bucket, bucketStart, nextBucketStart);
+    return ret;
+  });
+}
+
+} // folly
+
+#endif // FOLLY_STATS_BUCKETEDTIMESERIES_INL_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/stats/BucketedTimeSeries.h
@@ -0,0 +1,396 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_STATS_BUCKETEDTIMESERIES_H_
+#define FOLLY_STATS_BUCKETEDTIMESERIES_H_
+
+#include <chrono>
+#include <vector>
+
+#include "folly/detail/Stats.h"
+
+namespace folly {
+
+/*
+ * This class represents a bucketed time series which keeps track of values
+ * added in the recent past, and merges these values together into a fixed
+ * number of buckets to keep a lid on memory use if the number of values
+ * added is very large.
+ *
+ * For example, a BucketedTimeSeries() with duration == 60s and 10 buckets
+ * will keep track of 10 6-second buckets, and discard all data added more
+ * than 1 minute ago.  As time ticks by, a 6-second bucket at a time will
+ * be discarded and new data will go into the newly opened bucket.  Internally,
+ * it uses a circular array of buckets that it reuses as time advances.
+ *
+ * The class assumes that time advances forward --  you can't retroactively add
+ * values for events in the past -- the 'now' argument is provided for better
+ * efficiency and ease of unittesting.
+ *
+ *
+ * This class is not thread-safe -- use your own synchronization!
+ */
+template <typename VT, typename TT=std::chrono::seconds>
+class BucketedTimeSeries {
+ public:
+  typedef VT ValueType;
+  typedef TT TimeType;
+  typedef detail::Bucket<ValueType> Bucket;
+
+  /*
+   * Create a new BucketedTimeSeries.
+   *
+   * This creates a new BucketedTimeSeries with the specified number of
+   * buckets, storing data for the specified amount of time.
+   *
+   * If the duration is 0, the BucketedTimeSeries will track data forever,
+   * and does not need the rolling buckets.  The numBuckets parameter is
+   * ignored when duration is 0.
+   */
+  BucketedTimeSeries(size_t numBuckets, TimeType duration);
+
+  /*
+   * Adds the value 'val' at time 'now'
+   *
+   * This function expects time to always move forwards: it cannot be used to
+   * add historical data points that have occurred in the past.  If now is
+   * older than the another timestamp that has already been passed to
+   * addValue() or update(), now will be ignored and the latest timestamp will
+   * be used.
+   */
+  void addValue(TimeType now, const ValueType& val);
+
+  /*
+   * Adds the value 'val' the given number of 'times' at time 'now'
+   */
+  void addValue(TimeType now, const ValueType& val, int64_t times);
+
+  /*
+   * Adds the value 'sum' as the sum of 'nsamples' samples
+   */
+  void addValueAggregated(TimeType now, const ValueType& sum, int64_t nsamples);
+
+  /*
+   * Updates the container to the specified time, doing all the necessary
+   * work to rotate the buckets and remove any stale data points.
+   *
+   * The addValue() methods automatically call update() when adding new data
+   * points.  However, when reading data from the timeseries, you should make
+   * sure to manually call update() before accessing the data.  Otherwise you
+   * may be reading stale data if update() has not been called recently.
+   *
+   * Returns the current bucket index after the update.
+   */
+  size_t update(TimeType now);
+
+  /*
+   * Reset the timeseries to an empty state,
+   * as if no data points have ever been added to it.
+   */
+  void clear();
+
+  /*
+   * Get the latest time that has ever been passed to update() or addValue().
+   *
+   * If no data has ever been added to this timeseries, 0 will be returned.
+   */
+  TimeType getLatestTime() const {
+    return latestTime_;
+  }
+
+  /*
+   * Get the time of the earliest data point stored in this timeseries.
+   *
+   * If no data has ever been added to this timeseries, 0 will be returned.
+   *
+   * If isAllTime() is true, this is simply the time when the first data point
+   * was recorded.
+   *
+   * For non-all-time data, the timestamp reflects the first data point still
+   * remembered.  As new data points are added, old data will be expired.
+   * getEarliestTime() returns the timestamp of the oldest bucket still present
+   * in the timeseries.  This will never be older than (getLatestTime() -
+   * duration()).
+   */
+  TimeType getEarliestTime() const;
+
+  /*
+   * Return the number of buckets.
+   */
+  size_t numBuckets() const {
+    return buckets_.size();
+  }
+
+  /*
+   * Return the maximum duration of data that can be tracked by this
+   * BucketedTimeSeries.
+   */
+  TimeType duration() const {
+    return duration_;
+  }
+
+  /*
+   * Returns true if this BucketedTimeSeries stores data for all-time, without
+   * ever rolling over into new buckets.
+   */
+  bool isAllTime() const {
+    return (duration_ == TimeType(0));
+  }
+
+  /*
+   * Returns true if no calls to update() have been made since the last call to
+   * clear().
+   */
+  bool empty() const {
+    // We set firstTime_ greater than latestTime_ in the constructor and in
+    // clear, so we use this to distinguish if the timeseries is empty.
+    //
+    // Once a data point has been added, latestTime_ will always be greater
+    // than or equal to firstTime_.
+    return firstTime_ > latestTime_;
+  }
+
+  /*
+   * Get the amount of time tracked by this timeseries.
+   *
+   * For an all-time timeseries, this returns the length of time since the
+   * first data point was added to the time series.
+   *
+   * Otherwise, this never returns a value greater than the overall timeseries
+   * duration.  If the first data point was recorded less than a full duration
+   * ago, the time since the first data point is returned.  If a full duration
+   * has elapsed, and we have already thrown away some data, the time since the
+   * oldest bucket is returned.
+   *
+   * For example, say we are tracking 600 seconds worth of data, in 60 buckets.
+   * - If less than 600 seconds have elapsed since the first data point,
+   *   elapsed() returns the total elapsed time so far.
+   * - If more than 600 seconds have elapsed, we have already thrown away some
+   *   data.  However, we throw away a full bucket (10 seconds worth) at once,
+   *   so at any point in time we have from 590 to 600 seconds worth of data.
+   *   elapsed() will therefore return a value between 590 and 600.
+   *
+   * Note that you generally should call update() before calling elapsed(), to
+   * make sure you are not reading stale data.
+   */
+  TimeType elapsed() const;
+
+  /*
+   * Get the amount of time tracked by this timeseries, between the specified
+   * start and end times.
+   *
+   * If the timeseries contains data for the entire time range specified, this
+   * simply returns (end - start).  However, if start is earlier than
+   * getEarliestTime(), this returns (end - getEarliestTime()).
+   */
+  TimeType elapsed(TimeType start, TimeType end) const;
+
+  /*
+   * Return the sum of all the data points currently tracked by this
+   * BucketedTimeSeries.
+   *
+   * Note that you generally should call update() before calling sum(), to
+   * make sure you are not reading stale data.
+   */
+  const ValueType& sum() const {
+    return total_.sum;
+  }
+
+  /*
+   * Return the number of data points currently tracked by this
+   * BucketedTimeSeries.
+   *
+   * Note that you generally should call update() before calling count(), to
+   * make sure you are not reading stale data.
+   */
+  uint64_t count() const {
+    return total_.count;
+  }
+
+  /*
+   * Return the average value (sum / count).
+   *
+   * The return type may be specified to control whether floating-point or
+   * integer division should be performed.
+   *
+   * Note that you generally should call update() before calling avg(), to
+   * make sure you are not reading stale data.
+   */
+  template <typename ReturnType=double>
+  ReturnType avg() const {
+    return total_.template avg<ReturnType>();
+  }
+
+  /*
+   * Return the sum divided by the elapsed time.
+   *
+   * Note that you generally should call update() before calling rate(), to
+   * make sure you are not reading stale data.
+   */
+  template <typename ReturnType=double, typename Interval=TimeType>
+  ReturnType rate() const {
+    return rateHelper<ReturnType, Interval>(total_.sum, elapsed());
+  }
+
+  /*
+   * Return the count divided by the elapsed time.
+   *
+   * The Interval template parameter causes the elapsed time to be converted to
+   * the Interval type before using it.  For example, if Interval is
+   * std::chrono::seconds, the return value will be the count per second.
+   * If Interval is std::chrono::hours, the return value will be the count per
+   * hour.
+   *
+   * Note that you generally should call update() before calling countRate(),
+   * to make sure you are not reading stale data.
+   */
+  template <typename ReturnType=double, typename Interval=TimeType>
+  ReturnType countRate() const {
+    return rateHelper<ReturnType, Interval>(total_.count, elapsed());
+  }
+
+  /*
+   * Estimate the sum of the data points that occurred in the specified time
+   * period.
+   *
+   * The range queried is [start, end).
+   * That is, start is inclusive, and end is exclusive.
+   *
+   * Note that data outside of the timeseries duration will no longer be
+   * available for use in the estimation.  Specifying a start time earlier than
+   * getEarliestTime() will not have much effect, since only data points after
+   * that point in time will be counted.
+   *
+   * Note that the value returned is an estimate, and may not be precise.
+   */
+  ValueType sum(TimeType start, TimeType end) const;
+
+  /*
+   * Estimate the number of data points that occurred in the specified time
+   * period.
+   *
+   * The same caveats documented in the sum(TimeType start, TimeType end)
+   * comments apply here as well.
+   */
+  uint64_t count(TimeType start, TimeType end) const;
+
+  /*
+   * Estimate the average value during the specified time period.
+   *
+   * The same caveats documented in the sum(TimeType start, TimeType end)
+   * comments apply here as well.
+   */
+  template <typename ReturnType=double>
+  ReturnType avg(TimeType start, TimeType end) const;
+
+  /*
+   * Estimate the rate during the specified time period.
+   *
+   * The same caveats documented in the sum(TimeType start, TimeType end)
+   * comments apply here as well.
+   */
+  template <typename ReturnType=double, typename Interval=TimeType>
+  ReturnType rate(TimeType start, TimeType end) const {
+    ValueType intervalSum = sum(start, end);
+    TimeType interval = elapsed(start, end);
+    return rateHelper<ReturnType, Interval>(intervalSum, interval);
+  }
+
+  /*
+   * Estimate the rate of data points being added during the specified time
+   * period.
+   *
+   * The same caveats documented in the sum(TimeType start, TimeType end)
+   * comments apply here as well.
+   */
+  template <typename ReturnType=double, typename Interval=TimeType>
+  ReturnType countRate(TimeType start, TimeType end) const {
+    uint64_t intervalCount = count(start, end);
+    TimeType interval = elapsed(start, end);
+    return rateHelper<ReturnType, Interval>(intervalCount, interval);
+  }
+
+  /*
+   * Invoke a function for each bucket.
+   *
+   * The function will take as arguments the bucket index,
+   * the bucket start time, and the start time of the subsequent bucket.
+   *
+   * It should return true to continue iterating through the buckets, and false
+   * to break out of the loop and stop, without calling the function on any
+   * more buckets.
+   *
+   * bool function(const Bucket& bucket, TimeType bucketStart,
+   *               TimeType nextBucketStart)
+   */
+  template <typename Function>
+  void forEachBucket(Function fn) const;
+
+  /*
+   * Get the index for the bucket containing the specified time.
+   *
+   * Note that the index is only valid if this time actually falls within one
+   * of the current buckets.  If you pass in a value more recent than
+   * getLatestTime() or older than (getLatestTime() - elapsed()), the index
+   * returned will not be valid.
+   *
+   * This method may not be called for all-time data.
+   */
+  size_t getBucketIdx(TimeType time) const;
+
+  /*
+   * Get the bucket at the specified index.
+   *
+   * This method may not be called for all-time data.
+   */
+  const Bucket& getBucketByIndex(size_t idx) const {
+    return buckets_[idx];
+  }
+
+  /*
+   * Compute the bucket index that the specified time falls into,
+   * as well as the bucket start time and the next bucket's start time.
+   *
+   * This method may not be called for all-time data.
+   */
+  void getBucketInfo(TimeType time, size_t* bucketIdx,
+                     TimeType* bucketStart, TimeType* nextBucketStart) const;
+
+ private:
+  template <typename ReturnType=double, typename Interval=TimeType>
+  ReturnType rateHelper(ReturnType numerator, TimeType elapsed) const {
+    return detail::rateHelper<ReturnType, TimeType, Interval>(numerator,
+                                                              elapsed);
+  }
+
+  ValueType rangeAdjust(TimeType bucketStart, TimeType nextBucketStart,
+                        TimeType start, TimeType end,
+                        ValueType input) const;
+
+  template <typename Function>
+  void forEachBucket(TimeType start, TimeType end, Function fn) const;
+
+  TimeType firstTime_;   // time of first update() since clear()/constructor
+  TimeType latestTime_;  // time of last update()
+  TimeType duration_;    // total duration ("window length") of the time series
+
+  Bucket total_;                 // sum and count of everything in time series
+  std::vector<Bucket> buckets_;  // actual buckets of values
+};
+
+} // folly
+
+#endif // FOLLY_STATS_BUCKETEDTIMESERIES_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/stats/Histogram-defs.h
@@ -0,0 +1,269 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_HISTOGRAM_DEFS_H_
+#define FOLLY_HISTOGRAM_DEFS_H_
+
+#include "folly/Conv.h"
+
+#include <glog/logging.h>
+
+namespace folly {
+
+namespace detail {
+
+template <typename T, typename BucketT>
+HistogramBuckets<T, BucketT>::HistogramBuckets(ValueType bucketSize,
+                                               ValueType min,
+                                               ValueType max,
+                                               const BucketType& defaultBucket)
+  : bucketSize_(bucketSize),
+    min_(min),
+    max_(max) {
+  CHECK_GT(bucketSize_, ValueType(0));
+  CHECK_LT(min_, max_);
+
+  unsigned int numBuckets = (max - min) / bucketSize;
+  // Round up if the bucket size does not fit evenly
+  if (numBuckets * bucketSize < max - min) {
+    ++numBuckets;
+  }
+  // Add 2 for the extra 'below min' and 'above max' buckets
+  numBuckets += 2;
+  buckets_.assign(numBuckets, defaultBucket);
+}
+
+template <typename T, typename BucketType>
+unsigned int HistogramBuckets<T, BucketType>::getBucketIdx(
+    ValueType value) const {
+  if (value < min_) {
+    return 0;
+  } else if (value >= max_) {
+    return buckets_.size() - 1;
+  } else {
+    // the 1 is the below_min bucket
+    return ((value - min_) / bucketSize_) + 1;
+  }
+}
+
+template <typename T, typename BucketType>
+template <typename CountFn>
+unsigned int HistogramBuckets<T, BucketType>::getPercentileBucketIdx(
+    double pct,
+    CountFn countFromBucket,
+    double* lowPct, double* highPct) const {
+  CHECK_GE(pct, 0.0);
+  CHECK_LE(pct, 1.0);
+
+  unsigned int numBuckets = buckets_.size();
+
+  // Compute the counts in each bucket
+  std::vector<uint64_t> counts(numBuckets);
+  uint64_t totalCount = 0;
+  for (unsigned int n = 0; n < numBuckets; ++n) {
+    uint64_t bucketCount =
+      countFromBucket(const_cast<const BucketType&>(buckets_[n]));
+    counts[n] = bucketCount;
+    totalCount += bucketCount;
+  }
+
+  // If there are no elements, just return the lowest bucket.
+  // Note that we return bucket 1, which is the first bucket in the
+  // histogram range; bucket 0 is for all values below min_.
+  if (totalCount == 0) {
+    // Set lowPct and highPct both to 0.
+    // getPercentileEstimate() will recognize this to mean that the histogram
+    // is empty.
+    if (lowPct) {
+      *lowPct = 0.0;
+    }
+    if (highPct) {
+      *highPct = 0.0;
+    }
+    return 1;
+  }
+
+  // Loop through all the buckets, keeping track of each bucket's
+  // percentile range: [0,10], [10,17], [17,45], etc.  When we find a range
+  // that includes our desired percentile, we return that bucket index.
+  double prevPct = 0.0;
+  double curPct = 0.0;
+  uint64_t curCount = 0;
+  unsigned int idx;
+  for (idx = 0; idx < numBuckets; ++idx) {
+    if (counts[idx] == 0) {
+      // skip empty buckets
+      continue;
+    }
+
+    prevPct = curPct;
+    curCount += counts[idx];
+    curPct = static_cast<double>(curCount) / totalCount;
+    if (pct <= curPct) {
+      // This is the desired bucket
+      break;
+    }
+  }
+
+  if (lowPct) {
+    *lowPct = prevPct;
+  }
+  if (highPct) {
+    *highPct = curPct;
+  }
+  return idx;
+}
+
+template <typename T, typename BucketType>
+template <typename CountFn, typename AvgFn>
+T HistogramBuckets<T, BucketType>::getPercentileEstimate(
+    double pct,
+    CountFn countFromBucket,
+    AvgFn avgFromBucket) const {
+
+  // Find the bucket where this percentile falls
+  double lowPct;
+  double highPct;
+  unsigned int bucketIdx = getPercentileBucketIdx(pct, countFromBucket,
+                                                  &lowPct, &highPct);
+  if (lowPct == 0.0 && highPct == 0.0) {
+    // Invalid range -- the buckets must all be empty
+    // Return the default value for ValueType.
+    return ValueType();
+  }
+  if (lowPct == highPct) {
+    // Unlikely to have exact equality,
+    // but just return the bucket average in this case.
+    // We handle this here to avoid division by 0 below.
+    return avgFromBucket(buckets_[bucketIdx]);
+  }
+
+  CHECK_GE(pct, lowPct);
+  CHECK_LE(pct, highPct);
+  CHECK_LT(lowPct, highPct);
+
+  // Compute information about this bucket
+  ValueType avg = avgFromBucket(buckets_[bucketIdx]);
+  ValueType low;
+  ValueType high;
+  if (bucketIdx == 0) {
+    if (avg > min_) {
+      // This normally shouldn't happen.  This bucket is only supposed to track
+      // values less than min_.  Most likely this means that integer overflow
+      // occurred, and the code in avgFromBucket() returned a huge value
+      // instead of a small one.  Just return the minimum possible value for
+      // now.
+      //
+      // (Note that if the counter keeps being decremented, eventually it will
+      // wrap and become small enough that we won't detect this any more, and
+      // we will return bogus information.)
+      LOG(ERROR) << "invalid average value in histogram minimum bucket: " <<
+        avg << " > " << min_ << ": possible integer overflow?";
+      return getBucketMin(bucketIdx);
+    }
+    // For the below-min bucket, just assume the lowest value ever seen is
+    // twice as far away from min_ as avg.
+    high = min_;
+    low = high - (2 * (high - avg));
+    // Adjust low in case it wrapped
+    if (low > avg) {
+      low = std::numeric_limits<ValueType>::min();
+    }
+  } else if (bucketIdx == buckets_.size() - 1) {
+    if (avg < max_) {
+      // Most likely this means integer overflow occurred.  See the comments
+      // above in the minimum case.
+      LOG(ERROR) << "invalid average value in histogram maximum bucket: " <<
+        avg << " < " << max_ << ": possible integer overflow?";
+      return getBucketMax(bucketIdx);
+    }
+    // Similarly for the above-max bucket, assume the highest value ever seen
+    // is twice as far away from max_ as avg.
+    low = max_;
+    high = low + (2 * (avg - low));
+    // Adjust high in case it wrapped
+    if (high < avg) {
+      high = std::numeric_limits<ValueType>::max();
+    }
+  } else {
+    low = getBucketMin(bucketIdx);
+    high = getBucketMax(bucketIdx);
+    if (avg < low || avg > high) {
+      // Most likely this means an integer overflow occurred.
+      // See the comments above.  Return the midpoint between low and high
+      // as a best guess, since avg is meaningless.
+      LOG(ERROR) << "invalid average value in histogram bucket: " <<
+        avg << " not in range [" << low << ", " << high <<
+        "]: possible integer overflow?";
+      return (low + high) / 2;
+    }
+  }
+
+  // Since we know the average value in this bucket, we can do slightly better
+  // than just assuming the data points in this bucket are uniformly
+  // distributed between low and high.
+  //
+  // Assume that the median value in this bucket is the same as the average
+  // value.
+  double medianPct = (lowPct + highPct) / 2.0;
+  if (pct < medianPct) {
+    // Assume that the data points lower than the median of this bucket
+    // are uniformly distributed between low and avg
+    double pctThroughSection = (pct - lowPct) / (medianPct - lowPct);
+    return low + ((avg - low) * pctThroughSection);
+  } else {
+    // Assume that the data points greater than the median of this bucket
+    // are uniformly distributed between avg and high
+    double pctThroughSection = (pct - medianPct) / (highPct - medianPct);
+    return avg + ((high - avg) * pctThroughSection);
+  }
+}
+
+} // detail
+
+
+template <typename T>
+std::string Histogram<T>::debugString() const {
+  std::string ret = folly::to<std::string>(
+      "num buckets: ", buckets_.getNumBuckets(),
+      ", bucketSize: ", buckets_.getBucketSize(),
+      ", min: ", buckets_.getMin(), ", max: ", buckets_.getMax(), "\n");
+
+  for (unsigned int i = 0; i < buckets_.getNumBuckets(); ++i) {
+    folly::toAppend("  ", buckets_.getBucketMin(i), ": ",
+                    buckets_.getByIndex(i).count, "\n",
+                    &ret);
+  }
+
+  return ret;
+}
+
+template <typename T>
+void Histogram<T>::toTSV(std::ostream& out, bool skipEmptyBuckets) const {
+  for (unsigned int i = 0; i < buckets_.getNumBuckets(); ++i) {
+    // Do not output empty buckets in order to reduce data file size.
+    if (skipEmptyBuckets && getBucketByIndex(i).count == 0) {
+      continue;
+    }
+    const auto& bucket = getBucketByIndex(i);
+    out << getBucketMin(i) << '\t' << getBucketMax(i) << '\t'
+        << bucket.count << '\t' << bucket.sum << '\n';
+  }
+}
+
+} // folly
+
+#endif // FOLLY_HISTOGRAM_DEFS_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/stats/Histogram.h
@@ -0,0 +1,422 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_HISTOGRAM_H_
+#define FOLLY_HISTOGRAM_H_
+
+#include <cstddef>
+#include <limits>
+#include <ostream>
+#include <string>
+#include <vector>
+#include <stdexcept>
+
+#include "folly/detail/Stats.h"
+
+namespace folly {
+
+namespace detail {
+
+/*
+ * A helper class to manage a set of histogram buckets.
+ */
+template <typename T, typename BucketT>
+class HistogramBuckets {
+ public:
+  typedef T ValueType;
+  typedef BucketT BucketType;
+
+  /*
+   * Create a set of histogram buckets.
+   *
+   * One bucket will be created for each bucketSize interval of values within
+   * the specified range.  Additionally, one bucket will be created to track
+   * all values that fall below the specified minimum, and one bucket will be
+   * created for all values above the specified maximum.
+   *
+   * If (max - min) is not a multiple of bucketSize, the last bucket will cover
+   * a smaller range of values than the other buckets.
+   *
+   * (max - min) must be larger than or equal to bucketSize.
+   */
+  HistogramBuckets(ValueType bucketSize, ValueType min, ValueType max,
+                   const BucketType& defaultBucket);
+
+  /* Returns the bucket size of each bucket in the histogram. */
+  ValueType getBucketSize() const {
+    return bucketSize_;
+  }
+
+  /* Returns the min value at which bucketing begins. */
+  ValueType getMin() const {
+    return min_;
+  }
+
+  /* Returns the max value at which bucketing ends. */
+  ValueType getMax() const {
+    return max_;
+  }
+
+  /*
+   * Returns the number of buckets.
+   *
+   * This includes the total number of buckets for the [min, max) range,
+   * plus 2 extra buckets, one for handling values less than min, and one for
+   * values greater than max.
+   */
+  unsigned int getNumBuckets() const {
+    return buckets_.size();
+  }
+
+  /* Returns the bucket index into which the given value would fall. */
+  unsigned int getBucketIdx(ValueType value) const;
+
+  /* Returns the bucket for the specified value */
+  BucketType& getByValue(ValueType value) {
+    return buckets_[getBucketIdx(value)];
+  }
+
+  /* Returns the bucket for the specified value */
+  const BucketType& getByValue(ValueType value) const {
+    return buckets_[getBucketIdx(value)];
+  }
+
+  /*
+   * Returns the bucket at the specified index.
+   *
+   * Note that index 0 is the bucket for all values less than the specified
+   * minimum.  Index 1 is the first bucket in the specified bucket range.
+   */
+  BucketType& getByIndex(unsigned int idx) {
+    return buckets_[idx];
+  }
+
+  /* Returns the bucket at the specified index. */
+  const BucketType& getByIndex(unsigned int idx) const {
+    return buckets_[idx];
+  }
+
+  /*
+   * Returns the minimum threshold for the bucket at the given index.
+   *
+   * The bucket at the specified index will store values in the range
+   * [bucketMin, bucketMin + bucketSize), or [bucketMin, max), if the overall
+   * max is smaller than bucketMin + bucketSize.
+   */
+  ValueType getBucketMin(unsigned int idx) const {
+    if (idx == 0) {
+      return std::numeric_limits<ValueType>::min();
+    }
+    if (idx == buckets_.size() - 1) {
+      return max_;
+    }
+
+    return min_ + ((idx - 1) * bucketSize_);
+  }
+
+  /*
+   * Returns the maximum threshold for the bucket at the given index.
+   *
+   * The bucket at the specified index will store values in the range
+   * [bucketMin, bucketMin + bucketSize), or [bucketMin, max), if the overall
+   * max is smaller than bucketMin + bucketSize.
+   */
+  ValueType getBucketMax(unsigned int idx) const {
+    if (idx == buckets_.size() - 1) {
+      return std::numeric_limits<ValueType>::max();
+    }
+
+    return min_ + (idx * bucketSize_);
+  }
+
+  /**
+   * Determine which bucket the specified percentile falls into.
+   *
+   * Looks for the bucket that contains the Nth percentile data point.
+   *
+   * @param pct     The desired percentile to find, as a value from 0.0 to 1.0.
+   * @param countFn A function that takes a const BucketType&, and returns the
+   *                number of values in that bucket.
+   * @param lowPct  The lowest percentile stored in the selected bucket will be
+   *                returned via this parameter.
+   * @param highPct The highest percentile stored in the selected bucket will
+   *                be returned via this parameter.
+   *
+   * @return Returns the index of the bucket that contains the Nth percentile
+   *         data point.
+   */
+  template <typename CountFn>
+  unsigned int getPercentileBucketIdx(double pct,
+                                      CountFn countFromBucket,
+                                      double* lowPct = NULL,
+                                      double* highPct = NULL) const;
+
+  /**
+   * Estimate the value at the specified percentile.
+   *
+   * @param pct     The desired percentile to find, as a value from 0.0 to 1.0.
+   * @param countFn A function that takes a const BucketType&, and returns the
+   *                number of values in that bucket.
+   * @param avgFn   A function that takes a const BucketType&, and returns the
+   *                average of all the values in that bucket.
+   *
+   * @return Returns an estimate for N, where N is the number where exactly pct
+   *         percentage of the data points in the histogram are less than N.
+   */
+  template <typename CountFn, typename AvgFn>
+  ValueType getPercentileEstimate(double pct,
+                                  CountFn countFromBucket,
+                                  AvgFn avgFromBucket) const;
+
+  /*
+   * Iterator access to the buckets.
+   *
+   * Note that the first bucket is for all values less than min, and the last
+   * bucket is for all values greater than max.  The buckets tracking values in
+   * the [min, max) actually start at the second bucket.
+   */
+  typename std::vector<BucketType>::const_iterator begin() const {
+    return buckets_.begin();
+  }
+  typename std::vector<BucketType>::iterator begin() {
+    return buckets_.begin();
+  }
+  typename std::vector<BucketType>::const_iterator end() const {
+    return buckets_.end();
+  }
+  typename std::vector<BucketType>::iterator end() {
+    return buckets_.end();
+  }
+
+ private:
+  ValueType bucketSize_;
+  ValueType min_;
+  ValueType max_;
+  std::vector<BucketType> buckets_;
+};
+
+} // detail
+
+
+/*
+ * A basic histogram class.
+ *
+ * Groups data points into equally-sized buckets, and stores the overall sum of
+ * the data points in each bucket, as well as the number of data points in the
+ * bucket.
+ *
+ * The caller must specify the minimum and maximum data points to expect ahead
+ * of time, as well as the bucket width.
+ */
+template <typename T>
+class Histogram {
+ public:
+  typedef T ValueType;
+  typedef detail::Bucket<T> Bucket;
+
+  Histogram(ValueType bucketSize, ValueType min, ValueType max)
+    : buckets_(bucketSize, min, max, Bucket()) {}
+
+  /* Add a data point to the histogram */
+  void addValue(ValueType value) {
+    Bucket& bucket = buckets_.getByValue(value);
+    // TODO: It would be nice to handle overflow here.
+    bucket.sum += value;
+    bucket.count += 1;
+  }
+
+  /*
+   * Remove a data point to the histogram
+   *
+   * Note that this method does not actually verify that this exact data point
+   * had previously been added to the histogram; it merely subtracts the
+   * requested value from the appropriate bucket's sum.
+   */
+  void removeValue(ValueType value) {
+    Bucket& bucket = buckets_.getByValue(value);
+    // TODO: It would be nice to handle overflow here.
+    bucket.sum -= value;
+    bucket.count -= 1;
+  }
+
+  /* Remove all data points from the histogram */
+  void clear() {
+    for (int i = 0; i < buckets_.getNumBuckets(); i++) {
+      buckets_.getByIndex(i).clear();
+    }
+  }
+
+  /* Subtract another histogram data from the histogram */
+  void subtract(const Histogram &hist) {
+    // the two histogram bucket definitions must match to support
+    // subtract.
+    if (getBucketSize() != hist.getBucketSize() ||
+        getMin() != hist.getMin() ||
+        getMax() != hist.getMax() ||
+        getNumBuckets() != hist.getNumBuckets() ) {
+      throw std::invalid_argument("Cannot subtract input histogram.");
+    }
+
+    for (int i = 0; i < buckets_.getNumBuckets(); i++) {
+      buckets_.getByIndex(i) -= hist.buckets_.getByIndex(i);
+    }
+  }
+
+  /* Merge two histogram data together */
+  void merge(const Histogram &hist) {
+    // the two histogram bucket definitions must match to support
+    // a merge.
+    if (getBucketSize() != hist.getBucketSize() ||
+        getMin() != hist.getMin() ||
+        getMax() != hist.getMax() ||
+        getNumBuckets() != hist.getNumBuckets() ) {
+      throw std::invalid_argument("Cannot merge from input histogram.");
+    }
+
+    for (int i = 0; i < buckets_.getNumBuckets(); i++) {
+      buckets_.getByIndex(i) += hist.buckets_.getByIndex(i);
+    }
+  }
+
+  /* Copy bucket values from another histogram */
+  void copy(const Histogram &hist) {
+    // the two histogram bucket definition must match
+    if (getBucketSize() != hist.getBucketSize() ||
+        getMin() != hist.getMin() ||
+        getMax() != hist.getMax() ||
+        getNumBuckets() != hist.getNumBuckets() ) {
+      throw std::invalid_argument("Cannot copy from input histogram.");
+    }
+
+    for (int i = 0; i < buckets_.getNumBuckets(); i++) {
+      buckets_.getByIndex(i) = hist.buckets_.getByIndex(i);
+    }
+  }
+
+  /* Returns the bucket size of each bucket in the histogram. */
+  ValueType getBucketSize() const {
+    return buckets_.getBucketSize();
+  }
+  /* Returns the min value at which bucketing begins. */
+  ValueType getMin() const {
+    return buckets_.getMin();
+  }
+  /* Returns the max value at which bucketing ends. */
+  ValueType getMax() const {
+    return buckets_.getMax();
+  }
+  /* Returns the number of buckets */
+  unsigned int getNumBuckets() const {
+    return buckets_.getNumBuckets();
+  }
+
+  /* Returns the specified bucket (for reading only!) */
+  const Bucket& getBucketByIndex(int idx) const {
+    return buckets_.getByIndex(idx);
+  }
+
+  /*
+   * Returns the minimum threshold for the bucket at the given index.
+   *
+   * The bucket at the specified index will store values in the range
+   * [bucketMin, bucketMin + bucketSize), or [bucketMin, max), if the overall
+   * max is smaller than bucketMin + bucketSize.
+   */
+  ValueType getBucketMin(unsigned int idx) const {
+    return buckets_.getBucketMin(idx);
+  }
+
+  /*
+   * Returns the maximum threshold for the bucket at the given index.
+   *
+   * The bucket at the specified index will store values in the range
+   * [bucketMin, bucketMin + bucketSize), or [bucketMin, max), if the overall
+   * max is smaller than bucketMin + bucketSize.
+   */
+  ValueType getBucketMax(unsigned int idx) const {
+    return buckets_.getBucketMax(idx);
+  }
+
+  /*
+   * Get the bucket that the specified percentile falls into
+   *
+   * The lowest and highest percentile data points in returned bucket will be
+   * returned in the lowPct and highPct arguments, if they are non-NULL.
+   */
+  unsigned int getPercentileBucketIdx(double pct,
+                                      double* lowPct = NULL,
+                                      double* highPct = NULL) const {
+    // We unfortunately can't use lambdas here yet;
+    // Some users of this code are still built with gcc-4.4.
+    CountFromBucket countFn;
+    return buckets_.getPercentileBucketIdx(pct, countFn, lowPct, highPct);
+  }
+
+  /**
+   * Estimate the value at the specified percentile.
+   *
+   * @param pct     The desired percentile to find, as a value from 0.0 to 1.0.
+   *
+   * @return Returns an estimate for N, where N is the number where exactly pct
+   *         percentage of the data points in the histogram are less than N.
+   */
+  ValueType getPercentileEstimate(double pct) const {
+    CountFromBucket countFn;
+    AvgFromBucket avgFn;
+    return buckets_.getPercentileEstimate(pct, countFn, avgFn);
+  }
+
+  /*
+   * Get a human-readable string describing the histogram contents
+   */
+  std::string debugString() const;
+
+  /*
+   * Write the histogram contents in tab-separated values (TSV) format.
+   * Format is "min max count sum".
+   */
+  void toTSV(std::ostream& out, bool skipEmptyBuckets = true) const;
+
+ private:
+  struct CountFromBucket {
+    uint64_t operator()(const Bucket& bucket) const {
+      return bucket.count;
+    }
+  };
+  struct AvgFromBucket {
+    ValueType operator()(const Bucket& bucket) const {
+      if (bucket.count == 0) {
+        return ValueType(0);
+      }
+      // Cast bucket.count to a signed integer type.  This ensures that we
+      // perform division properly here: If bucket.sum is a signed integer
+      // type but we divide by an unsigned number, unsigned division will be
+      // performed and bucket.sum will be converted to unsigned first.
+      // If bucket.sum is unsigned, the code will still do unsigned division
+      // correctly.
+      //
+      // The only downside is if bucket.count is large enough to be negative
+      // when treated as signed.  That should be extremely unlikely, though.
+      return bucket.sum / static_cast<int64_t>(bucket.count);
+    }
+  };
+
+  detail::HistogramBuckets<ValueType, Bucket> buckets_;
+};
+
+} // folly
+
+#endif // FOLLY_HISTOGRAM_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/stats/Instantiations.cpp
@@ -0,0 +1,44 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+ * This file contains explicit instantiations of stats template types.
+ *
+ * This allows most users to avoid having to include the template definition
+ * header files.
+ */
+
+#include "folly/stats/BucketedTimeSeries.h"
+#include "folly/stats/BucketedTimeSeries-defs.h"
+
+#include "folly/stats/Histogram.h"
+#include "folly/stats/Histogram-defs.h"
+
+#include "folly/stats/MultiLevelTimeSeries.h"
+#include "folly/stats/MultiLevelTimeSeries-defs.h"
+
+#include "folly/stats/TimeseriesHistogram.h"
+#include "folly/stats/TimeseriesHistogram-defs.h"
+
+namespace folly {
+
+template class BucketedTimeSeries<int64_t>;
+template class Histogram<int64_t>;
+template class detail::HistogramBuckets<int64_t, Histogram<int64_t>::Bucket>;
+template class MultiLevelTimeSeries<int64_t>;
+template class TimeseriesHistogram<int64_t>;
+
+} // folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/stats/MultiLevelTimeSeries-defs.h
@@ -0,0 +1,105 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_STATS_MULTILEVELTIMESERIES_DEFS_H_
+#define FOLLY_STATS_MULTILEVELTIMESERIES_DEFS_H_
+
+#include <glog/logging.h>
+
+namespace folly {
+
+template <typename VT, typename TT>
+MultiLevelTimeSeries<VT, TT>::MultiLevelTimeSeries(
+  size_t numBuckets,
+  size_t numLevels,
+  const TimeType levelDurations[])
+    : numBuckets_(numBuckets),
+      cachedTime_(0),
+      cachedSum_(0),
+      cachedCount_(0) {
+    CHECK_GT(numLevels, 0);
+    CHECK(levelDurations);
+
+    levels_.reserve(numLevels);
+    for (int i = 0; i < numLevels; ++i) {
+      if (levelDurations[i] == TT(0)) {
+        CHECK_EQ(i, numLevels - 1);
+      } else if (i > 0) {
+        CHECK(levelDurations[i-1] < levelDurations[i]);
+      }
+      levels_.emplace_back(numBuckets, levelDurations[i]);
+    }
+}
+
+template <typename VT, typename TT>
+void MultiLevelTimeSeries<VT, TT>::addValue(TimeType now,
+                                            const ValueType& val) {
+  addValueAggregated(now, val, 1);
+}
+
+template <typename VT, typename TT>
+void MultiLevelTimeSeries<VT, TT>::addValue(TimeType now,
+                                            const ValueType& val,
+                                            int64_t times) {
+  addValueAggregated(now, val * times, times);
+}
+
+template <typename VT, typename TT>
+void MultiLevelTimeSeries<VT, TT>::addValueAggregated(TimeType now,
+                                                      const ValueType& sum,
+                                                      int64_t nsamples) {
+  if (cachedTime_ != now) {
+    flush();
+    cachedTime_ = now;
+  }
+  cachedSum_ += sum;
+  cachedCount_ += nsamples;
+}
+
+template <typename VT, typename TT>
+void MultiLevelTimeSeries<VT, TT>::update(TimeType now) {
+  flush();
+  for (int i = 0; i < levels_.size(); ++i) {
+    levels_[i].update(now);
+  }
+}
+
+template <typename VT, typename TT>
+void MultiLevelTimeSeries<VT, TT>::flush() {
+  // update all the underlying levels
+  if (cachedCount_ > 0) {
+    for (int i = 0; i < levels_.size(); ++i) {
+      levels_[i].addValueAggregated(cachedTime_, cachedSum_, cachedCount_);
+    }
+    cachedCount_ = 0;
+    cachedSum_ = 0;
+  }
+}
+
+template <typename VT, typename TT>
+void MultiLevelTimeSeries<VT, TT>::clear() {
+  for (auto & level : levels_) {
+    level.clear();
+  }
+
+  cachedTime_ = TimeType(0);
+  cachedSum_ = 0;
+  cachedCount_ = 0;
+}
+
+}  // folly
+
+#endif // FOLLY_STATS_MULTILEVELTIMESERIES_DEFS_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/stats/MultiLevelTimeSeries.h
@@ -0,0 +1,313 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_STATS_MULTILEVELTIMESERIES_H_
+#define FOLLY_STATS_MULTILEVELTIMESERIES_H_
+
+#include <chrono>
+#include <string>
+#include <vector>
+
+#include <glog/logging.h>
+#include "folly/stats/BucketedTimeSeries.h"
+
+namespace folly {
+
+/*
+ * This class represents a timeseries which keeps several levels of data
+ * granularity (similar in principle to the loads reported by the UNIX
+ * 'uptime' command).  It uses several instances (one per level) of
+ * BucketedTimeSeries as the underlying storage.
+ *
+ * This can easily be used to track sums (and thus rates or averages) over
+ * several predetermined time periods, as well as all-time sums.  For example,
+ * you would use to it to track query rate or response speed over the last
+ * 5, 15, 30, and 60 minutes.
+ *
+ * The MultiLevelTimeSeries takes a list of level durations as an input; the
+ * durations must be strictly increasing.  Furthermore a special level can be
+ * provided with a duration of '0' -- this will be an "all-time" level.  If
+ * an all-time level is provided, it MUST be the last level present.
+ *
+ * The class assumes that time advances forward --  you can't retroactively add
+ * values for events in the past -- the 'now' argument is provided for better
+ * efficiency and ease of unittesting.
+ *
+ * The class is not thread-safe -- use your own synchronization!
+ */
+template <typename VT, typename TT=std::chrono::seconds>
+class MultiLevelTimeSeries {
+ public:
+  typedef VT ValueType;
+  typedef TT TimeType;
+  typedef folly::BucketedTimeSeries<ValueType, TimeType> Level;
+
+  /*
+   * Create a new MultiLevelTimeSeries.
+   *
+   * This creates a new MultiLevelTimeSeries that tracks time series data at the
+   * specified time durations (level). The time series data tracked at each
+   * level is then further divided by numBuckets for memory efficiency.
+   *
+   * The durations must be strictly increasing. Furthermore a special level can
+   * be provided with a duration of '0' -- this will be an "all-time" level. If
+   * an all-time level is provided, it MUST be the last level present.
+   */
+  MultiLevelTimeSeries(size_t numBuckets,
+                       size_t numLevels,
+                       const TimeType levelDurations[]);
+
+  /*
+   * Return the number of buckets used to track time series at each level.
+   */
+  size_t numBuckets() const { return numBuckets_; }
+
+  /*
+   * Return the number of levels tracked by MultiLevelTimeSeries.
+   */
+  size_t numLevels() const { return levels_.size(); }
+
+  /*
+   * Get the BucketedTimeSeries backing the specified level.
+   *
+   * Note: you should generally call update() or flush() before accessing the
+   * data. Otherwise you may be reading stale data if update() or flush() has
+   * not been called recently.
+   */
+  const Level& getLevel(int level) const {
+    CHECK(level >= 0);
+    CHECK_LT(level, levels_.size());
+    return levels_[level];
+  }
+
+  /*
+   * Get the highest granularity level that is still large enough to contain
+   * data going back to the specified start time.
+   *
+   * Note: you should generally call update() or flush() before accessing the
+   * data. Otherwise you may be reading stale data if update() or flush() has
+   * not been called recently.
+   */
+  const Level& getLevel(TimeType start) const {
+    for (const auto& level : levels_) {
+      if (level.isAllTime()) {
+        return level;
+      }
+      // Note that we use duration() here rather than elapsed().
+      // If duration is large enough to contain the start time then this level
+      // is good enough, even if elapsed() indicates that no data was recorded
+      // before the specified start time.
+      if (level.getLatestTime() - level.duration() <= start) {
+        return level;
+      }
+    }
+    // We should always have an all-time level, so this is never reached.
+    LOG(FATAL) << "No level of timeseries covers internval"
+               << " from " << start.count() << " to now";
+    return levels_.back();
+  }
+
+  /*
+   * Return the sum of all the data points currently tracked at this level.
+   *
+   * Note: you should generally call update() or flush() before accessing the
+   * data. Otherwise you may be reading stale data if update() or flush() has
+   * not been called recently.
+   */
+  ValueType sum(int level) const {
+    return getLevel(level).sum();
+  }
+
+  /*
+   * Return the average (sum / count) of all the data points currently tracked
+   * at this level.
+   *
+   * The return type may be specified to control whether floating-point or
+   * integer division should be performed.
+   *
+   * Note: you should generally call update() or flush() before accessing the
+   * data. Otherwise you may be reading stale data if update() or flush() has
+   * not been called recently.
+   */
+  template <typename ReturnType=double>
+  ReturnType avg(int level) const {
+    return getLevel(level).template avg<ReturnType>();
+  }
+
+  /*
+   * Return the rate (sum divided by elaspsed time) of the all data points
+   * currently tracked at this level.
+   *
+   * Note: you should generally call update() or flush() before accessing the
+   * data. Otherwise you may be reading stale data if update() or flush() has
+   * not been called recently.
+   */
+  template <typename ReturnType=double, typename Interval=TimeType>
+  ValueType rate(int level) const {
+    return getLevel(level).template rate<ReturnType, Interval>();
+  }
+
+  /*
+   * Return the number of data points currently tracked at this level.
+   *
+   * Note: you should generally call update() or flush() before accessing the
+   * data. Otherwise you may be reading stale data if update() or flush() has
+   * not been called recently.
+   */
+  int64_t count(int level) const {
+    return getLevel(level).count();
+  }
+
+  /*
+   * Return the count divided by the elapsed time tracked at this level.
+   *
+   * Note: you should generally call update() or flush() before accessing the
+   * data. Otherwise you may be reading stale data if update() or flush() has
+   * not been called recently.
+   */
+  template <typename ReturnType=double, typename Interval=TimeType>
+  ReturnType countRate(int level) const {
+    return getLevel(level).template countRate<ReturnType, Interval>();
+  }
+
+  /*
+   * Estimate the sum of the data points that occurred in the specified time
+   * period at this level.
+   *
+   * The range queried is [start, end).
+   * That is, start is inclusive, and end is exclusive.
+   *
+   * Note that data outside of the timeseries duration will no longer be
+   * available for use in the estimation.  Specifying a start time earlier than
+   * getEarliestTime() will not have much effect, since only data points after
+   * that point in time will be counted.
+   *
+   * Note that the value returned is an estimate, and may not be precise.
+   *
+   * Note: you should generally call update() or flush() before accessing the
+   * data. Otherwise you may be reading stale data if update() or flush() has
+   * not been called recently.
+   */
+  ValueType sum(TimeType start, TimeType end) const {
+    return getLevel(start).sum(start, end);
+  }
+
+  /*
+   * Estimate the average value during the specified time period.
+   *
+   * The same caveats documented in the sum(TimeType start, TimeType end)
+   * comments apply here as well.
+   *
+   * Note: you should generally call update() or flush() before accessing the
+   * data. Otherwise you may be reading stale data if update() or flush() has
+   * not been called recently.
+   */
+  template <typename ReturnType=double>
+  ReturnType avg(TimeType start, TimeType end) const {
+    return getLevel(start).template avg<ReturnType>(start, end);
+  }
+
+  /*
+   * Estimate the rate during the specified time period.
+   *
+   * The same caveats documented in the sum(TimeType start, TimeType end)
+   * comments apply here as well.
+   *
+   * Note: you should generally call update() or flush() before accessing the
+   * data. Otherwise you may be reading stale data if update() or flush() has
+   * not been called recently.
+   */
+  template <typename ReturnType=double>
+  ReturnType rate(TimeType start, TimeType end) const {
+    return getLevel(start).template rate<ReturnType>(start, end);
+  }
+
+  /*
+   * Estimate the count during the specified time period.
+   *
+   * The same caveats documented in the sum(TimeType start, TimeType end)
+   * comments apply here as well.
+   *
+   * Note: you should generally call update() or flush() before accessing the
+   * data. Otherwise you may be reading stale data if update() or flush() has
+   * not been called recently.
+   */
+  int64_t count(TimeType start, TimeType end) const {
+    return getLevel(start).count(start, end);
+  }
+
+  /*
+   * Adds the value 'val' at time 'now' to all levels.
+   *
+   * Data points added at the same time point is cached internally here and not
+   * propagated to the underlying levels until either flush() is called or when
+   * update from a different time comes.
+   *
+   * This function expects time to always move forwards: it cannot be used to
+   * add historical data points that have occurred in the past.  If now is
+   * older than the another timestamp that has already been passed to
+   * addValue() or update(), now will be ignored and the latest timestamp will
+   * be used.
+   */
+  void addValue(TimeType now, const ValueType& val);
+
+  /*
+   * Adds the value 'val' at time 'now' to all levels.
+   */
+  void addValue(TimeType now, const ValueType& val, int64_t times);
+
+  /*
+   * Adds the value 'val' at time 'now' to all levels as the sum of 'nsamples'
+   * samples.
+   */
+  void addValueAggregated(TimeType now, const ValueType& sum, int64_t nsamples);
+
+  /*
+   * Update all the levels to the specified time, doing all the necessary
+   * work to rotate the buckets and remove any stale data points.
+   *
+   * When reading data from the timeseries, you should make sure to manually
+   * call update() before accessing the data. Otherwise you may be reading
+   * stale data if update() has not been called recently.
+   */
+  void update(TimeType now);
+
+  /*
+   * Reset all the timeseries to an empty state as if no data points have ever
+   * been added to it.
+   */
+  void clear();
+
+  /*
+   * Flush all cached updates.
+   */
+  void flush();
+
+ private:
+  size_t numBuckets_;
+  std::vector<Level> levels_;
+
+  // Updates within the same time interval are cached
+  // They are flushed out when updates from a different time comes,
+  // or flush() is called.
+  TimeType cachedTime_;
+  ValueType cachedSum_;
+  int cachedCount_;
+};
+
+} // folly
+
+#endif // FOLLY_STATS_MULTILEVELTIMESERIES_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/stats/TimeseriesHistogram-defs.h
@@ -0,0 +1,231 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_TIMESERIES_HISTOGRAM_DEF_H_
+#define FOLLY_TIMESERIES_HISTOGRAM_DEF_H_
+
+#include "folly/Conv.h"
+#include "folly/stats/Histogram-defs.h"
+#include "folly/stats/MultiLevelTimeSeries-defs.h"
+#include "folly/stats/BucketedTimeSeries-defs.h"
+
+namespace folly {
+
+template <class T, class TT, class C>
+template <typename ReturnType>
+ReturnType TimeseriesHistogram<T, TT, C>::avg(int level) const {
+  ValueType total = ValueType();
+  int64_t count = 0;
+  for (int b = 0; b < buckets_.getNumBuckets(); ++b) {
+    const auto& levelObj = buckets_.getByIndex(b).getLevel(level);
+    total += levelObj.sum();
+    count += levelObj.count();
+  }
+  return folly::detail::avgHelper<ReturnType>(total, count);
+}
+
+template <class T, class TT, class C>
+template <typename ReturnType>
+ReturnType TimeseriesHistogram<T, TT, C>::avg(TimeType start,
+                                              TimeType end) const {
+  ValueType total = ValueType();
+  int64_t count = 0;
+  for (int b = 0; b < buckets_.getNumBuckets(); ++b) {
+    const auto& levelObj = buckets_.getByIndex(b).getLevel(start, end);
+    total += levelObj.sum(start, end);
+    count += levelObj.count(start, end);
+  }
+  return folly::detail::avgHelper<ReturnType>(total, count);
+}
+
+template <class T, class TT, class C>
+template <typename ReturnType>
+ReturnType TimeseriesHistogram<T, TT, C>::rate(TimeType start,
+                                               TimeType end) const {
+  ValueType total = ValueType();
+  TimeType elapsed(0);
+  for (int b = 0; b < buckets_.getNumBuckets(); ++b) {
+    const auto& level = buckets_.getByIndex(b).getLevel(start);
+    total += level.sum(start, end);
+    elapsed = std::max(elapsed, level.elapsed(start, end));
+  }
+  return folly::detail::rateHelper<ReturnType, TimeType, TimeType>(
+      total, elapsed);
+}
+
+template <typename T, typename TT, typename C>
+TimeseriesHistogram<T, TT, C>::TimeseriesHistogram(ValueType bucketSize,
+                                            ValueType min,
+                                            ValueType max,
+                                            const ContainerType& copyMe)
+  : buckets_(bucketSize, min, max, copyMe),
+    haveNotSeenValue_(true),
+    singleUniqueValue_(false) {
+}
+
+template <typename T, typename TT, typename C>
+void TimeseriesHistogram<T, TT, C>::addValue(TimeType now,
+                                             const ValueType& value) {
+  buckets_.getByValue(value).addValue(now, value);
+  maybeHandleSingleUniqueValue(value);
+}
+
+template <typename T, typename TT, typename C>
+void TimeseriesHistogram<T, TT, C>::addValue(TimeType now,
+                                      const ValueType& value,
+                                      int64_t times) {
+  buckets_.getByValue(value).addValue(now, value, times);
+  maybeHandleSingleUniqueValue(value);
+}
+
+template <typename T, typename TT, typename C>
+void TimeseriesHistogram<T, TT, C>::addValues(
+    TimeType now, const folly::Histogram<ValueType>& hist) {
+  CHECK_EQ(hist.getMin(), getMin());
+  CHECK_EQ(hist.getMax(), getMax());
+  CHECK_EQ(hist.getBucketSize(), getBucketSize());
+  CHECK_EQ(hist.getNumBuckets(), getNumBuckets());
+
+  for (unsigned int n = 0; n < hist.getNumBuckets(); ++n) {
+    const typename folly::Histogram<ValueType>::Bucket& histBucket =
+      hist.getBucketByIndex(n);
+    Bucket& myBucket = buckets_.getByIndex(n);
+    myBucket.addValueAggregated(now, histBucket.sum, histBucket.count);
+  }
+
+  // We don't bother with the singleUniqueValue_ tracking.
+  haveNotSeenValue_ = false;
+  singleUniqueValue_ = false;
+}
+
+template <typename T, typename TT, typename C>
+void TimeseriesHistogram<T, TT, C>::maybeHandleSingleUniqueValue(
+  const ValueType& value) {
+  if (haveNotSeenValue_) {
+    firstValue_ = value;
+    singleUniqueValue_ = true;
+    haveNotSeenValue_ = false;
+  } else if (singleUniqueValue_) {
+    if (value != firstValue_) {
+      singleUniqueValue_ = false;
+    }
+  }
+}
+
+template <typename T, typename TT, typename C>
+T TimeseriesHistogram<T, TT, C>::getPercentileEstimate(double pct,
+                                                       int level) const {
+  if (singleUniqueValue_) {
+    return firstValue_;
+  }
+
+  return buckets_.getPercentileEstimate(pct / 100.0, CountFromLevel(level),
+                                        AvgFromLevel(level));
+}
+
+template <typename T, typename TT, typename C>
+T TimeseriesHistogram<T, TT, C>::getPercentileEstimate(double pct,
+                                                TimeType start,
+                                                TimeType end) const {
+  if (singleUniqueValue_) {
+    return firstValue_;
+  }
+
+  return buckets_.getPercentileEstimate(pct / 100.0,
+                                        CountFromInterval(start, end),
+                                        AvgFromInterval<T>(start, end));
+}
+
+template <typename T, typename TT, typename C>
+int TimeseriesHistogram<T, TT, C>::getPercentileBucketIdx(
+  double pct,
+  int level
+) const {
+  return buckets_.getPercentileBucketIdx(pct / 100.0, CountFromLevel(level));
+}
+
+template <typename T, typename TT, typename C>
+int TimeseriesHistogram<T, TT, C>::getPercentileBucketIdx(double pct,
+                                                   TimeType start,
+                                                   TimeType end) const {
+  return buckets_.getPercentileBucketIdx(pct / 100.0,
+                                         CountFromInterval(start, end));
+}
+
+template <typename T, typename TT, typename C>
+T TimeseriesHistogram<T, TT, C>::rate(int level) const {
+  ValueType total = ValueType();
+  TimeType elapsed(0);
+  for (int b = 0; b < buckets_.getNumBuckets(); ++b) {
+    const auto& levelObj = buckets_.getByIndex(b).getLevel(level);
+    total += levelObj.sum();
+    elapsed = std::max(elapsed, levelObj.elapsed());
+  }
+  return elapsed == TimeType(0) ? 0 : (total / elapsed.count());
+}
+
+template <typename T, typename TT, typename C>
+void TimeseriesHistogram<T, TT, C>::clear() {
+  for (int i = 0; i < buckets_.getNumBuckets(); i++) {
+    buckets_.getByIndex(i).clear();
+  }
+}
+
+template <typename T, typename TT, typename C>
+void TimeseriesHistogram<T, TT, C>::update(TimeType now) {
+  for (int i = 0; i < buckets_.getNumBuckets(); i++) {
+    buckets_.getByIndex(i).update(now);
+  }
+}
+
+template <typename T, typename TT, typename C>
+std::string TimeseriesHistogram<T, TT, C>::getString(int level) const {
+  std::string result;
+
+  for (int i = 0; i < buckets_.getNumBuckets(); i++) {
+    if (i > 0) {
+      toAppend(",", &result);
+    }
+    const ContainerType& cont = buckets_.getByIndex(i);
+    toAppend(buckets_.getBucketMin(i),
+             ":", cont.count(level),
+             ":", cont.template avg<ValueType>(level), &result);
+  }
+
+  return result;
+}
+
+template <typename T, typename TT, typename C>
+std::string TimeseriesHistogram<T, TT, C>::getString(TimeType start,
+                                                     TimeType end) const {
+  std::string result;
+
+  for (int i = 0; i < buckets_.getNumBuckets(); i++) {
+    if (i > 0) {
+      toAppend(",", &result);
+    }
+    const ContainerType& cont = buckets_.getByIndex(i);
+    toAppend(buckets_.getBucketMin(i),
+             ":", cont.count(start, end),
+             ":", cont.avg(start, end), &result);
+  }
+
+  return result;
+}
+
+}  // namespace folly
+
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/stats/TimeseriesHistogram.h
@@ -0,0 +1,335 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_TIMESERIES_HISTOGRAM_H_
+#define FOLLY_TIMESERIES_HISTOGRAM_H_
+
+#include <string>
+#include <boost/static_assert.hpp>
+#include "folly/stats/Histogram.h"
+#include "folly/stats/MultiLevelTimeSeries.h"
+
+namespace folly {
+
+/*
+ * TimeseriesHistogram tracks data distributions as they change over time.
+ *
+ * Specifically, it is a bucketed histogram with different value ranges assigned
+ * to each bucket.  Within each bucket is a MultiLevelTimeSeries from
+ * 'folly/stats/MultiLevelTimeSeries.h'. This means that each bucket contains a
+ * different set of data for different historical time periods, and one can
+ * query data distributions over different trailing time windows.
+ *
+ * For example, this can answer questions: "What is the data distribution over
+ * the last minute? Over the last 10 minutes?  Since I last cleared this
+ * histogram?"
+ *
+ * The class can also estimate percentiles and answer questions like: "What was
+ * the 99th percentile data value over the last 10 minutes?"
+ *
+ * Note: that depending on the size of your buckets and the smoothness
+ * of your data distribution, the estimate may be way off from the actual
+ * value.  In particular, if the given percentile falls outside of the bucket
+ * range (i.e. your buckets range in 0 - 100,000 but the 99th percentile is
+ * around 115,000) this estimate may be very wrong.
+ *
+ * The memory usage for a typical histogram is roughly 3k * (# of buckets).  All
+ * insertion operations are amortized O(1), and all queries are O(# of buckets).
+ */
+template <class T, class TT=std::chrono::seconds,
+          class C=folly::MultiLevelTimeSeries<T, TT>>
+class TimeseriesHistogram {
+ private:
+   // NOTE: T must be equivalent to _signed_ numeric type for our math.
+   BOOST_STATIC_ASSERT(std::numeric_limits<T>::is_signed);
+
+ public:
+  // values to be inserted into container
+  typedef T ValueType;
+  // the container type we use internally for each bucket
+  typedef C ContainerType;
+  // The time type.
+  typedef TT TimeType;
+
+  /*
+   * Create a TimeSeries histogram and initialize the bucketing and levels.
+   *
+   * The buckets are created by chopping the range [min, max) into pieces
+   * of size bucketSize, with the last bucket being potentially shorter.  Two
+   * additional buckets are always created -- the "under" bucket for the range
+   * (-inf, min) and the "over" bucket for the range [max, +inf).
+   *
+   * @param bucketSize the width of each bucket
+   * @param min the smallest value for the bucket range.
+   * @param max the largest value for the bucket range
+   * @param defaultContainer a pre-initialized timeseries with the desired
+   *                         number of levels and their durations.
+   */
+  TimeseriesHistogram(ValueType bucketSize, ValueType min, ValueType max,
+                      const ContainerType& defaultContainer);
+
+  /* Return the bucket size of each bucket in the histogram. */
+  ValueType getBucketSize() const { return buckets_.getBucketSize(); }
+
+  /* Return the min value at which bucketing begins. */
+  ValueType getMin() const { return buckets_.getMin(); }
+
+  /* Return the max value at which bucketing ends. */
+  ValueType getMax() const { return buckets_.getMax(); }
+
+  /* Return the number of levels of the Timeseries object in each bucket */
+  int getNumLevels() const {
+    return buckets_.getByIndex(0).numLevels();
+  }
+
+  /* Return the number of buckets */
+  int getNumBuckets() const { return buckets_.getNumBuckets(); }
+
+  /* Return the bucket index into which the given value would fall. */
+  int getBucketIdx(const ValueType& value) const;
+
+  /*
+   * Return the threshold of the bucket for the given index in range
+   * [0..numBuckets).  The bucket will have range [thresh, thresh + bucketSize)
+   * or [thresh, max), whichever is shorter.
+   */
+  ValueType getBucketMin(int bucketIdx) const {
+    return buckets_.getBucketMin(bucketIdx);
+  }
+
+  /* Return the actual timeseries in the given bucket (for reading only!) */
+  const ContainerType& getBucket(int bucketIdx) const {
+    return buckets_.getByIndex(bucketIdx);
+  }
+
+  /* Total count of values at the given timeseries level (all buckets). */
+  int64_t count(int level) const {
+    int64_t total = 0;
+    for (int b = 0; b < buckets_.getNumBuckets(); ++b) {
+      total += buckets_.getByIndex(b).count(level);
+    }
+    return total;
+  }
+
+  /* Total count of values added during the given interval (all buckets). */
+  int64_t count(TimeType start, TimeType end) const {
+    int64_t total = 0;
+    for (int b = 0; b < buckets_.getNumBuckets(); ++b) {
+      total += buckets_.getByIndex(b).count(start, end);
+    }
+    return total;
+  }
+
+  /* Total sum of values at the given timeseries level (all buckets). */
+  ValueType sum(int level) const {
+    ValueType total = ValueType();
+    for (int b = 0; b < buckets_.getNumBuckets(); ++b) {
+      total += buckets_.getByIndex(b).sum(level);
+    }
+    return total;
+  }
+
+  /* Total sum of values added during the given interval (all buckets). */
+  ValueType sum(TimeType start, TimeType end) const {
+    ValueType total = ValueType();
+    for (int b = 0; b < buckets_.getNumBuckets(); ++b) {
+      total += buckets_.getByIndex(b).sum(start, end);
+    }
+    return total;
+  }
+
+  /* Average of values at the given timeseries level (all buckets). */
+  template <typename ReturnType=double>
+  ReturnType avg(int level) const;
+
+  /* Average of values added during the given interval (all buckets). */
+  template <typename ReturnType=double>
+  ReturnType avg(TimeType start, TimeType end) const;
+
+  /*
+   * Rate at the given timeseries level (all buckets).
+   * This is the sum of all values divided by the time interval (in seconds).
+   */
+  ValueType rate(int level) const;
+
+  /*
+   * Rate for the given interval (all buckets).
+   * This is the sum of all values divided by the time interval (in seconds).
+   */
+  template <typename ReturnType=double>
+  ReturnType rate(TimeType start, TimeType end) const;
+
+  /*
+   * Update every underlying timeseries object with the given timestamp. You
+   * must call this directly before querying to ensure that the data in all
+   * buckets is decayed properly.
+   */
+  void update(TimeType now);
+
+  /* clear all the data from the histogram. */
+  void clear();
+
+  /* Add a value into the histogram with timestamp 'now' */
+  void addValue(TimeType now, const ValueType& value);
+  /* Add a value the given number of times with timestamp 'now' */
+  void addValue(TimeType now, const ValueType& value, int64_t times);
+
+  /*
+   * Add all of the values from the specified histogram.
+   *
+   * All of the values will be added to the current time-slot.
+   *
+   * One use of this is for thread-local caching of frequently updated
+   * histogram data.  For example, each thread can store a thread-local
+   * Histogram that is updated frequently, and only add it to the global
+   * TimeseriesHistogram once a second.
+   */
+  void addValues(TimeType now, const folly::Histogram<ValueType>& values);
+
+  /*
+   * Return an estimate of the value at the given percentile in the histogram
+   * in the given timeseries level.  The percentile is estimated as follows:
+   *
+   * - We retrieve a count of the values in each bucket (at the given level)
+   * - We determine via the counts which bucket the given percentile falls in.
+   * - We assume the average value in the bucket is also its median
+   * - We then linearly interpolate within the bucket, by assuming that the
+   *   distribution is uniform in the two value ranges [left, median) and
+   *   [median, right) where [left, right) is the bucket value range.
+   *
+   * Caveats:
+   * - If the histogram is empty, this always returns ValueType(), usually 0.
+   * - For the 'under' and 'over' special buckets, their range is unbounded
+   *   on one side.  In order for the interpolation to work, we assume that
+   *   the average value in the bucket is equidistant from the two edges of
+   *   the bucket.  In other words, we assume that the distance between the
+   *   average and the known bound is equal to the distance between the average
+   *   and the unknown bound.
+   */
+  ValueType getPercentileEstimate(double pct, int level) const;
+  /*
+   * Return an estimate of the value at the given percentile in the histogram
+   * in the given historical interval.  Please see the documentation for
+   * getPercentileEstimate(int pct, int level) for the explanation of the
+   * estimation algorithm.
+   */
+  ValueType getPercentileEstimate(double pct, TimeType start, TimeType end)
+    const;
+
+  /*
+   * Return the bucket index that the given percentile falls into (in the
+   * given timeseries level).  This index can then be used to retrieve either
+   * the bucket threshold, or other data from inside the bucket.
+   */
+  int getPercentileBucketIdx(double pct, int level) const;
+  /*
+   * Return the bucket index that the given percentile falls into (in the
+   * given historical interval).  This index can then be used to retrieve either
+   * the bucket threshold, or other data from inside the bucket.
+   */
+  int getPercentileBucketIdx(double pct, TimeType start, TimeType end) const;
+
+  /* Get the bucket threshold for the bucket containing the given pct. */
+  int getPercentileBucketMin(double pct, int level) const {
+    return getBucketMin(getPercentileBucketIdx(pct, level));
+  }
+  /* Get the bucket threshold for the bucket containing the given pct. */
+  int getPercentileBucketMin(double pct, TimeType start, TimeType end) const {
+    return getBucketMin(getPercentileBucketIdx(pct, start, end));
+  }
+
+  /*
+   * Print out serialized data from all buckets at the given level.
+   * Format is: BUCKET [',' BUCKET ...]
+   * Where: BUCKET == bucketMin ':' count ':' avg
+   */
+  std::string getString(int level) const;
+
+  /*
+   * Print out serialized data for all buckets in the historical interval.
+   * For format, please see getString(int level).
+   */
+  std::string getString(TimeType start, TimeType end) const;
+
+ private:
+  typedef ContainerType Bucket;
+  struct CountFromLevel {
+    explicit CountFromLevel(int level) : level_(level) {}
+
+    uint64_t operator()(const ContainerType& bucket) const {
+      return bucket.count(level_);
+    }
+
+   private:
+    int level_;
+  };
+  struct CountFromInterval {
+    explicit CountFromInterval(TimeType start, TimeType end)
+      : start_(start),
+        end_(end) {}
+
+    uint64_t operator()(const ContainerType& bucket) const {
+      return bucket.count(start_, end_);
+    }
+
+   private:
+    TimeType start_;
+    TimeType end_;
+  };
+
+  struct AvgFromLevel {
+    explicit AvgFromLevel(int level) : level_(level) {}
+
+    ValueType operator()(const ContainerType& bucket) const {
+      return bucket.template avg<ValueType>(level_);
+    }
+
+   private:
+    int level_;
+  };
+
+  template <typename ReturnType>
+  struct AvgFromInterval {
+    explicit AvgFromInterval(TimeType start, TimeType end)
+      : start_(start),
+        end_(end) {}
+
+    ReturnType operator()(const ContainerType& bucket) const {
+      return bucket.template avg<ReturnType>(start_, end_);
+    }
+
+   private:
+    TimeType start_;
+    TimeType end_;
+  };
+
+  /*
+   * Special logic for the case of only one unique value registered
+   * (this can happen when clients don't pick good bucket ranges or have
+   * other bugs).  It's a lot easier for clients to track down these issues
+   * if they are getting the correct value.
+   */
+  void maybeHandleSingleUniqueValue(const ValueType& value);
+
+  folly::detail::HistogramBuckets<ValueType, ContainerType> buckets_;
+  bool haveNotSeenValue_;
+  bool singleUniqueValue_;
+  ValueType firstValue_;
+};
+
+}  // folly
+
+#endif  // FOLLY_TIMESERIES_HISTOGRAM_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/String.cpp
@@ -0,0 +1,317 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/String.h"
+#include "folly/Format.h"
+
+#include <cerrno>
+#include <cstdarg>
+#include <cstring>
+#include <stdexcept>
+#include <iterator>
+#include <glog/logging.h>
+
+namespace folly {
+
+namespace {
+
+inline void stringPrintfImpl(std::string& output, const char* format,
+                             va_list args) {
+  // Tru to the space at the end of output for our output buffer.
+  // Find out write point then inflate its size temporarily to its
+  // capacity; we will later shrink it to the size needed to represent
+  // the formatted string.  If this buffer isn't large enough, we do a
+  // resize and try again.
+
+  const auto write_point = output.size();
+  auto remaining = output.capacity() - write_point;
+  output.resize(output.capacity());
+
+  va_list args_copy;
+  va_copy(args_copy, args);
+  int bytes_used = vsnprintf(&output[write_point], remaining, format,
+                             args_copy);
+  va_end(args_copy);
+  if (bytes_used < 0) {
+    throw std::runtime_error(
+      to<std::string>("Invalid format string; snprintf returned negative "
+                      "with format string: ", format));
+  } else if (bytes_used < remaining) {
+    // There was enough room, just shrink and return.
+    output.resize(write_point + bytes_used);
+  } else {
+    output.resize(write_point + bytes_used + 1);
+    remaining = bytes_used + 1;
+    va_list args_copy;
+    va_copy(args_copy, args);
+    bytes_used = vsnprintf(&output[write_point], remaining, format,
+                           args_copy);
+    va_end(args_copy);
+    if (bytes_used + 1 != remaining) {
+      throw std::runtime_error(
+        to<std::string>("vsnprint retry did not manage to work "
+                        "with format string: ", format));
+    }
+    output.resize(write_point + bytes_used);
+  }
+}
+
+}  // anon namespace
+
+std::string stringPrintf(const char* format, ...) {
+  // snprintf will tell us how large the output buffer should be, but
+  // we then have to call it a second time, which is costly.  By
+  // guestimating the final size, we avoid the double snprintf in many
+  // cases, resulting in a performance win.  We use this constructor
+  // of std::string to avoid a double allocation, though it does pad
+  // the resulting string with nul bytes.  Our guestimation is twice
+  // the format string size, or 32 bytes, whichever is larger.  This
+  // is a hueristic that doesn't affect correctness but attempts to be
+  // reasonably fast for the most common cases.
+  std::string ret(std::max(32UL, strlen(format) * 2), '\0');
+  ret.resize(0);
+
+  va_list ap;
+  va_start(ap, format);
+  stringPrintfImpl(ret, format, ap);
+  va_end(ap);
+  return ret;
+}
+
+// Basic declarations; allow for parameters of strings and string
+// pieces to be specified.
+std::string& stringAppendf(std::string* output, const char* format, ...) {
+  va_list ap;
+  va_start(ap, format);
+  stringPrintfImpl(*output, format, ap);
+  va_end(ap);
+  return *output;
+}
+
+void stringPrintf(std::string* output, const char* format, ...) {
+  output->clear();
+  va_list ap;
+  va_start(ap, format);
+  stringPrintfImpl(*output, format, ap);
+  va_end(ap);
+};
+
+namespace {
+
+struct PrettySuffix {
+  const char* suffix;
+  double val;
+};
+
+const PrettySuffix kPrettyTimeSuffixes[] = {
+  { "s ", 1e0L },
+  { "ms", 1e-3L },
+  { "us", 1e-6L },
+  { "ns", 1e-9L },
+  { "ps", 1e-12L },
+  { "s ", 0 },
+  { 0, 0 },
+};
+
+const PrettySuffix kPrettyBytesMetricSuffixes[] = {
+  { "TB", 1e12L },
+  { "GB", 1e9L },
+  { "MB", 1e6L },
+  { "kB", 1e3L },
+  { "B ", 0L },
+  { 0, 0 },
+};
+
+const PrettySuffix kPrettyBytesBinarySuffixes[] = {
+  { "TB", int64_t(1) << 40 },
+  { "GB", int64_t(1) << 30 },
+  { "MB", int64_t(1) << 20 },
+  { "kB", int64_t(1) << 10 },
+  { "B ", 0L },
+  { 0, 0 },
+};
+
+const PrettySuffix kPrettyBytesBinaryIECSuffixes[] = {
+  { "TiB", int64_t(1) << 40 },
+  { "GiB", int64_t(1) << 30 },
+  { "MiB", int64_t(1) << 20 },
+  { "KiB", int64_t(1) << 10 },
+  { "B  ", 0L },
+  { 0, 0 },
+};
+
+const PrettySuffix kPrettyUnitsMetricSuffixes[] = {
+  { "tril", 1e12L },
+  { "bil",  1e9L },
+  { "M",    1e6L },
+  { "k",    1e3L },
+  { " ",      0  },
+  { 0, 0 },
+};
+
+const PrettySuffix kPrettyUnitsBinarySuffixes[] = {
+  { "T", int64_t(1) << 40 },
+  { "G", int64_t(1) << 30 },
+  { "M", int64_t(1) << 20 },
+  { "k", int64_t(1) << 10 },
+  { " ", 0 },
+  { 0, 0 },
+};
+
+const PrettySuffix kPrettyUnitsBinaryIECSuffixes[] = {
+  { "Ti", int64_t(1) << 40 },
+  { "Gi", int64_t(1) << 30 },
+  { "Mi", int64_t(1) << 20 },
+  { "Ki", int64_t(1) << 10 },
+  { "  ", 0 },
+  { 0, 0 },
+};
+
+const PrettySuffix* const kPrettySuffixes[PRETTY_NUM_TYPES] = {
+  kPrettyTimeSuffixes,
+  kPrettyBytesMetricSuffixes,
+  kPrettyBytesBinarySuffixes,
+  kPrettyBytesBinaryIECSuffixes,
+  kPrettyUnitsMetricSuffixes,
+  kPrettyUnitsBinarySuffixes,
+  kPrettyUnitsBinaryIECSuffixes,
+};
+
+}  // namespace
+
+std::string prettyPrint(double val, PrettyType type, bool addSpace) {
+  char buf[100];
+
+  // pick the suffixes to use
+  assert(type >= 0);
+  assert(type < PRETTY_NUM_TYPES);
+  const PrettySuffix* suffixes = kPrettySuffixes[type];
+
+  // find the first suffix we're bigger than -- then use it
+  double abs_val = fabs(val);
+  for (int i = 0; suffixes[i].suffix; ++i) {
+    if (abs_val >= suffixes[i].val) {
+      snprintf(buf, sizeof buf, "%.4g%s%s",
+               (suffixes[i].val ? (val / suffixes[i].val)
+                                : val),
+               (addSpace ? " " : ""),
+               suffixes[i].suffix);
+      return std::string(buf);
+    }
+  }
+
+  // no suffix, we've got a tiny value -- just print it in sci-notation
+  snprintf(buf, sizeof buf, "%.4g", val);
+  return std::string(buf);
+}
+
+std::string hexDump(const void* ptr, size_t size) {
+  std::ostringstream os;
+  hexDump(ptr, size, std::ostream_iterator<StringPiece>(os, "\n"));
+  return os.str();
+}
+
+fbstring errnoStr(int err) {
+  int savedErrno = errno;
+
+  // Ensure that we reset errno upon exit.
+  auto guard(makeGuard([&] { errno = savedErrno; }));
+
+  char buf[1024];
+  buf[0] = '\0';
+
+  fbstring result;
+
+  // https://developer.apple.com/library/mac/documentation/Darwin/Reference/ManPages/man3/strerror_r.3.html
+  // http://www.kernel.org/doc/man-pages/online/pages/man3/strerror.3.html
+#if defined(__APPLE__) || defined(__FreeBSD__) || \
+    ((_POSIX_C_SOURCE >= 200112L || _XOPEN_SOURCE >= 600) && !_GNU_SOURCE)
+  // Using XSI-compatible strerror_r
+  int r = strerror_r(err, buf, sizeof(buf));
+
+  // OSX/FreeBSD use EINVAL and Linux uses -1 so just check for non-zero
+  if (r != 0) {
+    result = to<fbstring>(
+      "Unknown error ", err,
+      " (strerror_r failed with error ", errno, ")");
+  } else {
+    result.assign(buf);
+  }
+#else
+  // Using GNU strerror_r
+  result.assign(strerror_r(err, buf, sizeof(buf)));
+#endif
+
+  return result;
+}
+
+namespace detail {
+
+size_t hexDumpLine(const void* ptr, size_t offset, size_t size,
+                   std::string& line) {
+  // Line layout:
+  // 8: address
+  // 1: space
+  // (1+2)*16: hex bytes, each preceded by a space
+  // 1: space separating the two halves
+  // 3: "  |"
+  // 16: characters
+  // 1: "|"
+  // Total: 78
+  line.clear();
+  line.reserve(78);
+  const uint8_t* p = reinterpret_cast<const uint8_t*>(ptr) + offset;
+  size_t n = std::min(size - offset, size_t(16));
+  format("{:08x} ", offset).appendTo(line);
+
+  for (size_t i = 0; i < n; i++) {
+    if (i == 8) {
+      line.push_back(' ');
+    }
+    format(" {:02x}", p[i]).appendTo(line);
+  }
+
+  // 3 spaces for each byte we're not printing, one separating the halves
+  // if necessary
+  line.append(3 * (16 - n) + (n <= 8), ' ');
+  line.append("  |");
+
+  for (size_t i = 0; i < n; i++) {
+    char c = (p[i] >= 32 && p[i] <= 126 ? static_cast<char>(p[i]) : '.');
+    line.push_back(c);
+  }
+  line.append(16 - n, ' ');
+  line.push_back('|');
+  DCHECK_EQ(line.size(), 78);
+
+  return n;
+}
+
+} // namespace detail
+
+}   // namespace folly
+
+#ifdef FOLLY_DEFINED_DMGL
+# undef FOLLY_DEFINED_DMGL
+# undef DMGL_NO_OPTS
+# undef DMGL_PARAMS
+# undef DMGL_ANSI
+# undef DMGL_JAVA
+# undef DMGL_VERBOSE
+# undef DMGL_TYPES
+# undef DMGL_RET_POSTFIX
+#endif
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/String.h
@@ -0,0 +1,516 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_BASE_STRING_H_
+#define FOLLY_BASE_STRING_H_
+
+#include <exception>
+#include <string>
+#include <boost/type_traits.hpp>
+
+#ifdef _GLIBCXX_SYMVER
+#include <ext/hash_set>
+#include <ext/hash_map>
+#endif
+
+#include <unordered_set>
+#include <unordered_map>
+
+#include "folly/Conv.h"
+#include "folly/Demangle.h"
+#include "folly/FBString.h"
+#include "folly/FBVector.h"
+#include "folly/Portability.h"
+#include "folly/Range.h"
+#include "folly/ScopeGuard.h"
+
+// Compatibility function, to make sure toStdString(s) can be called
+// to convert a std::string or fbstring variable s into type std::string
+// with very little overhead if s was already std::string
+namespace folly {
+
+inline
+std::string toStdString(const folly::fbstring& s) {
+  return std::string(s.data(), s.size());
+}
+
+inline
+const std::string& toStdString(const std::string& s) {
+  return s;
+}
+
+// If called with a temporary, the compiler will select this overload instead
+// of the above, so we don't return a (lvalue) reference to a temporary.
+inline
+std::string&& toStdString(std::string&& s) {
+  return std::move(s);
+}
+
+/**
+ * C-Escape a string, making it suitable for representation as a C string
+ * literal.  Appends the result to the output string.
+ *
+ * Backslashes all occurrences of backslash and double-quote:
+ *   "  ->  \"
+ *   \  ->  \\
+ *
+ * Replaces all non-printable ASCII characters with backslash-octal
+ * representation:
+ *   <ASCII 254> -> \376
+ *
+ * Note that we use backslash-octal instead of backslash-hex because the octal
+ * representation is guaranteed to consume no more than 3 characters; "\3760"
+ * represents two characters, one with value 254, and one with value 48 ('0'),
+ * whereas "\xfe0" represents only one character (with value 4064, which leads
+ * to implementation-defined behavior).
+ */
+template <class String>
+void cEscape(StringPiece str, String& out);
+
+/**
+ * Similar to cEscape above, but returns the escaped string.
+ */
+template <class String>
+String cEscape(StringPiece str) {
+  String out;
+  cEscape(str, out);
+  return out;
+}
+
+/**
+ * C-Unescape a string; the opposite of cEscape above.  Appends the result
+ * to the output string.
+ *
+ * Recognizes the standard C escape sequences:
+ *
+ * \' \" \? \\ \a \b \f \n \r \t \v
+ * \[0-7]+
+ * \x[0-9a-fA-F]+
+ *
+ * In strict mode (default), throws std::invalid_argument if it encounters
+ * an unrecognized escape sequence.  In non-strict mode, it leaves
+ * the escape sequence unchanged.
+ */
+template <class String>
+void cUnescape(StringPiece str, String& out, bool strict = true);
+
+/**
+ * Similar to cUnescape above, but returns the escaped string.
+ */
+template <class String>
+String cUnescape(StringPiece str, bool strict = true) {
+  String out;
+  cUnescape(str, out, strict);
+  return out;
+}
+
+/**
+ * URI-escape a string.  Appends the result to the output string.
+ *
+ * Alphanumeric characters and other characters marked as "unreserved" in RFC
+ * 3986 ( -_.~ ) are left unchanged.  In PATH mode, the forward slash (/) is
+ * also left unchanged.  In QUERY mode, spaces are replaced by '+'.  All other
+ * characters are percent-encoded.
+ */
+enum class UriEscapeMode : unsigned char {
+  // The values are meaningful, see generate_escape_tables.py
+  ALL = 0,
+  QUERY = 1,
+  PATH = 2
+};
+template <class String>
+void uriEscape(StringPiece str,
+               String& out,
+               UriEscapeMode mode = UriEscapeMode::ALL);
+
+/**
+ * Similar to uriEscape above, but returns the escaped string.
+ */
+template <class String>
+String uriEscape(StringPiece str, UriEscapeMode mode = UriEscapeMode::ALL) {
+  String out;
+  uriEscape(str, out, mode);
+  return out;
+}
+
+/**
+ * URI-unescape a string.  Appends the result to the output string.
+ *
+ * In QUERY mode, '+' are replaced by space.  %XX sequences are decoded if
+ * XX is a valid hex sequence, otherwise we throw invalid_argument.
+ */
+template <class String>
+void uriUnescape(StringPiece str,
+                 String& out,
+                 UriEscapeMode mode = UriEscapeMode::ALL);
+
+/**
+ * Similar to uriUnescape above, but returns the unescaped string.
+ */
+template <class String>
+String uriUnescape(StringPiece str, UriEscapeMode mode = UriEscapeMode::ALL) {
+  String out;
+  uriUnescape(str, out, mode);
+  return out;
+}
+
+/**
+ * stringPrintf is much like printf but deposits its result into a
+ * string. Two signatures are supported: the first simply returns the
+ * resulting string, and the second appends the produced characters to
+ * the specified string and returns a reference to it.
+ */
+std::string stringPrintf(const char* format, ...)
+  __attribute__ ((format (printf, 1, 2)));
+
+/** Similar to stringPrintf, with different signiture.
+  */
+void stringPrintf(std::string* out, const char* fmt, ...)
+  __attribute__ ((format (printf, 2, 3)));
+
+std::string& stringAppendf(std::string* output, const char* format, ...)
+  __attribute__ ((format (printf, 2, 3)));
+
+/**
+ * Backslashify a string, that is, replace non-printable characters
+ * with C-style (but NOT C compliant) "\xHH" encoding.  If hex_style
+ * is false, then shorthand notations like "\0" will be used instead
+ * of "\x00" for the most common backslash cases.
+ *
+ * There are two forms, one returning the input string, and one
+ * creating output in the specified output string.
+ *
+ * This is mainly intended for printing to a terminal, so it is not
+ * particularly optimized.
+ *
+ * Do *not* use this in situations where you expect to be able to feed
+ * the string to a C or C++ compiler, as there are nuances with how C
+ * parses such strings that lead to failures.  This is for display
+ * purposed only.  If you want a string you can embed for use in C or
+ * C++, use cEscape instead.  This function is for display purposes
+ * only.
+ */
+template <class String1, class String2>
+void backslashify(const String1& input, String2& output, bool hex_style=false);
+
+template <class String>
+String backslashify(const String& input, bool hex_style=false) {
+  String output;
+  backslashify(input, output, hex_style);
+  return output;
+}
+
+/**
+ * Take a string and "humanify" it -- that is, make it look better.
+ * Since "better" is subjective, caveat emptor.  The basic approach is
+ * to count the number of unprintable characters.  If there are none,
+ * then the output is the input.  If there are relatively few, or if
+ * there is a long "enough" prefix of printable characters, use
+ * backslashify.  If it is mostly binary, then simply hex encode.
+ *
+ * This is an attempt to make a computer smart, and so likely is wrong
+ * most of the time.
+ */
+template <class String1, class String2>
+void humanify(const String1& input, String2& output);
+
+template <class String>
+String humanify(const String& input) {
+  String output;
+  humanify(input, output);
+  return output;
+}
+
+/**
+ * Same functionality as Python's binascii.hexlify.  Returns true
+ * on successful conversion.
+ *
+ * If append_output is true, append data to the output rather than
+ * replace it.
+ */
+template<class InputString, class OutputString>
+bool hexlify(const InputString& input, OutputString& output,
+             bool append=false);
+
+/**
+ * Same functionality as Python's binascii.unhexlify.  Returns true
+ * on successful conversion.
+ */
+template<class InputString, class OutputString>
+bool unhexlify(const InputString& input, OutputString& output);
+
+/*
+ * A pretty-printer for numbers that appends suffixes of units of the
+ * given type.  It prints 4 sig-figs of value with the most
+ * appropriate unit.
+ *
+ * If `addSpace' is true, we put a space between the units suffix and
+ * the value.
+ *
+ * Current types are:
+ *   PRETTY_TIME         - s, ms, us, ns, etc.
+ *   PRETTY_BYTES_METRIC - kB, MB, GB, etc (goes up by 10^3 = 1000 each time)
+ *   PRETTY_BYTES        - kB, MB, GB, etc (goes up by 2^10 = 1024 each time)
+ *   PRETTY_BYTES_IEC    - KiB, MiB, GiB, etc
+ *   PRETTY_UNITS_METRIC - k, M, G, etc (goes up by 10^3 = 1000 each time)
+ *   PRETTY_UNITS_BINARY - k, M, G, etc (goes up by 2^10 = 1024 each time)
+ *   PRETTY_UNITS_BINARY_IEC - Ki, Mi, Gi, etc
+ *
+ * @author Mark Rabkin <mrabkin@fb.com>
+ */
+enum PrettyType {
+  PRETTY_TIME,
+
+  PRETTY_BYTES_METRIC,
+  PRETTY_BYTES_BINARY,
+  PRETTY_BYTES = PRETTY_BYTES_BINARY,
+  PRETTY_BYTES_BINARY_IEC,
+  PRETTY_BYTES_IEC = PRETTY_BYTES_BINARY_IEC,
+
+  PRETTY_UNITS_METRIC,
+  PRETTY_UNITS_BINARY,
+  PRETTY_UNITS_BINARY_IEC,
+
+  PRETTY_NUM_TYPES
+};
+
+std::string prettyPrint(double val, PrettyType, bool addSpace = true);
+
+/**
+ * Write a hex dump of size bytes starting at ptr to out.
+ *
+ * The hex dump is formatted as follows:
+ *
+ * for the string "abcdefghijklmnopqrstuvwxyz\x02"
+00000000  61 62 63 64 65 66 67 68  69 6a 6b 6c 6d 6e 6f 70  |abcdefghijklmnop|
+00000010  71 72 73 74 75 76 77 78  79 7a 02                 |qrstuvwxyz.     |
+ *
+ * that is, we write 16 bytes per line, both as hex bytes and as printable
+ * characters.  Non-printable characters are replaced with '.'
+ * Lines are written to out one by one (one StringPiece at a time) without
+ * delimiters.
+ */
+template <class OutIt>
+void hexDump(const void* ptr, size_t size, OutIt out);
+
+/**
+ * Return the hex dump of size bytes starting at ptr as a string.
+ */
+std::string hexDump(const void* ptr, size_t size);
+
+/**
+ * Return a fbstring containing the description of the given errno value.
+ * Takes care not to overwrite the actual system errno, so calling
+ * errnoStr(errno) is valid.
+ */
+fbstring errnoStr(int err);
+
+/**
+ * Debug string for an exception: include type and what().
+ */
+inline fbstring exceptionStr(const std::exception& e) {
+  return folly::to<fbstring>(demangle(typeid(e)), ": ", e.what());
+}
+
+inline fbstring exceptionStr(std::exception_ptr ep) {
+  try {
+    std::rethrow_exception(ep);
+  } catch (const std::exception& e) {
+    return exceptionStr(e);
+  } catch (...) {
+    return "<unknown exception>";
+  }
+}
+
+/*
+ * Split a string into a list of tokens by delimiter.
+ *
+ * The split interface here supports different output types, selected
+ * at compile time: StringPiece, fbstring, or std::string.  If you are
+ * using a vector to hold the output, it detects the type based on
+ * what your vector contains.  If the output vector is not empty, split
+ * will append to the end of the vector.
+ *
+ * You can also use splitTo() to write the output to an arbitrary
+ * OutputIterator (e.g. std::inserter() on a std::set<>), in which
+ * case you have to tell the function the type.  (Rationale:
+ * OutputIterators don't have a value_type, so we can't detect the
+ * type in splitTo without being told.)
+ *
+ * Examples:
+ *
+ *   std::vector<folly::StringPiece> v;
+ *   folly::split(":", "asd:bsd", v);
+ *
+ *   std::set<StringPiece> s;
+ *   folly::splitTo<StringPiece>(":", "asd:bsd:asd:csd",
+ *    std::inserter(s, s.begin()));
+ *
+ * Split also takes a flag (ignoreEmpty) that indicates whether adjacent
+ * delimiters should be treated as one single separator (ignoring empty tokens)
+ * or not (generating empty tokens).
+ */
+
+template<class Delim, class String, class OutputType>
+void split(const Delim& delimiter,
+           const String& input,
+           std::vector<OutputType>& out,
+           bool ignoreEmpty = false);
+
+template<class Delim, class String, class OutputType>
+void split(const Delim& delimiter,
+           const String& input,
+           folly::fbvector<OutputType>& out,
+           bool ignoreEmpty = false);
+
+template<class OutputValueType, class Delim, class String,
+         class OutputIterator>
+void splitTo(const Delim& delimiter,
+             const String& input,
+             OutputIterator out,
+             bool ignoreEmpty = false);
+
+/*
+ * Split a string into a fixed number of string pieces and/or numeric types
+ * by delimiter. Any numeric type that folly::to<> can convert to from a
+ * string piece is supported as a target. Returns 'true' if the fields were
+ * all successfully populated.
+ *
+ * Examples:
+ *
+ *  folly::StringPiece name, key, value;
+ *  if (folly::split('\t', line, name, key, value))
+ *    ...
+ *
+ *  folly::StringPiece name;
+ *  double value;
+ *  int id;
+ *  if (folly::split('\t', line, name, value, id))
+ *    ...
+ *
+ * The 'exact' template parameter specifies how the function behaves when too
+ * many fields are present in the input string. When 'exact' is set to its
+ * default value of 'true', a call to split will fail if the number of fields in
+ * the input string does not exactly match the number of output parameters
+ * passed. If 'exact' is overridden to 'false', all remaining fields will be
+ * stored, unsplit, in the last field, as shown below:
+ *
+ *  folly::StringPiece x, y.
+ *  if (folly::split<false>(':', "a:b:c", x, y))
+ *    assert(x == "a" && y == "b:c");
+ *
+ * Note that this will likely not work if the last field's target is of numeric
+ * type, in which case folly::to<> will throw an exception.
+ */
+template <class T>
+using IsSplitTargetType = std::integral_constant<bool,
+  std::is_arithmetic<T>::value ||
+  std::is_same<T, StringPiece>::value>;
+
+template<bool exact = true,
+         class Delim,
+         class OutputType,
+         class... OutputTypes>
+typename std::enable_if<IsSplitTargetType<OutputType>::value, bool>::type
+split(const Delim& delimiter,
+      StringPiece input,
+      OutputType& outHead,
+      OutputTypes&... outTail);
+
+/*
+ * Join list of tokens.
+ *
+ * Stores a string representation of tokens in the same order with
+ * deliminer between each element.
+ */
+
+template <class Delim, class Iterator, class String>
+void join(const Delim& delimiter,
+          Iterator begin,
+          Iterator end,
+          String& output);
+
+template <class Delim, class Container, class String>
+void join(const Delim& delimiter,
+          const Container& container,
+          String& output) {
+  join(delimiter, container.begin(), container.end(), output);
+}
+
+template <class Delim, class Value, class String>
+void join(const Delim& delimiter,
+          const std::initializer_list<Value>& values,
+          String& output) {
+  join(delimiter, values.begin(), values.end(), output);
+}
+
+template <class Delim, class Container>
+std::string join(const Delim& delimiter,
+                 const Container& container) {
+  std::string output;
+  join(delimiter, container.begin(), container.end(), output);
+  return output;
+}
+
+template <class Delim, class Value>
+std::string join(const Delim& delimiter,
+                 const std::initializer_list<Value>& values) {
+  std::string output;
+  join(delimiter, values.begin(), values.end(), output);
+  return output;
+}
+
+} // namespace folly
+
+// Hash functions to make std::string usable with e.g. hash_map
+//
+// Handle interaction with different C++ standard libraries, which
+// expect these types to be in different namespaces.
+namespace std {
+
+template <class C>
+struct hash<std::basic_string<C> > : private hash<const C*> {
+  size_t operator()(const std::basic_string<C> & s) const {
+    return hash<const C*>::operator()(s.c_str());
+  }
+};
+
+}
+
+#if defined(_GLIBCXX_SYMVER) && !defined(__BIONIC__)
+namespace __gnu_cxx {
+
+template <class C>
+struct hash<std::basic_string<C> > : private hash<const C*> {
+  size_t operator()(const std::basic_string<C> & s) const {
+    return hash<const C*>::operator()(s.c_str());
+  }
+};
+
+}
+#endif
+
+// Hook into boost's type traits
+namespace boost {
+template <class T>
+struct has_nothrow_constructor<folly::basic_fbstring<T> > : true_type {
+  enum { value = true };
+};
+} // namespace boost
+
+#include "folly/String-inl.h"
+
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/String-inl.h
@@ -0,0 +1,663 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_STRING_INL_H_
+#define FOLLY_STRING_INL_H_
+
+#include <stdexcept>
+#include <iterator>
+
+#ifndef FOLLY_BASE_STRING_H_
+#error This file may only be included from String.h
+#endif
+
+namespace folly {
+
+namespace detail {
+// Map from character code to value of one-character escape sequence
+// ('\n' = 10 maps to 'n'), 'O' if the character should be printed as
+// an octal escape sequence, or 'P' if the character is printable and
+// should be printed as is.
+extern const char cEscapeTable[];
+}  // namespace detail
+
+template <class String>
+void cEscape(StringPiece str, String& out) {
+  char esc[4];
+  esc[0] = '\\';
+  out.reserve(out.size() + str.size());
+  auto p = str.begin();
+  auto last = p;  // last regular character
+  // We advance over runs of regular characters (printable, not double-quote or
+  // backslash) and copy them in one go; this is faster than calling push_back
+  // repeatedly.
+  while (p != str.end()) {
+    char c = *p;
+    unsigned char v = static_cast<unsigned char>(c);
+    char e = detail::cEscapeTable[v];
+    if (e == 'P') {  // printable
+      ++p;
+    } else if (e == 'O') {  // octal
+      out.append(&*last, p - last);
+      esc[1] = '0' + ((v >> 6) & 7);
+      esc[2] = '0' + ((v >> 3) & 7);
+      esc[3] = '0' + (v & 7);
+      out.append(esc, 4);
+      ++p;
+      last = p;
+    } else {  // special 1-character escape
+      out.append(&*last, p - last);
+      esc[1] = e;
+      out.append(esc, 2);
+      ++p;
+      last = p;
+    }
+  }
+  out.append(&*last, p - last);
+}
+
+namespace detail {
+// Map from the character code of the character following a backslash to
+// the unescaped character if a valid one-character escape sequence
+// ('n' maps to 10 = '\n'), 'O' if this is the first character of an
+// octal escape sequence, 'X' if this is the first character of a
+// hexadecimal escape sequence, or 'I' if this escape sequence is invalid.
+extern const char cUnescapeTable[];
+
+// Map from the character code to the hex value, or 16 if invalid hex char.
+extern const unsigned char hexTable[];
+}  // namespace detail
+
+template <class String>
+void cUnescape(StringPiece str, String& out, bool strict) {
+  out.reserve(out.size() + str.size());
+  auto p = str.begin();
+  auto last = p;  // last regular character (not part of an escape sequence)
+  // We advance over runs of regular characters (not backslash) and copy them
+  // in one go; this is faster than calling push_back repeatedly.
+  while (p != str.end()) {
+    char c = *p;
+    if (c != '\\') {  // normal case
+      ++p;
+      continue;
+    }
+    out.append(&*last, p - last);
+    if (p == str.end()) {  // backslash at end of string
+      if (strict) {
+        throw std::invalid_argument("incomplete escape sequence");
+      }
+      out.push_back('\\');
+      last = p;
+      continue;
+    }
+    ++p;
+    char e = detail::cUnescapeTable[static_cast<unsigned char>(*p)];
+    if (e == 'O') {  // octal
+      unsigned char val = 0;
+      for (int i = 0; i < 3 && p != str.end() && *p >= '0' && *p <= '7';
+           ++i, ++p) {
+        val = (val << 3) | (*p - '0');
+      }
+      out.push_back(val);
+      last = p;
+    } else if (e == 'X') {  // hex
+      ++p;
+      if (p == str.end()) {  // \x at end of string
+        if (strict) {
+          throw std::invalid_argument("incomplete hex escape sequence");
+        }
+        out.append("\\x");
+        last = p;
+        continue;
+      }
+      unsigned char val = 0;
+      unsigned char h;
+      for (; (p != str.end() &&
+              (h = detail::hexTable[static_cast<unsigned char>(*p)]) < 16);
+           ++p) {
+        val = (val << 4) | h;
+      }
+      out.push_back(val);
+      last = p;
+    } else if (e == 'I') {  // invalid
+      if (strict) {
+        throw std::invalid_argument("invalid escape sequence");
+      }
+      out.push_back('\\');
+      out.push_back(*p);
+      ++p;
+      last = p;
+    } else {  // standard escape sequence, \' etc
+      out.push_back(e);
+      ++p;
+      last = p;
+    }
+  }
+  out.append(&*last, p - last);
+}
+
+namespace detail {
+// Map from character code to escape mode:
+// 0 = pass through
+// 1 = unused
+// 2 = pass through in PATH mode
+// 3 = space, replace with '+' in QUERY mode
+// 4 = percent-encode
+extern const unsigned char uriEscapeTable[];
+}  // namespace detail
+
+template <class String>
+void uriEscape(StringPiece str, String& out, UriEscapeMode mode) {
+  static const char hexValues[] = "0123456789abcdef";
+  char esc[3];
+  esc[0] = '%';
+  // Preallocate assuming that 25% of the input string will be escaped
+  out.reserve(out.size() + str.size() + 3 * (str.size() / 4));
+  auto p = str.begin();
+  auto last = p;  // last regular character
+  // We advance over runs of passthrough characters and copy them in one go;
+  // this is faster than calling push_back repeatedly.
+  unsigned char minEncode = static_cast<unsigned char>(mode);
+  while (p != str.end()) {
+    char c = *p;
+    unsigned char v = static_cast<unsigned char>(c);
+    unsigned char discriminator = detail::uriEscapeTable[v];
+    if (LIKELY(discriminator <= minEncode)) {
+      ++p;
+    } else if (mode == UriEscapeMode::QUERY && discriminator == 3) {
+      out.append(&*last, p - last);
+      out.push_back('+');
+      ++p;
+      last = p;
+    } else {
+      out.append(&*last, p - last);
+      esc[1] = hexValues[v >> 4];
+      esc[2] = hexValues[v & 0x0f];
+      out.append(esc, 3);
+      ++p;
+      last = p;
+    }
+  }
+  out.append(&*last, p - last);
+}
+
+template <class String>
+void uriUnescape(StringPiece str, String& out, UriEscapeMode mode) {
+  out.reserve(out.size() + str.size());
+  auto p = str.begin();
+  auto last = p;
+  // We advance over runs of passthrough characters and copy them in one go;
+  // this is faster than calling push_back repeatedly.
+  while (p != str.end()) {
+    char c = *p;
+    unsigned char v = static_cast<unsigned char>(v);
+    switch (c) {
+    case '%':
+      {
+        if (UNLIKELY(std::distance(p, str.end()) < 3)) {
+          throw std::invalid_argument("incomplete percent encode sequence");
+        }
+        auto h1 = detail::hexTable[static_cast<unsigned char>(p[1])];
+        auto h2 = detail::hexTable[static_cast<unsigned char>(p[2])];
+        if (UNLIKELY(h1 == 16 || h2 == 16)) {
+          throw std::invalid_argument("invalid percent encode sequence");
+        }
+        out.append(&*last, p - last);
+        out.push_back((h1 << 4) | h2);
+        p += 3;
+        last = p;
+        break;
+      }
+    case '+':
+      if (mode == UriEscapeMode::QUERY) {
+        out.append(&*last, p - last);
+        out.push_back(' ');
+        ++p;
+        last = p;
+        break;
+      }
+      // else fallthrough
+    default:
+      ++p;
+      break;
+    }
+  }
+  out.append(&*last, p - last);
+}
+
+namespace detail {
+
+/*
+ * The following functions are type-overloaded helpers for
+ * internalSplit().
+ */
+inline size_t delimSize(char)          { return 1; }
+inline size_t delimSize(StringPiece s) { return s.size(); }
+inline bool atDelim(const char* s, char c) {
+ return *s == c;
+}
+inline bool atDelim(const char* s, StringPiece sp) {
+  return !std::memcmp(s, sp.start(), sp.size());
+}
+
+// These are used to short-circuit internalSplit() in the case of
+// 1-character strings.
+inline char delimFront(char c) {
+  // This one exists only for compile-time; it should never be called.
+  std::abort();
+  return c;
+}
+inline char delimFront(StringPiece s) {
+  assert(!s.empty() && s.start() != nullptr);
+  return *s.start();
+}
+
+/*
+ * These output conversion templates allow us to support multiple
+ * output string types, even when we are using an arbitrary
+ * OutputIterator.
+ */
+template<class OutStringT> struct OutputConverter {};
+
+template<> struct OutputConverter<std::string> {
+  std::string operator()(StringPiece sp) const {
+    return sp.toString();
+  }
+};
+
+template<> struct OutputConverter<fbstring> {
+  fbstring operator()(StringPiece sp) const {
+    return sp.toFbstring();
+  }
+};
+
+template<> struct OutputConverter<StringPiece> {
+  StringPiece operator()(StringPiece sp) const { return sp; }
+};
+
+/*
+ * Shared implementation for all the split() overloads.
+ *
+ * This uses some external helpers that are overloaded to let this
+ * algorithm be more performant if the deliminator is a single
+ * character instead of a whole string.
+ *
+ * @param ignoreEmpty iff true, don't copy empty segments to output
+ */
+template<class OutStringT, class DelimT, class OutputIterator>
+void internalSplit(DelimT delim, StringPiece sp, OutputIterator out,
+    bool ignoreEmpty) {
+  assert(sp.empty() || sp.start() != nullptr);
+
+  const char* s = sp.start();
+  const size_t strSize = sp.size();
+  const size_t dSize = delimSize(delim);
+
+  OutputConverter<OutStringT> conv;
+
+  if (dSize > strSize || dSize == 0) {
+    if (!ignoreEmpty || strSize > 0) {
+      *out++ = conv(sp);
+    }
+    return;
+  }
+  if (boost::is_same<DelimT,StringPiece>::value && dSize == 1) {
+    // Call the char version because it is significantly faster.
+    return internalSplit<OutStringT>(delimFront(delim), sp, out,
+      ignoreEmpty);
+  }
+
+  int tokenStartPos = 0;
+  int tokenSize = 0;
+  for (int i = 0; i <= strSize - dSize; ++i) {
+    if (atDelim(&s[i], delim)) {
+      if (!ignoreEmpty || tokenSize > 0) {
+        *out++ = conv(StringPiece(&s[tokenStartPos], tokenSize));
+      }
+
+      tokenStartPos = i + dSize;
+      tokenSize = 0;
+      i += dSize - 1;
+    } else {
+      ++tokenSize;
+    }
+  }
+
+  if (!ignoreEmpty || tokenSize > 0) {
+    tokenSize = strSize - tokenStartPos;
+    *out++ = conv(StringPiece(&s[tokenStartPos], tokenSize));
+  }
+}
+
+template<class String> StringPiece prepareDelim(const String& s) {
+  return StringPiece(s);
+}
+inline char prepareDelim(char c) { return c; }
+
+template <class Dst>
+struct convertTo {
+  template <class Src>
+  static Dst from(const Src& src) { return folly::to<Dst>(src); }
+  static Dst from(const Dst& src) { return src; }
+};
+
+template<bool exact,
+         class Delim,
+         class OutputType>
+typename std::enable_if<IsSplitTargetType<OutputType>::value, bool>::type
+splitFixed(const Delim& delimiter,
+           StringPiece input,
+           OutputType& out) {
+  if (exact && UNLIKELY(std::string::npos != input.find(delimiter))) {
+    return false;
+  }
+  out = convertTo<OutputType>::from(input);
+  return true;
+}
+
+template<bool exact,
+         class Delim,
+         class OutputType,
+         class... OutputTypes>
+typename std::enable_if<IsSplitTargetType<OutputType>::value, bool>::type
+splitFixed(const Delim& delimiter,
+           StringPiece input,
+           OutputType& outHead,
+           OutputTypes&... outTail) {
+  size_t cut = input.find(delimiter);
+  if (UNLIKELY(cut == std::string::npos)) {
+    return false;
+  }
+  StringPiece head(input.begin(), input.begin() + cut);
+  StringPiece tail(input.begin() + cut + detail::delimSize(delimiter),
+                   input.end());
+  if (LIKELY(splitFixed<exact>(delimiter, tail, outTail...))) {
+    outHead = convertTo<OutputType>::from(head);
+    return true;
+  }
+  return false;
+}
+
+}
+
+//////////////////////////////////////////////////////////////////////
+
+template<class Delim, class String, class OutputType>
+void split(const Delim& delimiter,
+           const String& input,
+           std::vector<OutputType>& out,
+           bool ignoreEmpty) {
+  detail::internalSplit<OutputType>(
+    detail::prepareDelim(delimiter),
+    StringPiece(input),
+    std::back_inserter(out),
+    ignoreEmpty);
+}
+
+template<class Delim, class String, class OutputType>
+void split(const Delim& delimiter,
+           const String& input,
+           fbvector<OutputType>& out,
+           bool ignoreEmpty) {
+  detail::internalSplit<OutputType>(
+    detail::prepareDelim(delimiter),
+    StringPiece(input),
+    std::back_inserter(out),
+    ignoreEmpty);
+}
+
+template<class OutputValueType, class Delim, class String,
+         class OutputIterator>
+void splitTo(const Delim& delimiter,
+             const String& input,
+             OutputIterator out,
+             bool ignoreEmpty) {
+  detail::internalSplit<OutputValueType>(
+    detail::prepareDelim(delimiter),
+    StringPiece(input),
+    out,
+    ignoreEmpty);
+}
+
+template<bool exact,
+         class Delim,
+         class OutputType,
+         class... OutputTypes>
+typename std::enable_if<IsSplitTargetType<OutputType>::value, bool>::type
+split(const Delim& delimiter,
+      StringPiece input,
+      OutputType& outHead,
+      OutputTypes&... outTail) {
+  return detail::splitFixed<exact>(
+    detail::prepareDelim(delimiter),
+    input,
+    outHead,
+    outTail...);
+}
+
+namespace detail {
+
+template <class Iterator>
+struct IsStringContainerIterator :
+  IsSomeString<typename std::iterator_traits<Iterator>::value_type> {
+};
+
+template <class Delim, class Iterator, class String>
+void internalJoinAppend(Delim delimiter,
+                        Iterator begin,
+                        Iterator end,
+                        String& output) {
+  assert(begin != end);
+  if (std::is_same<Delim, StringPiece>::value &&
+      delimSize(delimiter) == 1) {
+    internalJoinAppend(delimFront(delimiter), begin, end, output);
+    return;
+  }
+  toAppend(*begin, &output);
+  while (++begin != end) {
+    toAppend(delimiter, *begin, &output);
+  }
+}
+
+template <class Delim, class Iterator, class String>
+typename std::enable_if<IsStringContainerIterator<Iterator>::value>::type
+internalJoin(Delim delimiter,
+             Iterator begin,
+             Iterator end,
+             String& output) {
+  output.clear();
+  if (begin == end) {
+    return;
+  }
+  const size_t dsize = delimSize(delimiter);
+  Iterator it = begin;
+  size_t size = it->size();
+  while (++it != end) {
+    size += dsize + it->size();
+  }
+  output.reserve(size);
+  internalJoinAppend(delimiter, begin, end, output);
+}
+
+template <class Delim, class Iterator, class String>
+typename std::enable_if<!IsStringContainerIterator<Iterator>::value>::type
+internalJoin(Delim delimiter,
+             Iterator begin,
+             Iterator end,
+             String& output) {
+  output.clear();
+  if (begin == end) {
+    return;
+  }
+  internalJoinAppend(delimiter, begin, end, output);
+}
+
+}  // namespace detail
+
+template <class Delim, class Iterator, class String>
+void join(const Delim& delimiter,
+          Iterator begin,
+          Iterator end,
+          String& output) {
+  detail::internalJoin(
+    detail::prepareDelim(delimiter),
+    begin,
+    end,
+    output);
+}
+
+template <class String1, class String2>
+void backslashify(const String1& input, String2& output, bool hex_style) {
+  static const char hexValues[] = "0123456789abcdef";
+  output.clear();
+  output.reserve(3 * input.size());
+  for (unsigned char c : input) {
+    // less than space or greater than '~' are considered unprintable
+    if (c < 0x20 || c > 0x7e || c == '\\') {
+      bool hex_append = false;
+      output.push_back('\\');
+      if (hex_style) {
+        hex_append = true;
+      } else {
+        if (c == '\r') output += 'r';
+        else if (c == '\n') output += 'n';
+        else if (c == '\t') output += 't';
+        else if (c == '\a') output += 'a';
+        else if (c == '\b') output += 'b';
+        else if (c == '\0') output += '0';
+        else if (c == '\\') output += '\\';
+        else {
+          hex_append = true;
+        }
+      }
+      if (hex_append) {
+        output.push_back('x');
+        output.push_back(hexValues[(c >> 4) & 0xf]);
+        output.push_back(hexValues[c & 0xf]);
+      }
+    } else {
+      output += c;
+    }
+  }
+}
+
+template <class String1, class String2>
+void humanify(const String1& input, String2& output) {
+  int numUnprintable = 0;
+  int numPrintablePrefix = 0;
+  for (unsigned char c : input) {
+    if (c < 0x20 || c > 0x7e || c == '\\') {
+      ++numUnprintable;
+    }
+    if (numUnprintable == 0) {
+      ++numPrintablePrefix;
+    }
+  }
+
+  // hexlify doubles a string's size; backslashify can potentially
+  // explode it by 4x.  Now, the printable range of the ascii
+  // "spectrum" is around 95 out of 256 values, so a "random" binary
+  // string should be around 60% unprintable.  We use a 50% hueristic
+  // here, so if a string is 60% unprintable, then we just use hex
+  // output.  Otherwise we backslash.
+  //
+  // UTF8 is completely ignored; as a result, utf8 characters will
+  // likely be \x escaped (since most common glyphs fit in two bytes).
+  // This is a tradeoff of complexity/speed instead of a convenience
+  // that likely would rarely matter.  Moreover, this function is more
+  // about displaying underlying bytes, not about displaying glyphs
+  // from languages.
+  if (numUnprintable == 0) {
+    output = input;
+  } else if (5 * numUnprintable >= 3 * input.size()) {
+    // However!  If we have a "meaningful" prefix of printable
+    // characters, say 20% of the string, we backslashify under the
+    // assumption viewing the prefix as ascii is worth blowing the
+    // output size up a bit.
+    if (5 * numPrintablePrefix >= input.size()) {
+      backslashify(input, output);
+    } else {
+      output = "0x";
+      hexlify(input, output, true /* append output */);
+    }
+  } else {
+    backslashify(input, output);
+  }
+}
+
+template<class InputString, class OutputString>
+bool hexlify(const InputString& input, OutputString& output,
+             bool append_output) {
+  if (!append_output) output.clear();
+
+  static char hexValues[] = "0123456789abcdef";
+  int j = output.size();
+  output.resize(2 * input.size() + output.size());
+  for (int i = 0; i < input.size(); ++i) {
+    int ch = input[i];
+    output[j++] = hexValues[(ch >> 4) & 0xf];
+    output[j++] = hexValues[ch & 0xf];
+  }
+  return true;
+}
+
+template<class InputString, class OutputString>
+bool unhexlify(const InputString& input, OutputString& output) {
+  if (input.size() % 2 != 0) {
+    return false;
+  }
+  output.resize(input.size() / 2);
+  int j = 0;
+  auto unhex = [](char c) -> int {
+    return c >= '0' && c <= '9' ? c - '0' :
+           c >= 'A' && c <= 'F' ? c - 'A' + 10 :
+           c >= 'a' && c <= 'f' ? c - 'a' + 10 :
+           -1;
+  };
+
+  for (int i = 0; i < input.size(); i += 2) {
+    int highBits = unhex(input[i]);
+    int lowBits = unhex(input[i + 1]);
+    if (highBits < 0 || lowBits < 0) {
+      return false;
+    }
+    output[j++] = (highBits << 4) + lowBits;
+  }
+  return true;
+}
+
+namespace detail {
+/**
+ * Hex-dump at most 16 bytes starting at offset from a memory area of size
+ * bytes.  Return the number of bytes actually dumped.
+ */
+size_t hexDumpLine(const void* ptr, size_t offset, size_t size,
+                   std::string& line);
+}  // namespace detail
+
+template <class OutIt>
+void hexDump(const void* ptr, size_t size, OutIt out) {
+  size_t offset = 0;
+  std::string line;
+  while (offset < size) {
+    offset += detail::hexDumpLine(ptr, offset, size, line);
+    *out++ = line;
+  }
+}
+
+}  // namespace folly
+
+#endif /* FOLLY_STRING_INL_H_ */
--- /dev/null
+++ b/hphp/submodules/folly/folly/Subprocess.cpp
@@ -0,0 +1,782 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Subprocess.h"
+
+#if __linux__
+#include <sys/prctl.h>
+#endif
+#include <fcntl.h>
+#include <poll.h>
+#include <unistd.h>
+
+#include <array>
+#include <algorithm>
+#include <system_error>
+
+#include <boost/container/flat_set.hpp>
+#include <boost/range/adaptors.hpp>
+
+#include <glog/logging.h>
+
+#include "folly/Conv.h"
+#include "folly/Exception.h"
+#include "folly/FileUtil.h"
+#include "folly/ScopeGuard.h"
+#include "folly/String.h"
+#include "folly/io/Cursor.h"
+
+extern char** environ;
+
+constexpr int kExecFailure = 127;
+constexpr int kChildFailure = 126;
+
+namespace folly {
+
+ProcessReturnCode::State ProcessReturnCode::state() const {
+  if (rawStatus_ == RV_NOT_STARTED) return NOT_STARTED;
+  if (rawStatus_ == RV_RUNNING) return RUNNING;
+  if (WIFEXITED(rawStatus_)) return EXITED;
+  if (WIFSIGNALED(rawStatus_)) return KILLED;
+  throw std::runtime_error(to<std::string>(
+      "Invalid ProcessReturnCode: ", rawStatus_));
+}
+
+void ProcessReturnCode::enforce(State expected) const {
+  State s = state();
+  if (s != expected) {
+    throw std::logic_error(to<std::string>("Invalid state ", s,
+                                           " expected ", expected));
+  }
+}
+
+int ProcessReturnCode::exitStatus() const {
+  enforce(EXITED);
+  return WEXITSTATUS(rawStatus_);
+}
+
+int ProcessReturnCode::killSignal() const {
+  enforce(KILLED);
+  return WTERMSIG(rawStatus_);
+}
+
+bool ProcessReturnCode::coreDumped() const {
+  enforce(KILLED);
+  return WCOREDUMP(rawStatus_);
+}
+
+std::string ProcessReturnCode::str() const {
+  switch (state()) {
+  case NOT_STARTED:
+    return "not started";
+  case RUNNING:
+    return "running";
+  case EXITED:
+    return to<std::string>("exited with status ", exitStatus());
+  case KILLED:
+    return to<std::string>("killed by signal ", killSignal(),
+                           (coreDumped() ? " (core dumped)" : ""));
+  }
+  CHECK(false);  // unreached
+}
+
+CalledProcessError::CalledProcessError(ProcessReturnCode rc)
+  : returnCode_(rc),
+    what_(returnCode_.str()) {
+}
+
+SubprocessSpawnError::SubprocessSpawnError(const char* executable,
+                                           int errCode,
+                                           int errnoValue)
+  : errnoValue_(errnoValue),
+    what_(to<std::string>(errCode == kExecFailure ?
+                            "failed to execute " :
+                            "error preparing to execute ",
+                          executable, ": ", errnoStr(errnoValue))) {
+}
+
+namespace {
+
+// Copy pointers to the given strings in a format suitable for posix_spawn
+std::unique_ptr<const char*[]> cloneStrings(const std::vector<std::string>& s) {
+  std::unique_ptr<const char*[]> d(new const char*[s.size() + 1]);
+  for (int i = 0; i < s.size(); i++) {
+    d[i] = s[i].c_str();
+  }
+  d[s.size()] = nullptr;
+  return d;
+}
+
+// Check a wait() status, throw on non-successful
+void checkStatus(ProcessReturnCode returnCode) {
+  if (returnCode.state() != ProcessReturnCode::EXITED ||
+      returnCode.exitStatus() != 0) {
+    throw CalledProcessError(returnCode);
+  }
+}
+
+}  // namespace
+
+Subprocess::Options& Subprocess::Options::fd(int fd, int action) {
+  if (action == Subprocess::PIPE) {
+    if (fd == 0) {
+      action = Subprocess::PIPE_IN;
+    } else if (fd == 1 || fd == 2) {
+      action = Subprocess::PIPE_OUT;
+    } else {
+      throw std::invalid_argument(
+          to<std::string>("Only fds 0, 1, 2 are valid for action=PIPE: ", fd));
+    }
+  }
+  fdActions_[fd] = action;
+  return *this;
+}
+
+Subprocess::Subprocess(
+    const std::vector<std::string>& argv,
+    const Options& options,
+    const char* executable,
+    const std::vector<std::string>* env)
+  : pid_(-1),
+    returnCode_(RV_NOT_STARTED) {
+  if (argv.empty()) {
+    throw std::invalid_argument("argv must not be empty");
+  }
+  if (!executable) executable = argv[0].c_str();
+  spawn(cloneStrings(argv), executable, options, env);
+}
+
+Subprocess::Subprocess(
+    const std::string& cmd,
+    const Options& options,
+    const std::vector<std::string>* env)
+  : pid_(-1),
+    returnCode_(RV_NOT_STARTED) {
+  if (options.usePath_) {
+    throw std::invalid_argument("usePath() not allowed when running in shell");
+  }
+  const char* shell = getenv("SHELL");
+  if (!shell) {
+    shell = "/bin/sh";
+  }
+
+  std::unique_ptr<const char*[]> argv(new const char*[4]);
+  argv[0] = shell;
+  argv[1] = "-c";
+  argv[2] = cmd.c_str();
+  argv[3] = nullptr;
+  spawn(std::move(argv), shell, options, env);
+}
+
+Subprocess::~Subprocess() {
+  CHECK_NE(returnCode_.state(), ProcessReturnCode::RUNNING)
+    << "Subprocess destroyed without reaping child";
+  closeAll();
+}
+
+namespace {
+void closeChecked(int fd) {
+  checkUnixError(::close(fd), "close");
+}
+
+struct ChildErrorInfo {
+  int errCode;
+  int errnoValue;
+};
+
+void childError(int errFd, int errCode, int errnoValue) FOLLY_NORETURN;
+void childError(int errFd, int errCode, int errnoValue) {
+  ChildErrorInfo info = {errCode, errnoValue};
+  // Write the error information over the pipe to our parent process.
+  // We can't really do anything else if this write call fails.
+  writeNoInt(errFd, &info, sizeof(info));
+  // exit
+  _exit(errCode);
+}
+
+}  // namespace
+
+void Subprocess::closeAll() {
+  for (auto& p : pipes_) {
+    closeChecked(p.parentFd);
+  }
+  pipes_.clear();
+}
+
+void Subprocess::setAllNonBlocking() {
+  for (auto& p : pipes_) {
+    int fd = p.parentFd;
+    int flags = ::fcntl(fd, F_GETFL);
+    checkUnixError(flags, "fcntl");
+    int r = ::fcntl(fd, F_SETFL, flags | O_NONBLOCK);
+    checkUnixError(r, "fcntl");
+  }
+}
+
+void Subprocess::spawn(
+    std::unique_ptr<const char*[]> argv,
+    const char* executable,
+    const Options& optionsIn,
+    const std::vector<std::string>* env) {
+  if (optionsIn.usePath_ && env) {
+    throw std::invalid_argument(
+        "usePath() not allowed when overriding environment");
+  }
+
+  // Make a copy, we'll mutate options
+  Options options(optionsIn);
+
+  // On error, close all of the pipes_
+  auto pipesGuard = makeGuard([&] {
+    for (auto& p : this->pipes_) {
+      CHECK_ERR(::close(p.parentFd));
+    }
+  });
+
+  // Create a pipe to use to receive error information from the child,
+  // in case it fails before calling exec()
+  int errFds[2];
+  int r = ::pipe(errFds);
+  checkUnixError(r, "pipe");
+  SCOPE_EXIT {
+    CHECK_ERR(::close(errFds[0]));
+    if (errFds[1] >= 0) {
+      CHECK_ERR(::close(errFds[1]));
+    }
+  };
+  // Ask the child to close the read end of the error pipe.
+  options.fdActions_[errFds[0]] = CLOSE;
+  // Set the close-on-exec flag on the write side of the pipe.
+  // This way the pipe will be closed automatically in the child if execve()
+  // succeeds.  If the exec fails the child can write error information to the
+  // pipe.
+  r = fcntl(errFds[1], F_SETFD, FD_CLOEXEC);
+  checkUnixError(r, "set FD_CLOEXEC");
+
+  // Perform the actual work of setting up pipes then forking and
+  // executing the child.
+  spawnInternal(std::move(argv), executable, options, env, errFds[1]);
+
+  // After spawnInternal() returns the child is alive.  We have to be very
+  // careful about throwing after this point.  We are inside the constructor,
+  // so if we throw the Subprocess object will have never existed, and the
+  // destructor will never be called.
+  //
+  // We should only throw if we got an error via the errFd, and we know the
+  // child has exited and can be immediately waited for.  In all other cases,
+  // we have no way of cleaning up the child.
+
+  // Close writable side of the errFd pipe in the parent process
+  CHECK_ERR(::close(errFds[1]));
+  errFds[1] = -1;
+
+  // Read from the errFd pipe, to tell if the child ran into any errors before
+  // calling exec()
+  readChildErrorPipe(errFds[0], executable);
+
+  // We have fully succeeded now, so release the guard on pipes_
+  pipesGuard.dismiss();
+}
+
+void Subprocess::spawnInternal(
+    std::unique_ptr<const char*[]> argv,
+    const char* executable,
+    Options& options,
+    const std::vector<std::string>* env,
+    int errFd) {
+  // Parent work, pre-fork: create pipes
+  std::vector<int> childFds;
+  // Close all of the childFds as we leave this scope
+  SCOPE_EXIT {
+    // These are only pipes, closing them shouldn't fail
+    for (int cfd : childFds) {
+      CHECK_ERR(::close(cfd));
+    }
+  };
+
+  int r;
+  for (auto& p : options.fdActions_) {
+    if (p.second == PIPE_IN || p.second == PIPE_OUT) {
+      int fds[2];
+      r = ::pipe(fds);
+      checkUnixError(r, "pipe");
+      PipeInfo pinfo;
+      pinfo.direction = p.second;
+      int cfd;
+      if (p.second == PIPE_IN) {
+        // Child gets reading end
+        pinfo.parentFd = fds[1];
+        cfd = fds[0];
+      } else {
+        pinfo.parentFd = fds[0];
+        cfd = fds[1];
+      }
+      p.second = cfd;  // ensure it gets dup2()ed
+      pinfo.childFd = p.first;
+      childFds.push_back(cfd);
+      pipes_.push_back(pinfo);
+    }
+  }
+
+  // This should already be sorted, as options.fdActions_ is
+  DCHECK(std::is_sorted(pipes_.begin(), pipes_.end()));
+
+  // Note that the const casts below are legit, per
+  // http://pubs.opengroup.org/onlinepubs/009695399/functions/exec.html
+
+  char** argVec = const_cast<char**>(argv.get());
+
+  // Set up environment
+  std::unique_ptr<const char*[]> envHolder;
+  char** envVec;
+  if (env) {
+    envHolder = cloneStrings(*env);
+    envVec = const_cast<char**>(envHolder.get());
+  } else {
+    envVec = environ;
+  }
+
+  // Block all signals around vfork; see http://ewontfix.com/7/.
+  //
+  // As the child may run in the same address space as the parent until
+  // the actual execve() system call, any (custom) signal handlers that
+  // the parent has might alter parent's memory if invoked in the child,
+  // with undefined results.  So we block all signals in the parent before
+  // vfork(), which will cause them to be blocked in the child as well (we
+  // rely on the fact that Linux, just like all sane implementations, only
+  // clones the calling thread).  Then, in the child, we reset all signals
+  // to their default dispositions (while still blocked), and unblock them
+  // (so the exec()ed process inherits the parent's signal mask)
+  //
+  // The parent also unblocks all signals as soon as vfork() returns.
+  sigset_t allBlocked;
+  r = sigfillset(&allBlocked);
+  checkUnixError(r, "sigfillset");
+  sigset_t oldSignals;
+
+  r = pthread_sigmask(SIG_SETMASK, &allBlocked, &oldSignals);
+  checkPosixError(r, "pthread_sigmask");
+  SCOPE_EXIT {
+    // Restore signal mask
+    r = pthread_sigmask(SIG_SETMASK, &oldSignals, nullptr);
+    CHECK_EQ(r, 0) << "pthread_sigmask: " << errnoStr(r);  // shouldn't fail
+  };
+
+  pid_t pid = vfork();
+  if (pid == 0) {
+    int errnoValue = prepareChild(options, &oldSignals);
+    if (errnoValue != 0) {
+      childError(errFd, kChildFailure, errnoValue);
+    }
+
+    errnoValue = runChild(executable, argVec, envVec, options);
+    // If we get here, exec() failed.
+    childError(errFd, kExecFailure, errnoValue);
+  }
+  // In parent.  Make sure vfork() succeeded.
+  checkUnixError(pid, errno, "vfork");
+
+  // Child is alive.  We have to be very careful about throwing after this
+  // point.  We are inside the constructor, so if we throw the Subprocess
+  // object will have never existed, and the destructor will never be called.
+  //
+  // We should only throw if we got an error via the errFd, and we know the
+  // child has exited and can be immediately waited for.  In all other cases,
+  // we have no way of cleaning up the child.
+  pid_ = pid;
+  returnCode_ = ProcessReturnCode(RV_RUNNING);
+}
+
+int Subprocess::prepareChild(const Options& options,
+                             const sigset_t* sigmask) const {
+  // While all signals are blocked, we must reset their
+  // dispositions to default.
+  for (int sig = 1; sig < NSIG; ++sig) {
+    ::signal(sig, SIG_DFL);
+  }
+  // Unblock signals; restore signal mask.
+  int r = pthread_sigmask(SIG_SETMASK, sigmask, nullptr);
+  if (r != 0) {
+    return r;  // pthread_sigmask() returns an errno value
+  }
+
+  // Close parent's ends of all pipes
+  for (auto& p : pipes_) {
+    r = ::close(p.parentFd);
+    if (r == -1) {
+      return errno;
+    }
+  }
+
+  // Close all fds that we're supposed to close.
+  // Note that we're ignoring errors here, in case some of these
+  // fds were set to close on exec.
+  for (auto& p : options.fdActions_) {
+    if (p.second == CLOSE) {
+      ::close(p.first);
+    } else {
+      r = ::dup2(p.second, p.first);
+      if (r == -1) {
+        return errno;
+      }
+    }
+  }
+
+  // If requested, close all other file descriptors.  Don't close
+  // any fds in options.fdActions_, and don't touch stdin, stdout, stderr.
+  // Ignore errors.
+  if (options.closeOtherFds_) {
+    for (int fd = getdtablesize() - 1; fd >= 3; --fd) {
+      if (options.fdActions_.count(fd) == 0) {
+        ::close(fd);
+      }
+    }
+  }
+
+#if __linux__
+  // Opt to receive signal on parent death, if requested
+  if (options.parentDeathSignal_ != 0) {
+    r = prctl(PR_SET_PDEATHSIG, options.parentDeathSignal_, 0, 0, 0);
+    if (r == -1) {
+      return errno;
+    }
+  }
+#endif
+
+  return 0;
+}
+
+int Subprocess::runChild(const char* executable,
+                         char** argv, char** env,
+                         const Options& options) const {
+  // Now, finally, exec.
+  int r;
+  if (options.usePath_) {
+    ::execvp(executable, argv);
+  } else {
+    ::execve(executable, argv, env);
+  }
+  return errno;
+}
+
+void Subprocess::readChildErrorPipe(int pfd, const char* executable) {
+  ChildErrorInfo info;
+  auto rc = readNoInt(pfd, &info, sizeof(info));
+  if (rc == 0) {
+    // No data means the child executed successfully, and the pipe
+    // was closed due to the close-on-exec flag being set.
+    return;
+  } else if (rc != sizeof(ChildErrorInfo)) {
+    // An error occurred trying to read from the pipe, or we got a partial read.
+    // Neither of these cases should really occur in practice.
+    //
+    // We can't get any error data from the child in this case, and we don't
+    // know if it is successfully running or not.  All we can do is to return
+    // normally, as if the child executed successfully.  If something bad
+    // happened the caller should at least get a non-normal exit status from
+    // the child.
+    LOG(ERROR) << "unexpected error trying to read from child error pipe " <<
+      "rc=" << rc << ", errno=" << errno;
+    return;
+  }
+
+  // We got error data from the child.  The child should exit immediately in
+  // this case, so wait on it to clean up.
+  wait();
+
+  // Throw to signal the error
+  throw SubprocessSpawnError(executable, info.errCode, info.errnoValue);
+}
+
+ProcessReturnCode Subprocess::poll() {
+  returnCode_.enforce(ProcessReturnCode::RUNNING);
+  DCHECK_GT(pid_, 0);
+  int status;
+  pid_t found = ::waitpid(pid_, &status, WNOHANG);
+  checkUnixError(found, "waitpid");
+  if (found != 0) {
+    returnCode_ = ProcessReturnCode(status);
+    pid_ = -1;
+  }
+  return returnCode_;
+}
+
+bool Subprocess::pollChecked() {
+  if (poll().state() == ProcessReturnCode::RUNNING) {
+    return false;
+  }
+  checkStatus(returnCode_);
+  return true;
+}
+
+ProcessReturnCode Subprocess::wait() {
+  returnCode_.enforce(ProcessReturnCode::RUNNING);
+  DCHECK_GT(pid_, 0);
+  int status;
+  pid_t found;
+  do {
+    found = ::waitpid(pid_, &status, 0);
+  } while (found == -1 && errno == EINTR);
+  checkUnixError(found, "waitpid");
+  DCHECK_EQ(found, pid_);
+  returnCode_ = ProcessReturnCode(status);
+  pid_ = -1;
+  return returnCode_;
+}
+
+void Subprocess::waitChecked() {
+  wait();
+  checkStatus(returnCode_);
+}
+
+void Subprocess::sendSignal(int signal) {
+  returnCode_.enforce(ProcessReturnCode::RUNNING);
+  int r = ::kill(pid_, signal);
+  checkUnixError(r, "kill");
+}
+
+pid_t Subprocess::pid() const {
+  return pid_;
+}
+
+namespace {
+
+std::pair<const uint8_t*, size_t> queueFront(const IOBufQueue& queue) {
+  auto* p = queue.front();
+  if (!p) return std::make_pair(nullptr, 0);
+  return io::Cursor(p).peek();
+}
+
+// fd write
+bool handleWrite(int fd, IOBufQueue& queue) {
+  for (;;) {
+    auto p = queueFront(queue);
+    if (p.second == 0) {
+      return true;  // EOF
+    }
+
+    ssize_t n;
+    do {
+      n = ::write(fd, p.first, p.second);
+    } while (n == -1 && errno == EINTR);
+    if (n == -1 && errno == EAGAIN) {
+      return false;
+    }
+    checkUnixError(n, "write");
+    queue.trimStart(n);
+  }
+}
+
+// fd read
+bool handleRead(int fd, IOBufQueue& queue) {
+  for (;;) {
+    auto p = queue.preallocate(100, 65000);
+    ssize_t n;
+    do {
+      n = ::read(fd, p.first, p.second);
+    } while (n == -1 && errno == EINTR);
+    if (n == -1 && errno == EAGAIN) {
+      return false;
+    }
+    checkUnixError(n, "read");
+    if (n == 0) {
+      return true;
+    }
+    queue.postallocate(n);
+  }
+}
+
+bool discardRead(int fd) {
+  static const size_t bufSize = 65000;
+  // Thread unsafe, but it doesn't matter.
+  static std::unique_ptr<char[]> buf(new char[bufSize]);
+
+  for (;;) {
+    ssize_t n;
+    do {
+      n = ::read(fd, buf.get(), bufSize);
+    } while (n == -1 && errno == EINTR);
+    if (n == -1 && errno == EAGAIN) {
+      return false;
+    }
+    checkUnixError(n, "read");
+    if (n == 0) {
+      return true;
+    }
+  }
+}
+
+}  // namespace
+
+std::pair<std::string, std::string> Subprocess::communicate(
+    StringPiece input) {
+  IOBufQueue inputQueue;
+  inputQueue.wrapBuffer(input.data(), input.size());
+
+  auto outQueues = communicateIOBuf(std::move(inputQueue));
+  auto outBufs = std::make_pair(outQueues.first.move(),
+                                outQueues.second.move());
+  std::pair<std::string, std::string> out;
+  if (outBufs.first) {
+    outBufs.first->coalesce();
+    out.first.assign(reinterpret_cast<const char*>(outBufs.first->data()),
+                     outBufs.first->length());
+  }
+  if (outBufs.second) {
+    outBufs.second->coalesce();
+    out.second.assign(reinterpret_cast<const char*>(outBufs.second->data()),
+                     outBufs.second->length());
+  }
+  return out;
+}
+
+std::pair<IOBufQueue, IOBufQueue> Subprocess::communicateIOBuf(
+    IOBufQueue input) {
+  // If the user supplied a non-empty input buffer, make sure
+  // that stdin is a pipe so we can write the data.
+  if (!input.empty()) {
+    // findByChildFd() will throw std::invalid_argument if no pipe for
+    // STDIN_FILENO exists
+    findByChildFd(STDIN_FILENO);
+  }
+
+  std::pair<IOBufQueue, IOBufQueue> out;
+
+  auto readCallback = [&] (int pfd, int cfd) -> bool {
+    if (cfd == STDOUT_FILENO) {
+      return handleRead(pfd, out.first);
+    } else if (cfd == STDERR_FILENO) {
+      return handleRead(pfd, out.second);
+    } else {
+      // Don't close the file descriptor, the child might not like SIGPIPE,
+      // just read and throw the data away.
+      return discardRead(pfd);
+    }
+  };
+
+  auto writeCallback = [&] (int pfd, int cfd) -> bool {
+    if (cfd == STDIN_FILENO) {
+      return handleWrite(pfd, input);
+    } else {
+      // If we don't want to write to this fd, just close it.
+      return true;
+    }
+  };
+
+  communicate(std::move(readCallback), std::move(writeCallback));
+
+  return out;
+}
+
+void Subprocess::communicate(FdCallback readCallback,
+                             FdCallback writeCallback) {
+  returnCode_.enforce(ProcessReturnCode::RUNNING);
+  setAllNonBlocking();
+
+  std::vector<pollfd> fds;
+  fds.reserve(pipes_.size());
+  std::vector<int> toClose;
+  toClose.reserve(pipes_.size());
+
+  while (!pipes_.empty()) {
+    fds.clear();
+    toClose.clear();
+
+    for (auto& p : pipes_) {
+      pollfd pfd;
+      pfd.fd = p.parentFd;
+      // Yes, backwards, PIPE_IN / PIPE_OUT are defined from the
+      // child's point of view.
+      pfd.events = (p.direction == PIPE_IN ?  POLLOUT : POLLIN);
+      fds.push_back(pfd);
+    }
+
+    int r;
+    do {
+      r = ::poll(fds.data(), fds.size(), -1);
+    } while (r == -1 && errno == EINTR);
+    checkUnixError(r, "poll");
+
+    for (int i = 0; i < pipes_.size(); ++i) {
+      auto& p = pipes_[i];
+      DCHECK_EQ(fds[i].fd, p.parentFd);
+      short events = fds[i].revents;
+
+      bool closed = false;
+      if (events & POLLOUT) {
+        DCHECK(!(events & POLLIN));
+        if (writeCallback(p.parentFd, p.childFd)) {
+          toClose.push_back(i);
+          closed = true;
+        }
+      }
+
+      if (events & POLLIN) {
+        DCHECK(!(events & POLLOUT));
+        if (readCallback(p.parentFd, p.childFd)) {
+          toClose.push_back(i);
+          closed = true;
+        }
+      }
+
+      if ((events & (POLLHUP | POLLERR)) && !closed) {
+        toClose.push_back(i);
+        closed = true;
+      }
+    }
+
+    // Close the fds in reverse order so the indexes hold after erase()
+    for (int idx : boost::adaptors::reverse(toClose)) {
+      auto pos = pipes_.begin() + idx;
+      closeChecked(pos->parentFd);
+      pipes_.erase(pos);
+    }
+  }
+}
+
+int Subprocess::findByChildFd(int childFd) const {
+  auto pos = std::lower_bound(
+      pipes_.begin(), pipes_.end(), childFd,
+      [] (const PipeInfo& info, int fd) { return info.childFd < fd; });
+  if (pos == pipes_.end() || pos->childFd != childFd) {
+    throw std::invalid_argument(folly::to<std::string>(
+        "child fd not found ", childFd));
+  }
+  return pos - pipes_.begin();
+}
+
+void Subprocess::closeParentFd(int childFd) {
+  int idx = findByChildFd(childFd);
+  closeChecked(pipes_[idx].parentFd);
+  pipes_.erase(pipes_.begin() + idx);
+}
+
+namespace {
+
+class Initializer {
+ public:
+  Initializer() {
+    // We like EPIPE, thanks.
+    ::signal(SIGPIPE, SIG_IGN);
+  }
+};
+
+Initializer initializer;
+
+}  // namespace
+
+}  // namespace folly
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/Subprocess.h
@@ -0,0 +1,535 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Subprocess library, modeled after Python's subprocess module
+ * (http://docs.python.org/2/library/subprocess.html)
+ *
+ * This library defines one class (Subprocess) which represents a child
+ * process.  Subprocess has two constructors: one that takes a vector<string>
+ * and executes the given executable without using the shell, and one
+ * that takes a string and executes the given command using the shell.
+ * Subprocess allows you to redirect the child's standard input, standard
+ * output, and standard error to/from child descriptors in the parent,
+ * or to create communication pipes between the child and the parent.
+ *
+ * The simplest example is a thread-safe version of the system() library
+ * function:
+ *    Subprocess(cmd).wait();
+ * which executes the command using the default shell and waits for it
+ * to complete, returning the exit status.
+ *
+ * A thread-safe version of popen() (type="r", to read from the child):
+ *    Subprocess proc(cmd, Subprocess::pipeStdout());
+ *    // read from proc.stdout()
+ *    proc.wait();
+ *
+ * A thread-safe version of popen() (type="w", to write to the child):
+ *    Subprocess proc(cmd, Subprocess::pipeStdin());
+ *    // write to proc.stdin()
+ *    proc.wait();
+ *
+ * If you want to redirect both stdin and stdout to pipes, you can, but
+ * note that you're subject to a variety of deadlocks.  You'll want to use
+ * nonblocking I/O; look at the implementation of communicate() for an example.
+ *
+ * communicate() is a way to communicate to a child via its standard input,
+ * standard output, and standard error.  It buffers everything in memory,
+ * so it's not great for large amounts of data (or long-running processes),
+ * but it insulates you from the deadlocks mentioned above.
+ */
+#ifndef FOLLY_SUBPROCESS_H_
+#define FOLLY_SUBPROCESS_H_
+
+#include <sys/types.h>
+#include <signal.h>
+#if __APPLE__
+#include <sys/wait.h>
+#else
+#include <wait.h>
+#endif
+
+#include <exception>
+#include <vector>
+#include <string>
+
+#include <boost/container/flat_map.hpp>
+#include <boost/operators.hpp>
+#include <boost/noncopyable.hpp>
+
+#include "folly/io/IOBufQueue.h"
+#include "folly/MapUtil.h"
+#include "folly/Portability.h"
+#include "folly/Range.h"
+
+namespace folly {
+
+/**
+ * Class to wrap a process return code.
+ */
+class Subprocess;
+class ProcessReturnCode {
+  friend class Subprocess;
+ public:
+  enum State {
+    NOT_STARTED,
+    RUNNING,
+    EXITED,
+    KILLED
+  };
+
+  /**
+   * Process state.  One of:
+   * NOT_STARTED: process hasn't been started successfully
+   * RUNNING: process is currently running
+   * EXITED: process exited (successfully or not)
+   * KILLED: process was killed by a signal.
+   */
+  State state() const;
+
+  /**
+   * Helper wrappers around state().
+   */
+  bool notStarted() const { return state() == NOT_STARTED; }
+  bool running() const { return state() == RUNNING; }
+  bool exited() const { return state() == EXITED; }
+  bool killed() const { return state() == KILLED; }
+
+  /**
+   * Exit status.  Only valid if state() == EXITED; throws otherwise.
+   */
+  int exitStatus() const;
+
+  /**
+   * Signal that caused the process's termination.  Only valid if
+   * state() == KILLED; throws otherwise.
+   */
+  int killSignal() const;
+
+  /**
+   * Was a core file generated?  Only valid if state() == KILLED; throws
+   * otherwise.
+   */
+  bool coreDumped() const;
+
+  /**
+   * String representation; one of
+   * "not started"
+   * "running"
+   * "exited with status <status>"
+   * "killed by signal <signal>"
+   * "killed by signal <signal> (core dumped)"
+   */
+  std::string str() const;
+
+  /**
+   * Helper function to enforce a precondition based on this.
+   * Throws std::logic_error if in an unexpected state.
+   */
+  void enforce(State state) const;
+ private:
+  explicit ProcessReturnCode(int rv) : rawStatus_(rv) { }
+  static constexpr int RV_NOT_STARTED = -2;
+  static constexpr int RV_RUNNING = -1;
+
+  int rawStatus_;
+};
+
+/**
+ * Base exception thrown by the Subprocess methods.
+ */
+class SubprocessError : public std::exception {};
+
+/**
+ * Exception thrown by *Checked methods of Subprocess.
+ */
+class CalledProcessError : public SubprocessError {
+ public:
+  explicit CalledProcessError(ProcessReturnCode rc);
+  ~CalledProcessError() throw() { }
+  const char* what() const throw() FOLLY_OVERRIDE { return what_.c_str(); }
+  ProcessReturnCode returnCode() const { return returnCode_; }
+ private:
+  ProcessReturnCode returnCode_;
+  std::string what_;
+};
+
+/**
+ * Exception thrown if the subprocess cannot be started.
+ */
+class SubprocessSpawnError : public SubprocessError {
+ public:
+  SubprocessSpawnError(const char* executable, int errCode, int errnoValue);
+  ~SubprocessSpawnError() throw() {}
+  const char* what() const throw() FOLLY_OVERRIDE { return what_.c_str(); }
+  int errnoValue() const { return errnoValue_; }
+
+ private:
+  int errnoValue_;
+  std::string what_;
+};
+
+/**
+ * Subprocess.
+ */
+class Subprocess : private boost::noncopyable {
+ public:
+  static const int CLOSE = -1;
+  static const int PIPE = -2;
+  static const int PIPE_IN = -3;
+  static const int PIPE_OUT = -4;
+
+  /**
+   * Class representing various options: file descriptor behavior, and
+   * whether to use $PATH for searching for the executable,
+   *
+   * By default, we don't use $PATH, file descriptors are closed if
+   * the close-on-exec flag is set (fcntl FD_CLOEXEC) and inherited
+   * otherwise.
+   */
+  class Options : private boost::orable<Options> {
+    friend class Subprocess;
+   public:
+    Options()
+      : closeOtherFds_(false),
+        usePath_(false) {
+    }
+
+    /**
+     * Change action for file descriptor fd.
+     *
+     * "action" may be another file descriptor number (dup2()ed before the
+     * child execs), or one of CLOSE, PIPE_IN, and PIPE_OUT.
+     *
+     * CLOSE: close the file descriptor in the child
+     * PIPE_IN: open a pipe *from* the child
+     * PIPE_OUT: open a pipe *to* the child
+     *
+     * PIPE is a shortcut; same as PIPE_IN for stdin (fd 0), same as
+     * PIPE_OUT for stdout (fd 1) or stderr (fd 2), and an error for
+     * other file descriptors.
+     */
+    Options& fd(int fd, int action);
+
+    /**
+     * Shortcut to change the action for standard input.
+     */
+    Options& stdin(int action) { return fd(STDIN_FILENO, action); }
+
+    /**
+     * Shortcut to change the action for standard output.
+     */
+    Options& stdout(int action) { return fd(STDOUT_FILENO, action); }
+
+    /**
+     * Shortcut to change the action for standard error.
+     * Note that stderr(1) will redirect the standard error to the same
+     * file descriptor as standard output; the equivalent of bash's "2>&1"
+     */
+    Options& stderr(int action) { return fd(STDERR_FILENO, action); }
+
+    Options& pipeStdin() { return fd(STDIN_FILENO, PIPE_IN); }
+    Options& pipeStdout() { return fd(STDOUT_FILENO, PIPE_OUT); }
+    Options& pipeStderr() { return fd(STDERR_FILENO, PIPE_OUT); }
+
+    /**
+     * Close all other fds (other than standard input, output, error,
+     * and file descriptors explicitly specified with fd()).
+     *
+     * This is potentially slow; it's generally a better idea to
+     * set the close-on-exec flag on all file descriptors that shouldn't
+     * be inherited by the child.
+     *
+     * Even with this option set, standard input, output, and error are
+     * not closed; use stdin(CLOSE), stdout(CLOSE), stderr(CLOSE) if you
+     * desire this.
+     */
+    Options& closeOtherFds() { closeOtherFds_ = true; return *this; }
+
+    /**
+     * Use the search path ($PATH) when searching for the executable.
+     */
+    Options& usePath() { usePath_ = true; return *this; }
+
+#if __linux__
+    /**
+     * Child will receive a signal when the parent exits.
+     */
+    Options& parentDeathSignal(int sig) {
+      parentDeathSignal_ = sig;
+      return *this;
+    }
+#endif
+
+    /**
+     * Helpful way to combine Options.
+     */
+    Options& operator|=(const Options& other);
+
+   private:
+    typedef boost::container::flat_map<int, int> FdMap;
+    FdMap fdActions_;
+    bool closeOtherFds_;
+    bool usePath_;
+#if __linux__
+    int parentDeathSignal_{0};
+#endif
+  };
+
+  static Options pipeStdin() { return Options().stdin(PIPE); }
+  static Options pipeStdout() { return Options().stdout(PIPE); }
+  static Options pipeStderr() { return Options().stderr(PIPE); }
+
+  /**
+   * Create a subprocess from the given arguments.  argv[0] must be listed.
+   * If not-null, executable must be the actual executable
+   * being used (otherwise it's the same as argv[0]).
+   *
+   * If env is not-null, it must contain name=value strings to be used
+   * as the child's environment; otherwise, we inherit the environment
+   * from the parent.  env must be null if options.usePath is set.
+   */
+  explicit Subprocess(
+      const std::vector<std::string>& argv,
+      const Options& options = Options(),
+      const char* executable = nullptr,
+      const std::vector<std::string>* env = nullptr);
+  ~Subprocess();
+
+  /**
+   * Create a subprocess run as a shell command (as shell -c 'command')
+   *
+   * The shell to use is taken from the environment variable $SHELL,
+   * or /bin/sh if $SHELL is unset.
+   */
+  explicit Subprocess(
+      const std::string& cmd,
+      const Options& options = Options(),
+      const std::vector<std::string>* env = nullptr);
+
+  /**
+   * Communicate with the child until all pipes to/from the child are closed.
+   *
+   * The input buffer is written to the process' stdin pipe, and data is read
+   * from the stdout and stderr pipes.  Non-blocking I/O is performed on all
+   * pipes simultaneously to avoid deadlocks.
+   *
+   * The stdin pipe will be closed after the full input buffer has been written.
+   * An error will be thrown if a non-empty input buffer is supplied but stdin
+   * was not configured as a pipe.
+   *
+   * Returns a pair of buffers containing the data read from stdout and stderr.
+   * If stdout or stderr is not a pipe, an empty IOBuf queue will be returned
+   * for the respective buffer.
+   *
+   * Note that communicate() returns when all pipes to/from the child are
+   * closed; the child might stay alive after that, so you must still wait().
+   *
+   * communicateIOBuf uses IOBufQueue for buffering (which has the advantage
+   * that it won't try to allocate all data at once).  communicate
+   * uses strings for simplicity.
+   */
+  std::pair<IOBufQueue, IOBufQueue> communicateIOBuf(
+      IOBufQueue input = IOBufQueue());
+
+  std::pair<std::string, std::string> communicate(
+      StringPiece input = StringPiece());
+
+  /**
+   * Communicate with the child until all pipes to/from the child are closed.
+   *
+   * readCallback(pfd, cfd) will be called whenever there's data available
+   * on any pipe *from* the child (PIPE_OUT).  pfd is the file descriptor
+   * in the parent (that you use to read from); cfd is the file descriptor
+   * in the child (used for identifying the stream; 1 = child's standard
+   * output, 2 = child's standard error, etc)
+   *
+   * writeCallback(pfd, cfd) will be called whenever a pipe *to* the child is
+   * writable (PIPE_IN).  pfd is the file descriptor in the parent (that you
+   * use to write to); cfd is the file descriptor in the child (used for
+   * identifying the stream; 0 = child's standard input, etc)
+   *
+   * The read and write callbacks must read from / write to pfd and return
+   * false during normal operation or true at end-of-file;
+   * communicate() will then close the pipe.  Note that pfd is
+   * nonblocking, so be prepared for read() / write() to return -1 and
+   * set errno to EAGAIN (in which case you should return false).
+   *
+   * NOTE that you MUST consume all data passed to readCallback (or return
+   * true, which will close the pipe, possibly sending SIGPIPE to the child or
+   * making its writes fail with EPIPE), and you MUST write to a writable pipe
+   * (or return true, which will close the pipe).  To do otherwise is an
+   * error.  You must do this even for pipes you are not interested in.
+   *
+   * Note that communicate() returns when all pipes to/from the child are
+   * closed; the child might stay alive after that, so you must still wait().
+   *
+   * Most users won't need to use this; the simpler version of communicate
+   * (which buffers data in memory) will probably work fine.
+   */
+  typedef std::function<bool(int, int)> FdCallback;
+  void communicate(FdCallback readCallback, FdCallback writeCallback);
+
+  /**
+   * Return the child's pid, or -1 if the child wasn't successfully spawned
+   * or has already been wait()ed upon.
+   */
+  pid_t pid() const;
+
+  /**
+   * Return the child's status (as per wait()) if the process has already
+   * been waited on, -1 if the process is still running, or -2 if the process
+   * hasn't been successfully started.  NOTE that this does not poll, but
+   * returns the status stored in the Subprocess object.
+   */
+  ProcessReturnCode returnCode() const { return returnCode_; }
+
+  /**
+   * Poll the child's status and return it, return -1 if the process
+   * is still running.  NOTE that it is illegal to call poll again after
+   * poll indicated that the process has terminated, or to call poll on a
+   * process that hasn't been successfully started (the constructor threw an
+   * exception).
+   */
+  ProcessReturnCode poll();
+
+  /**
+   * Poll the child's status.  If the process is still running, return false.
+   * Otherwise, return true if the process exited with status 0 (success),
+   * or throw CalledProcessError if the process exited with a non-zero status.
+   */
+  bool pollChecked();
+
+  /**
+   * Wait for the process to terminate and return its status.
+   * Similarly to poll, it is illegal to call wait after the process
+   * has already been reaped or if the process has not successfully started.
+   */
+  ProcessReturnCode wait();
+
+  /**
+   * Wait for the process to terminate, throw if unsuccessful.
+   */
+  void waitChecked();
+
+  /**
+   * Set all pipes from / to child non-blocking.  communicate() does
+   * this for you.
+   */
+  void setAllNonBlocking();
+
+  /**
+   * Get parent file descriptor corresponding to the given file descriptor
+   * in the child.  Throws if childFd isn't a pipe (PIPE_IN / PIPE_OUT).
+   * Do not close() the return file descriptor; use closeParentFd, below.
+   */
+  int parentFd(int childFd) const {
+    return pipes_[findByChildFd(childFd)].parentFd;
+  }
+  int stdin() const { return parentFd(0); }
+  int stdout() const { return parentFd(1); }
+  int stderr() const { return parentFd(2); }
+
+  /**
+   * Close the parent file descriptor given a file descriptor in the child.
+   */
+  void closeParentFd(int childFd);
+
+  /**
+   * Send a signal to the child.  Shortcuts for the commonly used Unix
+   * signals are below.
+   */
+  void sendSignal(int signal);
+  void terminate() { sendSignal(SIGTERM); }
+  void kill() { sendSignal(SIGKILL); }
+
+ private:
+  static const int RV_RUNNING = ProcessReturnCode::RV_RUNNING;
+  static const int RV_NOT_STARTED = ProcessReturnCode::RV_NOT_STARTED;
+
+  // spawn() sets up a pipe to read errors from the child,
+  // then calls spawnInternal() to do the bulk of the work.  Once
+  // spawnInternal() returns it reads the error pipe to see if the child
+  // encountered any errors.
+  void spawn(
+      std::unique_ptr<const char*[]> argv,
+      const char* executable,
+      const Options& options,
+      const std::vector<std::string>* env);
+  void spawnInternal(
+      std::unique_ptr<const char*[]> argv,
+      const char* executable,
+      Options& options,
+      const std::vector<std::string>* env,
+      int errFd);
+
+  // Actions to run in child.
+  // Note that this runs after vfork(), so tread lightly.
+  // Returns 0 on success, or an errno value on failure.
+  int prepareChild(const Options& options, const sigset_t* sigmask) const;
+  int runChild(const char* executable, char** argv, char** env,
+               const Options& options) const;
+
+  /**
+   * Read from the error pipe, and throw SubprocessSpawnError if the child
+   * failed before calling exec().
+   */
+  void readChildErrorPipe(int pfd, const char* executable);
+
+  /**
+   * Close all file descriptors.
+   */
+  void closeAll();
+
+  // return index in pipes_
+  int findByChildFd(int childFd) const;
+
+  pid_t pid_;
+  ProcessReturnCode returnCode_;
+
+  // The number of pipes between parent and child is assumed to be small,
+  // so we're happy with a vector here, even if it means linear erase.
+  // sorted by childFd
+  struct PipeInfo : private boost::totally_ordered<PipeInfo> {
+    int parentFd;
+    int childFd;
+    int direction;  // one of PIPE_IN / PIPE_OUT
+    bool operator<(const PipeInfo& other) const {
+      return childFd < other.childFd;
+    }
+    bool operator==(const PipeInfo& other) const {
+      return childFd == other.childFd;
+    }
+  };
+  std::vector<PipeInfo> pipes_;
+};
+
+inline Subprocess::Options& Subprocess::Options::operator|=(
+    const Subprocess::Options& other) {
+  if (this == &other) return *this;
+  // Replace
+  for (auto& p : other.fdActions_) {
+    fdActions_[p.first] = p.second;
+  }
+  closeOtherFds_ |= other.closeOtherFds_;
+  usePath_ |= other.usePath_;
+  return *this;
+}
+
+}  // namespace folly
+
+#endif /* FOLLY_SUBPROCESS_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/Synchronized.h
@@ -0,0 +1,697 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * This module implements a Synchronized abstraction useful in
+ * mutex-based concurrency.
+ *
+ * @author: Andrei Alexandrescu (andrei.alexandrescu@fb.com)
+ */
+
+#ifndef SYNCHRONIZED_H_
+#define SYNCHRONIZED_H_
+
+#include <type_traits>
+#include <mutex>
+#include <boost/thread.hpp>
+#include "folly/Preprocessor.h"
+#include "folly/Traits.h"
+
+namespace folly {
+
+namespace detail {
+enum InternalDoNotUse {};
+
+/**
+ * Free function adaptors for std:: and boost::
+ */
+
+/**
+ * Yields true iff T has .lock() and .unlock() member functions. This
+ * is done by simply enumerating the mutexes with this interface in
+ * std and boost.
+ */
+template <class T>
+struct HasLockUnlock {
+  enum { value = IsOneOf<T,
+         std::mutex, std::recursive_mutex,
+         boost::mutex, boost::recursive_mutex, boost::shared_mutex
+#ifndef __APPLE__ // OSX doesn't have timed mutexes
+        ,std::timed_mutex, std::recursive_timed_mutex,
+         boost::timed_mutex, boost::recursive_timed_mutex
+#endif
+         >::value };
+};
+
+/**
+ * Acquires a mutex for reading by calling .lock(). The exception is
+ * boost::shared_mutex, which has a special read-lock primitive called
+ * .lock_shared().
+ */
+template <class T>
+typename std::enable_if<
+  HasLockUnlock<T>::value && !std::is_same<T, boost::shared_mutex>::value>::type
+acquireRead(T& mutex) {
+  mutex.lock();
+}
+
+/**
+ * Special case for boost::shared_mutex.
+ */
+template <class T>
+typename std::enable_if<std::is_same<T, boost::shared_mutex>::value>::type
+acquireRead(T& mutex) {
+  mutex.lock_shared();
+}
+
+/**
+ * Acquires a mutex for reading with timeout by calling .timed_lock(). This
+ * applies to three of the boost mutex classes as enumerated below.
+ */
+template <class T>
+typename std::enable_if<std::is_same<T, boost::shared_mutex>::value, bool>::type
+acquireRead(T& mutex,
+            unsigned int milliseconds) {
+  return mutex.timed_lock_shared(boost::posix_time::milliseconds(milliseconds));
+}
+
+/**
+ * Acquires a mutex for reading and writing by calling .lock().
+ */
+template <class T>
+typename std::enable_if<HasLockUnlock<T>::value>::type
+acquireReadWrite(T& mutex) {
+  mutex.lock();
+}
+
+#ifndef __APPLE__ // OSX doesn't have timed mutexes
+/**
+ * Acquires a mutex for reading and writing with timeout by calling
+ * .try_lock_for(). This applies to two of the std mutex classes as
+ * enumerated below.
+ */
+template <class T>
+typename std::enable_if<
+  IsOneOf<T, std::timed_mutex, std::recursive_timed_mutex>::value, bool>::type
+acquireReadWrite(T& mutex,
+                 unsigned int milliseconds) {
+  // work around try_lock_for bug in some gcc versions, see
+  // http://gcc.gnu.org/bugzilla/show_bug.cgi?id=54562
+  return mutex.try_lock()
+      || (milliseconds > 0 &&
+          mutex.try_lock_until(std::chrono::system_clock::now() +
+                               std::chrono::milliseconds(milliseconds)));
+}
+
+/**
+ * Acquires a mutex for reading and writing with timeout by calling
+ * .timed_lock(). This applies to three of the boost mutex classes as
+ * enumerated below.
+ */
+template <class T>
+typename std::enable_if<
+  IsOneOf<T, boost::shared_mutex, boost::timed_mutex,
+          boost::recursive_timed_mutex>::value, bool>::type
+acquireReadWrite(T& mutex,
+                 unsigned int milliseconds) {
+  return mutex.timed_lock(boost::posix_time::milliseconds(milliseconds));
+}
+#endif // __APPLE__
+
+/**
+ * Releases a mutex previously acquired for reading by calling
+ * .unlock(). The exception is boost::shared_mutex, which has a
+ * special primitive called .unlock_shared().
+ */
+template <class T>
+typename std::enable_if<
+  HasLockUnlock<T>::value && !std::is_same<T, boost::shared_mutex>::value>::type
+releaseRead(T& mutex) {
+  mutex.unlock();
+}
+
+/**
+ * Special case for boost::shared_mutex.
+ */
+template <class T>
+typename std::enable_if<std::is_same<T, boost::shared_mutex>::value>::type
+releaseRead(T& mutex) {
+  mutex.unlock_shared();
+}
+
+/**
+ * Releases a mutex previously acquired for reading-writing by calling
+ * .unlock().
+ */
+template <class T>
+typename std::enable_if<HasLockUnlock<T>::value>::type
+releaseReadWrite(T& mutex) {
+  mutex.unlock();
+}
+
+} // namespace detail
+
+/**
+ * Synchronized<T> encapsulates an object of type T (a "datum") paired
+ * with a mutex. The only way to access the datum is while the mutex
+ * is locked, and Synchronized makes it virtually impossible to do
+ * otherwise. The code that would access the datum in unsafe ways
+ * would look odd and convoluted, thus readily alerting the human
+ * reviewer. In contrast, the code that uses Synchronized<T> correctly
+ * looks simple and intuitive.
+ *
+ * The second parameter must be a mutex type. Supported mutexes are
+ * std::mutex, std::recursive_mutex, std::timed_mutex,
+ * std::recursive_timed_mutex, boost::mutex, boost::recursive_mutex,
+ * boost::shared_mutex, boost::timed_mutex,
+ * boost::recursive_timed_mutex, and the folly/RWSpinLock.h
+ * classes.
+ *
+ * You may define Synchronized support by defining 4-6 primitives in
+ * the same namespace as the mutex class (found via ADL).  The
+ * primitives are: acquireRead, acquireReadWrite, releaseRead, and
+ * releaseReadWrite. Two optional primitives for timout operations are
+ * overloads of acquireRead and acquireReadWrite. For signatures,
+ * refer to the namespace detail below, which implements the
+ * primitives for mutexes in std and boost.
+ */
+template <class T, class Mutex = boost::shared_mutex>
+struct Synchronized {
+  /**
+   * Default constructor leaves both members call their own default
+   * constructor.
+   */
+  Synchronized() = default;
+
+  /**
+   * Copy constructor copies the data (with locking the source and
+   * all) but does NOT copy the mutex. Doing so would result in
+   * deadlocks.
+   */
+  Synchronized(const Synchronized& rhs) {
+    auto guard = rhs.operator->();
+    datum_ = rhs.datum_;
+  }
+
+  /**
+   * Move constructor moves the data (with locking the source and all)
+   * but does not move the mutex.
+   */
+  Synchronized(Synchronized&& rhs) {
+    auto guard = rhs.operator->();
+    datum_ = std::move(rhs.datum_);
+  }
+
+  /**
+   * Constructor taking a datum as argument copies it. There is no
+   * need to lock the constructing object.
+   */
+  explicit Synchronized(const T& rhs) : datum_(rhs) {}
+
+  /**
+   * Constructor taking a datum rvalue as argument moves it. Again,
+   * there is no need to lock the constructing object.
+   */
+  explicit Synchronized(T&& rhs) : datum_(std::move(rhs)) {}
+
+  /**
+   * The canonical assignment operator only assigns the data, NOT the
+   * mutex. It locks the two objects in ascending order of their
+   * addresses.
+   */
+  Synchronized& operator=(const Synchronized& rhs) {
+    if (this == &rhs) {
+      // Self-assignment, pass.
+    } else if (this < &rhs) {
+      auto guard1 = operator->();
+      auto guard2 = rhs.operator->();
+      datum_ = rhs.datum_;
+    } else {
+      auto guard1 = rhs.operator->();
+      auto guard2 = operator->();
+      datum_ = rhs.datum_;
+    }
+    return *this;
+  }
+
+  /**
+   * Move assignment operator, only assigns the data, NOT the
+   * mutex. It locks the two objects in ascending order of their
+   * addresses.
+   */
+  Synchronized& operator=(Synchronized&& rhs) {
+    if (this == &rhs) {
+      // Self-assignment, pass.
+    } else if (this < &rhs) {
+      auto guard1 = operator->();
+      auto guard2 = rhs.operator->();
+      datum_ = std::move(rhs.datum_);
+    } else {
+      auto guard1 = rhs.operator->();
+      auto guard2 = operator->();
+      datum_ = std::move(rhs.datum_);
+    }
+    return *this;
+  }
+
+  /**
+   * Lock object, assign datum.
+   */
+  Synchronized& operator=(const T& rhs) {
+    auto guard = operator->();
+    datum_ = rhs;
+    return *this;
+  }
+
+  /**
+   * Lock object, move-assign datum.
+   */
+  Synchronized& operator=(T&& rhs) {
+    auto guard = operator->();
+    datum_ = std::move(rhs);
+    return *this;
+  }
+
+  /**
+   * A LockedPtr lp keeps a modifiable (i.e. non-const)
+   * Synchronized<T> object locked for the duration of lp's
+   * existence. Because of this, you get to access the datum's methods
+   * directly by using lp->fun().
+   */
+  struct LockedPtr {
+    /**
+     * Found no reason to leave this hanging.
+     */
+    LockedPtr() = delete;
+
+    /**
+     * Takes a Synchronized and locks it.
+     */
+    explicit LockedPtr(Synchronized* parent) : parent_(parent) {
+      acquire();
+    }
+
+    /**
+     * Takes a Synchronized and attempts to lock it for some
+     * milliseconds. If not, the LockedPtr will be subsequently null.
+     */
+    LockedPtr(Synchronized* parent, unsigned int milliseconds) {
+      using namespace detail;
+      if (acquireReadWrite(parent->mutex_, milliseconds)) {
+        parent_ = parent;
+        return;
+      }
+      // Could not acquire the resource, pointer is null
+      parent_ = NULL;
+    }
+
+    /**
+     * This is used ONLY inside SYNCHRONIZED_DUAL. It initializes
+     * everything properly, but does not lock the parent because it
+     * "knows" someone else will lock it. Please do not use.
+     */
+    LockedPtr(Synchronized* parent, detail::InternalDoNotUse)
+        : parent_(parent) {
+    }
+
+    /**
+     * Copy ctor adds one lock.
+     */
+    LockedPtr(const LockedPtr& rhs) : parent_(rhs.parent_) {
+      acquire();
+    }
+
+    /**
+     * Assigning from another LockedPtr results in freeing the former
+     * lock and acquiring the new one. The method works with
+     * self-assignment (does nothing).
+     */
+    LockedPtr& operator=(const LockedPtr& rhs) {
+      if (parent_ != rhs.parent_) {
+        if (parent_) parent_->mutex_.unlock();
+        parent_ = rhs.parent_;
+        acquire();
+      }
+      return *this;
+    }
+
+    /**
+     * Destructor releases.
+     */
+    ~LockedPtr() {
+      using namespace detail;
+      if (parent_) releaseReadWrite(parent_->mutex_);
+    }
+
+    /**
+     * Safe to access the data. Don't save the obtained pointer by
+     * invoking lp.operator->() by hand. Also, if the method returns a
+     * handle stored inside the datum, don't use this idiom - use
+     * SYNCHRONIZED below.
+     */
+    T* operator->() {
+      return parent_ ? &parent_->datum_ : NULL;
+    }
+
+    /**
+     * This class temporarily unlocks a LockedPtr in a scoped
+     * manner. It is used inside of the UNSYNCHRONIZED macro.
+     */
+    struct Unsynchronizer {
+      explicit Unsynchronizer(LockedPtr* p) : parent_(p) {
+        using namespace detail;
+        releaseReadWrite(parent_->parent_->mutex_);
+      }
+      Unsynchronizer(const Unsynchronizer&) = delete;
+      Unsynchronizer& operator=(const Unsynchronizer&) = delete;
+      ~Unsynchronizer() {
+        parent_->acquire();
+      }
+      LockedPtr* operator->() const {
+        return parent_;
+      }
+    private:
+      LockedPtr* parent_;
+    };
+    friend struct Unsynchronizer;
+    Unsynchronizer typeHackDoNotUse();
+
+    template <class P1, class P2>
+    friend void lockInOrder(P1& p1, P2& p2);
+
+  private:
+    void acquire() {
+      using namespace detail;
+      if (parent_) acquireReadWrite(parent_->mutex_);
+    }
+
+    // This is the entire state of LockedPtr.
+    Synchronized* parent_;
+  };
+
+  /**
+   * ConstLockedPtr does exactly what LockedPtr does, but for const
+   * Synchronized objects. Of interest is that ConstLockedPtr only
+   * uses a read lock, which is faster but more restrictive - you only
+   * get to call const methods of the datum.
+   *
+   * Much of the code between LockedPtr and
+   * ConstLockedPtr is identical and could be factor out, but there
+   * are enough nagging little differences to not justify the trouble.
+   */
+  struct ConstLockedPtr {
+    ConstLockedPtr() = delete;
+    explicit ConstLockedPtr(const Synchronized* parent) : parent_(parent) {
+      acquire();
+    }
+    ConstLockedPtr(const Synchronized* parent, detail::InternalDoNotUse)
+        : parent_(parent) {
+    }
+    ConstLockedPtr(const ConstLockedPtr& rhs) : parent_(rhs.parent_) {
+      acquire();
+    }
+    explicit ConstLockedPtr(const LockedPtr& rhs) : parent_(rhs.parent_) {
+      acquire();
+    }
+    ConstLockedPtr(const Synchronized* parent, unsigned int milliseconds) {
+      if (parent->mutex_.timed_lock(
+            boost::posix_time::milliseconds(milliseconds))) {
+        parent_ = parent;
+        return;
+      }
+      // Could not acquire the resource, pointer is null
+      parent_ = NULL;
+    }
+
+    ConstLockedPtr& operator=(const ConstLockedPtr& rhs) {
+      if (parent_ != rhs.parent_) {
+        if (parent_) parent_->mutex_.unlock_shared();
+        parent_ = rhs.parent_;
+        acquire();
+      }
+    }
+    ~ConstLockedPtr() {
+      using namespace detail;
+      if (parent_) releaseRead(parent_->mutex_);
+    }
+
+    const T* operator->() const {
+      return parent_ ? &parent_->datum_ : NULL;
+    }
+
+    struct Unsynchronizer {
+      explicit Unsynchronizer(ConstLockedPtr* p) : parent_(p) {
+        using namespace detail;
+        releaseRead(parent_->parent_->mutex_);
+      }
+      Unsynchronizer(const Unsynchronizer&) = delete;
+      Unsynchronizer& operator=(const Unsynchronizer&) = delete;
+      ~Unsynchronizer() {
+        using namespace detail;
+        acquireRead(parent_->parent_->mutex_);
+      }
+      ConstLockedPtr* operator->() const {
+        return parent_;
+      }
+    private:
+      ConstLockedPtr* parent_;
+    };
+    friend struct Unsynchronizer;
+    Unsynchronizer typeHackDoNotUse();
+
+    template <class P1, class P2>
+    friend void lockInOrder(P1& p1, P2& p2);
+
+  private:
+    void acquire() {
+      using namespace detail;
+      if (parent_) acquireRead(parent_->mutex_);
+    }
+
+    const Synchronized* parent_;
+  };
+
+  /**
+   * This accessor offers a LockedPtr. In turn. LockedPtr offers
+   * operator-> returning a pointer to T. The operator-> keeps
+   * expanding until it reaches a pointer, so syncobj->foo() will lock
+   * the object and call foo() against it.
+  */
+  LockedPtr operator->() {
+    return LockedPtr(this);
+  }
+
+  /**
+   * Same, for constant objects. You will be able to invoke only const
+   * methods.
+   */
+  ConstLockedPtr operator->() const {
+    return ConstLockedPtr(this);
+  }
+
+  /**
+   * Attempts to acquire for a given number of milliseconds. If
+   * acquisition is unsuccessful, the returned LockedPtr is NULL.
+   */
+  LockedPtr timedAcquire(unsigned int milliseconds) {
+    return LockedPtr(this, milliseconds);
+  }
+
+  /**
+   * As above, for a constant object.
+   */
+  ConstLockedPtr timedAcquire(unsigned int milliseconds) const {
+    return ConstLockedPtr(this, milliseconds);
+  }
+
+  /**
+   * Used by SYNCHRONIZED_DUAL.
+   */
+  LockedPtr internalDoNotUse() {
+    return LockedPtr(this, detail::InternalDoNotUse());
+  }
+
+  /**
+   * ditto
+   */
+  ConstLockedPtr internalDoNotUse() const {
+    return ConstLockedPtr(this, detail::InternalDoNotUse());
+  }
+
+  /**
+   * Sometimes, although you have a mutable object, you only want to
+   * call a const method against it. The most efficient way to achieve
+   * that is by using a read lock. You get to do so by using
+   * obj.asConst()->method() instead of obj->method().
+   */
+  const Synchronized& asConst() const {
+    return *this;
+  }
+
+  /**
+   * Swaps with another Synchronized. Protected against
+   * self-swap. Only data is swapped. Locks are acquired in increasing
+   * address order.
+   */
+  void swap(Synchronized& rhs) {
+    if (this == &rhs) {
+      return;
+    }
+    if (this > &rhs) {
+      return rhs.swap(*this);
+    }
+    auto guard1 = operator->();
+    auto guard2 = rhs.operator->();
+
+    using std::swap;
+    swap(datum_, rhs.datum_);
+  }
+
+  /**
+   * Swap with another datum. Recommended because it keeps the mutex
+   * held only briefly.
+   */
+  void swap(T& rhs) {
+    LockedPtr guard = operator->();
+
+    using std::swap;
+    swap(datum_, rhs);
+  }
+
+  /**
+   * Copies datum to a given target.
+   */
+  void copy(T* target) const {
+    ConstLockedPtr guard = operator->();
+    *target = datum_;
+  }
+
+  /**
+   * Returns a fresh copy of the datum.
+   */
+  T copy() const {
+    ConstLockedPtr guard = operator->();
+    return datum_;
+  }
+
+private:
+  T datum_;
+  mutable Mutex mutex_;
+};
+
+// Non-member swap primitive
+template <class T, class M>
+void swap(Synchronized<T, M>& lhs, Synchronized<T, M>& rhs) {
+  lhs.swap(rhs);
+}
+
+/**
+ * SYNCHRONIZED is the main facility that makes Synchronized<T>
+ * helpful. It is a pseudo-statement that introduces a scope where the
+ * object is locked. Inside that scope you get to access the unadorned
+ * datum.
+ *
+ * Example:
+ *
+ * Synchronized<vector<int>> svector;
+ * ...
+ * SYNCHRONIZED (svector) { ... use svector as a vector<int> ... }
+ * or
+ * SYNCHRONIZED (v, svector) { ... use v as a vector<int> ... }
+ *
+ * Refer to folly/docs/Synchronized.md for a detailed explanation and more
+ * examples.
+ */
+#define SYNCHRONIZED(...)                                       \
+  if (bool SYNCHRONIZED_state = false) {} else                  \
+    for (auto SYNCHRONIZED_lockedPtr =                          \
+           (FB_ARG_2_OR_1(__VA_ARGS__)).operator->();           \
+         !SYNCHRONIZED_state; SYNCHRONIZED_state = true)        \
+      for (auto& FB_ARG_1(__VA_ARGS__) =                        \
+             *SYNCHRONIZED_lockedPtr.operator->();              \
+           !SYNCHRONIZED_state; SYNCHRONIZED_state = true)
+
+#define TIMED_SYNCHRONIZED(timeout, ...)                           \
+  if (bool SYNCHRONIZED_state = false) {} else                     \
+    for (auto SYNCHRONIZED_lockedPtr =                             \
+           (FB_ARG_2_OR_1(__VA_ARGS__)).timedAcquire(timeout);     \
+         !SYNCHRONIZED_state; SYNCHRONIZED_state = true)           \
+      for (auto FB_ARG_1(__VA_ARGS__) =                            \
+             SYNCHRONIZED_lockedPtr.operator->();                  \
+           !SYNCHRONIZED_state; SYNCHRONIZED_state = true)
+
+/**
+ * Similar to SYNCHRONIZED, but only uses a read lock.
+ */
+#define SYNCHRONIZED_CONST(...)                         \
+  SYNCHRONIZED(FB_ARG_1(__VA_ARGS__),                   \
+               (FB_ARG_2_OR_1(__VA_ARGS__)).asConst())
+
+/**
+ * Similar to TIMED_SYNCHRONIZED, but only uses a read lock.
+ */
+#define TIMED_SYNCHRONIZED_CONST(timeout, ...)                  \
+  TIMED_SYNCHRONIZED(timeout, FB_ARG_1(__VA_ARGS__),            \
+                     (FB_ARG_2_OR_1(__VA_ARGS__)).asConst())
+
+/**
+ * Temporarily disables synchronization inside a SYNCHRONIZED block.
+ */
+#define UNSYNCHRONIZED(name)                                    \
+  for (decltype(SYNCHRONIZED_lockedPtr.typeHackDoNotUse())      \
+         SYNCHRONIZED_state3(&SYNCHRONIZED_lockedPtr);          \
+       !SYNCHRONIZED_state; SYNCHRONIZED_state = true)          \
+    for (auto name = *SYNCHRONIZED_state3.operator->();         \
+         !SYNCHRONIZED_state; SYNCHRONIZED_state = true)
+
+/**
+ * Locks two objects in increasing order of their addresses.
+ */
+template <class P1, class P2>
+void lockInOrder(P1& p1, P2& p2) {
+  if (static_cast<const void*>(p1.operator->()) >
+      static_cast<const void*>(p2.operator->())) {
+    p2.acquire();
+    p1.acquire();
+  } else {
+    p1.acquire();
+    p2.acquire();
+  }
+}
+
+/**
+ * Synchronizes two Synchronized objects (they may encapsulate
+ * different data). Synchronization is done in increasing address of
+ * object order, so there is no deadlock risk.
+ */
+#define SYNCHRONIZED_DUAL(n1, e1, n2, e2)                       \
+  if (bool SYNCHRONIZED_state = false) {} else                  \
+    for (auto SYNCHRONIZED_lp1 = (e1).internalDoNotUse();       \
+         !SYNCHRONIZED_state; SYNCHRONIZED_state = true)        \
+      for (auto& n1 = *SYNCHRONIZED_lp1.operator->();           \
+           !SYNCHRONIZED_state;  SYNCHRONIZED_state = true)     \
+        for (auto SYNCHRONIZED_lp2 = (e2).internalDoNotUse();   \
+             !SYNCHRONIZED_state;  SYNCHRONIZED_state = true)   \
+          for (auto& n2 = *SYNCHRONIZED_lp2.operator->();       \
+               !SYNCHRONIZED_state; SYNCHRONIZED_state = true)  \
+            if ((::folly::lockInOrder(                          \
+                   SYNCHRONIZED_lp1, SYNCHRONIZED_lp2),         \
+                 false)) {}                                     \
+            else
+
+} /* namespace folly */
+
+#endif // SYNCHRONIZED_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/AHMIntStressTest.cpp
@@ -0,0 +1,126 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <gtest/gtest.h>
+
+#include <thread>
+#include <memory>
+#include <mutex>
+
+#include "folly/AtomicHashMap.h"
+#include "folly/ScopeGuard.h"
+#include "folly/Memory.h"
+
+namespace {
+
+struct MyObject {
+  explicit MyObject(int i) : i(i) {}
+  int i;
+};
+
+typedef folly::AtomicHashMap<int,std::shared_ptr<MyObject>> MyMap;
+typedef std::lock_guard<std::mutex> Guard;
+
+std::unique_ptr<MyMap> newMap() { return folly::make_unique<MyMap>(100); }
+
+struct MyObjectDirectory {
+  MyObjectDirectory()
+    : cur_(newMap())
+    , prev_(newMap())
+  {}
+
+  std::shared_ptr<MyObject> get(int key) {
+    auto val = tryGet(key);
+    if (val) {
+      return val;
+    }
+
+    std::shared_ptr<MyMap> cur;
+    {
+      Guard g(lock_);
+      cur = cur_;
+    }
+
+    auto ret = cur->insert(key, std::make_shared<MyObject>(key));
+    return ret.first->second;
+  }
+
+  std::shared_ptr<MyObject> tryGet(int key) {
+    std::shared_ptr<MyMap> cur;
+    std::shared_ptr<MyMap> prev;
+    {
+      Guard g(lock_);
+      cur = cur_;
+      prev = prev_;
+    }
+
+    auto it = cur->find(key);
+    if (it != cur->end()) {
+      return it->second;
+    }
+
+    it = prev->find(key);
+    if (it != prev->end()) {
+      auto ret = cur->insert(key, it->second);
+      return ret.first->second;
+    }
+
+    return nullptr;
+  }
+
+  void archive() {
+    std::shared_ptr<MyMap> cur(newMap());
+
+    Guard g(lock_);
+    prev_ = cur_;
+    cur_ = cur;
+  }
+
+  std::mutex lock_;
+  std::shared_ptr<MyMap> cur_;
+  std::shared_ptr<MyMap> prev_;
+};
+
+}
+
+//////////////////////////////////////////////////////////////////////
+
+/*
+ * This test case stresses ThreadLocal allocation/deallocation heavily
+ * via ThreadCachedInt and AtomicHashMap, and a bunch of other
+ * mallocing.
+ */
+TEST(AHMIntStressTest, Test) {
+  auto const objs = new MyObjectDirectory();
+  SCOPE_EXIT { delete objs; };
+
+  std::vector<std::thread> threads;
+  for (int threadId = 0; threadId < 64; ++threadId) {
+    threads.emplace_back(
+      [objs,threadId] {
+        for (int recycles = 0; recycles < 500; ++recycles) {
+          for (int i = 0; i < 10; i++) {
+            auto val = objs->get(i);
+          }
+
+          objs->archive();
+        }
+      }
+    );
+  }
+
+  for (auto& t : threads) t.join();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/ApplyTupleTest.cpp
@@ -0,0 +1,162 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <iostream>
+
+#include "folly/ApplyTuple.h"
+#include <gtest/gtest.h>
+
+#include <memory>
+
+namespace {
+
+void func(int a, int b, double c) {
+  EXPECT_EQ(a, 1);
+  EXPECT_EQ(b, 2);
+  EXPECT_EQ(c, 3.0);
+}
+
+struct Wat {
+  void func(int a, int b, double c) {
+    ::func(a, b, c);
+  }
+
+  double retVal(int a, double b) {
+    return a + b;
+  }
+
+  Wat() {}
+  Wat(Wat const&) = delete;
+
+  int foo;
+};
+
+struct Overloaded {
+  int func(int) { return 0; }
+  bool func(bool) { return true; }
+};
+
+struct Func {
+  int operator()() const {
+    return 1;
+  }
+};
+
+struct CopyCount {
+  CopyCount() {}
+  CopyCount(CopyCount const&) {
+    std::cout << "copy count copy ctor\n";
+  }
+};
+
+void anotherFunc(CopyCount const&) {}
+
+std::function<void (int, int, double)> makeFunc() {
+  return &func;
+}
+
+struct GuardObjBase {
+  GuardObjBase(GuardObjBase&&) {}
+  GuardObjBase() {}
+  GuardObjBase(GuardObjBase const&) = delete;
+  GuardObjBase& operator=(GuardObjBase const&) = delete;
+};
+typedef GuardObjBase const& Guard;
+
+template<class F, class Tuple>
+struct GuardObj : GuardObjBase {
+  explicit GuardObj(F&& f, Tuple&& args)
+    : f_(std::move(f))
+    , args_(std::move(args))
+  {}
+  GuardObj(GuardObj&& g)
+    : GuardObjBase(std::move(g))
+    , f_(std::move(g.f_))
+    , args_(std::move(g.args_))
+  {}
+
+  ~GuardObj() {
+    folly::applyTuple(f_, args_);
+  }
+
+  GuardObj(const GuardObj&) = delete;
+  GuardObj& operator=(const GuardObj&) = delete;
+
+private:
+  F f_;
+  Tuple args_;
+};
+
+template<class F, class ...Args>
+GuardObj<typename std::decay<F>::type,std::tuple<Args...>>
+guard(F&& f, Args&&... args) {
+  return GuardObj<typename std::decay<F>::type,std::tuple<Args...>>(
+    std::forward<F>(f),
+    std::tuple<Args...>(std::forward<Args>(args)...)
+  );
+}
+
+struct Mover {
+  Mover() {}
+  Mover(Mover&&) {}
+  Mover(const Mover&) = delete;
+  Mover& operator=(const Mover&) = delete;
+};
+
+void move_only_func(Mover&&) {}
+
+}
+
+TEST(ApplyTuple, Test) {
+  auto argsTuple = std::make_tuple(1, 2, 3.0);
+  auto func2 = func;
+  folly::applyTuple(func2, argsTuple);
+  folly::applyTuple(func, argsTuple);
+  folly::applyTuple(func, std::make_tuple(1, 2, 3.0));
+  folly::applyTuple(makeFunc(), std::make_tuple(1, 2, 3.0));
+  folly::applyTuple(makeFunc(), argsTuple);
+
+  std::unique_ptr<Wat> wat(new Wat);
+  folly::applyTuple(&Wat::func, std::make_tuple(wat.get(), 1, 2, 3.0));
+  auto argsTuple2 = std::make_tuple(wat.get(), 1, 2, 3.0);
+  folly::applyTuple(&Wat::func, argsTuple2);
+
+  EXPECT_EQ(10.0,
+            folly::applyTuple(&Wat::retVal,
+                              std::make_tuple(wat.get(), 1, 9.0)));
+
+  auto test = guard(func, 1, 2, 3.0);
+  CopyCount cpy;
+  auto test2 = guard(anotherFunc, cpy);
+  auto test3 = guard(anotherFunc, std::cref(cpy));
+
+  Overloaded ovl;
+  EXPECT_EQ(0,
+            folly::applyTuple(
+              static_cast<int (Overloaded::*)(int)>(&Overloaded::func),
+              std::make_tuple(&ovl, 12)));
+  EXPECT_EQ(true,
+            folly::applyTuple(
+              static_cast<bool (Overloaded::*)(bool)>(&Overloaded::func),
+              std::make_tuple(&ovl, false)));
+
+  int x = folly::applyTuple(std::plus<int>(), std::make_tuple(12, 12));
+  EXPECT_EQ(24, x);
+
+  Mover m;
+  folly::applyTuple(move_only_func,
+                    std::forward_as_tuple(std::forward<Mover>(Mover())));
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/ArenaSmartPtrTest.cpp
@@ -0,0 +1,183 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+ * @author: Marcelo Juchem <marcelo@fb.com>
+ */
+
+#include "folly/Memory.h"
+#include "folly/Arena.h"
+
+#include <gtest/gtest.h>
+
+using namespace folly;
+
+static_assert(
+  is_simple_allocator<int,SysArena>::value,
+  "SysArena should be a simple allocator"
+);
+
+struct global_counter {
+  global_counter(): count_(0) {}
+
+  void increase() { ++count_; }
+  void decrease() {
+    EXPECT_GT(count_, 0);
+    --count_;
+  }
+
+  unsigned count() const { return count_; }
+
+private:
+  unsigned count_;
+};
+
+struct Foo {
+  explicit Foo(global_counter& counter):
+    counter_(counter)
+  {
+    counter_.increase();
+  }
+
+  ~Foo() {
+    counter_.decrease();
+  }
+
+private:
+  global_counter& counter_;
+};
+
+template <typename Allocator>
+void unique_ptr_test(Allocator& allocator) {
+  typedef typename AllocatorUniquePtr<Foo, Allocator>::type ptr_type;
+
+  global_counter counter;
+  EXPECT_EQ(counter.count(), 0);
+
+  Foo* foo = nullptr;
+
+  {
+    auto p = folly::allocate_unique<Foo>(allocator, counter);
+    EXPECT_EQ(counter.count(), 1);
+
+    p.reset();
+    EXPECT_EQ(counter.count(), 0);
+
+    p = folly::allocate_unique<Foo>(allocator, counter);
+    EXPECT_EQ(counter.count(), 1);
+
+    foo = p.release();
+    EXPECT_EQ(counter.count(), 1);
+  }
+  EXPECT_EQ(counter.count(), 1);
+
+  {
+    auto p = folly::allocate_unique<Foo>(allocator, counter);
+    EXPECT_EQ(counter.count(), 2);
+
+    [&](ptr_type g) {
+      EXPECT_EQ(counter.count(), 2);
+      g.reset();
+      EXPECT_EQ(counter.count(), 1);
+    }(std::move(p));
+  }
+  EXPECT_EQ(counter.count(), 1);
+
+  StlAllocator<Allocator, Foo>().destroy(foo);
+  EXPECT_EQ(counter.count(), 0);
+}
+
+TEST(ArenaSmartPtr, unique_ptr_SysArena) {
+  SysArena arena;
+  unique_ptr_test(arena);
+}
+
+TEST(ArenaSmartPtr, unique_ptr_StlAlloc_SysArena) {
+  SysArena arena;
+  StlAllocator<SysArena, Foo> alloc(&arena);
+  unique_ptr_test(alloc);
+}
+
+template <typename Allocator>
+void shared_ptr_test(Allocator& allocator) {
+  typedef std::shared_ptr<Foo> ptr_type;
+
+  global_counter counter;
+  EXPECT_EQ(counter.count(), 0);
+
+  ptr_type foo;
+  EXPECT_EQ(counter.count(), 0);
+  EXPECT_EQ(foo.use_count(), 0);
+
+  {
+    auto p = folly::allocate_shared<Foo>(allocator, counter);
+    EXPECT_EQ(counter.count(), 1);
+    EXPECT_EQ(p.use_count(), 1);
+
+    p.reset();
+    EXPECT_EQ(counter.count(), 0);
+    EXPECT_EQ(p.use_count(), 0);
+
+    p = folly::allocate_shared<Foo>(allocator, counter);
+    EXPECT_EQ(counter.count(), 1);
+    EXPECT_EQ(p.use_count(), 1);
+
+    foo = p;
+    EXPECT_EQ(p.use_count(), 2);
+  }
+  EXPECT_EQ(counter.count(), 1);
+  EXPECT_EQ(foo.use_count(), 1);
+
+  {
+    auto p = foo;
+    EXPECT_EQ(counter.count(), 1);
+    EXPECT_EQ(p.use_count(), 2);
+
+    [&](ptr_type g) {
+      EXPECT_EQ(counter.count(), 1);
+      EXPECT_EQ(p.use_count(), 3);
+      EXPECT_EQ(g.use_count(), 3);
+      g.reset();
+      EXPECT_EQ(counter.count(), 1);
+      EXPECT_EQ(p.use_count(), 2);
+      EXPECT_EQ(g.use_count(), 0);
+    }(p);
+    EXPECT_EQ(counter.count(), 1);
+    EXPECT_EQ(p.use_count(), 2);
+  }
+  EXPECT_EQ(counter.count(), 1);
+  EXPECT_EQ(foo.use_count(), 1);
+
+  foo.reset();
+  EXPECT_EQ(counter.count(), 0);
+  EXPECT_EQ(foo.use_count(), 0);
+}
+
+TEST(ArenaSmartPtr, shared_ptr_SysArena) {
+  SysArena arena;
+  shared_ptr_test(arena);
+}
+
+TEST(ArenaSmartPtr, shared_ptr_StlAlloc_SysArena) {
+  SysArena arena;
+  StlAllocator<SysArena, Foo> alloc(&arena);
+  shared_ptr_test(alloc);
+}
+
+int main(int argc, char *argv[]) {
+  testing::InitGoogleTest(&argc, argv);
+  return RUN_ALL_TESTS();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/ArenaTest.cpp
@@ -0,0 +1,161 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Arena.h"
+#include "folly/Memory.h"
+
+#include <set>
+#include <vector>
+
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+using namespace folly;
+
+TEST(Arena, SizeSanity) {
+  std::set<size_t*> allocatedItems;
+
+  static const size_t requestedBlockSize = 64;
+  SysArena arena(requestedBlockSize);
+  size_t minimum_size = sizeof(SysArena), maximum_size = minimum_size;
+  EXPECT_EQ(arena.totalSize(), minimum_size);
+
+  // Insert a single small element to get a new block
+  size_t* ptr = static_cast<size_t*>(arena.allocate(sizeof(long)));
+  allocatedItems.insert(ptr);
+  minimum_size += requestedBlockSize;
+  maximum_size += goodMallocSize(requestedBlockSize + 1);
+  EXPECT_TRUE(arena.totalSize() >= minimum_size);
+  EXPECT_TRUE(arena.totalSize() <= maximum_size);
+  VLOG(4) << minimum_size << " < " << arena.totalSize() << " < "
+          << maximum_size;
+
+  // Insert a larger element, size should be the same
+  ptr = static_cast<size_t*>(arena.allocate(requestedBlockSize / 2));
+  allocatedItems.insert(ptr);
+  EXPECT_TRUE(arena.totalSize() >= minimum_size);
+  EXPECT_TRUE(arena.totalSize() <= maximum_size);
+  VLOG(4) << minimum_size << " < " << arena.totalSize() << " < "
+          << maximum_size;
+
+  // Insert 10 full block sizes to get 10 new blocks
+  for (int i = 0; i < 10; i++) {
+    ptr = static_cast<size_t*>(arena.allocate(requestedBlockSize));
+    allocatedItems.insert(ptr);
+  }
+  minimum_size += 10 * requestedBlockSize;
+  maximum_size += 10 * goodMallocSize(requestedBlockSize + 1);
+  EXPECT_TRUE(arena.totalSize() >= minimum_size);
+  EXPECT_TRUE(arena.totalSize() <= maximum_size);
+  VLOG(4) << minimum_size << " < " << arena.totalSize() << " < "
+          << maximum_size;
+
+  // Insert something huge
+  ptr = static_cast<size_t*>(arena.allocate(10 * requestedBlockSize));
+  allocatedItems.insert(ptr);
+  minimum_size += 10 * requestedBlockSize;
+  maximum_size += goodMallocSize(10 * requestedBlockSize + 1);
+  EXPECT_TRUE(arena.totalSize() >= minimum_size);
+  EXPECT_TRUE(arena.totalSize() <= maximum_size);
+  VLOG(4) << minimum_size << " < " << arena.totalSize() << " < "
+          << maximum_size;
+
+  // Nuke 'em all
+  for (const auto& item : allocatedItems) {
+    arena.deallocate(item);
+  }
+  //The total size should be the same
+  EXPECT_TRUE(arena.totalSize() >= minimum_size);
+  EXPECT_TRUE(arena.totalSize() <= maximum_size);
+  VLOG(4) << minimum_size << " < " << arena.totalSize() << " < "
+          << maximum_size;
+}
+
+TEST(Arena, BytesUsedSanity) {
+  static const size_t smallChunkSize = 1024;
+  static const size_t blockSize = goodMallocSize(16 * smallChunkSize);
+  const size_t bigChunkSize = blockSize - 4 * smallChunkSize;
+
+  size_t bytesUsed = 0;
+
+  SysArena arena(blockSize);
+  EXPECT_EQ(arena.bytesUsed(), bytesUsed);
+
+  // Insert 2 small chunks
+  arena.allocate(smallChunkSize);
+  arena.allocate(smallChunkSize);
+  bytesUsed += 2 * smallChunkSize;
+  EXPECT_EQ(arena.bytesUsed(), bytesUsed);
+  EXPECT_TRUE(arena.totalSize() >= blockSize);
+  EXPECT_TRUE(arena.totalSize() <= 2 * blockSize);
+
+  // Insert big chunk, should still fit in one block
+  arena.allocate(bigChunkSize);
+  bytesUsed += bigChunkSize;
+  EXPECT_EQ(arena.bytesUsed(), bytesUsed);
+  EXPECT_TRUE(arena.totalSize() >= blockSize);
+  EXPECT_TRUE(arena.totalSize() <= 2 * blockSize);
+
+  // Insert big chunk once more, should trigger new block allocation
+  arena.allocate(bigChunkSize);
+  bytesUsed += bigChunkSize;
+  EXPECT_EQ(arena.bytesUsed(), bytesUsed);
+  EXPECT_TRUE(arena.totalSize() >= 2 * blockSize);
+  EXPECT_TRUE(arena.totalSize() <= 3 * blockSize);
+
+  // Test that bytesUsed() accounts for alignment
+  static const size_t tinyChunkSize = 7;
+  arena.allocate(tinyChunkSize);
+  EXPECT_TRUE(arena.bytesUsed() >= bytesUsed + tinyChunkSize);
+  size_t delta = arena.bytesUsed() - bytesUsed;
+  EXPECT_EQ(delta & (delta - 1), 0);
+}
+
+TEST(Arena, Vector) {
+  static const size_t requestedBlockSize = 64;
+  SysArena arena(requestedBlockSize);
+
+  EXPECT_EQ(arena.totalSize(), sizeof(SysArena));
+
+  std::vector<size_t, StlAllocator<SysArena, size_t>>
+    vec { {}, StlAllocator<SysArena, size_t>(&arena) };
+
+  for (size_t i = 0; i < 1000; i++) {
+    vec.push_back(i);
+  }
+
+  for (size_t i = 0; i < 1000; i++) {
+    EXPECT_EQ(i, vec[i]);
+  }
+}
+
+TEST(Arena, SizeLimit) {
+  static const size_t requestedBlockSize = sizeof(size_t);
+  static const size_t maxSize = 10 * requestedBlockSize;
+
+  SysArena arena(requestedBlockSize, maxSize);
+
+  void* a = arena.allocate(sizeof(size_t));
+  EXPECT_TRUE(a != nullptr);
+  EXPECT_THROW(arena.allocate(maxSize + 1), std::bad_alloc);
+}
+
+int main(int argc, char *argv[]) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  auto ret = RUN_ALL_TESTS();
+  return ret;
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/AtomicBitSetTest.cpp
@@ -0,0 +1,62 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/AtomicBitSet.h"
+
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+namespace folly { namespace test {
+
+TEST(AtomicBitSet, Simple) {
+  constexpr size_t kSize = 1000;
+  AtomicBitSet<kSize> bs;
+
+  EXPECT_EQ(kSize, bs.size());
+
+  for (size_t i = 0; i < kSize; ++i) {
+    EXPECT_FALSE(bs[i]);
+  }
+
+  bs.set(42);
+  for (size_t i = 0; i < kSize; ++i) {
+    EXPECT_EQ(i == 42, bs[i]);
+  }
+
+  bs.set(43);
+  for (size_t i = 0; i < kSize; ++i) {
+    EXPECT_EQ((i == 42 || i == 43), bs[i]);
+  }
+
+  bs.reset(42);
+  for (size_t i = 0; i < kSize; ++i) {
+    EXPECT_EQ((i == 43), bs[i]);
+  }
+
+  bs.reset(43);
+  for (size_t i = 0; i < kSize; ++i) {
+    EXPECT_FALSE(bs[i]);
+  }
+}
+
+}}  // namespaces
+
+int main(int argc, char *argv[]) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  return RUN_ALL_TESTS();
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/AtomicHashArrayTest.cpp
@@ -0,0 +1,193 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <sys/mman.h>
+
+#include <cstddef>
+#include <map>
+#include <stdexcept>
+
+#include "folly/AtomicHashArray.h"
+#include "folly/Hash.h"
+#include "folly/Conv.h"
+#include "folly/Memory.h"
+#include <gtest/gtest.h>
+
+#if !defined(MAP_ANONYMOUS) && defined(MAP_ANON)
+#define MAP_ANONYMOUS MAP_ANON
+#endif
+
+using namespace std;
+using namespace folly;
+
+template <class T>
+class MmapAllocator {
+ public:
+  typedef T value_type;
+  typedef T* pointer;
+  typedef const T* const_pointer;
+  typedef T& reference;
+  typedef const T& const_reference;
+
+  typedef ptrdiff_t difference_type;
+  typedef size_t size_type;
+
+  T* address(T& x) const {
+    return std::addressof(x);
+  }
+
+  const T* address(const T& x) const {
+    return std::addressof(x);
+  }
+
+  size_t max_size() const {
+    return std::numeric_limits<size_t>::max();
+  }
+
+  template <class U> struct rebind {
+    typedef MmapAllocator<U> other;
+  };
+
+  bool operator!=(const MmapAllocator<T>& other) const {
+    return !(*this == other);
+  }
+
+  bool operator==(const MmapAllocator<T>& other) const {
+    return true;
+  }
+
+  template <class... Args>
+  void construct(T* p, Args&&... args) {
+    new (p) T(std::forward<Args>(args)...);
+  }
+
+  void destroy(T* p) {
+    p->~T();
+  }
+
+  T *allocate(size_t n) {
+    void *p = mmap(nullptr, n * sizeof(T), PROT_READ | PROT_WRITE,
+        MAP_SHARED | MAP_ANONYMOUS, -1, 0);
+    if (!p) throw std::bad_alloc();
+    return (T *)p;
+  }
+
+  void deallocate(T *p, size_t n) {
+    munmap(p, n * sizeof(T));
+  }
+};
+
+template<class KeyT, class ValueT>
+pair<KeyT,ValueT> createEntry(int i) {
+  return pair<KeyT,ValueT>(to<KeyT>(folly::hash::jenkins_rev_mix32(i) % 1000),
+                           to<ValueT>(i + 3));
+}
+
+template<class KeyT, class ValueT, class Allocator = std::allocator<char>>
+void testMap() {
+  typedef AtomicHashArray<KeyT, ValueT, std::hash<KeyT>,
+                          std::equal_to<KeyT>, Allocator> MyArr;
+  auto arr = MyArr::create(150);
+  map<KeyT, ValueT> ref;
+  for (int i = 0; i < 100; ++i) {
+    auto e = createEntry<KeyT, ValueT>(i);
+    auto ret = arr->insert(e);
+    EXPECT_EQ(!ref.count(e.first), ret.second);  // succeed iff not in ref
+    ref.insert(e);
+    EXPECT_EQ(ref.size(), arr->size());
+    if (ret.first == arr->end()) {
+      EXPECT_FALSE("AHA should not have run out of space.");
+      continue;
+    }
+    EXPECT_EQ(e.first, ret.first->first);
+    EXPECT_EQ(ref.find(e.first)->second, ret.first->second);
+  }
+
+  for (int i = 125; i > 0; i -= 10) {
+    auto e = createEntry<KeyT, ValueT>(i);
+    auto ret = arr->erase(e.first);
+    auto refRet = ref.erase(e.first);
+    EXPECT_EQ(ref.size(), arr->size());
+    EXPECT_EQ(refRet, ret);
+  }
+
+  for (int i = 155; i > 0; i -= 10) {
+    auto e = createEntry<KeyT, ValueT>(i);
+    auto ret = arr->insert(e);
+    auto refRet = ref.insert(e);
+    EXPECT_EQ(ref.size(), arr->size());
+    EXPECT_EQ(*refRet.first, *ret.first);
+    EXPECT_EQ(refRet.second, ret.second);
+  }
+
+  for (const auto& e : ref) {
+    auto ret = arr->find(e.first);
+    if (ret == arr->end()) {
+      EXPECT_FALSE("Key was not in AHA");
+      continue;
+    }
+    EXPECT_EQ(e.first, ret->first);
+    EXPECT_EQ(e.second, ret->second);
+  }
+}
+
+template<class KeyT, class ValueT, class Allocator = std::allocator<char>>
+void testNoncopyableMap() {
+  typedef AtomicHashArray<KeyT, std::unique_ptr<ValueT>, std::hash<KeyT>,
+                          std::equal_to<KeyT>, Allocator> MyArr;
+  auto arr = MyArr::create(150);
+  for (int i = 0; i < 100; i++) {
+    arr->insert(make_pair(i,std::unique_ptr<ValueT>(new ValueT(i))));
+  }
+  for (int i = 0; i < 100; i++) {
+    auto ret = arr->find(i);
+    EXPECT_EQ(*(ret->second), i);
+  }
+}
+
+
+TEST(Aha, InsertErase_i32_i32) {
+  testMap<int32_t, int32_t>();
+  testMap<int32_t, int32_t, MmapAllocator<char>>();
+  testNoncopyableMap<int32_t, int32_t>();
+  testNoncopyableMap<int32_t, int32_t, MmapAllocator<char>>();
+}
+TEST(Aha, InsertErase_i64_i32) {
+  testMap<int64_t, int32_t>();
+  testMap<int64_t, int32_t, MmapAllocator<char>>();
+  testNoncopyableMap<int64_t, int32_t>();
+  testNoncopyableMap<int64_t, int32_t, MmapAllocator<char>>();
+}
+TEST(Aha, InsertErase_i64_i64) {
+  testMap<int64_t, int64_t>();
+  testMap<int64_t, int64_t, MmapAllocator<char>>();
+  testNoncopyableMap<int64_t, int64_t>();
+  testNoncopyableMap<int64_t, int64_t, MmapAllocator<char>>();
+}
+TEST(Aha, InsertErase_i32_i64) {
+  testMap<int32_t, int64_t>();
+  testMap<int32_t, int64_t, MmapAllocator<char>>();
+  testNoncopyableMap<int32_t, int64_t>();
+  testNoncopyableMap<int32_t, int64_t, MmapAllocator<char>>();
+}
+TEST(Aha, InsertErase_i32_str) {
+  testMap<int32_t, string>();
+  testMap<int32_t, string, MmapAllocator<char>>();
+}
+TEST(Aha, InsertErase_i64_str) {
+  testMap<int64_t, string>();
+  testMap<int64_t, string, MmapAllocator<char>>();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/AtomicHashMapTest.cpp
@@ -0,0 +1,830 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/AtomicHashMap.h"
+
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+#include <sys/time.h>
+#include <thread>
+#include <atomic>
+#include <memory>
+#include "folly/Benchmark.h"
+#include "folly/Conv.h"
+
+using std::vector;
+using std::string;
+using folly::AtomicHashMap;
+using folly::AtomicHashArray;
+
+// Tunables:
+DEFINE_double(targetLoadFactor, 0.75, "Target memory utilization fraction.");
+DEFINE_double(maxLoadFactor, 0.80, "Max before growth.");
+DEFINE_int32(numThreads, 8, "Threads to use for concurrency tests.");
+DEFINE_int64(numBMElements, 12 * 1000 * 1000, "Size of maps for benchmarks.");
+
+const double LF = FLAGS_maxLoadFactor / FLAGS_targetLoadFactor;
+const int maxBMElements = int(FLAGS_numBMElements * LF); // hit our target LF.
+
+static int64_t nowInUsec() {
+  timeval tv;
+  gettimeofday(&tv, 0);
+  return int64_t(tv.tv_sec) * 1000 * 1000 + tv.tv_usec;
+}
+
+TEST(Ahm, BasicStrings) {
+  typedef AtomicHashMap<int64_t,string> AHM;
+  AHM myMap(1024);
+  EXPECT_TRUE(myMap.begin() == myMap.end());
+
+  for (int i = 0; i < 100; ++i) {
+    myMap.insert(make_pair(i, folly::to<string>(i)));
+  }
+  for (int i = 0; i < 100; ++i) {
+    EXPECT_EQ(myMap.find(i)->second, folly::to<string>(i));
+  }
+
+  myMap.insert(std::make_pair(999, "A"));
+  myMap.insert(std::make_pair(999, "B"));
+  EXPECT_EQ(myMap.find(999)->second, "A"); // shouldn't have overwritten
+  myMap.find(999)->second = "B";
+  myMap.find(999)->second = "C";
+  EXPECT_EQ(myMap.find(999)->second, "C");
+  EXPECT_EQ(myMap.find(999)->first, 999);
+}
+
+
+TEST(Ahm, BasicNoncopyable) {
+  typedef AtomicHashMap<int64_t,std::unique_ptr<int>> AHM;
+  AHM myMap(1024);
+  EXPECT_TRUE(myMap.begin() == myMap.end());
+
+  for (int i = 0; i < 50; ++i) {
+    myMap.insert(make_pair(i, std::unique_ptr<int>(new int(i))));
+  }
+  for (int i = 50; i < 100; ++i) {
+    myMap.insert(i, std::unique_ptr<int>(new int (i)));
+  }
+  for (int i = 0; i < 100; ++i) {
+    EXPECT_EQ(*(myMap.find(i)->second), i);
+  }
+  for (int i = 0; i < 100; i+=4) {
+    myMap.erase(i);
+  }
+  for (int i = 0; i < 100; i+=4) {
+    EXPECT_EQ(myMap.find(i), myMap.end());
+  }
+}
+
+typedef int32_t     KeyT;
+typedef int64_t     KeyTBig;
+typedef int32_t     ValueT;
+
+typedef AtomicHashMap<KeyT,ValueT> AHMapT;
+typedef AHMapT::value_type RecordT;
+typedef AtomicHashArray<KeyT,ValueT> AHArrayT;
+
+AHArrayT::Config config;
+static AHArrayT::SmartPtr globalAHA(nullptr);
+static std::unique_ptr<AHMapT> globalAHM;
+
+// Generate a deterministic value based on an input key
+static int genVal(int key) {
+  return key / 3;
+}
+
+TEST(Ahm, grow) {
+  VLOG(1) << "Overhead: " << sizeof(AHArrayT) << " (array) " <<
+    sizeof(AHMapT) + sizeof(AHArrayT) << " (map/set) Bytes.";
+  int numEntries = 10000;
+  float sizeFactor = 0.46;
+
+  std::unique_ptr<AHMapT> m(new AHMapT(int(numEntries * sizeFactor), config));
+
+  // load map - make sure we succeed and the index is accurate
+  bool success = true;
+  for (uint64_t i = 0; i < numEntries; i++) {
+    auto ret = m->insert(RecordT(i, genVal(i)));
+    success &= ret.second;
+    success &= (m->findAt(ret.first.getIndex())->second == genVal(i));
+  }
+  // Overwrite vals to make sure there are no dups
+  // Every insert should fail because the keys are already in the map.
+  success = true;
+  for (uint64_t i = 0; i < numEntries; i++) {
+    auto ret = m->insert(RecordT(i, genVal(i * 2)));
+    success &= (ret.second == false);  // fail on collision
+    success &= (ret.first->second == genVal(i)); // return the previous value
+    success &= (m->findAt(ret.first.getIndex())->second == genVal(i));
+  }
+  EXPECT_TRUE(success);
+
+  // check correctness
+  size_t cap = m->capacity();
+  ValueT val;
+  EXPECT_GT(m->numSubMaps(), 1);  // make sure we grew
+  success = true;
+  EXPECT_EQ(m->size(), numEntries);
+  for (int i = 0; i < numEntries; i++) {
+    success &= (m->find(i)->second == genVal(i));
+  }
+  EXPECT_TRUE(success);
+
+  // Check findAt
+  success = true;
+  KeyT key(0);
+  AHMapT::const_iterator retIt;
+  for (uint64_t i = 0; i < numEntries; i++) {
+    retIt = m->find(i);
+    retIt = m->findAt(retIt.getIndex());
+    success &= (retIt->second == genVal(i));
+    success &= (retIt->first == i);
+  }
+  EXPECT_TRUE(success);
+
+  // Try modifying value
+  m->find(8)->second = 5309;
+  EXPECT_EQ(m->find(8)->second, 5309);
+
+  // check clear()
+  m->clear();
+  success = true;
+  for (uint64_t i = 0; i < numEntries / 2; i++) {
+    success &= m->insert(RecordT(i, genVal(i))).second;
+  }
+  EXPECT_TRUE(success);
+  EXPECT_EQ(m->size(), numEntries / 2);
+}
+
+TEST(Ahm, iterator) {
+  int numEntries = 10000;
+  float sizeFactor = .46;
+  std::unique_ptr<AHMapT> m(new AHMapT(int(numEntries * sizeFactor), config));
+
+  // load map - make sure we succeed and the index is accurate
+  for (uint64_t i = 0; i < numEntries; i++) {
+    m->insert(RecordT(i, genVal(i)));
+  }
+
+  bool success = true;
+  int count = 0;
+  FOR_EACH(it, *m) {
+    success &= (it->second == genVal(it->first));
+    ++count;
+  }
+  EXPECT_TRUE(success);
+  EXPECT_EQ(count, numEntries);
+}
+
+class Counters {
+private:
+  // Note: Unfortunately can't currently put a std::atomic<int64_t> in
+  // the value in ahm since it doesn't support types that are both non-copy
+  // and non-move constructible yet.
+  AtomicHashMap<int64_t,int64_t> ahm;
+
+public:
+  explicit Counters(size_t numCounters) : ahm(numCounters) {}
+
+  void increment(int64_t obj_id) {
+    auto ret = ahm.insert(std::make_pair(obj_id, 1));
+    if (!ret.second) {
+      // obj_id already exists, increment count
+      __sync_fetch_and_add(&ret.first->second, 1);
+    }
+  }
+
+  int64_t getValue(int64_t obj_id) {
+    auto ret = ahm.find(obj_id);
+    return ret != ahm.end() ? ret->second : 0;
+  }
+
+  // export the counters without blocking increments
+  string toString() {
+    string ret = "{\n";
+    ret.reserve(ahm.size() * 32);
+    for (const auto& e : ahm) {
+      ret += folly::to<string>(
+        "  [", e.first, ":", e.second, "]\n");
+    }
+    ret += "}\n";
+    return ret;
+  }
+};
+
+// If you get an error "terminate called without an active exception", there
+// might be too many threads getting created - decrease numKeys and/or mult.
+TEST(Ahm, counter) {
+  const int numKeys = 10;
+  const int mult = 10;
+  Counters c(numKeys);
+  vector<int64_t> keys;
+  FOR_EACH_RANGE(i, 1, numKeys) {
+    keys.push_back(i);
+  }
+  vector<std::thread> threads;
+  for (auto key : keys) {
+    FOR_EACH_RANGE(i, 0, key * mult) {
+      threads.push_back(std::thread([&, key] { c.increment(key); }));
+    }
+  }
+  for (auto& t : threads) {
+    t.join();
+  }
+  string str = c.toString();
+  for (auto key : keys) {
+    int val = key * mult;
+    EXPECT_EQ(val, c.getValue(key));
+    EXPECT_NE(string::npos, str.find(folly::to<string>("[",key,":",val,"]")));
+  }
+}
+
+class Integer {
+
+ public:
+  explicit Integer(KeyT v = 0) : v_(v) {}
+
+  Integer& operator=(const Integer& a) {
+    static bool throwException_ = false;
+    throwException_ = !throwException_;
+    if (throwException_) {
+      throw 1;
+    }
+    v_ = a.v_;
+    return *this;
+  }
+
+  bool operator==(const Integer& a) const { return v_ == a.v_; }
+
+ private:
+  KeyT v_;
+};
+
+TEST(Ahm, map_exception_safety) {
+  typedef AtomicHashMap<KeyT,Integer> MyMapT;
+
+  int numEntries = 10000;
+  float sizeFactor = 0.46;
+  std::unique_ptr<MyMapT> m(new MyMapT(int(numEntries * sizeFactor)));
+
+  bool success = true;
+  int count = 0;
+  for (int i = 0; i < numEntries; i++) {
+    try {
+      m->insert(i, Integer(genVal(i)));
+      success &= (m->find(i)->second == Integer(genVal(i)));
+      ++count;
+    } catch (...) {
+      success &= !m->count(i);
+    }
+  }
+  EXPECT_EQ(count, m->size());
+  EXPECT_TRUE(success);
+}
+
+TEST(Ahm, basicErase) {
+  int numEntries = 3000;
+
+  std::unique_ptr<AHMapT> s(new AHMapT(numEntries, config));
+  // Iterate filling up the map and deleting all keys a few times
+  // to test more than one subMap.
+  for (int iterations = 0; iterations < 4; ++iterations) {
+    // Testing insertion of keys
+    bool success = true;
+    for (uint64_t i = 0; i < numEntries; ++i) {
+      success &= !(s->count(i));
+      auto ret = s->insert(RecordT(i, i));
+      success &= s->count(i);
+      success &= ret.second;
+    }
+    EXPECT_TRUE(success);
+    EXPECT_EQ(s->size(), numEntries);
+
+    // Delete every key in the map and verify that the key is gone and the the
+    // size is correct.
+    success = true;
+    for (uint64_t i = 0; i < numEntries; ++i) {
+      success &= s->erase(i);
+      success &= (s->size() == numEntries - 1 - i);
+      success &= !(s->count(i));
+      success &= !(s->erase(i));
+    }
+    EXPECT_TRUE(success);
+  }
+  VLOG(1) << "Final number of subMaps = " << s->numSubMaps();
+}
+
+namespace {
+
+inline KeyT randomizeKey(int key) {
+  // We deterministically randomize the key to more accurately simulate
+  // real-world usage, and to avoid pathalogical performance patterns (e.g.
+  // those related to __gnu_cxx::hash<int64_t>()(1) == 1).
+  //
+  // Use a hash function we don't normally use for ints to avoid interactions.
+  return folly::hash::jenkins_rev_mix32(key);
+}
+
+int numOpsPerThread = 0;
+
+void* insertThread(void* jj) {
+  int64_t j = (int64_t) jj;
+  for (int i = 0; i < numOpsPerThread; ++i) {
+    KeyT key = randomizeKey(i + j * numOpsPerThread);
+    globalAHM->insert(key, genVal(key));
+  }
+  return NULL;
+}
+
+void* insertThreadArr(void* jj) {
+  int64_t j = (int64_t) jj;
+  for (int i = 0; i < numOpsPerThread; ++i) {
+    KeyT key = randomizeKey(i + j * numOpsPerThread);
+    globalAHA->insert(std::make_pair(key, genVal(key)));
+  }
+  return NULL;
+}
+
+std::atomic<bool> runThreadsCreatedAllThreads;
+void runThreads(void *(*thread)(void*), int numThreads, void **statuses) {
+  folly::BenchmarkSuspender susp;
+  runThreadsCreatedAllThreads.store(false);
+  vector<pthread_t> threadIds;
+  for (int64_t j = 0; j < numThreads; j++) {
+    pthread_t tid;
+    if (pthread_create(&tid, NULL, thread, (void*) j) != 0) {
+       LOG(ERROR) << "Could not start thread";
+    } else {
+      threadIds.push_back(tid);
+    }
+  }
+  susp.dismiss();
+
+  runThreadsCreatedAllThreads.store(true);
+  for (int i = 0; i < threadIds.size(); ++i) {
+    pthread_join(threadIds[i], statuses == NULL ? NULL : &statuses[i]);
+  }
+}
+
+void runThreads(void *(*thread)(void*)) {
+  runThreads(thread, FLAGS_numThreads, NULL);
+}
+
+}
+
+TEST(Ahm, collision_test) {
+  const int numInserts = 1000000 / 4;
+
+  // Doing the same number on each thread so we collide.
+  numOpsPerThread = numInserts;
+
+  float sizeFactor = 0.46;
+  int entrySize = sizeof(KeyT) + sizeof(ValueT);
+  VLOG(1) << "Testing " << numInserts << " unique " << entrySize <<
+    " Byte entries replicated in " << FLAGS_numThreads <<
+    " threads with " << FLAGS_maxLoadFactor * 100.0 << "% max load factor.";
+
+  globalAHM.reset(new AHMapT(int(numInserts * sizeFactor), config));
+
+  size_t sizeInit = globalAHM->capacity();
+  VLOG(1) << "  Initial capacity: " << sizeInit;
+
+  double start = nowInUsec();
+  runThreads([](void*) -> void* { // collisionInsertThread
+    for (int i = 0; i < numOpsPerThread; ++i) {
+      KeyT key = randomizeKey(i);
+      globalAHM->insert(key, genVal(key));
+    }
+    return nullptr;
+  });
+  double elapsed = nowInUsec() - start;
+
+  size_t finalCap = globalAHM->capacity();
+  size_t sizeAHM = globalAHM->size();
+  VLOG(1) << elapsed/sizeAHM << " usec per " << FLAGS_numThreads <<
+    " duplicate inserts (atomic).";
+  VLOG(1) << "  Final capacity: " << finalCap << " in " <<
+    globalAHM->numSubMaps() << " sub maps (" <<
+    sizeAHM * 100 / finalCap << "% load factor, " <<
+    (finalCap - sizeInit) * 100 / sizeInit << "% growth).";
+
+  // check correctness
+  EXPECT_EQ(sizeAHM, numInserts);
+  bool success = true;
+  ValueT val;
+  for (int i = 0; i < numInserts; ++i) {
+    KeyT key = randomizeKey(i);
+    success &= (globalAHM->find(key)->second == genVal(key));
+  }
+  EXPECT_TRUE(success);
+
+  // check colliding finds
+  start = nowInUsec();
+  runThreads([](void*) -> void* { // collisionFindThread
+    KeyT key(0);
+    for (int i = 0; i < numOpsPerThread; ++i) {
+      globalAHM->find(key);
+    }
+    return nullptr;
+  });
+
+  elapsed = nowInUsec() - start;
+
+  VLOG(1) << elapsed/sizeAHM << " usec per " << FLAGS_numThreads <<
+    " duplicate finds (atomic).";
+}
+
+namespace {
+
+const int kInsertPerThread = 100000;
+int raceFinalSizeEstimate;
+
+void* raceIterateThread(void* jj) {
+  int64_t j = (int64_t) jj;
+  int count = 0;
+
+  AHMapT::iterator it = globalAHM->begin();
+  AHMapT::iterator end = globalAHM->end();
+  for (; it != end; ++it) {
+    ++count;
+    if (count > raceFinalSizeEstimate) {
+      EXPECT_FALSE("Infinite loop in iterator.");
+      return NULL;
+    }
+  }
+  return NULL;
+}
+
+void* raceInsertRandomThread(void* jj) {
+  int64_t j = (int64_t) jj;
+  for (int i = 0; i < kInsertPerThread; ++i) {
+    KeyT key = rand();
+    globalAHM->insert(key, genVal(key));
+  }
+  return NULL;
+}
+
+}
+
+// Test for race conditions when inserting and iterating at the same time and
+// creating multiple submaps.
+TEST(Ahm, race_insert_iterate_thread_test) {
+  const int kInsertThreads = 20;
+  const int kIterateThreads = 20;
+  raceFinalSizeEstimate = kInsertThreads * kInsertPerThread;
+
+  VLOG(1) << "Testing iteration and insertion with " << kInsertThreads
+    << " threads inserting and " << kIterateThreads << " threads iterating.";
+
+  globalAHM.reset(new AHMapT(raceFinalSizeEstimate / 9, config));
+
+  vector<pthread_t> threadIds;
+  for (int64_t j = 0; j < kInsertThreads + kIterateThreads; j++) {
+    pthread_t tid;
+    void *(*thread)(void*) =
+      (j < kInsertThreads ? raceInsertRandomThread : raceIterateThread);
+    if (pthread_create(&tid, NULL, thread, (void*) j) != 0) {
+      LOG(ERROR) << "Could not start thread";
+    } else {
+      threadIds.push_back(tid);
+    }
+  }
+  for (int i = 0; i < threadIds.size(); ++i) {
+    pthread_join(threadIds[i], NULL);
+  }
+  VLOG(1) << "Ended up with " << globalAHM->numSubMaps() << " submaps";
+  VLOG(1) << "Final size of map " << globalAHM->size();
+}
+
+namespace {
+
+const int kTestEraseInsertions = 200000;
+std::atomic<int32_t> insertedLevel;
+
+void* testEraseInsertThread(void*) {
+  for (int i = 0; i < kTestEraseInsertions; ++i) {
+    KeyT key = randomizeKey(i);
+    globalAHM->insert(key, genVal(key));
+    insertedLevel.store(i, std::memory_order_release);
+  }
+  insertedLevel.store(kTestEraseInsertions, std::memory_order_release);
+  return NULL;
+}
+
+void* testEraseEraseThread(void*) {
+  for (int i = 0; i < kTestEraseInsertions; ++i) {
+    /*
+     * Make sure that we don't get ahead of the insert thread, because
+     * part of the condition for this unit test succeeding is that the
+     * map ends up empty.
+     *
+     * Note, there is a subtle case here when a new submap is
+     * allocated: the erasing thread might get 0 from count(key)
+     * because it hasn't seen numSubMaps_ update yet.  To avoid this
+     * race causing problems for the test (it's ok for real usage), we
+     * lag behind the inserter by more than just element.
+     */
+    const int lag = 10;
+    int currentLevel;
+    do {
+      currentLevel = insertedLevel.load(std::memory_order_acquire);
+      if (currentLevel == kTestEraseInsertions) currentLevel += lag + 1;
+    } while (currentLevel - lag < i);
+
+    KeyT key = randomizeKey(i);
+    while (globalAHM->count(key)) {
+      if (globalAHM->erase(key)) {
+        break;
+      }
+    }
+  }
+  return NULL;
+}
+
+}
+
+// Here we have a single thread inserting some values, and several threads
+// racing to delete the values in the order they were inserted.
+TEST(Ahm, thread_erase_insert_race) {
+  const int kInsertThreads = 1;
+  const int kEraseThreads = 10;
+
+  VLOG(1) << "Testing insertion and erase with " << kInsertThreads
+    << " thread inserting and " << kEraseThreads << " threads erasing.";
+
+  globalAHM.reset(new AHMapT(kTestEraseInsertions / 4, config));
+
+  vector<pthread_t> threadIds;
+  for (int64_t j = 0; j < kInsertThreads + kEraseThreads; j++) {
+    pthread_t tid;
+    void *(*thread)(void*) =
+      (j < kInsertThreads ? testEraseInsertThread : testEraseEraseThread);
+    if (pthread_create(&tid, NULL, thread, (void*) j) != 0) {
+      LOG(ERROR) << "Could not start thread";
+    } else {
+      threadIds.push_back(tid);
+    }
+  }
+  for (int i = 0; i < threadIds.size(); i++) {
+    pthread_join(threadIds[i], NULL);
+  }
+
+  EXPECT_TRUE(globalAHM->empty());
+  EXPECT_EQ(globalAHM->size(), 0);
+
+  VLOG(1) << "Ended up with " << globalAHM->numSubMaps() << " submaps";
+}
+
+// Repro for T#483734: Duplicate AHM inserts due to incorrect AHA return value.
+typedef AtomicHashArray<int32_t, int32_t> AHA;
+AHA::Config configRace;
+auto atomicHashArrayInsertRaceArray = AHA::create(2, configRace);
+void* atomicHashArrayInsertRaceThread(void* j) {
+  AHA* arr = atomicHashArrayInsertRaceArray.get();
+  uintptr_t numInserted = 0;
+  while (!runThreadsCreatedAllThreads.load());
+  for (int i = 0; i < 2; i++) {
+    if (arr->insert(RecordT(randomizeKey(i), 0)).first != arr->end()) {
+      numInserted++;
+    }
+  }
+  pthread_exit((void *) numInserted);
+}
+TEST(Ahm, atomic_hash_array_insert_race) {
+  AHA* arr = atomicHashArrayInsertRaceArray.get();
+  int numIterations = 50000, FLAGS_numThreads = 4;
+  void* statuses[FLAGS_numThreads];
+  for (int i = 0; i < numIterations; i++) {
+    arr->clear();
+    runThreads(atomicHashArrayInsertRaceThread, FLAGS_numThreads, statuses);
+    EXPECT_GE(arr->size(), 1);
+    for (int j = 0; j < FLAGS_numThreads; j++) {
+      EXPECT_EQ(arr->size(), uintptr_t(statuses[j]));
+    }
+  }
+}
+
+namespace {
+
+void loadGlobalAha() {
+  std::cout << "loading global AHA with " << FLAGS_numThreads
+            << " threads...\n";
+  uint64_t start = nowInUsec();
+  globalAHA = AHArrayT::create(maxBMElements, config);
+  numOpsPerThread = FLAGS_numBMElements / FLAGS_numThreads;
+  CHECK_EQ(0, FLAGS_numBMElements % FLAGS_numThreads) <<
+    "kNumThreads must evenly divide kNumInserts.";
+  runThreads(insertThreadArr);
+  uint64_t elapsed = nowInUsec() - start;
+  std::cout << "  took " << elapsed / 1000 << " ms (" <<
+    (elapsed * 1000 / FLAGS_numBMElements) << " ns/insert).\n";
+  EXPECT_EQ(globalAHA->size(), FLAGS_numBMElements);
+}
+
+void loadGlobalAhm() {
+  std::cout << "loading global AHM with " << FLAGS_numThreads
+            << " threads...\n";
+  uint64_t start = nowInUsec();
+  globalAHM.reset(new AHMapT(maxBMElements, config));
+  numOpsPerThread = FLAGS_numBMElements / FLAGS_numThreads;
+  runThreads(insertThread);
+  uint64_t elapsed = nowInUsec() - start;
+  std::cout << "  took " << elapsed / 1000 << " ms (" <<
+    (elapsed * 1000 / FLAGS_numBMElements) << " ns/insert).\n";
+  EXPECT_EQ(globalAHM->size(), FLAGS_numBMElements);
+}
+
+}
+
+BENCHMARK(st_aha_find, iters) {
+  CHECK_LE(iters, FLAGS_numBMElements);
+  for (int i = 0; i < iters; i++) {
+    KeyT key = randomizeKey(i);
+    folly::doNotOptimizeAway(globalAHA->find(key)->second);
+  }
+}
+
+BENCHMARK(st_ahm_find, iters) {
+  CHECK_LE(iters, FLAGS_numBMElements);
+  for (int i = 0; i < iters; i++) {
+    KeyT key = randomizeKey(i);
+    folly::doNotOptimizeAway(globalAHM->find(key)->second);
+  }
+}
+
+BENCHMARK_DRAW_LINE()
+
+BENCHMARK(mt_ahm_miss, iters) {
+  CHECK_LE(iters, FLAGS_numBMElements);
+  numOpsPerThread = iters / FLAGS_numThreads;
+  runThreads([](void* jj) -> void* {
+    int64_t j = (int64_t) jj;
+    while (!runThreadsCreatedAllThreads.load());
+    for (int i = 0; i < numOpsPerThread; ++i) {
+      KeyT key = i + j * numOpsPerThread * 100;
+      folly::doNotOptimizeAway(globalAHM->find(key) == globalAHM->end());
+    }
+    return nullptr;
+  });
+}
+
+BENCHMARK(st_ahm_miss, iters) {
+  CHECK_LE(iters, FLAGS_numBMElements);
+  for (int i = 0; i < iters; i++) {
+    KeyT key = randomizeKey(i + iters * 100);
+    folly::doNotOptimizeAway(globalAHM->find(key) == globalAHM->end());
+  }
+}
+
+BENCHMARK(mt_ahm_find_insert_mix, iters) {
+  CHECK_LE(iters, FLAGS_numBMElements);
+  numOpsPerThread = iters / FLAGS_numThreads;
+  runThreads([](void* jj) -> void* {
+    int64_t j = (int64_t) jj;
+    while (!runThreadsCreatedAllThreads.load());
+    for (int i = 0; i < numOpsPerThread; ++i) {
+      if (i % 128) {  // ~1% insert mix
+        KeyT key = randomizeKey(i + j * numOpsPerThread);
+        folly::doNotOptimizeAway(globalAHM->find(key)->second);
+      } else {
+        KeyT key = randomizeKey(i + j * numOpsPerThread * 100);
+        globalAHM->insert(key, genVal(key));
+      }
+    }
+    return nullptr;
+  });
+}
+
+BENCHMARK(mt_aha_find, iters) {
+  CHECK_LE(iters, FLAGS_numBMElements);
+  numOpsPerThread = iters / FLAGS_numThreads;
+  runThreads([](void* jj) -> void* {
+      int64_t j = (int64_t) jj;
+      while (!runThreadsCreatedAllThreads.load());
+      for (int i = 0; i < numOpsPerThread; ++i) {
+        KeyT key = randomizeKey(i + j * numOpsPerThread);
+        folly::doNotOptimizeAway(globalAHA->find(key)->second);
+      }
+      return nullptr;
+    });
+}
+
+BENCHMARK(mt_ahm_find, iters) {
+  CHECK_LE(iters, FLAGS_numBMElements);
+  numOpsPerThread = iters / FLAGS_numThreads;
+  runThreads([](void* jj) -> void* {
+    int64_t j = (int64_t) jj;
+    while (!runThreadsCreatedAllThreads.load());
+    for (int i = 0; i < numOpsPerThread; ++i) {
+      KeyT key = randomizeKey(i + j * numOpsPerThread);
+      folly::doNotOptimizeAway(globalAHM->find(key)->second);
+    }
+    return nullptr;
+  });
+}
+
+KeyT k;
+BENCHMARK(st_baseline_modulus_and_random, iters) {
+  for (int i = 0; i < iters; ++i) {
+    k = randomizeKey(i) % iters;
+  }
+}
+
+// insertions go last because they reset the map
+
+BENCHMARK(mt_ahm_insert, iters) {
+  BENCHMARK_SUSPEND {
+    globalAHM.reset(new AHMapT(int(iters * LF), config));
+    numOpsPerThread = iters / FLAGS_numThreads;
+  }
+  runThreads(insertThread);
+}
+
+BENCHMARK(st_ahm_insert, iters) {
+  folly::BenchmarkSuspender susp;
+  std::unique_ptr<AHMapT> ahm(new AHMapT(int(iters * LF), config));
+  susp.dismiss();
+
+  for (int i = 0; i < iters; i++) {
+    KeyT key = randomizeKey(i);
+    ahm->insert(key, genVal(key));
+  }
+}
+
+void benchmarkSetup() {
+  config.maxLoadFactor = FLAGS_maxLoadFactor;
+  configRace.maxLoadFactor = 0.5;
+  int numCores = sysconf(_SC_NPROCESSORS_ONLN);
+  loadGlobalAha();
+  loadGlobalAhm();
+  string numIters = folly::to<string>(
+    std::min(1000000, int(FLAGS_numBMElements)));
+
+  google::SetCommandLineOptionWithMode(
+    "bm_max_iters", numIters.c_str(), google::SET_FLAG_IF_DEFAULT
+  );
+  google::SetCommandLineOptionWithMode(
+    "bm_min_iters", numIters.c_str(), google::SET_FLAG_IF_DEFAULT
+  );
+  string numCoresStr = folly::to<string>(numCores);
+  google::SetCommandLineOptionWithMode(
+    "numThreads", numCoresStr.c_str(), google::SET_FLAG_IF_DEFAULT
+  );
+
+  std::cout << "\nRunning AHM benchmarks on machine with " << numCores
+    << " logical cores.\n"
+       "  num elements per map: " << FLAGS_numBMElements << "\n"
+    << "  num threads for mt tests: " << FLAGS_numThreads << "\n"
+    << "  AHM load factor: " << FLAGS_targetLoadFactor << "\n\n";
+}
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  auto ret = RUN_ALL_TESTS();
+  if (!ret && FLAGS_benchmark) {
+    benchmarkSetup();
+    folly::runBenchmarks();
+  }
+  return ret;
+}
+
+/*
+Benchmarks run on dual Xeon X5650's @ 2.67GHz w/hyperthreading enabled
+  (12 physical cores, 12 MB cache, 72 GB RAM)
+
+Running AHM benchmarks on machine with 24 logical cores.
+  num elements per map: 12000000
+  num threads for mt tests: 24
+  AHM load factor: 0.75
+
+Benchmark                               Iters   Total t    t/iter iter/sec
+------------------------------------------------------------------------------
+Comparing benchmarks: BM_mt_aha_find,BM_mt_ahm_find
+*       BM_mt_aha_find                1000000  7.767 ms  7.767 ns  122.8 M
+ +0.81% BM_mt_ahm_find                1000000   7.83 ms   7.83 ns  121.8 M
+------------------------------------------------------------------------------
+Comparing benchmarks: BM_st_aha_find,BM_st_ahm_find
+*       BM_st_aha_find                1000000  57.83 ms  57.83 ns  16.49 M
+ +77.9% BM_st_ahm_find                1000000  102.9 ms  102.9 ns   9.27 M
+------------------------------------------------------------------------------
+BM_mt_ahm_miss                        1000000  2.937 ms  2.937 ns  324.7 M
+BM_st_ahm_miss                        1000000  164.2 ms  164.2 ns  5.807 M
+BM_mt_ahm_find_insert_mix             1000000  8.797 ms  8.797 ns  108.4 M
+BM_mt_ahm_insert                      1000000  17.39 ms  17.39 ns  54.83 M
+BM_st_ahm_insert                      1000000  106.8 ms  106.8 ns   8.93 M
+BM_st_baseline_modulus_and_rando      1000000  6.223 ms  6.223 ns  153.2 M
+*/
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/AtomicStructTest.cpp
@@ -0,0 +1,76 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/AtomicStruct.h"
+
+#include <gflags/gflags.h>
+#include <gtest/gtest.h>
+
+using namespace folly;
+
+struct TwoBy32 {
+  uint32_t left;
+  uint32_t right;
+};
+
+TEST(AtomicStruct, two_by_32) {
+  AtomicStruct<TwoBy32> a(TwoBy32{ 10, 20 });
+  TwoBy32 av = a;
+  EXPECT_EQ(av.left, 10);
+  EXPECT_EQ(av.right, 20);
+  EXPECT_TRUE(a.compare_exchange_strong(av, TwoBy32{ 30, 40 }));
+  EXPECT_FALSE(a.compare_exchange_weak(av, TwoBy32{ 31, 41 }));
+  EXPECT_EQ(av.left, 30);
+  EXPECT_TRUE(a.is_lock_free());
+  auto b = a.exchange(TwoBy32{ 50, 60 });
+  EXPECT_EQ(b.left, 30);
+  EXPECT_EQ(b.right, 40);
+  EXPECT_EQ(a.load().left, 50);
+  a = TwoBy32{ 70, 80 };
+  EXPECT_EQ(a.load().right, 80);
+  a.store(TwoBy32{ 90, 100 });
+  av = a;
+  EXPECT_EQ(av.left, 90);
+  AtomicStruct<TwoBy32> c;
+  c = b;
+  EXPECT_EQ(c.load().right, 40);
+}
+
+TEST(AtomicStruct, size_selection) {
+  struct S1 { char x[1]; };
+  struct S2 { char x[2]; };
+  struct S3 { char x[3]; };
+  struct S4 { char x[4]; };
+  struct S5 { char x[5]; };
+  struct S6 { char x[6]; };
+  struct S7 { char x[7]; };
+  struct S8 { char x[8]; };
+
+  EXPECT_EQ(sizeof(AtomicStruct<S1>), 1);
+  EXPECT_EQ(sizeof(AtomicStruct<S2>), 2);
+  EXPECT_EQ(sizeof(AtomicStruct<S3>), 4);
+  EXPECT_EQ(sizeof(AtomicStruct<S4>), 4);
+  EXPECT_EQ(sizeof(AtomicStruct<S5>), 8);
+  EXPECT_EQ(sizeof(AtomicStruct<S6>), 8);
+  EXPECT_EQ(sizeof(AtomicStruct<S7>), 8);
+  EXPECT_EQ(sizeof(AtomicStruct<S8>), 8);
+}
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  return RUN_ALL_TESTS();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/BatonTest.cpp
@@ -0,0 +1,100 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <folly/Baton.h>
+#include <folly/test/DeterministicSchedule.h>
+#include <thread>
+#include <semaphore.h>
+#include <gflags/gflags.h>
+#include <gtest/gtest.h>
+#include <folly/Benchmark.h>
+
+using namespace folly;
+using namespace folly::test;
+
+typedef DeterministicSchedule DSched;
+
+TEST(Baton, basic) {
+  Baton<> b;
+  b.post();
+  b.wait();
+}
+
+template <template<typename> class Atom>
+void run_pingpong_test(int numRounds) {
+  Baton<Atom> batons[17];
+  Baton<Atom>& a = batons[0];
+  Baton<Atom>& b = batons[16]; // to get it on a different cache line
+  auto thr = DSched::thread([&]{
+    for (int i = 0; i < numRounds; ++i) {
+      a.wait();
+      a.reset();
+      b.post();
+    }
+  });
+  for (int i = 0; i < numRounds; ++i) {
+    a.post();
+    b.wait();
+    b.reset();
+  }
+  DSched::join(thr);
+}
+
+TEST(Baton, pingpong) {
+  DSched sched(DSched::uniform(0));
+
+  run_pingpong_test<DeterministicAtomic>(1000);
+}
+
+BENCHMARK(baton_pingpong, iters) {
+  run_pingpong_test<std::atomic>(iters);
+}
+
+BENCHMARK(posix_sem_pingpong, iters) {
+  sem_t sems[3];
+  sem_t* a = sems + 0;
+  sem_t* b = sems + 2; // to get it on a different cache line
+
+  sem_init(a, 0, 0);
+  sem_init(b, 0, 0);
+  auto thr = std::thread([=]{
+    for (int i = 0; i < iters; ++i) {
+      sem_wait(a);
+      sem_post(b);
+    }
+  });
+  for (int i = 0; i < iters; ++i) {
+    sem_post(a);
+    sem_wait(b);
+  }
+  thr.join();
+}
+
+// I am omitting a benchmark result snapshot because these microbenchmarks
+// mainly illustrate that PreBlockAttempts is very effective for rapid
+// handoffs.  The performance of Baton and sem_t is essentially identical
+// to the required futex calls for the blocking case
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+
+  auto rv = RUN_ALL_TESTS();
+  if (!rv && FLAGS_benchmark) {
+    folly::runBenchmarks();
+  }
+  return rv;
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/BenchmarkTest.cpp
@@ -0,0 +1,74 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Benchmark.h"
+#include "folly/Foreach.h"
+#include "folly/String.h"
+#include <iostream>
+using namespace folly;
+using namespace std;
+
+void fun() {
+  static double x = 1;
+  ++x;
+  doNotOptimizeAway(x);
+}
+BENCHMARK(bmFun) { fun(); }
+BENCHMARK(bmRepeatedFun, n) {
+  FOR_EACH_RANGE (i, 0, n) {
+    fun();
+  }
+}
+BENCHMARK_DRAW_LINE()
+
+BENCHMARK(gun) {
+  static double x = 1;
+  x *= 2000;
+  doNotOptimizeAway(x);
+}
+
+BENCHMARK_DRAW_LINE()
+
+BENCHMARK(baselinevector) {
+  vector<int> v;
+
+  BENCHMARK_SUSPEND {
+    v.resize(1000);
+  }
+
+  FOR_EACH_RANGE (i, 0, 100) {
+    v.push_back(42);
+  }
+}
+
+BENCHMARK_RELATIVE(bmVector) {
+  vector<int> v;
+  FOR_EACH_RANGE (i, 0, 100) {
+    v.resize(v.size() + 1, 42);
+  }
+}
+
+BENCHMARK_DRAW_LINE()
+
+BENCHMARK(superslow) {
+  sleep(1);
+}
+
+int main(int argc, char** argv) {
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  runBenchmarks();
+  runBenchmarksOnFlag();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/BitIteratorTest.cpp
@@ -0,0 +1,187 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Bits.h"
+#include "folly/Benchmark.h"
+
+#include <algorithm>
+#include <type_traits>
+#include <limits>
+#include <vector>
+#include <gtest/gtest.h>
+
+#include <gflags/gflags.h>
+
+using namespace folly;
+using namespace folly::bititerator_detail;
+
+namespace {
+
+template <class INT, class IT>
+void checkIt(INT exp, IT& it) {
+  typedef typename std::make_unsigned<INT>::type utype;
+  size_t bits = std::numeric_limits<utype>::digits;
+  utype uexp = exp;
+  for (size_t i = 0; i < bits; ++i) {
+    bool e = uexp & 1;
+    EXPECT_EQ(e, *it++);
+    uexp >>= 1;
+  }
+}
+
+template <class INT, class IT>
+void checkRange(INT exp, IT begin, IT end) {
+  typedef typename std::make_unsigned<INT>::type utype;
+  utype uexp = exp;
+  size_t i = 0;
+  auto bitEnd = makeBitIterator(end);
+  for (BitIterator<IT> it = makeBitIterator(begin); it != bitEnd; ++it, ++i) {
+    bool e = uexp & 1;
+    EXPECT_EQ(e, *it);
+    uexp >>= 1;
+  }
+}
+
+}  // namespace
+
+TEST(BitIterator, Simple) {
+  std::vector<int> v;
+  v.push_back(0x10);
+  v.push_back(0x42);
+  auto bi(makeBitIterator(v.begin()));
+  checkIt(0x10, bi);
+  checkIt(0x42, bi);
+  checkRange(0x0000004200000010ULL, v.begin(), v.end());
+
+  v[0] = 0;
+  bi = v.begin();
+  *bi++ = true;     // 1
+  *bi++ = false;
+  *bi++ = true;     // 4
+  *bi++ = false;
+  *bi++ = false;
+  *bi++ = true;     // 32
+  *++bi = true;     // 128 (note pre-increment)
+
+  EXPECT_EQ(165, v[0]);
+}
+
+TEST(BitIterator, Const) {
+  std::vector<int> v;
+  v.push_back(0x10);
+  v.push_back(0x42);
+  auto bi(makeBitIterator(v.cbegin()));
+  checkIt(0x10, bi);
+  checkIt(0x42, bi);
+}
+
+namespace {
+
+template <class BaseIter>
+BitIterator<BaseIter> simpleFFS(BitIterator<BaseIter> begin,
+                                BitIterator<BaseIter> end) {
+  return std::find(begin, end, true);
+}
+
+template <class FFS>
+void runFFSTest(FFS fn) {
+  static const size_t bpb = 8 * sizeof(uint64_t);
+  std::vector<uint64_t> data;
+  for (size_t nblocks = 1; nblocks <= 3; ++nblocks) {
+    size_t nbits = nblocks * bpb;
+    data.resize(nblocks);
+    auto begin = makeBitIterator(data.cbegin());
+    auto end = makeBitIterator(data.cend());
+    EXPECT_EQ(nbits, end - begin);
+    EXPECT_FALSE(begin == end);
+
+    // Try every possible combination of first bit set (including none),
+    // start bit, end bit
+    for (size_t firstSet = 0; firstSet <= nbits; ++firstSet) {
+      data.assign(nblocks, 0);
+      if (firstSet) {
+        size_t b = firstSet - 1;
+        data[b / bpb] |= (1ULL << (b % bpb));
+      }
+      for (size_t startBit = 0; startBit <= nbits; ++startBit) {
+        for (size_t endBit = startBit; endBit <= nbits; ++endBit) {
+          auto p = begin + startBit;
+          auto q = begin + endBit;
+          p = fn(p, q);
+          if (firstSet < startBit + 1 || firstSet >= endBit + 1) {
+            EXPECT_EQ(endBit, p - begin)
+              << "  firstSet=" << firstSet << " startBit=" << startBit
+              << " endBit=" << endBit << " nblocks=" << nblocks;
+          } else {
+            EXPECT_EQ(firstSet - 1, p - begin)
+              << "  firstSet=" << firstSet << " startBit=" << startBit
+              << " endBit=" << endBit << " nblocks=" << nblocks;
+          }
+        }
+      }
+    }
+  }
+}
+
+void runSimpleFFSTest(int iters) {
+  auto fn = simpleFFS<std::vector<uint64_t>::const_iterator>;
+  while (iters--) {
+    runFFSTest(fn);
+  }
+}
+
+void runRealFFSTest(int iters) {
+  auto fn = findFirstSet<std::vector<uint64_t>::const_iterator>;
+  while (iters--) {
+    runFFSTest(fn);
+  }
+}
+
+}
+
+TEST(BitIterator, SimpleFindFirstSet) {
+  runSimpleFFSTest(1);
+}
+
+TEST(BitIterator, FindFirstSet) {
+  runRealFFSTest(1);
+}
+
+BENCHMARK(SimpleFFSTest, iters) {
+  runSimpleFFSTest(iters);
+}
+BENCHMARK(RealFFSTest, iters) {
+  runRealFFSTest(iters);
+}
+
+/* --bm_min_iters=10 --bm_max_iters=100
+
+Benchmark                               Iters   Total t    t/iter iter/sec
+------------------------------------------------------------------------------
+runSimpleFFSTest                           10   4.82 s     482 ms  2.075
+runRealFFSTest                             19  2.011 s   105.9 ms  9.447
+
+*/
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  auto ret = RUN_ALL_TESTS();
+  if (!ret && FLAGS_benchmark) {
+    folly::runBenchmarks();
+  }
+  return ret;
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/BitsTest.cpp
@@ -0,0 +1,182 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// @author Tudor Bosman (tudorb@fb.com)
+
+#include <gflags/gflags.h>
+#include "folly/Bits.h"
+#include "folly/Benchmark.h"
+#include <gtest/gtest.h>
+
+using namespace folly;
+
+// Test constexpr-ness.
+#ifndef __clang__
+static_assert(findFirstSet(2u) == 2, "findFirstSet");
+static_assert(findLastSet(2u) == 2, "findLastSet");
+static_assert(nextPowTwo(2u) == 2, "nextPowTwo");
+static_assert(isPowTwo(2u), "isPowTwo");
+#endif  // __clang__
+
+namespace {
+
+template <class INT>
+void testFFS() {
+  EXPECT_EQ(0, findFirstSet(static_cast<INT>(0)));
+  size_t bits = std::numeric_limits<
+    typename std::make_unsigned<INT>::type>::digits;
+  for (size_t i = 0; i < bits; i++) {
+    INT v = (static_cast<INT>(1) << (bits - 1)) |
+            (static_cast<INT>(1) << i);
+    EXPECT_EQ(i+1, findFirstSet(v));
+  }
+}
+
+template <class INT>
+void testFLS() {
+  typedef typename std::make_unsigned<INT>::type UINT;
+  EXPECT_EQ(0, findLastSet(static_cast<INT>(0)));
+  size_t bits = std::numeric_limits<UINT>::digits;
+  for (size_t i = 0; i < bits; i++) {
+    INT v1 = static_cast<UINT>(1) << i;
+    EXPECT_EQ(i + 1, findLastSet(v1));
+
+    INT v2 = (static_cast<UINT>(1) << i) - 1;
+    EXPECT_EQ(i, findLastSet(v2));
+  }
+}
+
+}  // namespace
+
+TEST(Bits, FindFirstSet) {
+  testFFS<char>();
+  testFFS<signed char>();
+  testFFS<unsigned char>();
+  testFFS<short>();
+  testFFS<unsigned short>();
+  testFFS<int>();
+  testFFS<unsigned int>();
+  testFFS<long>();
+  testFFS<unsigned long>();
+  testFFS<long long>();
+  testFFS<unsigned long long>();
+}
+
+TEST(Bits, FindLastSet) {
+  testFLS<char>();
+  testFLS<signed char>();
+  testFLS<unsigned char>();
+  testFLS<short>();
+  testFLS<unsigned short>();
+  testFLS<int>();
+  testFLS<unsigned int>();
+  testFLS<long>();
+  testFLS<unsigned long>();
+  testFLS<long long>();
+  testFLS<unsigned long long>();
+}
+
+#define testPowTwo(nextPowTwoFunc) {                              \
+  EXPECT_EQ(1, nextPowTwoFunc(0u));                               \
+  EXPECT_EQ(1, nextPowTwoFunc(1u));                               \
+  EXPECT_EQ(2, nextPowTwoFunc(2u));                               \
+  EXPECT_EQ(4, nextPowTwoFunc(3u));                               \
+  EXPECT_EQ(4, nextPowTwoFunc(4u));                               \
+  EXPECT_EQ(8, nextPowTwoFunc(5u));                               \
+  EXPECT_EQ(8, nextPowTwoFunc(6u));                               \
+  EXPECT_EQ(8, nextPowTwoFunc(7u));                               \
+  EXPECT_EQ(8, nextPowTwoFunc(8u));                               \
+  EXPECT_EQ(16, nextPowTwoFunc(9u));                              \
+  EXPECT_EQ(16, nextPowTwoFunc(13u));                             \
+  EXPECT_EQ(16, nextPowTwoFunc(16u));                             \
+  EXPECT_EQ(512, nextPowTwoFunc(510u));                           \
+  EXPECT_EQ(512, nextPowTwoFunc(511u));                           \
+  EXPECT_EQ(512, nextPowTwoFunc(512u));                           \
+  EXPECT_EQ(1024, nextPowTwoFunc(513u));                          \
+  EXPECT_EQ(1024, nextPowTwoFunc(777u));                          \
+  EXPECT_EQ(1ul << 31, nextPowTwoFunc((1ul << 31) - 1));          \
+  EXPECT_EQ(1ul << 32, nextPowTwoFunc((1ul << 32) - 1));          \
+  EXPECT_EQ(1ull << 63, nextPowTwoFunc((1ull << 62) + 1));        \
+}
+
+
+TEST(Bits, nextPowTwoClz) {
+  testPowTwo(nextPowTwo);
+}
+
+BENCHMARK(nextPowTwoClz, iters) {
+  for (unsigned long i = 0; i < iters; ++i) {
+    auto x = folly::nextPowTwo(iters);
+    folly::doNotOptimizeAway(x);
+  }
+}
+
+TEST(Bits, isPowTwo) {
+  EXPECT_FALSE(isPowTwo(0u));
+  EXPECT_TRUE(isPowTwo(1ul));
+  EXPECT_TRUE(isPowTwo(2ull));
+  EXPECT_FALSE(isPowTwo(3ul));
+  EXPECT_TRUE(isPowTwo(4ul));
+  EXPECT_FALSE(isPowTwo(5ul));
+  EXPECT_TRUE(isPowTwo(8ul));
+  EXPECT_FALSE(isPowTwo(15u));
+  EXPECT_TRUE(isPowTwo(16u));
+  EXPECT_FALSE(isPowTwo(17u));
+  EXPECT_FALSE(isPowTwo(511ul));
+  EXPECT_TRUE(isPowTwo(512ul));
+  EXPECT_FALSE(isPowTwo(513ul));
+  EXPECT_FALSE(isPowTwo((1ul<<31) - 1));
+  EXPECT_TRUE(isPowTwo(1ul<<31));
+  EXPECT_FALSE(isPowTwo((1ul<<31) + 1));
+  EXPECT_FALSE(isPowTwo((1ull<<63) - 1));
+  EXPECT_TRUE(isPowTwo(1ull<<63));
+  EXPECT_FALSE(isPowTwo((1ull<<63) + 1));
+}
+
+BENCHMARK_DRAW_LINE();
+BENCHMARK(isPowTwo, iters) {
+  bool b;
+  for (unsigned long i = 0; i < iters; ++i) {
+    b = folly::isPowTwo(i);
+    folly::doNotOptimizeAway(b);
+  }
+}
+
+TEST(Bits, popcount) {
+  EXPECT_EQ(0, popcount(0U));
+  EXPECT_EQ(1, popcount(1U));
+  EXPECT_EQ(32, popcount(uint32_t(-1)));
+  EXPECT_EQ(64, popcount(uint64_t(-1)));
+}
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  auto ret = RUN_ALL_TESTS();
+  if (!ret && FLAGS_benchmark) {
+    folly::runBenchmarks();
+  }
+  return ret;
+}
+
+/*
+Benchmarks run on dual Xeon X5650's @ 2.67GHz w/hyperthreading enabled
+  (12 physical cores, 12 MB cache, 72 GB RAM)
+
+Benchmark                               Iters   Total t    t/iter iter/sec
+------------------------------------------------------------------------------
+*       nextPowTwoClz                 1000000  1.659 ms  1.659 ns  574.8 M
+*/
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/CacheLocalityTest.cpp
@@ -0,0 +1,705 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/detail/CacheLocality.h"
+
+#include <sched.h>
+#include <memory>
+#include <thread>
+#include <type_traits>
+#include <unordered_map>
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+#include "folly/Benchmark.h"
+
+using namespace folly::detail;
+
+/// This is the relevant nodes from a production box's sysfs tree.  If you
+/// think this map is ugly you should see the version of this test that
+/// used a real directory tree.  To reduce the chance of testing error
+/// I haven't tried to remove the common prefix
+static std::unordered_map<std::string,std::string> fakeSysfsTree = {
+  { "/sys/devices/system/cpu/cpu0/cache/index0/shared_cpu_list", "0,17" },
+  { "/sys/devices/system/cpu/cpu0/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu0/cache/index1/shared_cpu_list", "0,17" },
+  { "/sys/devices/system/cpu/cpu0/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu0/cache/index2/shared_cpu_list", "0,17" },
+  { "/sys/devices/system/cpu/cpu0/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu0/cache/index3/shared_cpu_list", "0-8,17-23" },
+  { "/sys/devices/system/cpu/cpu0/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu1/cache/index0/shared_cpu_list", "1,18" },
+  { "/sys/devices/system/cpu/cpu1/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu1/cache/index1/shared_cpu_list", "1,18" },
+  { "/sys/devices/system/cpu/cpu1/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu1/cache/index2/shared_cpu_list", "1,18" },
+  { "/sys/devices/system/cpu/cpu1/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu1/cache/index3/shared_cpu_list", "0-8,17-23" },
+  { "/sys/devices/system/cpu/cpu1/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu2/cache/index0/shared_cpu_list", "2,19" },
+  { "/sys/devices/system/cpu/cpu2/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu2/cache/index1/shared_cpu_list", "2,19" },
+  { "/sys/devices/system/cpu/cpu2/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu2/cache/index2/shared_cpu_list", "2,19" },
+  { "/sys/devices/system/cpu/cpu2/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu2/cache/index3/shared_cpu_list", "0-8,17-23" },
+  { "/sys/devices/system/cpu/cpu2/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu3/cache/index0/shared_cpu_list", "3,20" },
+  { "/sys/devices/system/cpu/cpu3/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu3/cache/index1/shared_cpu_list", "3,20" },
+  { "/sys/devices/system/cpu/cpu3/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu3/cache/index2/shared_cpu_list", "3,20" },
+  { "/sys/devices/system/cpu/cpu3/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu3/cache/index3/shared_cpu_list", "0-8,17-23" },
+  { "/sys/devices/system/cpu/cpu3/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu4/cache/index0/shared_cpu_list", "4,21" },
+  { "/sys/devices/system/cpu/cpu4/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu4/cache/index1/shared_cpu_list", "4,21" },
+  { "/sys/devices/system/cpu/cpu4/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu4/cache/index2/shared_cpu_list", "4,21" },
+  { "/sys/devices/system/cpu/cpu4/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu4/cache/index3/shared_cpu_list", "0-8,17-23" },
+  { "/sys/devices/system/cpu/cpu4/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu5/cache/index0/shared_cpu_list", "5-6" },
+  { "/sys/devices/system/cpu/cpu5/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu5/cache/index1/shared_cpu_list", "5-6" },
+  { "/sys/devices/system/cpu/cpu5/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu5/cache/index2/shared_cpu_list", "5-6" },
+  { "/sys/devices/system/cpu/cpu5/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu5/cache/index3/shared_cpu_list", "0-8,17-23" },
+  { "/sys/devices/system/cpu/cpu5/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu6/cache/index0/shared_cpu_list", "5-6" },
+  { "/sys/devices/system/cpu/cpu6/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu6/cache/index1/shared_cpu_list", "5-6" },
+  { "/sys/devices/system/cpu/cpu6/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu6/cache/index2/shared_cpu_list", "5-6" },
+  { "/sys/devices/system/cpu/cpu6/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu6/cache/index3/shared_cpu_list", "0-8,17-23" },
+  { "/sys/devices/system/cpu/cpu6/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu7/cache/index0/shared_cpu_list", "7,22" },
+  { "/sys/devices/system/cpu/cpu7/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu7/cache/index1/shared_cpu_list", "7,22" },
+  { "/sys/devices/system/cpu/cpu7/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu7/cache/index2/shared_cpu_list", "7,22" },
+  { "/sys/devices/system/cpu/cpu7/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu7/cache/index3/shared_cpu_list", "0-8,17-23" },
+  { "/sys/devices/system/cpu/cpu7/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu8/cache/index0/shared_cpu_list", "8,23" },
+  { "/sys/devices/system/cpu/cpu8/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu8/cache/index1/shared_cpu_list", "8,23" },
+  { "/sys/devices/system/cpu/cpu8/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu8/cache/index2/shared_cpu_list", "8,23" },
+  { "/sys/devices/system/cpu/cpu8/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu8/cache/index3/shared_cpu_list", "0-8,17-23" },
+  { "/sys/devices/system/cpu/cpu8/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu9/cache/index0/shared_cpu_list", "9,24" },
+  { "/sys/devices/system/cpu/cpu9/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu9/cache/index1/shared_cpu_list", "9,24" },
+  { "/sys/devices/system/cpu/cpu9/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu9/cache/index2/shared_cpu_list", "9,24" },
+  { "/sys/devices/system/cpu/cpu9/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu9/cache/index3/shared_cpu_list", "9-16,24-31" },
+  { "/sys/devices/system/cpu/cpu9/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu10/cache/index0/shared_cpu_list", "10,25" },
+  { "/sys/devices/system/cpu/cpu10/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu10/cache/index1/shared_cpu_list", "10,25" },
+  { "/sys/devices/system/cpu/cpu10/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu10/cache/index2/shared_cpu_list", "10,25" },
+  { "/sys/devices/system/cpu/cpu10/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu10/cache/index3/shared_cpu_list", "9-16,24-31"},
+  { "/sys/devices/system/cpu/cpu10/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu11/cache/index0/shared_cpu_list", "11,26" },
+  { "/sys/devices/system/cpu/cpu11/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu11/cache/index1/shared_cpu_list", "11,26" },
+  { "/sys/devices/system/cpu/cpu11/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu11/cache/index2/shared_cpu_list", "11,26" },
+  { "/sys/devices/system/cpu/cpu11/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu11/cache/index3/shared_cpu_list", "9-16,24-31"},
+  { "/sys/devices/system/cpu/cpu11/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu12/cache/index0/shared_cpu_list", "12,27" },
+  { "/sys/devices/system/cpu/cpu12/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu12/cache/index1/shared_cpu_list", "12,27" },
+  { "/sys/devices/system/cpu/cpu12/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu12/cache/index2/shared_cpu_list", "12,27" },
+  { "/sys/devices/system/cpu/cpu12/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu12/cache/index3/shared_cpu_list", "9-16,24-31"},
+  { "/sys/devices/system/cpu/cpu12/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu13/cache/index0/shared_cpu_list", "13,28" },
+  { "/sys/devices/system/cpu/cpu13/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu13/cache/index1/shared_cpu_list", "13,28" },
+  { "/sys/devices/system/cpu/cpu13/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu13/cache/index2/shared_cpu_list", "13,28" },
+  { "/sys/devices/system/cpu/cpu13/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu13/cache/index3/shared_cpu_list", "9-16,24-31"},
+  { "/sys/devices/system/cpu/cpu13/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu14/cache/index0/shared_cpu_list", "14,29" },
+  { "/sys/devices/system/cpu/cpu14/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu14/cache/index1/shared_cpu_list", "14,29" },
+  { "/sys/devices/system/cpu/cpu14/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu14/cache/index2/shared_cpu_list", "14,29" },
+  { "/sys/devices/system/cpu/cpu14/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu14/cache/index3/shared_cpu_list", "9-16,24-31"},
+  { "/sys/devices/system/cpu/cpu14/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu15/cache/index0/shared_cpu_list", "15,30" },
+  { "/sys/devices/system/cpu/cpu15/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu15/cache/index1/shared_cpu_list", "15,30" },
+  { "/sys/devices/system/cpu/cpu15/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu15/cache/index2/shared_cpu_list", "15,30" },
+  { "/sys/devices/system/cpu/cpu15/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu15/cache/index3/shared_cpu_list", "9-16,24-31"},
+  { "/sys/devices/system/cpu/cpu15/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu16/cache/index0/shared_cpu_list", "16,31" },
+  { "/sys/devices/system/cpu/cpu16/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu16/cache/index1/shared_cpu_list", "16,31" },
+  { "/sys/devices/system/cpu/cpu16/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu16/cache/index2/shared_cpu_list", "16,31" },
+  { "/sys/devices/system/cpu/cpu16/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu16/cache/index3/shared_cpu_list", "9-16,24-31"},
+  { "/sys/devices/system/cpu/cpu16/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu17/cache/index0/shared_cpu_list", "0,17" },
+  { "/sys/devices/system/cpu/cpu17/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu17/cache/index1/shared_cpu_list", "0,17" },
+  { "/sys/devices/system/cpu/cpu17/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu17/cache/index2/shared_cpu_list", "0,17" },
+  { "/sys/devices/system/cpu/cpu17/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu17/cache/index3/shared_cpu_list", "0-8,17-23" },
+  { "/sys/devices/system/cpu/cpu17/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu18/cache/index0/shared_cpu_list", "1,18" },
+  { "/sys/devices/system/cpu/cpu18/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu18/cache/index1/shared_cpu_list", "1,18" },
+  { "/sys/devices/system/cpu/cpu18/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu18/cache/index2/shared_cpu_list", "1,18" },
+  { "/sys/devices/system/cpu/cpu18/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu18/cache/index3/shared_cpu_list", "0-8,17-23" },
+  { "/sys/devices/system/cpu/cpu18/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu19/cache/index0/shared_cpu_list", "2,19" },
+  { "/sys/devices/system/cpu/cpu19/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu19/cache/index1/shared_cpu_list", "2,19" },
+  { "/sys/devices/system/cpu/cpu19/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu19/cache/index2/shared_cpu_list", "2,19" },
+  { "/sys/devices/system/cpu/cpu19/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu19/cache/index3/shared_cpu_list", "0-8,17-23" },
+  { "/sys/devices/system/cpu/cpu19/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu20/cache/index0/shared_cpu_list", "3,20" },
+  { "/sys/devices/system/cpu/cpu20/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu20/cache/index1/shared_cpu_list", "3,20" },
+  { "/sys/devices/system/cpu/cpu20/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu20/cache/index2/shared_cpu_list", "3,20" },
+  { "/sys/devices/system/cpu/cpu20/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu20/cache/index3/shared_cpu_list", "0-8,17-23" },
+  { "/sys/devices/system/cpu/cpu20/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu21/cache/index0/shared_cpu_list", "4,21" },
+  { "/sys/devices/system/cpu/cpu21/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu21/cache/index1/shared_cpu_list", "4,21" },
+  { "/sys/devices/system/cpu/cpu21/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu21/cache/index2/shared_cpu_list", "4,21" },
+  { "/sys/devices/system/cpu/cpu21/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu21/cache/index3/shared_cpu_list", "0-8,17-23" },
+  { "/sys/devices/system/cpu/cpu21/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu22/cache/index0/shared_cpu_list", "7,22" },
+  { "/sys/devices/system/cpu/cpu22/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu22/cache/index1/shared_cpu_list", "7,22" },
+  { "/sys/devices/system/cpu/cpu22/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu22/cache/index2/shared_cpu_list", "7,22" },
+  { "/sys/devices/system/cpu/cpu22/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu22/cache/index3/shared_cpu_list", "0-8,17-23" },
+  { "/sys/devices/system/cpu/cpu22/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu23/cache/index0/shared_cpu_list", "8,23" },
+  { "/sys/devices/system/cpu/cpu23/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu23/cache/index1/shared_cpu_list", "8,23" },
+  { "/sys/devices/system/cpu/cpu23/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu23/cache/index2/shared_cpu_list", "8,23" },
+  { "/sys/devices/system/cpu/cpu23/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu23/cache/index3/shared_cpu_list", "0-8,17-23" },
+  { "/sys/devices/system/cpu/cpu23/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu24/cache/index0/shared_cpu_list", "9,24" },
+  { "/sys/devices/system/cpu/cpu24/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu24/cache/index1/shared_cpu_list", "9,24" },
+  { "/sys/devices/system/cpu/cpu24/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu24/cache/index2/shared_cpu_list", "9,24" },
+  { "/sys/devices/system/cpu/cpu24/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu24/cache/index3/shared_cpu_list", "9-16,24-31"},
+  { "/sys/devices/system/cpu/cpu24/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu25/cache/index0/shared_cpu_list", "10,25" },
+  { "/sys/devices/system/cpu/cpu25/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu25/cache/index1/shared_cpu_list", "10,25" },
+  { "/sys/devices/system/cpu/cpu25/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu25/cache/index2/shared_cpu_list", "10,25" },
+  { "/sys/devices/system/cpu/cpu25/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu25/cache/index3/shared_cpu_list", "9-16,24-31"},
+  { "/sys/devices/system/cpu/cpu25/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu26/cache/index0/shared_cpu_list", "11,26" },
+  { "/sys/devices/system/cpu/cpu26/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu26/cache/index1/shared_cpu_list", "11,26" },
+  { "/sys/devices/system/cpu/cpu26/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu26/cache/index2/shared_cpu_list", "11,26" },
+  { "/sys/devices/system/cpu/cpu26/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu26/cache/index3/shared_cpu_list", "9-16,24-31"},
+  { "/sys/devices/system/cpu/cpu26/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu27/cache/index0/shared_cpu_list", "12,27" },
+  { "/sys/devices/system/cpu/cpu27/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu27/cache/index1/shared_cpu_list", "12,27" },
+  { "/sys/devices/system/cpu/cpu27/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu27/cache/index2/shared_cpu_list", "12,27" },
+  { "/sys/devices/system/cpu/cpu27/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu27/cache/index3/shared_cpu_list", "9-16,24-31"},
+  { "/sys/devices/system/cpu/cpu27/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu28/cache/index0/shared_cpu_list", "13,28" },
+  { "/sys/devices/system/cpu/cpu28/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu28/cache/index1/shared_cpu_list", "13,28" },
+  { "/sys/devices/system/cpu/cpu28/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu28/cache/index2/shared_cpu_list", "13,28" },
+  { "/sys/devices/system/cpu/cpu28/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu28/cache/index3/shared_cpu_list", "9-16,24-31"},
+  { "/sys/devices/system/cpu/cpu28/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu29/cache/index0/shared_cpu_list", "14,29" },
+  { "/sys/devices/system/cpu/cpu29/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu29/cache/index1/shared_cpu_list", "14,29" },
+  { "/sys/devices/system/cpu/cpu29/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu29/cache/index2/shared_cpu_list", "14,29" },
+  { "/sys/devices/system/cpu/cpu29/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu29/cache/index3/shared_cpu_list", "9-16,24-31"},
+  { "/sys/devices/system/cpu/cpu29/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu30/cache/index0/shared_cpu_list", "15,30" },
+  { "/sys/devices/system/cpu/cpu30/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu30/cache/index1/shared_cpu_list", "15,30" },
+  { "/sys/devices/system/cpu/cpu30/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu30/cache/index2/shared_cpu_list", "15,30" },
+  { "/sys/devices/system/cpu/cpu30/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu30/cache/index3/shared_cpu_list", "9-16,24-31"},
+  { "/sys/devices/system/cpu/cpu30/cache/index3/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu31/cache/index0/shared_cpu_list", "16,31" },
+  { "/sys/devices/system/cpu/cpu31/cache/index0/type", "Data" },
+  { "/sys/devices/system/cpu/cpu31/cache/index1/shared_cpu_list", "16,31" },
+  { "/sys/devices/system/cpu/cpu31/cache/index1/type", "Instruction" },
+  { "/sys/devices/system/cpu/cpu31/cache/index2/shared_cpu_list", "16,31" },
+  { "/sys/devices/system/cpu/cpu31/cache/index2/type", "Unified" },
+  { "/sys/devices/system/cpu/cpu31/cache/index3/shared_cpu_list", "9-16,24-31"},
+  { "/sys/devices/system/cpu/cpu31/cache/index3/type", "Unified" }
+};
+
+/// This is the expected CacheLocality structure for fakeSysfsTree
+static const CacheLocality nonUniformExampleLocality = {
+  32,
+  { 16, 16, 2 },
+  { 0, 2, 4, 6, 8, 10, 11, 12, 14, 16, 18, 20, 22, 24, 26, 28,
+    30, 1, 3, 5, 7, 9, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31 }
+};
+
+TEST(CacheLocality, FakeSysfs) {
+  auto parsed = CacheLocality::readFromSysfsTree([](std::string name) {
+    auto iter = fakeSysfsTree.find(name);
+    return iter == fakeSysfsTree.end() ? std::string() : iter->second;
+  });
+
+  auto& expected = nonUniformExampleLocality;
+  EXPECT_EQ(expected.numCpus, parsed.numCpus);
+  EXPECT_EQ(expected.numCachesByLevel, parsed.numCachesByLevel);
+  EXPECT_EQ(expected.localityIndexByCpu, parsed.localityIndexByCpu);
+}
+
+TEST(Getcpu, VdsoGetcpu) {
+  unsigned cpu;
+  Getcpu::vdsoFunc()(&cpu, nullptr, nullptr);
+
+  EXPECT_TRUE(cpu < CPU_SETSIZE);
+}
+
+TEST(SequentialThreadId, Simple) {
+  unsigned cpu = 0;
+  auto rv = SequentialThreadId<std::atomic>::getcpu(&cpu, nullptr, nullptr);
+  EXPECT_EQ(rv, 0);
+  EXPECT_TRUE(cpu > 0);
+  unsigned again;
+  SequentialThreadId<std::atomic>::getcpu(&again, nullptr, nullptr);
+  EXPECT_EQ(cpu, again);
+}
+
+static __thread unsigned testingCpu = 0;
+
+static int testingGetcpu(unsigned* cpu, unsigned* node, void* unused) {
+  if (cpu != nullptr) {
+    *cpu = testingCpu;
+  }
+  if (node != nullptr) {
+    *node = testingCpu;
+  }
+  return 0;
+}
+
+TEST(AccessSpreader, Stubbed) {
+  std::vector<std::unique_ptr<AccessSpreader<>>> spreaders(100);
+  for (size_t s = 1; s < spreaders.size(); ++s) {
+    spreaders[s].reset(new AccessSpreader<>(
+        s, nonUniformExampleLocality, &testingGetcpu));
+  }
+  std::vector<size_t> cpusInLocalityOrder = {
+      0, 17, 1, 18, 2, 19, 3, 20, 4, 21, 5, 6, 7, 22, 8, 23, 9, 24, 10, 25,
+      11, 26, 12, 27, 13, 28, 14, 29, 15, 30, 16, 31 };
+  for (size_t i = 0; i < 32; ++i) {
+    // extra i * 32 is to check wrapping behavior of impl
+    testingCpu = cpusInLocalityOrder[i] + i * 64;
+    for (size_t s = 1; s < spreaders.size(); ++s) {
+      EXPECT_EQ((i * s) / 32, spreaders[s]->current())
+          << "i=" << i << ", cpu=" << testingCpu << ", s=" << s;
+    }
+  }
+}
+
+TEST(AccessSpreader, Default) {
+  AccessSpreader<> spreader(16);
+  EXPECT_LT(spreader.current(), 16);
+}
+
+TEST(AccessSpreader, Shared) {
+  for (size_t s = 1; s < 200; ++s) {
+    EXPECT_LT(AccessSpreader<>::shared(s).current(), s);
+  }
+}
+
+TEST(AccessSpreader, Statics) {
+  LOG(INFO) << "stripeByCore.numStripes() = "
+            << AccessSpreader<>::stripeByCore.numStripes();
+  LOG(INFO) << "stripeByChip.numStripes() = "
+            << AccessSpreader<>::stripeByChip.numStripes();
+  for (size_t s = 1; s < 200; ++s) {
+    EXPECT_LT(AccessSpreader<>::current(s), s);
+  }
+}
+
+TEST(AccessSpreader, Wrapping) {
+  // this test won't pass unless locality.numCpus divides kMaxCpus
+  auto numCpus = 16;
+  auto locality = CacheLocality::uniform(numCpus);
+  for (size_t s = 1; s < 200; ++s) {
+    AccessSpreader<> spreader(s, locality, &testingGetcpu);
+    for (size_t c = 0; c < 400; ++c) {
+      testingCpu = c;
+      auto observed = spreader.current();
+      testingCpu = c % numCpus;
+      auto expected = spreader.current();
+      EXPECT_EQ(expected, observed)
+          << "numCpus=" << numCpus << ", s=" << s << ", c=" << c;
+    }
+  }
+}
+
+// Benchmarked at ~21 nanos on fbk35 (2.6) and fbk18 (3.2) kernels with
+// a 2.2Ghz Xeon
+// ============================================================================
+// folly/test/CacheLocalityTest.cpp                relative  time/iter  iters/s
+// ============================================================================
+// LocalAccessSpreaderUse                                      20.77ns   48.16M
+// SharedAccessSpreaderUse                                     21.95ns   45.55M
+// AccessSpreaderConstruction                                 466.56ns    2.14M
+// ============================================================================
+
+BENCHMARK(LocalAccessSpreaderUse, iters) {
+  folly::BenchmarkSuspender braces;
+  AccessSpreader<> spreader(16);
+  braces.dismiss();
+
+  for (unsigned long i = 0; i < iters; ++i) {
+    auto x = spreader.current();
+    folly::doNotOptimizeAway(x);
+  }
+}
+
+BENCHMARK(SharedAccessSpreaderUse, iters) {
+  for (unsigned long i = 0; i < iters; ++i) {
+    auto x = AccessSpreader<>::current(16);
+    folly::doNotOptimizeAway(x);
+  }
+}
+
+BENCHMARK(AccessSpreaderConstruction, iters) {
+  std::aligned_storage<sizeof(AccessSpreader<>),
+                       std::alignment_of<AccessSpreader<>>::value>::type raw;
+  for (unsigned long i = 0; i < iters; ++i) {
+    auto x = new (&raw) AccessSpreader<>(16);
+    folly::doNotOptimizeAway(x);
+    x->~AccessSpreader();
+  }
+}
+
+enum class SpreaderType { GETCPU, SHARED, TLS_RR };
+
+// Benchmark scores here reflect the time for 32 threads to perform an
+// atomic increment on a dual-socket E5-2660 @ 2.2Ghz.  Surprisingly,
+// if we don't separate the counters onto unique 128 byte stripes the
+// 1_stripe and 2_stripe results are identical, even though the L3 is
+// claimed to have 64 byte cache lines.
+//
+// _stub means there was no call to getcpu or the tls round-robin
+// implementation, because for a single stripe the cpu doesn't matter.
+// _getcpu refers to the vdso getcpu implementation with a locally
+// constructed AccessSpreader.  _tls_rr refers to execution using
+// SequentialThreadId, the fallback if the vdso getcpu isn't available.
+// _shared refers to calling AccessSpreader<>::current(numStripes)
+// inside the hot loop.
+//
+// At 16_stripe_0_work and 32_stripe_0_work there is only L1 traffic,
+// so since the stripe selection is 21 nanos the atomic increments in
+// the L1 is ~15 nanos.  At width 8_stripe_0_work the line is expected
+// to ping-pong almost every operation, since the loops have the same
+// duration.  Widths 4 and 2 have the same behavior, but each tour of the
+// cache line is 4 and 8 cores long, respectively.  These all suggest a
+// lower bound of 60 nanos for intra-chip handoff and increment between
+// the L1s.
+//
+// With 455 nanos (1K cycles) of busywork per contended increment, the
+// system can hide all of the latency of a tour of length 4, but not
+// quite one of length 8.  I was a bit surprised at how much worse the
+// non-striped version got.  It seems that the inter-chip traffic also
+// interferes with the L1-only localWork.load().  When the local work is
+// doubled to about 1 microsecond we see that the inter-chip contention
+// is still very important, but subdivisions on the same chip don't matter.
+//
+// sudo nice -n -20
+//   _bin/folly/test/cache_locality_test --benchmark --bm_min_iters=1000000
+// ============================================================================
+// folly/test/CacheLocalityTest.cpp                relative  time/iter  iters/s
+// ============================================================================
+// contentionAtWidth(1_stripe_0_work_stub)                      1.14us  873.64K
+// contentionAtWidth(2_stripe_0_work_getcpu)                  495.58ns    2.02M
+// contentionAtWidth(4_stripe_0_work_getcpu)                  232.99ns    4.29M
+// contentionAtWidth(8_stripe_0_work_getcpu)                  101.16ns    9.88M
+// contentionAtWidth(16_stripe_0_work_getcpu)                  41.93ns   23.85M
+// contentionAtWidth(32_stripe_0_work_getcpu)                  42.04ns   23.79M
+// contentionAtWidth(64_stripe_0_work_getcpu)                  41.94ns   23.84M
+// contentionAtWidth(2_stripe_0_work_tls_rr)                    1.00us  997.41K
+// contentionAtWidth(4_stripe_0_work_tls_rr)                  694.41ns    1.44M
+// contentionAtWidth(8_stripe_0_work_tls_rr)                  590.27ns    1.69M
+// contentionAtWidth(16_stripe_0_work_tls_rr)                 222.13ns    4.50M
+// contentionAtWidth(32_stripe_0_work_tls_rr)                 169.49ns    5.90M
+// contentionAtWidth(64_stripe_0_work_tls_rr)                 162.20ns    6.17M
+// contentionAtWidth(2_stripe_0_work_shared)                  495.54ns    2.02M
+// contentionAtWidth(4_stripe_0_work_shared)                  236.27ns    4.23M
+// contentionAtWidth(8_stripe_0_work_shared)                  114.81ns    8.71M
+// contentionAtWidth(16_stripe_0_work_shared)                  44.65ns   22.40M
+// contentionAtWidth(32_stripe_0_work_shared)                  41.76ns   23.94M
+// contentionAtWidth(64_stripe_0_work_shared)                  43.47ns   23.00M
+// atomicIncrBaseline(local_incr_0_work)                       20.39ns   49.06M
+// ----------------------------------------------------------------------------
+// contentionAtWidth(1_stripe_500_work_stub)                    2.04us  491.13K
+// contentionAtWidth(2_stripe_500_work_getcpu)                610.98ns    1.64M
+// contentionAtWidth(4_stripe_500_work_getcpu)                507.72ns    1.97M
+// contentionAtWidth(8_stripe_500_work_getcpu)                542.53ns    1.84M
+// contentionAtWidth(16_stripe_500_work_getcpu)               496.55ns    2.01M
+// contentionAtWidth(32_stripe_500_work_getcpu)               500.67ns    2.00M
+// atomicIncrBaseline(local_incr_500_work)                    484.69ns    2.06M
+// ----------------------------------------------------------------------------
+// contentionAtWidth(1_stripe_1000_work_stub)                   2.11us  473.78K
+// contentionAtWidth(2_stripe_1000_work_getcpu)               970.64ns    1.03M
+// contentionAtWidth(4_stripe_1000_work_getcpu)               987.31ns    1.01M
+// contentionAtWidth(8_stripe_1000_work_getcpu)                 1.01us  985.52K
+// contentionAtWidth(16_stripe_1000_work_getcpu)              986.09ns    1.01M
+// contentionAtWidth(32_stripe_1000_work_getcpu)              960.23ns    1.04M
+// atomicIncrBaseline(local_incr_1000_work)                   950.63ns    1.05M
+// ============================================================================
+static void contentionAtWidth(size_t iters, size_t stripes, size_t work,
+                              SpreaderType spreaderType,
+                              size_t counterAlignment = 128,
+                              size_t numThreads = 32) {
+  folly::BenchmarkSuspender braces;
+
+  AccessSpreader<> spreader(
+      stripes,
+      CacheLocality::system<std::atomic>(),
+      spreaderType == SpreaderType::TLS_RR
+          ? SequentialThreadId<std::atomic>::getcpu : nullptr);
+
+  std::atomic<size_t> ready(0);
+  std::atomic<bool> go(false);
+
+  // while in theory the cache line size is 64 bytes, experiments show
+  // that we get contention on 128 byte boundaries for Ivy Bridge.  The
+  // extra indirection adds 1 or 2 nanos
+  assert(counterAlignment >= sizeof(std::atomic<size_t>));
+  char raw[counterAlignment * stripes];
+
+  // if we happen to be using the tlsRoundRobin, then sequentially
+  // assigning the thread identifiers is the unlikely best-case scenario.
+  // We don't want to unfairly benefit or penalize.  Computing the exact
+  // maximum likelihood of the probability distributions is annoying, so
+  // I approximate as 2/5 of the ids that have no threads, 2/5 that have
+  // 1, 2/15 that have 2, and 1/15 that have 3.  We accomplish this by
+  // wrapping back to slot 0 when we hit 1/15 and 1/5.
+
+  std::vector<std::thread> threads;
+  while (threads.size() < numThreads) {
+    threads.push_back(std::thread([&,iters,stripes,work]() {
+      std::atomic<size_t>* counters[stripes];
+      for (size_t i = 0; i < stripes; ++i) {
+        counters[i] = new (raw + counterAlignment * i) std::atomic<size_t>();
+      }
+
+      spreader.current();
+      ready++;
+      while (!go.load()) {
+        sched_yield();
+      }
+      std::atomic<int> localWork;
+      if (spreaderType == SpreaderType::SHARED) {
+        for (size_t i = iters; i > 0; --i) {
+          ++*(counters[AccessSpreader<>::current(stripes)]);
+          for (size_t j = work; j > 0; --j) {
+            localWork.load();
+          }
+        }
+      } else {
+        for (size_t i = iters; i > 0; --i) {
+          ++*(counters[spreader.current()]);
+          for (size_t j = work; j > 0; --j) {
+            localWork.load();
+          }
+        }
+      }
+    }));
+
+    if (threads.size() == numThreads / 15 ||
+        threads.size() == numThreads / 5) {
+      // create a few dummy threads to wrap back around to 0 mod numCpus
+      for (size_t i = threads.size(); i != numThreads; ++i) {
+        std::thread([&]() {
+          spreader.current();
+        }).join();
+      }
+    }
+  }
+
+  while (ready < numThreads) {
+    sched_yield();
+  }
+  braces.dismiss();
+  go = true;
+
+  for (auto& thr : threads) {
+    thr.join();
+  }
+}
+
+static void atomicIncrBaseline(size_t iters, size_t work,
+                               size_t numThreads = 32) {
+  folly::BenchmarkSuspender braces;
+
+  std::atomic<bool> go(false);
+
+  std::vector<std::thread> threads;
+  while (threads.size() < numThreads) {
+    threads.push_back(std::thread([&]() {
+      while (!go.load()) {
+        sched_yield();
+      }
+      std::atomic<size_t> localCounter;
+      std::atomic<int> localWork;
+      for (size_t i = iters; i > 0; --i) {
+        localCounter++;
+        for (size_t j = work; j > 0; --j) {
+          localWork.load();
+        }
+      }
+    }));
+  }
+
+  braces.dismiss();
+  go = true;
+
+  for (auto& thr : threads) {
+    thr.join();
+  }
+}
+
+BENCHMARK_DRAW_LINE()
+
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 1_stripe_0_work_stub,
+                      1, 0, SpreaderType::GETCPU)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 2_stripe_0_work_getcpu,
+                      2, 0, SpreaderType::GETCPU)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 4_stripe_0_work_getcpu,
+                      4, 0, SpreaderType::GETCPU)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 8_stripe_0_work_getcpu,
+                      8, 0, SpreaderType::GETCPU)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 16_stripe_0_work_getcpu,
+                      16, 0, SpreaderType::GETCPU)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 32_stripe_0_work_getcpu,
+                      32, 0, SpreaderType::GETCPU)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 64_stripe_0_work_getcpu,
+                      64, 0, SpreaderType::GETCPU)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 2_stripe_0_work_tls_rr,
+                      2, 0, SpreaderType::TLS_RR)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 4_stripe_0_work_tls_rr,
+                      4, 0, SpreaderType::TLS_RR)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 8_stripe_0_work_tls_rr,
+                      8, 0, SpreaderType::TLS_RR)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 16_stripe_0_work_tls_rr,
+                      16, 0, SpreaderType::TLS_RR)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 32_stripe_0_work_tls_rr,
+                      32, 0, SpreaderType::TLS_RR)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 64_stripe_0_work_tls_rr,
+                      64, 0, SpreaderType::TLS_RR)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 2_stripe_0_work_shared,
+                      2, 0, SpreaderType::SHARED)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 4_stripe_0_work_shared,
+                      4, 0, SpreaderType::SHARED)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 8_stripe_0_work_shared,
+                      8, 0, SpreaderType::SHARED)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 16_stripe_0_work_shared,
+                      16, 0, SpreaderType::SHARED)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 32_stripe_0_work_shared,
+                      32, 0, SpreaderType::SHARED)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 64_stripe_0_work_shared,
+                      64, 0, SpreaderType::SHARED)
+BENCHMARK_NAMED_PARAM(atomicIncrBaseline, local_incr_0_work, 0)
+BENCHMARK_DRAW_LINE()
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 1_stripe_500_work_stub,
+                      1, 500, SpreaderType::GETCPU)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 2_stripe_500_work_getcpu,
+                      2, 500, SpreaderType::GETCPU)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 4_stripe_500_work_getcpu,
+                      4, 500, SpreaderType::GETCPU)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 8_stripe_500_work_getcpu,
+                      8, 500, SpreaderType::GETCPU)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 16_stripe_500_work_getcpu,
+                      16, 500, SpreaderType::GETCPU)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 32_stripe_500_work_getcpu,
+                      32, 500, SpreaderType::GETCPU)
+BENCHMARK_NAMED_PARAM(atomicIncrBaseline, local_incr_500_work, 500)
+BENCHMARK_DRAW_LINE()
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 1_stripe_1000_work_stub,
+                      1, 1000, SpreaderType::GETCPU)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 2_stripe_1000_work_getcpu,
+                      2, 1000, SpreaderType::GETCPU)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 4_stripe_1000_work_getcpu,
+                      4, 1000, SpreaderType::GETCPU)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 8_stripe_1000_work_getcpu,
+                      8, 1000, SpreaderType::GETCPU)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 16_stripe_1000_work_getcpu,
+                      16, 1000, SpreaderType::GETCPU)
+BENCHMARK_NAMED_PARAM(contentionAtWidth, 32_stripe_1000_work_getcpu,
+                      32, 1000, SpreaderType::GETCPU)
+BENCHMARK_NAMED_PARAM(atomicIncrBaseline, local_incr_1000_work, 1000)
+
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  auto ret = RUN_ALL_TESTS();
+  if (!ret && FLAGS_benchmark) {
+    folly::runBenchmarks();
+  }
+  return ret;
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/ChecksumTest.cpp
@@ -0,0 +1,201 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Checksum.h"
+#include <gflags/gflags.h>
+#include <gtest/gtest.h>
+#include "folly/Benchmark.h"
+#include "folly/Hash.h"
+#include "folly/detail/ChecksumDetail.h"
+
+namespace {
+const unsigned int BUFFER_SIZE = 512 * 1024 * sizeof(uint64_t);
+uint8_t buffer[BUFFER_SIZE];
+
+struct ExpectedResult {
+  size_t offset;
+  size_t length;
+  uint32_t crc32c;
+};
+
+ExpectedResult expectedResults[] = {
+    // Zero-byte input
+    { 0, 0, ~0U },
+    // Small aligned inputs to test special cases in SIMD implementations
+    { 8, 1, 1543413366 },
+    { 8, 2, 523493126 },
+    { 8, 3, 1560427360 },
+    { 8, 4, 3422504776 },
+    { 8, 5, 447841138 },
+    { 8, 6, 3910050499 },
+    { 8, 7, 3346241981 },
+    // Small unaligned inputs
+    { 9, 1, 3855826643 },
+    { 10, 2, 560880875 },
+    { 11, 3, 1479707779 },
+    { 12, 4, 2237687071 },
+    { 13, 5, 4063855784 },
+    { 14, 6, 2553454047 },
+    { 15, 7, 1349220140 },
+    // Larger inputs to test leftover chunks at the end of aligned blocks
+    { 8, 8, 627613930 },
+    { 8, 9, 2105929409 },
+    { 8, 10, 2447068514 },
+    { 8, 11, 863807079 },
+    { 8, 12, 292050879 },
+    { 8, 13, 1411837737 },
+    { 8, 14, 2614515001 },
+    { 8, 15, 3579076296 },
+    { 8, 16, 2897079161 },
+    { 8, 17, 675168386 },
+    // Much larger inputs
+    { 0, BUFFER_SIZE, 2096790750 },
+    { 1, BUFFER_SIZE / 2, 3854797577 },
+};
+
+void testCRC32C(
+    std::function<uint32_t(const uint8_t*, size_t, uint32_t)> impl) {
+  for (auto expected : expectedResults) {
+    uint32_t result = impl(buffer + expected.offset, expected.length, ~0U);
+    EXPECT_EQ(expected.crc32c, result);
+  }
+}
+
+void testCRC32CContinuation(
+    std::function<uint32_t(const uint8_t*, size_t, uint32_t)> impl) {
+  for (auto expected : expectedResults) {
+    size_t partialLength = expected.length / 2;
+    uint32_t partialChecksum = impl(
+        buffer + expected.offset, partialLength, ~0U);
+    uint32_t result = impl(
+        buffer + expected.offset + partialLength,
+        expected.length - partialLength, partialChecksum);
+    EXPECT_EQ(expected.crc32c, result);
+  }
+}
+
+} // namespace
+
+TEST(Checksum, crc32c_software) {
+  testCRC32C(folly::detail::crc32c_sw);
+}
+
+TEST(Checksum, crc32c_continuation_software) {
+  testCRC32CContinuation(folly::detail::crc32c_sw);
+}
+
+
+TEST(Checksum, crc32c_hardware) {
+  if (folly::detail::crc32c_hw_supported()) {
+    testCRC32C(folly::detail::crc32c_hw);
+  } else {
+    LOG(WARNING) << "skipping hardware-accelerated CRC-32C tests" <<
+        " (not supported on this CPU)";
+  }
+}
+
+TEST(Checksum, crc32c_continuation_hardware) {
+  if (folly::detail::crc32c_hw_supported()) {
+    testCRC32CContinuation(folly::detail::crc32c_hw);
+  } else {
+    LOG(WARNING) << "skipping hardware-accelerated CRC-32C tests" <<
+        " (not supported on this CPU)";
+  }
+}
+
+TEST(Checksum, crc32c_autodetect) {
+  testCRC32C(folly::crc32c);
+}
+
+TEST(Checksum, crc32c_continuation_autodetect) {
+  testCRC32CContinuation(folly::crc32c);
+}
+
+void benchmarkHardwareCRC32C(unsigned long iters, size_t blockSize) {
+  if (folly::detail::crc32c_hw_supported()) {
+    uint32_t checksum;
+    for (unsigned long i = 0; i < iters; i++) {
+      checksum = folly::detail::crc32c_hw(buffer, blockSize);
+      folly::doNotOptimizeAway(checksum);
+    }
+  } else {
+    LOG(WARNING) << "skipping hardware-accelerated CRC-32C benchmarks" <<
+        " (not supported on this CPU)";
+  }
+}
+
+void benchmarkSoftwareCRC32C(unsigned long iters, size_t blockSize) {
+  uint32_t checksum;
+  for (unsigned long i = 0; i < iters; i++) {
+    checksum = folly::detail::crc32c_sw(buffer, blockSize);
+    folly::doNotOptimizeAway(checksum);
+  }
+}
+
+// This test fits easily in the L1 cache on modern server processors,
+// and thus it mainly measures the speed of the checksum computation.
+BENCHMARK(crc32c_hardware_1KB_block, iters) {
+  benchmarkHardwareCRC32C(iters, 1024);
+}
+
+BENCHMARK(crc32c_software_1KB_block, iters) {
+  benchmarkSoftwareCRC32C(iters, 1024);
+}
+
+BENCHMARK_DRAW_LINE();
+
+// This test is too big for the L1 cache but fits in L2
+BENCHMARK(crc32c_hardware_64KB_block, iters) {
+  benchmarkHardwareCRC32C(iters, 64 * 1024);
+}
+
+BENCHMARK(crc32c_software_64KB_block, iters) {
+  benchmarkSoftwareCRC32C(iters, 64 * 1024);
+}
+
+BENCHMARK_DRAW_LINE();
+
+// This test is too big for the L2 cache but fits in L3
+BENCHMARK(crc32c_hardware_512KB_block, iters) {
+  benchmarkHardwareCRC32C(iters, 512 * 1024);
+}
+
+BENCHMARK(crc32c_software_512KB_block, iters) {
+  benchmarkSoftwareCRC32C(iters, 512 * 1024);
+}
+
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+
+  // Populate a buffer with a deterministic pattern
+  // on which to compute checksums
+  const uint8_t* src = buffer;
+  uint64_t* dst = (uint64_t*)buffer;
+  const uint64_t* end = (const uint64_t*)(buffer + BUFFER_SIZE);
+  *dst++ = 0;
+  while (dst < end) {
+    *dst++ = folly::hash::fnv64_buf((const char*)src, sizeof(uint64_t));
+    src += sizeof(uint64_t);
+  }
+
+  auto ret = RUN_ALL_TESTS();
+  if (!ret && FLAGS_benchmark) {
+    folly::runBenchmarks();
+  }
+  return ret;
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/ConcurrentSkipListBenchmark.cpp
@@ -0,0 +1,698 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// @author: Xin Liu <xliux@fb.com>
+
+#include <map>
+#include <set>
+#include <thread>
+
+#include <gflags/gflags.h>
+#include <glog/logging.h>
+#include "folly/Benchmark.h"
+#include "folly/ConcurrentSkipList.h"
+#include "folly/Hash.h"
+#include "folly/RWSpinLock.h"
+
+
+DEFINE_int32(num_threads, 12, "num concurrent threads to test");
+
+// In some case, we may want to test worker threads operating on multiple
+// lists. For example in search, not all threads are visiting the same posting
+// list, but for the ones with some popular terms, they do get multiple
+// visitors at the same time.
+DEFINE_int32(num_sets, 1, "num of set to operate on");
+
+static const int kInitHeadHeight = 10;
+static const int kMaxValue = 0x1000000;
+
+namespace {
+
+using namespace folly;
+
+typedef int ValueType;
+typedef ConcurrentSkipList<ValueType> SkipListType;
+typedef SkipListType::Accessor SkipListAccessor;
+typedef std::set<ValueType> SetType;
+
+static std::vector<ValueType> gData;
+static void initData() {
+  gData.resize(kMaxValue);
+  for (int i = 0; i < kMaxValue; ++i) {
+    gData[i] = i;
+  }
+  std::random_shuffle(gData.begin(), gData.end());
+}
+
+// single thread benchmarks
+void BM_IterateOverSet(int iters, int size) {
+  SetType a_set;
+
+  BENCHMARK_SUSPEND {
+    CHECK_GT(size, 0);
+    for (int i = 0; i < size; ++i) {
+      a_set.insert(gData[rand() % kMaxValue]);
+    }
+  }
+
+  int64_t sum = 0;
+  auto iter = a_set.begin();
+  for (int i = 0; i < iters; ++i) {
+    sum += *iter++;
+    if (iter == a_set.end()) iter = a_set.begin();
+  }
+  BENCHMARK_SUSPEND {
+    // VLOG(20) << "sum = " << sum;
+  }
+}
+
+void BM_IterateSkipList(int iters, int size) {
+  BenchmarkSuspender susp;
+  CHECK_GT(size, 0);
+  auto skipList = SkipListType::create(kInitHeadHeight);
+  for (int i = 0; i < size; ++i) {
+    skipList.add(rand() % kMaxValue);
+  }
+  int64_t sum = 0;
+  susp.dismiss();
+
+  auto iter = skipList.begin();
+  for (int i = 0; i < iters; ++i) {
+    sum += *iter++;
+    if (iter == skipList.end()) iter = skipList.begin();
+  }
+
+  BENCHMARK_SUSPEND {
+    // VLOG(20) << "sum = " << sum;
+  }
+}
+
+void BM_SetMerge(int iters, int size) {
+  BenchmarkSuspender susp;
+  SetType a_set;
+  SetType b_set;
+  for (int i = 0; i < iters; ++i) {
+    a_set.insert(rand() % kMaxValue);
+  }
+  for (int i = 0; i < size; ++i) {
+    b_set.insert(rand() % kMaxValue);
+  }
+  susp.dismiss();
+
+  int64_t mergedSum = 0;
+  FOR_EACH(it, a_set) {
+    if (b_set.find(*it) != b_set.end()) mergedSum += *it;
+  }
+  BENCHMARK_SUSPEND {
+    // VLOG(20) << mergedSum;
+  }
+}
+
+void BM_CSLMergeLookup(int iters, int size) {
+  BenchmarkSuspender susp;
+  auto skipList = SkipListType::create(kInitHeadHeight);
+  auto skipList2 = SkipListType::create(kInitHeadHeight);
+
+  for (int i = 0; i < iters; ++i) {
+    skipList.add(rand() % kMaxValue);
+  }
+  for (int i = 0; i < size; ++i) {
+    skipList2.add(rand() % kMaxValue);
+  }
+  int64_t mergedSum = 0;
+  susp.dismiss();
+
+  SkipListType::Skipper skipper(skipList2);
+  FOR_EACH(it, skipList) {
+    if (skipper.to(*it)) mergedSum += *it;
+  }
+
+  BENCHMARK_SUSPEND {
+    // VLOG(20) << mergedSum;
+  }
+}
+
+// merge by two skippers
+void BM_CSLMergeIntersection(int iters, int size) {
+  BenchmarkSuspender susp;
+  auto skipList = SkipListType::create(kInitHeadHeight);
+  auto skipList2 = SkipListType::create(kInitHeadHeight);
+  for (int i = 0; i < iters; ++i) {
+    skipList.add(rand() % kMaxValue);
+  }
+  for (int i = 0; i < size; ++i) {
+    skipList2.add(rand() % kMaxValue);
+  }
+  susp.dismiss();
+
+  SkipListType::Skipper s1(skipList);
+  SkipListType::Skipper s2(skipList2);
+
+  int64_t mergedSum = 0;
+
+  while (s1.good() && s2.good()) {
+    int v1 = s1.data();
+    int v2 = s2.data();
+    if (v1 < v2) {
+      s1.to(v2);
+    } else if (v1 > v2) {
+      s2.to(v1);
+    } else {
+      mergedSum += v1;
+      ++s1;
+      ++s2;
+    }
+  }
+
+  BENCHMARK_SUSPEND {
+    // VLOG(20) << mergedSum;
+  }
+}
+
+void BM_SetContainsNotFound(int iters, int size) {
+  BenchmarkSuspender susp;
+  SetType aset;
+  CHECK_LT(size, kMaxValue);
+  for (int i = 0; i < size; ++i) {
+    aset.insert(2 * i);
+  }
+  int64_t sum = 0;
+  susp.dismiss();
+
+  for (int i = 0; i < iters; ++i) {
+    sum += (aset.end() == aset.find(2 * i + 1));
+  }
+
+  BENCHMARK_SUSPEND {
+    // VLOG(20) << sum;
+  }
+}
+
+void BM_SetContainsFound(int iters, int size) {
+  BenchmarkSuspender susp;
+  SetType aset;
+  CHECK_LT(size, kMaxValue);
+
+  for (int i = 0; i < size; ++i) {
+    aset.insert(i);
+  }
+
+  std::vector<int> values;
+  for (int i = 0; i < iters; ++i) {
+    values.push_back(rand() % size);
+  }
+  int64_t sum = 0;
+  susp.dismiss();
+
+  for (int i = 0; i < iters; ++i) {
+    sum += (aset.end() == aset.find(values[i]));
+  }
+
+  BENCHMARK_SUSPEND {
+    // VLOG(20) << sum;
+  }
+}
+
+void BM_CSLContainsFound(int iters, int size) {
+  BenchmarkSuspender susp;
+  auto skipList = SkipListType::create(kInitHeadHeight);
+  CHECK_LT(size, kMaxValue);
+
+  for (int i = 0; i < size; ++i) {
+    skipList.add(i);
+  }
+  std::vector<int> values;
+  for (int i = 0; i < iters; ++i) {
+    values.push_back(rand() % size);
+  }
+  int64_t sum = 0;
+  susp.dismiss();
+
+  for (int i = 0; i < iters; ++i) {
+    sum += skipList.contains(values[i]);
+  }
+
+  BENCHMARK_SUSPEND {
+    // VLOG(20) << sum;
+  }
+}
+
+void BM_CSLContainsNotFound(int iters, int size) {
+  BenchmarkSuspender susp;
+  auto skipList = SkipListType::create(kInitHeadHeight);
+  CHECK_LT(size, kMaxValue);
+
+  for (int i = 0; i < size; ++i) {
+    skipList.add(2 * i);
+  }
+  int64_t sum = 0;
+  susp.dismiss();
+
+  for (int i = 0; i < iters; ++i) {
+    sum += skipList.contains(2 * i + 1);
+  }
+
+  BENCHMARK_SUSPEND {
+    // VLOG(20) << sum;
+  }
+}
+
+void BM_AddSet(int iters, int size) {
+  BenchmarkSuspender susp;
+  SetType aset;
+  for (int i = 0; i < size; ++i) {
+    aset.insert(gData[i]);
+  }
+  susp.dismiss();
+
+  for (int i = size; i < size + iters; ++i) {
+    aset.insert(gData[i]);
+  }
+}
+
+void BM_AddSkipList(int iters, int size) {
+  BenchmarkSuspender susp;
+  auto skipList = SkipListType::create(kInitHeadHeight);
+  for (int i = 0; i < size; ++i) {
+    skipList.add(gData[i]);
+  }
+  susp.dismiss();
+
+  for (int i = size; i < size + iters; ++i) {
+    skipList.add(gData[i]);
+  }
+}
+
+BENCHMARK(Accessor, iters) {
+  BenchmarkSuspender susp;
+  auto skiplist = SkipListType::createInstance(kInitHeadHeight);
+  auto sl = skiplist.get();
+
+  susp.dismiss();
+  for (int i = 0; i < iters; ++i) {
+    SkipListAccessor accessor(sl);
+  }
+}
+
+// a benchmark to estimate the
+// low bound of doing a ref counting for an Accessor
+BENCHMARK(accessorBasicRefcounting, iters) {
+  BenchmarkSuspender susp;
+  auto* value = new std::atomic<int32_t>();
+  auto* dirty = new std::atomic<int32_t>();
+  *value = *dirty = 0;
+  folly::MicroSpinLock l;
+  l.init();
+
+  susp.dismiss();
+  for (int i = 0; i < iters; ++i) {
+    value->fetch_add(1, std::memory_order_relaxed);
+    if (dirty->load(std::memory_order_acquire) != 0) {
+      folly::MSLGuard g(l);
+    }
+    value->fetch_sub(1, std::memory_order_relaxed);
+  }
+
+  BENCHMARK_SUSPEND {
+    delete dirty;
+    delete value;
+  }
+}
+
+
+// Data For testing contention benchmark
+class ConcurrentAccessData {
+ public:
+  explicit ConcurrentAccessData(int size) :
+    skipList_(SkipListType::create(10)),
+    sets_(FLAGS_num_sets), locks_(FLAGS_num_sets) {
+
+    for (int i = 0; i < size; ++i) {
+      sets_[0].insert(i);
+      skipList_.add(i);
+    }
+
+    for (int i = 0; i < FLAGS_num_sets; ++i) {
+      locks_[i] = new RWSpinLock();
+      if (i > 0) sets_[i] = sets_[0];
+    }
+
+// This requires knowledge of the C++ library internals. Only use it if we're
+// using the GNU C++ library.
+#ifdef _GLIBCXX_SYMVER
+    // memory usage
+    int64_t setMemorySize = sets_[0].size() * sizeof(*sets_[0].begin()._M_node);
+    int64_t cslMemorySize = 0;
+    for (auto it = skipList_.begin(); it != skipList_.end(); ++it) {
+      cslMemorySize += it.nodeSize();
+    }
+
+    LOG(INFO) << "size=" << sets_[0].size()
+      << "; std::set memory size=" << setMemorySize
+      << "; csl memory size=" << cslMemorySize;
+#endif
+
+    readValues_.reserve(size);
+    deleteValues_.reserve(size);
+    writeValues_.reserve(size);
+    for (int i = size; i < 2 * size; ++i) {
+      readValues_.push_back(2 * i);
+      deleteValues_.push_back(2 * i);
+
+      // half new values and half already in the list
+      writeValues_.push_back((rand() % 2) + 2 * i);
+    }
+    std::random_shuffle(readValues_.begin(), readValues_.end());
+    std::random_shuffle(deleteValues_.begin(), deleteValues_.end());
+    std::random_shuffle(writeValues_.begin(), writeValues_.end());
+  }
+
+  ~ConcurrentAccessData() {
+    FOR_EACH(lock, locks_) delete *lock;
+  }
+
+  inline bool skipListFind(int idx, ValueType val) {
+    return skipList_.contains(val);
+  }
+  inline void skipListInsert(int idx, ValueType val) {
+    skipList_.add(val);
+  }
+  inline void skipListErase(int idx, ValueType val) {
+    skipList_.remove(val);
+  }
+
+  inline bool setFind(int idx, ValueType val) {
+    RWSpinLock::ReadHolder g(locks_[idx]);
+    return sets_[idx].find(val) == sets_[idx].end();
+  }
+  inline void setInsert(int idx, ValueType val) {
+    RWSpinLock::WriteHolder g(locks_[idx]);
+    sets_[idx].insert(val);
+  }
+  inline void setErase(int idx, ValueType val) {
+    RWSpinLock::WriteHolder g(locks_[idx]);
+    sets_[idx].erase(val);
+  }
+
+  void runSkipList(int id, int iters) {
+    int sum = 0;
+    for (int i = 0; i < iters; ++i) {
+      sum += accessSkipList(id, i);
+    }
+    // VLOG(20) << sum;
+  }
+
+  void runSet(int id, int iters) {
+    int sum = 0;
+    for (int i = 0; i < iters; ++i) {
+      sum += accessSet(id, i);
+    }
+    // VLOG(20) << sum;
+  }
+
+  bool accessSkipList(int64_t id, int t) {
+    if (t > readValues_.size()) {
+      t = t % readValues_.size();
+    }
+    uint32_t h = folly::hash::twang_32from64(t * id);
+    switch (h % 8) {
+      case 7:   // write
+        if ((h & 0x31) == 0) { // 1/4 chance to delete
+          skipListErase(0, deleteValues_[t]);
+        } else {
+          skipListInsert(0, writeValues_[t]);
+        }
+        return 0;
+      default:
+        return skipListFind(0, readValues_[t]);
+    }
+  }
+
+  bool accessSet(int64_t id, int t) {
+    if (t > readValues_.size()) {
+      t = t % readValues_.size();
+    }
+    uint32_t h = folly::hash::twang_32from64(t * id);
+    int idx = (h % FLAGS_num_sets);
+    switch (h % 8) {  // 1/8 chance to write
+      case 7:   // write
+        if ((h & 0x31) == 0) { // 1/32 chance to delete
+          setErase(idx, deleteValues_[t]);
+        } else {
+          setInsert(idx, writeValues_[t]);
+        }
+        return 0;
+      default:
+        return setFind(idx, readValues_[t]);
+    }
+  }
+
+ private:
+  SkipListType::Accessor skipList_;
+  std::vector<SetType> sets_;
+  std::vector<RWSpinLock*> locks_;
+
+  std::vector<ValueType> readValues_;
+  std::vector<ValueType> writeValues_;
+  std::vector<ValueType> deleteValues_;
+};
+
+static std::map<int, std::shared_ptr<ConcurrentAccessData> > g_data;
+
+static ConcurrentAccessData *mayInitTestData(int size) {
+  auto it = g_data.find(size);
+  if (it == g_data.end()) {
+    auto ptr = std::shared_ptr<ConcurrentAccessData>(
+        new ConcurrentAccessData(size));
+    g_data[size] = ptr;
+    return ptr.get();
+  }
+  return it->second.get();
+}
+
+void BM_ContentionCSL(int iters, int size) {
+  BenchmarkSuspender susp;
+  auto data = mayInitTestData(size);
+  std::vector<std::thread> threads;
+  susp.dismiss();
+
+  for (int i = 0; i < FLAGS_num_threads; ++i) {
+    threads.push_back(std::thread(
+          &ConcurrentAccessData::runSkipList, data, i, iters));
+  }
+  FOR_EACH(t, threads) {
+    (*t).join();
+  }
+}
+
+void BM_ContentionStdSet(int iters, int size) {
+  BenchmarkSuspender susp;
+  auto data = mayInitTestData(size);
+  std::vector<std::thread> threads;
+  susp.dismiss();
+
+  for (int i = 0; i < FLAGS_num_threads; ++i) {
+    threads.push_back(std::thread(
+          &ConcurrentAccessData::runSet, data, i, iters));
+  }
+  FOR_EACH(t, threads) {
+    (*t).join();
+  }
+  susp.rehire();
+}
+
+
+// Single-thread benchmarking
+
+BENCHMARK_DRAW_LINE();
+
+BENCHMARK_PARAM(BM_IterateOverSet,  1000);
+BENCHMARK_PARAM(BM_IterateSkipList, 1000);
+BENCHMARK_DRAW_LINE();
+BENCHMARK_PARAM(BM_IterateOverSet,  1000000);
+BENCHMARK_PARAM(BM_IterateSkipList, 1000000);
+BENCHMARK_DRAW_LINE();
+
+// find with keys in the set
+BENCHMARK_PARAM(BM_SetContainsFound, 1000);
+BENCHMARK_PARAM(BM_CSLContainsFound, 1000);
+BENCHMARK_DRAW_LINE();
+BENCHMARK_PARAM(BM_SetContainsFound, 100000);
+BENCHMARK_PARAM(BM_CSLContainsFound, 100000);
+BENCHMARK_DRAW_LINE();
+BENCHMARK_PARAM(BM_SetContainsFound, 1000000);
+BENCHMARK_PARAM(BM_CSLContainsFound, 1000000);
+BENCHMARK_DRAW_LINE();
+BENCHMARK_PARAM(BM_SetContainsFound, 10000000);
+BENCHMARK_PARAM(BM_CSLContainsFound, 10000000);
+BENCHMARK_DRAW_LINE();
+
+
+// find with keys not in the set
+BENCHMARK_PARAM(BM_SetContainsNotFound, 1000);
+BENCHMARK_PARAM(BM_CSLContainsNotFound, 1000);
+BENCHMARK_DRAW_LINE();
+BENCHMARK_PARAM(BM_SetContainsNotFound, 100000);
+BENCHMARK_PARAM(BM_CSLContainsNotFound, 100000);
+BENCHMARK_DRAW_LINE();
+BENCHMARK_PARAM(BM_SetContainsNotFound, 1000000);
+BENCHMARK_PARAM(BM_CSLContainsNotFound, 1000000);
+BENCHMARK_DRAW_LINE();
+
+
+BENCHMARK_PARAM(BM_AddSet,      1000);
+BENCHMARK_PARAM(BM_AddSkipList, 1000);
+BENCHMARK_DRAW_LINE();
+
+BENCHMARK_PARAM(BM_AddSet,      65536);
+BENCHMARK_PARAM(BM_AddSkipList, 65536);
+BENCHMARK_DRAW_LINE();
+
+BENCHMARK_PARAM(BM_AddSet,      1000000);
+BENCHMARK_PARAM(BM_AddSkipList, 1000000);
+BENCHMARK_DRAW_LINE();
+
+BENCHMARK_PARAM(BM_SetMerge,             1000);
+BENCHMARK_PARAM(BM_CSLMergeIntersection, 1000);
+BENCHMARK_PARAM(BM_CSLMergeLookup,       1000);
+BENCHMARK_DRAW_LINE();
+
+BENCHMARK_PARAM(BM_SetMerge,             65536);
+BENCHMARK_PARAM(BM_CSLMergeIntersection, 65536);
+BENCHMARK_PARAM(BM_CSLMergeLookup,       65536);
+BENCHMARK_DRAW_LINE();
+
+BENCHMARK_PARAM(BM_SetMerge,             1000000);
+BENCHMARK_PARAM(BM_CSLMergeIntersection, 1000000);
+BENCHMARK_PARAM(BM_CSLMergeLookup,       1000000);
+BENCHMARK_DRAW_LINE();
+
+
+// multithreaded benchmarking
+
+BENCHMARK_PARAM(BM_ContentionStdSet, 1024);
+BENCHMARK_PARAM(BM_ContentionCSL,    1024);
+BENCHMARK_DRAW_LINE();
+
+BENCHMARK_PARAM(BM_ContentionStdSet, 65536);
+BENCHMARK_PARAM(BM_ContentionCSL,    65536);
+BENCHMARK_DRAW_LINE();
+
+BENCHMARK_PARAM(BM_ContentionStdSet, 1048576);
+BENCHMARK_PARAM(BM_ContentionCSL,    1048576);
+BENCHMARK_DRAW_LINE();
+
+}
+
+int main(int argc, char** argv) {
+  google::InitGoogleLogging(argv[0]);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+
+  initData();
+  runBenchmarks();
+  return 0;
+}
+
+#if 0
+/*
+Benchmark on Intel(R) Xeon(R) CPU X5650 @2.67GHz
+
+==============================================================================
+1 thread Benchmark                     Iters   Total t    t/iter iter/sec
+------------------------------------------------------------------------------
+ +37.0% BM_Accessor                    100000  1.958 ms  19.58 ns  48.71 M
+*       BM_AccessorBasicRefcounting    100000  1.429 ms  14.29 ns  66.74 M
+------------------------------------------------------------------------------
+ + 603% BM_IterateOverSet/1000         100000  1.589 ms  15.89 ns  60.02 M
+*       BM_IterateSkipList/1000        100000    226 us   2.26 ns    422 M
+------------------------------------------------------------------------------
+ + 107% BM_IterateOverSet/976.6k       100000  8.324 ms  83.24 ns  11.46 M
+*       BM_IterateSkipList/976.6k      100000  4.016 ms  40.16 ns  23.75 M
+------------------------------------------------------------------------------
+*       BM_SetContainsFound/1000       100000  7.082 ms  70.82 ns  13.47 M
+ +39.9% BM_CSLContainsFound/1000       100000  9.908 ms  99.08 ns  9.625 M
+------------------------------------------------------------------------------
+*       BM_SetContainsFound/97.66k     100000   23.8 ms    238 ns  4.006 M
+ +5.97% BM_CSLContainsFound/97.66k     100000  25.23 ms  252.3 ns  3.781 M
+------------------------------------------------------------------------------
+ +33.6% BM_SetContainsFound/976.6k     100000   64.3 ms    643 ns  1.483 M
+*       BM_CSLContainsFound/976.6k     100000  48.13 ms  481.3 ns  1.981 M
+------------------------------------------------------------------------------
+ +30.3% BM_SetContainsFound/9.537M     100000  115.1 ms  1.151 us  848.6 k
+*       BM_CSLContainsFound/9.537M     100000  88.33 ms  883.3 ns   1.08 M
+------------------------------------------------------------------------------
+*       BM_SetContainsNotFound/1000    100000  2.081 ms  20.81 ns  45.83 M
+ +76.2% BM_CSLContainsNotFound/1000    100000  3.667 ms  36.67 ns  26.01 M
+------------------------------------------------------------------------------
+*       BM_SetContainsNotFound/97.66k  100000  6.049 ms  60.49 ns  15.77 M
+ +32.7% BM_CSLContainsNotFound/97.66k  100000  8.025 ms  80.25 ns  11.88 M
+------------------------------------------------------------------------------
+*       BM_SetContainsNotFound/976.6k  100000  7.464 ms  74.64 ns  12.78 M
+ +12.8% BM_CSLContainsNotFound/976.6k  100000  8.417 ms  84.17 ns  11.33 M
+------------------------------------------------------------------------------
+*       BM_AddSet/1000                 100000  29.26 ms  292.6 ns  3.259 M
+ +70.0% BM_AddSkipList/1000            100000  49.75 ms  497.5 ns  1.917 M
+------------------------------------------------------------------------------
+*       BM_AddSet/64k                  100000  38.73 ms  387.3 ns  2.462 M
+ +55.7% BM_AddSkipList/64k             100000   60.3 ms    603 ns  1.581 M
+------------------------------------------------------------------------------
+*       BM_AddSet/976.6k               100000  75.71 ms  757.1 ns   1.26 M
+ +33.6% BM_AddSkipList/976.6k          100000  101.2 ms  1.012 us  965.3 k
+------------------------------------------------------------------------------
+ + 716% BM_SetMerge/1000               100000  6.872 ms  68.72 ns  13.88 M
+*       BM_CSLMergeIntersection/1000   100000    842 us   8.42 ns  113.3 M
+ + 268% BM_CSLMergeLookup/1000         100000    3.1 ms     31 ns  30.76 M
+------------------------------------------------------------------------------
+ +36.3% BM_SetMerge/64k                100000  14.03 ms  140.3 ns  6.798 M
+ +39.4% BM_CSLMergeIntersection/64k    100000  14.35 ms  143.5 ns  6.645 M
+*       BM_CSLMergeLookup/64k          100000  10.29 ms  102.9 ns  9.266 M
+------------------------------------------------------------------------------
+ +10.3% BM_SetMerge/976.6k             100000  46.24 ms  462.4 ns  2.062 M
+ +25.1% BM_CSLMergeIntersection/976.6k 100000  52.47 ms  524.7 ns  1.818 M
+*       BM_CSLMergeLookup/976.6k       100000  41.94 ms  419.3 ns  2.274 M
+------------------------------------------------------------------------------
+
+
+==============================================================================
+Contention benchmark 7/8 find, 3/32 insert, 1/32 erase
+
+ 4 threads Benchmark                   Iters   Total t    t/iter iter/sec
+------------------------------------------------------------------------------
+ + 269% BM_ContentionStdSet/1k         100000  75.66 ms  756.6 ns   1.26 M
+*       BM_ContentionCSL/1k            100000  20.47 ms  204.7 ns  4.658 M
+------------------------------------------------------------------------------
+ + 228% BM_ContentionStdSet/64k        100000  105.6 ms  1.056 us  924.9 k
+*       BM_ContentionCSL/64k           100000  32.18 ms  321.8 ns  2.963 M
+------------------------------------------------------------------------------
+ + 224% BM_ContentionStdSet/1M         100000  117.4 ms  1.174 us  832.2 k
+*       BM_ContentionCSL/1M            100000  36.18 ms  361.8 ns  2.636 M
+------------------------------------------------------------------------------
+
+
+12 threads Benchmark                   Iters   Total t    t/iter iter/sec
+------------------------------------------------------------------------------
+ + 697% BM_ContentionStdSet/1k         100000  455.3 ms  4.553 us  214.5 k
+*       BM_ContentionCSL/1k            100000  57.12 ms  571.2 ns   1.67 M
+------------------------------------------------------------------------------
+ +1257% BM_ContentionStdSet/64k        100000  654.9 ms  6.549 us  149.1 k
+*       BM_ContentionCSL/64k           100000  48.24 ms  482.4 ns  1.977 M
+------------------------------------------------------------------------------
+ +1262% BM_ContentionStdSet/1M         100000  657.3 ms  6.573 us  148.6 k
+*       BM_ContentionCSL/1M            100000  48.25 ms  482.5 ns  1.977 M
+------------------------------------------------------------------------------
+
+*/
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/ConcurrentSkipListTest.cpp
@@ -0,0 +1,395 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// @author: Xin Liu <xliux@fb.com>
+
+#include <memory>
+#include <set>
+#include <vector>
+#include <thread>
+#include <system_error>
+
+#include <glog/logging.h>
+#include <gflags/gflags.h>
+#include "folly/ConcurrentSkipList.h"
+#include "folly/Foreach.h"
+#include "folly/String.h"
+#include <gtest/gtest.h>
+
+DEFINE_int32(num_threads, 12, "num concurrent threads to test");
+
+namespace {
+
+using namespace folly;
+using std::vector;
+
+typedef int ValueType;
+typedef detail::SkipListNode<ValueType> SkipListNodeType;
+typedef ConcurrentSkipList<ValueType> SkipListType;
+typedef SkipListType::Accessor SkipListAccessor;
+typedef vector<ValueType> VectorType;
+typedef std::set<ValueType> SetType;
+
+static const int kHeadHeight = 2;
+static const int kMaxValue = 5000;
+
+static void randomAdding(int size,
+    SkipListAccessor skipList,
+    SetType *verifier,
+    int maxValue = kMaxValue) {
+  for (int i = 0; i < size; ++i) {
+    int32_t r = rand() % maxValue;
+    verifier->insert(r);
+    skipList.add(r);
+  }
+}
+
+static void randomRemoval(int size,
+    SkipListAccessor skipList,
+    SetType *verifier,
+    int maxValue=kMaxValue) {
+  for (int i = 0; i < size; ++i) {
+    int32_t r = rand() % maxValue;
+    verifier->insert(r);
+    skipList.remove(r);
+  }
+}
+
+static void sumAllValues(SkipListAccessor skipList, int64_t *sum) {
+  *sum = 0;
+  FOR_EACH(it, skipList) {
+    *sum += *it;
+  }
+  VLOG(20) << "sum = " << sum;
+}
+
+static void concurrentSkip(const vector<ValueType> *values,
+    SkipListAccessor skipList) {
+  int64_t sum = 0;
+  SkipListAccessor::Skipper skipper(skipList);
+  FOR_EACH(it, *values) {
+    if (skipper.to(*it)) sum += *it;
+  }
+  VLOG(20) << "sum = " << sum;
+}
+
+bool verifyEqual(SkipListAccessor skipList,
+    const SetType &verifier) {
+  EXPECT_EQ(verifier.size(), skipList.size());
+  FOR_EACH(it, verifier) {
+    CHECK(skipList.contains(*it)) << *it;
+    SkipListType::const_iterator iter = skipList.find(*it);
+    CHECK(iter != skipList.end());
+    EXPECT_EQ(*iter, *it);
+  }
+  EXPECT_TRUE(std::equal(verifier.begin(), verifier.end(), skipList.begin()));
+  return true;
+}
+
+TEST(ConcurrentSkipList, SequentialAccess) {
+  {
+    LOG(INFO) << "nodetype size=" << sizeof(SkipListNodeType);
+
+    auto skipList(SkipListType::create(kHeadHeight));
+    EXPECT_TRUE(skipList.first() == NULL);
+    EXPECT_TRUE(skipList.last() == NULL);
+
+    skipList.add(3);
+    EXPECT_TRUE(skipList.contains(3));
+    EXPECT_FALSE(skipList.contains(2));
+    EXPECT_EQ(3, *skipList.first());
+    EXPECT_EQ(3, *skipList.last());
+
+    EXPECT_EQ(3, *skipList.find(3));
+    EXPECT_FALSE(skipList.find(3) == skipList.end());
+    EXPECT_TRUE(skipList.find(2) == skipList.end());
+
+    {
+      SkipListAccessor::Skipper skipper(skipList);
+      skipper.to(3);
+      CHECK_EQ(3, *skipper);
+    }
+
+    skipList.add(2);
+    EXPECT_EQ(2, *skipList.first());
+    EXPECT_EQ(3, *skipList.last());
+    skipList.add(5);
+    EXPECT_EQ(5, *skipList.last());
+    skipList.add(3);
+    EXPECT_EQ(5, *skipList.last());
+    auto ret = skipList.insert(9);
+    EXPECT_EQ(9, *ret.first);
+    EXPECT_TRUE(ret.second);
+
+    ret = skipList.insert(5);
+    EXPECT_EQ(5, *ret.first);
+    EXPECT_FALSE(ret.second);
+
+    EXPECT_EQ(2, *skipList.first());
+    EXPECT_EQ(9, *skipList.last());
+    EXPECT_TRUE(skipList.pop_back());
+    EXPECT_EQ(5, *skipList.last());
+    EXPECT_TRUE(skipList.pop_back());
+    EXPECT_EQ(3, *skipList.last());
+
+    skipList.add(9);
+    skipList.add(5);
+
+    CHECK(skipList.contains(2));
+    CHECK(skipList.contains(3));
+    CHECK(skipList.contains(5));
+    CHECK(skipList.contains(9));
+    CHECK(!skipList.contains(4));
+
+    // lower_bound
+    auto it = skipList.lower_bound(5);
+    EXPECT_EQ(5, *it);
+    it = skipList.lower_bound(4);
+    EXPECT_EQ(5, *it);
+    it = skipList.lower_bound(9);
+    EXPECT_EQ(9, *it);
+    it = skipList.lower_bound(12);
+    EXPECT_FALSE(it.good());
+
+    it = skipList.begin();
+    EXPECT_EQ(2, *it);
+
+    // skipper test
+    SkipListAccessor::Skipper skipper(skipList);
+    skipper.to(3);
+    EXPECT_EQ(3, skipper.data());
+    skipper.to(5);
+    EXPECT_EQ(5, skipper.data());
+    CHECK(!skipper.to(7));
+
+    skipList.remove(5);
+    skipList.remove(3);
+    CHECK(skipper.to(9));
+    EXPECT_EQ(9, skipper.data());
+
+    CHECK(!skipList.contains(3));
+    skipList.add(3);
+    CHECK(skipList.contains(3));
+    int pos = 0;
+    FOR_EACH(it, skipList) {
+      LOG(INFO) << "pos= " << pos++ << " value= " << *it;
+    }
+  }
+
+  {
+    auto skipList(SkipListType::create(kHeadHeight));
+
+    SetType verifier;
+    randomAdding(10000, skipList, &verifier);
+    verifyEqual(skipList, verifier);
+
+    // test skipper
+    SkipListAccessor::Skipper skipper(skipList);
+    int num_skips = 1000;
+    for (int i = 0; i < num_skips; ++i) {
+      int n = i * kMaxValue / num_skips;
+      bool found = skipper.to(n);
+      EXPECT_EQ(found, (verifier.find(n) != verifier.end()));
+    }
+  }
+
+}
+
+static std::string makeRandomeString(int len) {
+  std::string s;
+  for (int j = 0; j < len; j++) {
+    s.push_back((rand() % 26) + 'A');
+  }
+  return s;
+}
+
+TEST(ConcurrentSkipList, TestStringType) {
+  typedef folly::ConcurrentSkipList<std::string> SkipListT;
+  std::shared_ptr<SkipListT> skip = SkipListT::createInstance();
+  SkipListT::Accessor accessor(skip);
+  {
+    for (int i = 0; i < 100000; i++) {
+      std::string s = makeRandomeString(7);
+      accessor.insert(s);
+    }
+  }
+  EXPECT_TRUE(std::is_sorted(accessor.begin(), accessor.end()));
+}
+
+struct UniquePtrComp {
+  bool operator ()(
+      const std::unique_ptr<int> &x, const std::unique_ptr<int> &y) const {
+    if (!x) return false;
+    if (!y) return true;
+    return *x < *y;
+  }
+};
+
+TEST(ConcurrentSkipList, TestMovableData) {
+  typedef folly::ConcurrentSkipList<std::unique_ptr<int>, UniquePtrComp>
+    SkipListT;
+  auto sl = SkipListT::createInstance() ;
+  SkipListT::Accessor accessor(sl);
+
+  static const int N = 10;
+  for (int i = 0; i < N; ++i) {
+    accessor.insert(std::unique_ptr<int>(new int(i)));
+  }
+
+  for (int i = 0; i < N; ++i) {
+    EXPECT_TRUE(accessor.find(std::unique_ptr<int>(new int(i))) !=
+        accessor.end());
+  }
+  EXPECT_TRUE(accessor.find(std::unique_ptr<int>(new int(N))) ==
+      accessor.end());
+}
+
+void testConcurrentAdd(int numThreads) {
+  auto skipList(SkipListType::create(kHeadHeight));
+
+  vector<std::thread> threads;
+  vector<SetType> verifiers(numThreads);
+  try {
+    for (int i = 0; i < numThreads; ++i) {
+      threads.push_back(std::thread(
+            &randomAdding, 100, skipList, &verifiers[i], kMaxValue));
+    }
+  } catch (const std::system_error& e) {
+    LOG(WARNING)
+      << "Caught " << exceptionStr(e)
+      << ": could only create " << threads.size() << " threads out of "
+      << numThreads;
+  }
+  for (int i = 0; i < threads.size(); ++i) {
+    threads[i].join();
+  }
+
+  SetType all;
+  FOR_EACH(s, verifiers) {
+    all.insert(s->begin(), s->end());
+  }
+  verifyEqual(skipList, all);
+}
+
+TEST(ConcurrentSkipList, ConcurrentAdd) {
+  // test it many times
+  for (int numThreads = 10; numThreads < 10000; numThreads += 1000) {
+    testConcurrentAdd(numThreads);
+  }
+}
+
+void testConcurrentRemoval(int numThreads, int maxValue) {
+  auto skipList = SkipListType::create(kHeadHeight);
+  for (int i = 0; i < maxValue; ++i) {
+    skipList.add(i);
+  }
+
+  vector<std::thread> threads;
+  vector<SetType > verifiers(numThreads);
+  try {
+    for (int i = 0; i < numThreads; ++i) {
+      threads.push_back(std::thread(
+            &randomRemoval, 100, skipList, &verifiers[i], maxValue));
+    }
+  } catch (const std::system_error& e) {
+    LOG(WARNING)
+      << "Caught " << exceptionStr(e)
+      << ": could only create " << threads.size() << " threads out of "
+      << numThreads;
+  }
+  FOR_EACH(t, threads) {
+    (*t).join();
+  }
+
+  SetType all;
+  FOR_EACH(s, verifiers) {
+    all.insert(s->begin(), s->end());
+  }
+
+  CHECK_EQ(maxValue, all.size() + skipList.size());
+  for (int i = 0; i < maxValue; ++i) {
+    if (all.find(i) != all.end()) {
+      CHECK(!skipList.contains(i)) << i;
+    } else {
+      CHECK(skipList.contains(i)) << i;
+    }
+  }
+}
+
+TEST(ConcurrentSkipList, ConcurrentRemove) {
+  for (int numThreads = 10; numThreads < 1000; numThreads += 100) {
+    testConcurrentRemoval(numThreads, 100 * numThreads);
+  }
+}
+
+static void testConcurrentAccess(
+    int numInsertions, int numDeletions, int maxValue) {
+  auto skipList = SkipListType::create(kHeadHeight);
+
+  vector<SetType> verifiers(FLAGS_num_threads);
+  vector<int64_t> sums(FLAGS_num_threads);
+  vector<vector<ValueType> > skipValues(FLAGS_num_threads);
+
+  for (int i = 0; i < FLAGS_num_threads; ++i) {
+    for (int j = 0; j < numInsertions; ++j) {
+      skipValues[i].push_back(rand() % (maxValue + 1));
+    }
+    std::sort(skipValues[i].begin(), skipValues[i].end());
+  }
+
+  vector<std::thread> threads;
+  for (int i = 0; i < FLAGS_num_threads; ++i) {
+    switch (i % 8) {
+      case 0:
+      case 1:
+        threads.push_back(std::thread(
+              randomAdding, numInsertions, skipList, &verifiers[i], maxValue));
+        break;
+      case 2:
+        threads.push_back(std::thread(
+              randomRemoval, numDeletions, skipList, &verifiers[i], maxValue));
+        break;
+      case 3:
+        threads.push_back(std::thread(
+              concurrentSkip, &skipValues[i], skipList));
+        break;
+      default:
+        threads.push_back(std::thread(sumAllValues, skipList, &sums[i]));
+        break;
+    }
+  }
+
+  FOR_EACH(t, threads) {
+    (*t).join();
+  }
+  // just run through it, no need to verify the correctness.
+}
+
+TEST(ConcurrentSkipList, ConcurrentAccess) {
+  testConcurrentAccess(10000, 100, kMaxValue);
+  testConcurrentAccess(100000, 10000, kMaxValue * 10);
+  testConcurrentAccess(1000000, 100000, kMaxValue);
+}
+
+}  // namespace
+
+int main(int argc, char* argv[]) {
+  testing::InitGoogleTest(&argc, argv);
+  google::InitGoogleLogging(argv[0]);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+
+  return RUN_ALL_TESTS();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/ConvTest.cpp
@@ -0,0 +1,980 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Benchmark.h"
+#include "folly/Conv.h"
+#include "folly/Foreach.h"
+#include <boost/lexical_cast.hpp>
+#include <gtest/gtest.h>
+#include <limits>
+#include <stdexcept>
+
+using namespace std;
+using namespace folly;
+
+static int8_t s8;
+static uint8_t u8;
+static int16_t s16;
+static uint16_t u16;
+static int32_t s32;
+static uint32_t u32;
+static int64_t s64;
+static uint64_t u64;
+
+TEST(Conv, Integral2Integral) {
+  // Same size, different signs
+  s64 = numeric_limits<uint8_t>::max();
+  EXPECT_EQ(to<uint8_t>(s64), s64);
+
+  s64 = numeric_limits<int8_t>::max();
+  EXPECT_EQ(to<int8_t>(s64), s64);
+}
+
+TEST(Conv, Floating2Floating) {
+  float f1 = 1e3;
+  double d1 = to<double>(f1);
+  EXPECT_EQ(f1, d1);
+
+  double d2 = 23.0;
+  auto f2 = to<float>(d2);
+  EXPECT_EQ(double(f2), d2);
+
+  double invalidFloat = std::numeric_limits<double>::max();
+  EXPECT_ANY_THROW(to<float>(invalidFloat));
+  invalidFloat = -std::numeric_limits<double>::max();
+  EXPECT_ANY_THROW(to<float>(invalidFloat));
+
+  try {
+    auto shouldWork = to<float>(std::numeric_limits<double>::min());
+    // The value of `shouldWork' is an implementation defined choice
+    // between the following two alternatives.
+    EXPECT_TRUE(shouldWork == std::numeric_limits<float>::min() ||
+                shouldWork == 0.f);
+  } catch (...) {
+    EXPECT_TRUE(false);
+  }
+}
+
+template <class String>
+void testIntegral2String() {
+}
+
+template <class String, class Int, class... Ints>
+void testIntegral2String() {
+  typedef typename make_unsigned<Int>::type Uint;
+  typedef typename make_signed<Int>::type Sint;
+
+  Uint value = 123;
+  EXPECT_EQ(to<String>(value), "123");
+  Sint svalue = 123;
+  EXPECT_EQ(to<String>(svalue), "123");
+  svalue = -123;
+  EXPECT_EQ(to<String>(svalue), "-123");
+
+  value = numeric_limits<Uint>::min();
+  EXPECT_EQ(to<Uint>(to<String>(value)), value);
+  value = numeric_limits<Uint>::max();
+  EXPECT_EQ(to<Uint>(to<String>(value)), value);
+
+  svalue = numeric_limits<Sint>::min();
+  EXPECT_EQ(to<Sint>(to<String>(svalue)), svalue);
+  value = numeric_limits<Sint>::max();
+  EXPECT_EQ(to<Sint>(to<String>(svalue)), svalue);
+
+  testIntegral2String<String, Ints...>();
+}
+
+#if FOLLY_HAVE_INT128_T
+template <class String>
+void test128Bit2String() {
+  typedef unsigned __int128 Uint;
+  typedef __int128 Sint;
+
+  EXPECT_EQ(detail::digitsEnough<unsigned __int128>(), 39);
+
+  Uint value = 123;
+  EXPECT_EQ(to<String>(value), "123");
+  Sint svalue = 123;
+  EXPECT_EQ(to<String>(svalue), "123");
+  svalue = -123;
+  EXPECT_EQ(to<String>(svalue), "-123");
+
+  value = __int128(1) << 64;
+  EXPECT_EQ(to<String>(value), "18446744073709551616");
+
+  svalue =  -(__int128(1) << 64);
+  EXPECT_EQ(to<String>(svalue), "-18446744073709551616");
+
+  value = 0;
+  EXPECT_EQ(to<String>(value), "0");
+
+  svalue = 0;
+  EXPECT_EQ(to<String>(svalue), "0");
+
+  // TODO: the following do not compile to<__int128> ...
+
+#if 0
+  value = numeric_limits<Uint>::min();
+  EXPECT_EQ(to<Uint>(to<String>(value)), value);
+  value = numeric_limits<Uint>::max();
+  EXPECT_EQ(to<Uint>(to<String>(value)), value);
+
+  svalue = numeric_limits<Sint>::min();
+  EXPECT_EQ(to<Sint>(to<String>(svalue)), svalue);
+  value = numeric_limits<Sint>::max();
+  EXPECT_EQ(to<Sint>(to<String>(svalue)), svalue);
+#endif
+}
+
+#endif
+
+TEST(Conv, Integral2String) {
+  testIntegral2String<std::string, char, short, int, long>();
+  testIntegral2String<fbstring, char, short, int, long>();
+
+#if FOLLY_HAVE_INT128_T
+  test128Bit2String<std::string>();
+  test128Bit2String<fbstring>();
+#endif
+}
+
+template <class String>
+void testString2Integral() {
+}
+
+template <class String, class Int, class... Ints>
+void testString2Integral() {
+  typedef typename make_unsigned<Int>::type Uint;
+  typedef typename make_signed<Int>::type Sint;
+
+  // Unsigned numbers small enough to fit in a signed type
+  static const String strings[] = {
+    "0",
+    "00",
+    "2 ",
+    " 84",
+    " \n 123    \t\n",
+    " 127",
+    "0000000000000000000000000042"
+  };
+  static const Uint values[] = {
+    0,
+    0,
+    2,
+    84,
+    123,
+    127,
+    42
+  };
+  FOR_EACH_RANGE (i, 0, sizeof(strings) / sizeof(*strings)) {
+    EXPECT_EQ(to<Uint>(strings[i]), values[i]);
+    EXPECT_EQ(to<Sint>(strings[i]), values[i]);
+  }
+
+  // Unsigned numbers that won't fit in the signed variation
+  static const String uStrings[] = {
+    " 128",
+    "213",
+    "255"
+  };
+  static const Uint uValues[] = {
+    128,
+    213,
+    255
+  };
+  FOR_EACH_RANGE (i, 0, sizeof(uStrings)/sizeof(*uStrings)) {
+    EXPECT_EQ(to<Uint>(uStrings[i]), uValues[i]);
+    if (sizeof(Int) == 1) {
+      EXPECT_THROW(to<Sint>(uStrings[i]), std::range_error);
+    }
+  }
+
+  if (sizeof(Int) >= 4) {
+    static const String strings2[] = {
+      "256",
+      "6324 ",
+      "63245675 ",
+      "2147483647"
+    };
+    static const Uint values2[] = {
+      (Uint)256,
+      (Uint)6324,
+      (Uint)63245675,
+      (Uint)2147483647
+    };
+    FOR_EACH_RANGE (i, 0, sizeof(strings2)/sizeof(*strings2)) {
+      EXPECT_EQ(to<Uint>(strings2[i]), values2[i]);
+      EXPECT_EQ(to<Sint>(strings2[i]), values2[i]);
+    }
+
+    static const String uStrings2[] = {
+      "2147483648",
+      "3147483648",
+      "4147483648",
+      "4000000000",
+    };
+    static const Uint uValues2[] = {
+      (Uint)2147483648U,
+      (Uint)3147483648U,
+      (Uint)4147483648U,
+      (Uint)4000000000U,
+    };
+    FOR_EACH_RANGE (i, 0, sizeof(uStrings2)/sizeof(uStrings2)) {
+      EXPECT_EQ(to<Uint>(uStrings2[i]), uValues2[i]);
+      if (sizeof(Int) == 4) {
+        EXPECT_THROW(to<Sint>(uStrings2[i]), std::range_error);
+      }
+    }
+  }
+
+  if (sizeof(Int) >= 8) {
+    static_assert(sizeof(Int) <= 8, "Now that would be interesting");
+    static const String strings3[] = {
+      "2147483648",
+      "5000000001",
+      "25687346509278435",
+      "100000000000000000",
+      "9223372036854775807",
+    };
+    static const Uint values3[] = {
+      (Uint)2147483648ULL,
+      (Uint)5000000001ULL,
+      (Uint)25687346509278435ULL,
+      (Uint)100000000000000000ULL,
+      (Uint)9223372036854775807ULL,
+    };
+    FOR_EACH_RANGE (i, 0, sizeof(strings3)/sizeof(*strings3)) {
+      EXPECT_EQ(to<Uint>(strings3[i]), values3[i]);
+      EXPECT_EQ(to<Sint>(strings3[i]), values3[i]);
+    }
+
+    static const String uStrings3[] = {
+      "9223372036854775808",
+      "9987435987394857987",
+      "17873648761234698740",
+      "18446744073709551615",
+    };
+    static const Uint uValues3[] = {
+      (Uint)9223372036854775808ULL,
+      (Uint)9987435987394857987ULL,
+      (Uint)17873648761234698740ULL,
+      (Uint)18446744073709551615ULL,
+    };
+    FOR_EACH_RANGE (i, 0, sizeof(uStrings3)/sizeof(*uStrings3)) {
+      EXPECT_EQ(to<Uint>(uStrings3[i]), uValues3[i]);
+      if (sizeof(Int) == 8) {
+        EXPECT_THROW(to<Sint>(uStrings3[i]), std::range_error);
+      }
+    }
+  }
+
+  // Minimum possible negative values, and negative sign overflow
+  static const String strings4[] = {
+    "-128",
+    "-32768",
+    "-2147483648",
+    "-9223372036854775808",
+  };
+  static const String strings5[] = {
+    "-129",
+    "-32769",
+    "-2147483649",
+    "-9223372036854775809",
+  };
+  static const Sint values4[] = {
+    (Sint)-128LL,
+    (Sint)-32768LL,
+    (Sint)-2147483648LL,
+    (Sint)(-9223372036854775807LL - 1),
+  };
+  FOR_EACH_RANGE (i, 0, sizeof(strings4)/sizeof(*strings4)) {
+    if (sizeof(Int) > std::pow(2, i)) {
+      EXPECT_EQ(values4[i], to<Sint>(strings4[i]));
+      EXPECT_EQ(values4[i] - 1, to<Sint>(strings5[i]));
+    } else if (sizeof(Int) == std::pow(2, i)) {
+      EXPECT_EQ(values4[i], to<Sint>(strings4[i]));
+      EXPECT_THROW(to<Sint>(strings5[i]), std::range_error);
+    } else {
+      EXPECT_THROW(to<Sint>(strings4[i]), std::range_error);
+      EXPECT_THROW(to<Sint>(strings5[i]), std::range_error);
+    }
+  }
+
+  // Bogus string values
+  static const String bogusStrings[] = {
+    "",
+    "0x1234",
+    "123L",
+    "123a",
+    "x 123 ",
+    "234 y",
+    "- 42",  // whitespace is not allowed between the sign and the value
+    " +   13 ",
+    "12345678901234567890123456789",
+  };
+  for (const auto& str : bogusStrings) {
+    EXPECT_THROW(to<Sint>(str), std::range_error);
+    EXPECT_THROW(to<Uint>(str), std::range_error);
+  }
+
+  // A leading '+' character is only allowed when converting to signed types.
+  String posSign("+42");
+  EXPECT_EQ(42, to<Sint>(posSign));
+  EXPECT_THROW(to<Uint>(posSign), std::range_error);
+
+  testString2Integral<String, Ints...>();
+}
+
+TEST(Conv, String2Integral) {
+  testString2Integral<const char*, signed char, short, int, long, long long>();
+  testString2Integral<std::string, signed char, short, int, long, long long>();
+  testString2Integral<fbstring, signed char, short, int, long, long long>();
+
+  // Testing the behavior of the StringPiece* API
+  // StringPiece* normally parses as much valid data as it can,
+  // and advances the StringPiece to the end of the valid data.
+  char buf1[] = "100foo";
+  StringPiece sp1(buf1);
+  EXPECT_EQ(100, to<uint8_t>(&sp1));
+  EXPECT_EQ(buf1 + 3, sp1.begin());
+  // However, if the next character would cause an overflow it throws a
+  // range_error rather than consuming only as much as it can without
+  // overflowing.
+  char buf2[] = "1002";
+  StringPiece sp2(buf2);
+  EXPECT_THROW(to<uint8_t>(&sp2), std::range_error);
+  EXPECT_EQ(buf2, sp2.begin());
+}
+
+TEST(Conv, StringPiece2Integral) {
+  string s = "  +123  hello world  ";
+  StringPiece sp = s;
+  EXPECT_EQ(to<int>(&sp), 123);
+  EXPECT_EQ(sp, "  hello world  ");
+}
+
+TEST(Conv, StringPieceAppend) {
+  string s = "foobar";
+  {
+    StringPiece sp(s, 0, 3);
+    string result = to<string>(s, sp);
+    EXPECT_EQ(result, "foobarfoo");
+  }
+  {
+    StringPiece sp1(s, 0, 3);
+    StringPiece sp2(s, 3, 3);
+    string result = to<string>(sp1, sp2);
+    EXPECT_EQ(result, s);
+  }
+}
+
+TEST(Conv, BadStringToIntegral) {
+  // Note that leading spaces (e.g.  " 1") are valid.
+  vector<string> v = { "a", "", " ", "\n", " a0", "abcdef", "1Z", "!#" };
+  for (auto& s: v) {
+    EXPECT_THROW(to<int>(s), std::range_error) << "s=" << s;
+  }
+}
+
+template <class String>
+void testVariadicTo() {
+  String s;
+  toAppend(&s);
+  toAppend("Lorem ipsum ", 1234, String(" dolor amet "), 567.89, '!', &s);
+  EXPECT_EQ(s, "Lorem ipsum 1234 dolor amet 567.89!");
+
+  s = to<String>();
+  EXPECT_TRUE(s.empty());
+
+  s = to<String>("Lorem ipsum ", nullptr, 1234, " dolor amet ", 567.89, '.');
+  EXPECT_EQ(s, "Lorem ipsum 1234 dolor amet 567.89.");
+}
+
+template <class String>
+void testVariadicToDelim() {
+  String s;
+  toAppendDelim(":", &s);
+  toAppendDelim(
+      ":", "Lorem ipsum ", 1234, String(" dolor amet "), 567.89, '!', &s);
+  EXPECT_EQ(s, "Lorem ipsum :1234: dolor amet :567.89:!");
+
+  s = toDelim<String>(':');
+  EXPECT_TRUE(s.empty());
+
+  s = toDelim<String>(
+      ":", "Lorem ipsum ", nullptr, 1234, " dolor amet ", 567.89, '.');
+  EXPECT_EQ(s, "Lorem ipsum ::1234: dolor amet :567.89:.");
+}
+
+TEST(Conv, NullString) {
+  string s1 = to<string>((char *) NULL);
+  EXPECT_TRUE(s1.empty());
+  fbstring s2 = to<fbstring>((char *) NULL);
+  EXPECT_TRUE(s2.empty());
+}
+
+TEST(Conv, VariadicTo) {
+  testVariadicTo<string>();
+  testVariadicTo<fbstring>();
+}
+
+TEST(Conv, VariadicToDelim) {
+  testVariadicToDelim<string>();
+  testVariadicToDelim<fbstring>();
+}
+
+template <class String>
+void testDoubleToString() {
+  EXPECT_EQ(to<string>(0.0), "0");
+  EXPECT_EQ(to<string>(0.5), "0.5");
+  EXPECT_EQ(to<string>(10.25), "10.25");
+  EXPECT_EQ(to<string>(1.123e10), "11230000000");
+}
+
+TEST(Conv, DoubleToString) {
+  testDoubleToString<string>();
+  testDoubleToString<fbstring>();
+}
+
+TEST(Conv, FBStringToString) {
+  fbstring foo("foo");
+  string ret = to<string>(foo);
+  EXPECT_EQ(ret, "foo");
+  string ret2 = to<string>(foo, 2);
+  EXPECT_EQ(ret2, "foo2");
+}
+
+TEST(Conv, StringPieceToDouble) {
+  string s = "2134123.125 zorro";
+  StringPiece pc(s);
+  EXPECT_EQ(to<double>(&pc), 2134123.125);
+  EXPECT_EQ(pc, " zorro");
+
+  EXPECT_THROW(to<double>(StringPiece(s)), std::range_error);
+  EXPECT_EQ(to<double>(StringPiece(s.data(), pc.data())), 2134123.125);
+
+// Test NaN conversion
+  try {
+    to<double>("not a number");
+    EXPECT_TRUE(false);
+  } catch (const std::range_error &) {
+  }
+
+  EXPECT_TRUE(std::isnan(to<double>("NaN")));
+  EXPECT_EQ(to<double>("inf"), numeric_limits<double>::infinity());
+  EXPECT_EQ(to<double>("infinity"), numeric_limits<double>::infinity());
+  EXPECT_THROW(to<double>("infinitX"), std::range_error);
+  EXPECT_EQ(to<double>("-inf"), -numeric_limits<double>::infinity());
+  EXPECT_EQ(to<double>("-infinity"), -numeric_limits<double>::infinity());
+  EXPECT_THROW(to<double>("-infinitX"), std::range_error);
+}
+
+TEST(Conv, EmptyStringToInt) {
+  string s = "";
+  StringPiece pc(s);
+
+  try {
+    to<int>(pc);
+    EXPECT_TRUE(false);
+  } catch (const std::range_error &) {
+  }
+}
+
+TEST(Conv, CorruptedStringToInt) {
+  string s = "-1";
+  StringPiece pc(s.data(), s.data() + 1); // Only  "-"
+
+  try {
+    to<int64_t>(&pc);
+    EXPECT_TRUE(false);
+  } catch (const std::range_error &) {
+  }
+}
+
+TEST(Conv, EmptyStringToDouble) {
+  string s = "";
+  StringPiece pc(s);
+
+  try {
+    to<double>(pc);
+    EXPECT_TRUE(false);
+  } catch (const std::range_error &) {
+  }
+}
+
+TEST(Conv, IntToDouble) {
+  auto d = to<double>(42);
+  EXPECT_EQ(d, 42);
+  /* This seems not work in ubuntu11.10, gcc 4.6.1
+  try {
+    auto f = to<float>(957837589847);
+    EXPECT_TRUE(false);
+  } catch (std::range_error& e) {
+    //LOG(INFO) << e.what();
+  }
+  */
+}
+
+TEST(Conv, DoubleToInt) {
+  auto i = to<int>(42.0);
+  EXPECT_EQ(i, 42);
+  try {
+    auto i = to<int>(42.1);
+    EXPECT_TRUE(false);
+  } catch (std::range_error& e) {
+    //LOG(INFO) << e.what();
+  }
+}
+
+TEST(Conv, EnumToInt) {
+  enum A { x = 42, y = 420, z = 65 };
+  auto i = to<int>(x);
+  EXPECT_EQ(i, 42);
+  auto j = to<char>(x);
+  EXPECT_EQ(j, 42);
+  try {
+    auto i = to<char>(y);
+    LOG(ERROR) << static_cast<unsigned int>(i);
+    EXPECT_TRUE(false);
+  } catch (std::range_error& e) {
+    //LOG(INFO) << e.what();
+  }
+}
+
+TEST(Conv, EnumToString) {
+  // task 813959
+  enum A { x = 4, y = 420, z = 65 };
+  EXPECT_EQ("foo.4", to<string>("foo.", x));
+  EXPECT_EQ("foo.420", to<string>("foo.", y));
+  EXPECT_EQ("foo.65", to<string>("foo.", z));
+}
+
+TEST(Conv, IntToEnum) {
+  enum A { x = 42, y = 420 };
+  auto i = to<A>(42);
+  EXPECT_EQ(i, x);
+  auto j = to<A>(100);
+  EXPECT_EQ(j, 100);
+  try {
+    auto i = to<A>(5000000000L);
+    EXPECT_TRUE(false);
+  } catch (std::range_error& e) {
+    //LOG(INFO) << e.what();
+  }
+}
+
+TEST(Conv, UnsignedEnum) {
+  enum E : uint32_t { x = 3000000000U };
+  auto u = to<uint32_t>(x);
+  EXPECT_EQ(u, 3000000000U);
+  auto s = to<string>(x);
+  EXPECT_EQ("3000000000", s);
+  auto e = to<E>(3000000000U);
+  EXPECT_EQ(e, x);
+  try {
+    auto i = to<int32_t>(x);
+    LOG(ERROR) << to<uint32_t>(x);
+    EXPECT_TRUE(false);
+  } catch (std::range_error& e) {
+  }
+}
+
+#if defined(__clang__) || __GNUC_PREREQ(4, 7)
+// to<enum class> and to(enum class) only supported in gcc 4.7 onwards
+
+TEST(Conv, UnsignedEnumClass) {
+  enum class E : uint32_t { x = 3000000000U };
+  auto u = to<uint32_t>(E::x);
+  EXPECT_GT(u, 0);
+  EXPECT_EQ(u, 3000000000U);
+  auto s = to<string>(E::x);
+  EXPECT_EQ("3000000000", s);
+  auto e = to<E>(3000000000U);
+  EXPECT_EQ(e, E::x);
+  try {
+    auto i = to<int32_t>(E::x);
+    LOG(ERROR) << to<uint32_t>(E::x);
+    EXPECT_TRUE(false);
+  } catch (std::range_error& e) {
+  }
+}
+
+// Multi-argument to<string> uses toAppend, a different code path than
+// to<string>(enum).
+TEST(Conv, EnumClassToString) {
+  enum class A { x = 4, y = 420, z = 65 };
+  EXPECT_EQ("foo.4", to<string>("foo.", A::x));
+  EXPECT_EQ("foo.420", to<string>("foo.", A::y));
+  EXPECT_EQ("foo.65", to<string>("foo.", A::z));
+}
+
+#endif // gcc 4.7 onwards
+
+template<typename Src>
+void testStr2Bool() {
+  EXPECT_FALSE(to<bool>(Src("0")));
+  EXPECT_FALSE(to<bool>(Src("  000  ")));
+
+  EXPECT_FALSE(to<bool>(Src("n")));
+  EXPECT_FALSE(to<bool>(Src("no")));
+  EXPECT_FALSE(to<bool>(Src("false")));
+  EXPECT_FALSE(to<bool>(Src("False")));
+  EXPECT_FALSE(to<bool>(Src("  fAlSe"  )));
+  EXPECT_FALSE(to<bool>(Src("F")));
+  EXPECT_FALSE(to<bool>(Src("off")));
+
+  EXPECT_TRUE(to<bool>(Src("1")));
+  EXPECT_TRUE(to<bool>(Src("  001 ")));
+  EXPECT_TRUE(to<bool>(Src("y")));
+  EXPECT_TRUE(to<bool>(Src("yes")));
+  EXPECT_TRUE(to<bool>(Src("\nyEs\t")));
+  EXPECT_TRUE(to<bool>(Src("true")));
+  EXPECT_TRUE(to<bool>(Src("True")));
+  EXPECT_TRUE(to<bool>(Src("T")));
+  EXPECT_TRUE(to<bool>(Src("on")));
+
+  EXPECT_THROW(to<bool>(Src("")), std::range_error);
+  EXPECT_THROW(to<bool>(Src("2")), std::range_error);
+  EXPECT_THROW(to<bool>(Src("11")), std::range_error);
+  EXPECT_THROW(to<bool>(Src("19")), std::range_error);
+  EXPECT_THROW(to<bool>(Src("o")), std::range_error);
+  EXPECT_THROW(to<bool>(Src("fal")), std::range_error);
+  EXPECT_THROW(to<bool>(Src("tru")), std::range_error);
+  EXPECT_THROW(to<bool>(Src("ye")), std::range_error);
+  EXPECT_THROW(to<bool>(Src("yes foo")), std::range_error);
+  EXPECT_THROW(to<bool>(Src("bar no")), std::range_error);
+  EXPECT_THROW(to<bool>(Src("one")), std::range_error);
+  EXPECT_THROW(to<bool>(Src("true_")), std::range_error);
+  EXPECT_THROW(to<bool>(Src("bogus_token_that_is_too_long")),
+               std::range_error);
+}
+
+TEST(Conv, StringToBool) {
+  // testStr2Bool<const char *>();
+  testStr2Bool<std::string>();
+
+  // Test with strings that are not NUL terminated.
+  const char buf[] = "01234";
+  EXPECT_FALSE(to<bool>(StringPiece(buf, buf + 1)));  // "0"
+  EXPECT_TRUE(to<bool>(StringPiece(buf + 1, buf + 2)));  // "1"
+  const char buf2[] = "one two three";
+  EXPECT_TRUE(to<bool>(StringPiece(buf2, buf2 + 2)));  // "on"
+  const char buf3[] = "false";
+  EXPECT_THROW(to<bool>(StringPiece(buf3, buf3 + 3)),  // "fal"
+               std::range_error);
+
+  // Test the StringPiece* API
+  const char buf4[] = "001foo";
+  StringPiece sp4(buf4);
+  EXPECT_TRUE(to<bool>(&sp4));
+  EXPECT_EQ(buf4 + 3, sp4.begin());
+  const char buf5[] = "0012";
+  StringPiece sp5(buf5);
+  EXPECT_THROW(to<bool>(&sp5), std::range_error);
+  EXPECT_EQ(buf5, sp5.begin());
+}
+
+TEST(Conv, NewUint64ToString) {
+  char buf[21];
+
+#define THE_GREAT_EXPECTATIONS(n, len)                  \
+  do {                                                  \
+    EXPECT_EQ((len), uint64ToBufferUnsafe((n), buf));   \
+    buf[(len)] = 0;                                     \
+    auto s = string(#n);                                \
+    s = s.substr(0, s.size() - 2);                      \
+    EXPECT_EQ(s, buf);                                  \
+  } while (0)
+
+  THE_GREAT_EXPECTATIONS(0UL, 1);
+  THE_GREAT_EXPECTATIONS(1UL, 1);
+  THE_GREAT_EXPECTATIONS(12UL, 2);
+  THE_GREAT_EXPECTATIONS(123UL, 3);
+  THE_GREAT_EXPECTATIONS(1234UL, 4);
+  THE_GREAT_EXPECTATIONS(12345UL, 5);
+  THE_GREAT_EXPECTATIONS(123456UL, 6);
+  THE_GREAT_EXPECTATIONS(1234567UL, 7);
+  THE_GREAT_EXPECTATIONS(12345678UL, 8);
+  THE_GREAT_EXPECTATIONS(123456789UL, 9);
+  THE_GREAT_EXPECTATIONS(1234567890UL, 10);
+  THE_GREAT_EXPECTATIONS(12345678901UL, 11);
+  THE_GREAT_EXPECTATIONS(123456789012UL, 12);
+  THE_GREAT_EXPECTATIONS(1234567890123UL, 13);
+  THE_GREAT_EXPECTATIONS(12345678901234UL, 14);
+  THE_GREAT_EXPECTATIONS(123456789012345UL, 15);
+  THE_GREAT_EXPECTATIONS(1234567890123456UL, 16);
+  THE_GREAT_EXPECTATIONS(12345678901234567UL, 17);
+  THE_GREAT_EXPECTATIONS(123456789012345678UL, 18);
+  THE_GREAT_EXPECTATIONS(1234567890123456789UL, 19);
+  THE_GREAT_EXPECTATIONS(18446744073709551614UL, 20);
+  THE_GREAT_EXPECTATIONS(18446744073709551615UL, 20);
+
+#undef THE_GREAT_EXPECTATIONS
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Benchmarks for ASCII to int conversion
+////////////////////////////////////////////////////////////////////////////////
+// @author: Rajat Goel (rajat)
+
+static int64_t handwrittenAtoi(const char* start, const char* end) {
+
+  bool positive = true;
+  int64_t retVal = 0;
+
+  if (start == end) {
+    throw std::runtime_error("empty string");
+  }
+
+  while (start < end && isspace(*start)) {
+    ++start;
+  }
+
+  switch (*start) {
+    case '-':
+      positive = false;
+    case '+':
+      ++start;
+    default:;
+  }
+
+  while (start < end && *start >= '0' && *start <= '9') {
+    auto const newRetVal = retVal * 10 + (*start++ - '0');
+    if (newRetVal < retVal) {
+      throw std::runtime_error("overflow");
+    }
+    retVal = newRetVal;
+  }
+
+  if (start != end) {
+    throw std::runtime_error("extra chars at the end");
+  }
+
+  return positive ? retVal : -retVal;
+}
+
+static StringPiece pc1 = "1234567890123456789";
+
+void handwrittenAtoiMeasure(uint n, uint digits) {
+  auto p = pc1.subpiece(pc1.size() - digits, digits);
+  FOR_EACH_RANGE (i, 0, n) {
+    doNotOptimizeAway(handwrittenAtoi(p.begin(), p.end()));
+  }
+}
+
+void follyAtoiMeasure(uint n, uint digits) {
+  auto p = pc1.subpiece(pc1.size() - digits, digits);
+  FOR_EACH_RANGE (i, 0, n) {
+    doNotOptimizeAway(folly::to<int64_t>(p.begin(), p.end()));
+  }
+}
+
+void clibAtoiMeasure(uint n, uint digits) {
+  auto p = pc1.subpiece(pc1.size() - digits, digits);
+  assert(*p.end() == 0);
+  static_assert(sizeof(long) == 8, "64-bit long assumed");
+  FOR_EACH_RANGE (i, 0, n) {
+    doNotOptimizeAway(atol(p.begin()));
+  }
+}
+
+void clibStrtoulMeasure(uint n, uint digits) {
+  auto p = pc1.subpiece(pc1.size() - digits, digits);
+  assert(*p.end() == 0);
+  char * endptr;
+  FOR_EACH_RANGE (i, 0, n) {
+    doNotOptimizeAway(strtoul(p.begin(), &endptr, 10));
+  }
+}
+
+void lexicalCastMeasure(uint n, uint digits) {
+  auto p = pc1.subpiece(pc1.size() - digits, digits);
+  assert(*p.end() == 0);
+  FOR_EACH_RANGE (i, 0, n) {
+    doNotOptimizeAway(boost::lexical_cast<uint64_t>(p.begin()));
+  }
+}
+
+// Benchmarks for unsigned to string conversion, raw
+
+unsigned u64ToAsciiTable(uint64_t value, char* dst) {
+  static const char digits[201] =
+    "00010203040506070809"
+    "10111213141516171819"
+    "20212223242526272829"
+    "30313233343536373839"
+    "40414243444546474849"
+    "50515253545556575859"
+    "60616263646566676869"
+    "70717273747576777879"
+    "80818283848586878889"
+    "90919293949596979899";
+
+  uint32_t const length = digits10(value);
+  uint32_t next = length - 1;
+  while (value >= 100) {
+    auto const i = (value % 100) * 2;
+    value /= 100;
+    dst[next] = digits[i + 1];
+    dst[next - 1] = digits[i];
+    next -= 2;
+  }
+  // Handle last 1-2 digits
+  if (value < 10) {
+    dst[next] = '0' + uint32_t(value);
+  } else {
+    auto i = uint32_t(value) * 2;
+    dst[next] = digits[i + 1];
+    dst[next - 1] = digits[i];
+  }
+  return length;
+}
+
+void u64ToAsciiTableBM(uint n, uint64_t value) {
+  // This is too fast, need to do 10 times per iteration
+  char buf[20];
+  FOR_EACH_RANGE (i, 0, n) {
+    doNotOptimizeAway(u64ToAsciiTable(value + n, buf));
+  }
+}
+
+unsigned u64ToAsciiClassic(uint64_t value, char* dst) {
+  // Write backwards.
+  char* next = (char*)dst;
+  char* start = next;
+  do {
+    *next++ = '0' + (value % 10);
+    value /= 10;
+  } while (value != 0);
+  unsigned length = next - start;
+
+  // Reverse in-place.
+  next--;
+  while (next > start) {
+    char swap = *next;
+    *next = *start;
+    *start = swap;
+    next--;
+    start++;
+  }
+  return length;
+}
+
+void u64ToAsciiClassicBM(uint n, uint64_t value) {
+  // This is too fast, need to do 10 times per iteration
+  char buf[20];
+  FOR_EACH_RANGE (i, 0, n) {
+    doNotOptimizeAway(u64ToAsciiClassic(value + n, buf));
+  }
+}
+
+void u64ToAsciiFollyBM(uint n, uint64_t value) {
+  // This is too fast, need to do 10 times per iteration
+  char buf[20];
+  FOR_EACH_RANGE (i, 0, n) {
+    doNotOptimizeAway(uint64ToBufferUnsafe(value + n, buf));
+  }
+}
+
+// Benchmark uitoa with string append
+
+void u2aAppendClassicBM(uint n, uint64_t value) {
+  string s;
+  FOR_EACH_RANGE (i, 0, n) {
+    // auto buf = &s.back() + 1;
+    char buffer[20];
+    s.append(buffer, u64ToAsciiClassic(value, buffer));
+    doNotOptimizeAway(s.size());
+  }
+}
+
+void u2aAppendFollyBM(uint n, uint64_t value) {
+  string s;
+  FOR_EACH_RANGE (i, 0, n) {
+    // auto buf = &s.back() + 1;
+    char buffer[20];
+    s.append(buffer, uint64ToBufferUnsafe(value, buffer));
+    doNotOptimizeAway(s.size());
+  }
+}
+
+#define DEFINE_BENCHMARK_GROUP(n)                       \
+  BENCHMARK_PARAM(u64ToAsciiClassicBM, n);              \
+  BENCHMARK_RELATIVE_PARAM(u64ToAsciiTableBM, n);       \
+  BENCHMARK_RELATIVE_PARAM(u64ToAsciiFollyBM, n);       \
+  BENCHMARK_DRAW_LINE();
+
+DEFINE_BENCHMARK_GROUP(1);
+DEFINE_BENCHMARK_GROUP(12);
+DEFINE_BENCHMARK_GROUP(123);
+DEFINE_BENCHMARK_GROUP(1234);
+DEFINE_BENCHMARK_GROUP(12345);
+DEFINE_BENCHMARK_GROUP(123456);
+DEFINE_BENCHMARK_GROUP(1234567);
+DEFINE_BENCHMARK_GROUP(12345678);
+DEFINE_BENCHMARK_GROUP(123456789);
+DEFINE_BENCHMARK_GROUP(1234567890);
+DEFINE_BENCHMARK_GROUP(12345678901);
+DEFINE_BENCHMARK_GROUP(123456789012);
+DEFINE_BENCHMARK_GROUP(1234567890123);
+DEFINE_BENCHMARK_GROUP(12345678901234);
+DEFINE_BENCHMARK_GROUP(123456789012345);
+DEFINE_BENCHMARK_GROUP(1234567890123456);
+DEFINE_BENCHMARK_GROUP(12345678901234567);
+DEFINE_BENCHMARK_GROUP(123456789012345678);
+DEFINE_BENCHMARK_GROUP(1234567890123456789);
+DEFINE_BENCHMARK_GROUP(12345678901234567890U);
+
+#undef DEFINE_BENCHMARK_GROUP
+
+#define DEFINE_BENCHMARK_GROUP(n)                       \
+  BENCHMARK_PARAM(clibAtoiMeasure, n);                  \
+  BENCHMARK_RELATIVE_PARAM(lexicalCastMeasure, n);      \
+  BENCHMARK_RELATIVE_PARAM(handwrittenAtoiMeasure, n);  \
+  BENCHMARK_RELATIVE_PARAM(follyAtoiMeasure, n);        \
+  BENCHMARK_DRAW_LINE();
+
+DEFINE_BENCHMARK_GROUP(1);
+DEFINE_BENCHMARK_GROUP(2);
+DEFINE_BENCHMARK_GROUP(3);
+DEFINE_BENCHMARK_GROUP(4);
+DEFINE_BENCHMARK_GROUP(5);
+DEFINE_BENCHMARK_GROUP(6);
+DEFINE_BENCHMARK_GROUP(7);
+DEFINE_BENCHMARK_GROUP(8);
+DEFINE_BENCHMARK_GROUP(9);
+DEFINE_BENCHMARK_GROUP(10);
+DEFINE_BENCHMARK_GROUP(11);
+DEFINE_BENCHMARK_GROUP(12);
+DEFINE_BENCHMARK_GROUP(13);
+DEFINE_BENCHMARK_GROUP(14);
+DEFINE_BENCHMARK_GROUP(15);
+DEFINE_BENCHMARK_GROUP(16);
+DEFINE_BENCHMARK_GROUP(17);
+DEFINE_BENCHMARK_GROUP(18);
+DEFINE_BENCHMARK_GROUP(19);
+
+#undef DEFINE_BENCHMARK_GROUP
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  auto ret = RUN_ALL_TESTS();
+  if (!ret && FLAGS_benchmark) {
+    folly::runBenchmarks();
+  }
+  return ret;
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/CpuIdTest.cpp
@@ -0,0 +1,29 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/CpuId.h"
+
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+using namespace folly;
+
+TEST(CpuId, Simple) {
+  // All CPUs should support MMX
+  CpuId id;
+  EXPECT_TRUE(id.mmx());
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/DeterministicSchedule.cpp
@@ -0,0 +1,361 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "DeterministicSchedule.h"
+#include <algorithm>
+#include <list>
+#include <mutex>
+#include <random>
+#include <utility>
+#include <unordered_map>
+#include <assert.h>
+
+namespace folly { namespace test {
+
+__thread sem_t* DeterministicSchedule::tls_sem;
+__thread DeterministicSchedule* DeterministicSchedule::tls_sched;
+
+// access is protected by futexLock
+static std::unordered_map<detail::Futex<DeterministicAtomic>*,
+                          std::list<std::pair<uint32_t,bool*>>> futexQueues;
+
+static std::mutex futexLock;
+
+DeterministicSchedule::DeterministicSchedule(
+        const std::function<int(int)>& scheduler)
+  : scheduler_(scheduler)
+{
+  assert(tls_sem == nullptr);
+  assert(tls_sched == nullptr);
+
+  tls_sem = new sem_t;
+  sem_init(tls_sem, 0, 1);
+  sems_.push_back(tls_sem);
+
+  tls_sched = this;
+}
+
+DeterministicSchedule::~DeterministicSchedule() {
+  assert(tls_sched == this);
+  assert(sems_.size() == 1);
+  assert(sems_[0] == tls_sem);
+  beforeThreadExit();
+}
+
+std::function<int(int)>
+DeterministicSchedule::uniform(long seed) {
+  auto rand = std::make_shared<std::ranlux48>(seed);
+  return [rand](int numActive) {
+    auto dist = std::uniform_int_distribution<int>(0, numActive - 1);
+    return dist(*rand);
+  };
+}
+
+struct UniformSubset {
+  UniformSubset(long seed, int subsetSize, int stepsBetweenSelect)
+    : uniform_(DeterministicSchedule::uniform(seed))
+    , subsetSize_(subsetSize)
+    , stepsBetweenSelect_(stepsBetweenSelect)
+    , stepsLeft_(0)
+  {
+  }
+
+  int operator()(int numActive) {
+    adjustPermSize(numActive);
+    if (stepsLeft_-- == 0) {
+      stepsLeft_ = stepsBetweenSelect_ - 1;
+      shufflePrefix();
+    }
+    return perm_[uniform_(std::min(numActive, subsetSize_))];
+  }
+
+ private:
+  std::function<int(int)> uniform_;
+  const int subsetSize_;
+  const int stepsBetweenSelect_;
+
+  int stepsLeft_;
+  // only the first subsetSize_ is properly randomized
+  std::vector<int> perm_;
+
+  void adjustPermSize(int numActive) {
+    if (perm_.size() > numActive) {
+      perm_.erase(std::remove_if(perm_.begin(), perm_.end(),
+              [=](int x){ return x >= numActive; }), perm_.end());
+    } else {
+      while (perm_.size() < numActive) {
+        perm_.push_back(perm_.size());
+      }
+    }
+    assert(perm_.size() == numActive);
+  }
+
+  void shufflePrefix() {
+    for (int i = 0; i < std::min(int(perm_.size() - 1), subsetSize_); ++i) {
+      int j = uniform_(perm_.size() - i) + i;
+      std::swap(perm_[i], perm_[j]);
+    }
+  }
+};
+
+std::function<int(int)>
+DeterministicSchedule::uniformSubset(long seed, int n, int m) {
+  auto gen = std::make_shared<UniformSubset>(seed, n, m);
+  return [=](int numActive) { return (*gen)(numActive); };
+}
+
+void
+DeterministicSchedule::beforeSharedAccess() {
+  if (tls_sem) {
+    sem_wait(tls_sem);
+  }
+}
+
+void
+DeterministicSchedule::afterSharedAccess() {
+  auto sched = tls_sched;
+  if (!sched) {
+    return;
+  }
+
+  sem_post(sched->sems_[sched->scheduler_(sched->sems_.size())]);
+}
+
+int
+DeterministicSchedule::getRandNumber(int n) {
+  if (tls_sched) {
+    return tls_sched->scheduler_(n);
+  }
+  return std::rand() % n;
+}
+
+sem_t*
+DeterministicSchedule::beforeThreadCreate() {
+  sem_t* s = new sem_t;
+  sem_init(s, 0, 0);
+  beforeSharedAccess();
+  sems_.push_back(s);
+  afterSharedAccess();
+  return s;
+}
+
+void
+DeterministicSchedule::afterThreadCreate(sem_t* sem) {
+  assert(tls_sem == nullptr);
+  assert(tls_sched == nullptr);
+  tls_sem = sem;
+  tls_sched = this;
+  bool started = false;
+  while (!started) {
+    beforeSharedAccess();
+    if (active_.count(std::this_thread::get_id()) == 1) {
+      started = true;
+    }
+    afterSharedAccess();
+  }
+}
+
+void
+DeterministicSchedule::beforeThreadExit() {
+  assert(tls_sched == this);
+  beforeSharedAccess();
+  sems_.erase(std::find(sems_.begin(), sems_.end(), tls_sem));
+  active_.erase(std::this_thread::get_id());
+  if (sems_.size() > 0) {
+    afterSharedAccess();
+  }
+  sem_destroy(tls_sem);
+  delete tls_sem;
+  tls_sem = nullptr;
+  tls_sched = nullptr;
+}
+
+void
+DeterministicSchedule::join(std::thread& child) {
+  auto sched = tls_sched;
+  if (sched) {
+    bool done = false;
+    while (!done) {
+      beforeSharedAccess();
+      done = !sched->active_.count(child.get_id());
+      afterSharedAccess();
+    }
+  }
+  child.join();
+}
+
+void
+DeterministicSchedule::post(sem_t* sem) {
+  beforeSharedAccess();
+  sem_post(sem);
+  afterSharedAccess();
+}
+
+bool
+DeterministicSchedule::tryWait(sem_t* sem) {
+  beforeSharedAccess();
+  int rv = sem_trywait(sem);
+  afterSharedAccess();
+  if (rv == 0) {
+    return true;
+  } else {
+    assert(errno == EAGAIN);
+    return false;
+  }
+}
+
+void
+DeterministicSchedule::wait(sem_t* sem) {
+  while (!tryWait(sem)) {
+    // we're not busy waiting because this is a deterministic schedule
+  }
+}
+
+}}
+
+namespace folly { namespace detail {
+
+using namespace test;
+
+template<>
+bool Futex<DeterministicAtomic>::futexWait(uint32_t expected,
+                                           uint32_t waitMask) {
+  bool rv;
+  DeterministicSchedule::beforeSharedAccess();
+  futexLock.lock();
+  if (data != expected) {
+    rv = false;
+  } else {
+    auto& queue = futexQueues[this];
+    bool done = false;
+    queue.push_back(std::make_pair(waitMask, &done));
+    while (!done) {
+      futexLock.unlock();
+      DeterministicSchedule::afterSharedAccess();
+      DeterministicSchedule::beforeSharedAccess();
+      futexLock.lock();
+    }
+    rv = true;
+  }
+  futexLock.unlock();
+  DeterministicSchedule::afterSharedAccess();
+  return rv;
+}
+
+FutexResult futexWaitUntilImpl(Futex<DeterministicAtomic>* futex,
+                               uint32_t expected, uint32_t waitMask) {
+  if (futex == nullptr) {
+    return FutexResult::VALUE_CHANGED;
+  }
+
+  bool rv = false;
+  int futexErrno = 0;
+
+  DeterministicSchedule::beforeSharedAccess();
+  futexLock.lock();
+  if (futex->data == expected) {
+    auto& queue = futexQueues[futex];
+    queue.push_back(std::make_pair(waitMask, &rv));
+    auto ours = queue.end();
+    ours--;
+    while (!rv) {
+      futexLock.unlock();
+      DeterministicSchedule::afterSharedAccess();
+      DeterministicSchedule::beforeSharedAccess();
+      futexLock.lock();
+
+      // Simulate spurious wake-ups, timeouts each time with
+      // a 10% probability
+      if (DeterministicSchedule::getRandNumber(100) < 10) {
+        queue.erase(ours);
+        if (queue.empty()) {
+          futexQueues.erase(futex);
+        }
+        rv = false;
+        // Simulate ETIMEDOUT 90% of the time and other failures
+        // remaining time
+        futexErrno =
+          DeterministicSchedule::getRandNumber(100) >= 10 ? ETIMEDOUT : EINTR;
+        break;
+      }
+    }
+  }
+  futexLock.unlock();
+  DeterministicSchedule::afterSharedAccess();
+  return futexErrnoToFutexResult(rv ? 0 : -1, futexErrno);
+}
+
+template<>
+int Futex<DeterministicAtomic>::futexWake(int count, uint32_t wakeMask) {
+  int rv = 0;
+  DeterministicSchedule::beforeSharedAccess();
+  futexLock.lock();
+  if (futexQueues.count(this) > 0) {
+    auto& queue = futexQueues[this];
+    auto iter = queue.begin();
+    while (iter != queue.end() && rv < count) {
+      auto cur = iter++;
+      if ((cur->first & wakeMask) != 0) {
+        *(cur->second) = true;
+        rv++;
+        queue.erase(cur);
+      }
+    }
+    if (queue.empty()) {
+      futexQueues.erase(this);
+    }
+  }
+  futexLock.unlock();
+  DeterministicSchedule::afterSharedAccess();
+  return rv;
+}
+
+
+template<>
+CacheLocality const& CacheLocality::system<test::DeterministicAtomic>() {
+  static CacheLocality cache(CacheLocality::uniform(16));
+  return cache;
+}
+
+template<>
+test::DeterministicAtomic<size_t>
+    SequentialThreadId<test::DeterministicAtomic>::prevId(0);
+
+template<>
+__thread size_t SequentialThreadId<test::DeterministicAtomic>::currentId(0);
+
+template<>
+const AccessSpreader<test::DeterministicAtomic>
+AccessSpreader<test::DeterministicAtomic>::stripeByCore(
+    CacheLocality::system<>().numCachesByLevel.front());
+
+template<>
+const AccessSpreader<test::DeterministicAtomic>
+AccessSpreader<test::DeterministicAtomic>::stripeByChip(
+    CacheLocality::system<>().numCachesByLevel.back());
+
+template<>
+AccessSpreaderArray<test::DeterministicAtomic,128>
+AccessSpreaderArray<test::DeterministicAtomic,128>::sharedInstance = {};
+
+
+template<>
+Getcpu::Func
+AccessSpreader<test::DeterministicAtomic>::pickGetcpuFunc(size_t numStripes) {
+  return &SequentialThreadId<test::DeterministicAtomic>::getcpu;
+}
+
+}}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/DeterministicSchedule.h
@@ -0,0 +1,298 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <atomic>
+#include <functional>
+#include <thread>
+#include <unordered_set>
+#include <vector>
+#include <boost/noncopyable.hpp>
+#include <semaphore.h>
+#include <errno.h>
+#include <assert.h>
+
+#include <folly/ScopeGuard.h>
+#include <folly/detail/CacheLocality.h>
+#include <folly/detail/Futex.h>
+
+namespace folly { namespace test {
+
+/**
+ * DeterministicSchedule coordinates the inter-thread communication of a
+ * set of threads under test, so that despite concurrency the execution is
+ * the same every time.  It works by stashing a reference to the schedule
+ * in a thread-local variable, then blocking all but one thread at a time.
+ *
+ * In order for DeterministicSchedule to work, it needs to intercept
+ * all inter-thread communication.  To do this you should use
+ * DeterministicAtomic<T> instead of std::atomic<T>, create threads
+ * using DeterministicSchedule::thread() instead of the std::thread
+ * constructor, DeterministicSchedule::join(thr) instead of thr.join(),
+ * and access semaphores via the helper functions in DeterministicSchedule.
+ * Locks are not yet supported, although they would be easy to add with
+ * the same strategy as the mapping of sem_wait.
+ *
+ * The actual schedule is defined by a function from n -> [0,n). At
+ * each step, the function will be given the number of active threads
+ * (n), and it returns the index of the thread that should be run next.
+ * Invocations of the scheduler function will be serialized, but will
+ * occur from multiple threads.  A good starting schedule is uniform(0).
+ */
+class DeterministicSchedule : boost::noncopyable {
+ public:
+  /**
+   * Arranges for the current thread (and all threads created by
+   * DeterministicSchedule::thread on a thread participating in this
+   * schedule) to participate in a deterministic schedule.
+   */
+  explicit DeterministicSchedule(const std::function<int(int)>& scheduler);
+
+  /** Completes the schedule. */
+  ~DeterministicSchedule();
+
+  /**
+   * Returns a scheduling function that randomly chooses one of the
+   * runnable threads at each step, with no history.  This implements
+   * a schedule that is equivalent to one in which the steps between
+   * inter-thread communication are random variables following a poisson
+   * distribution.
+   */
+  static std::function<int(int)> uniform(long seed);
+
+  /**
+   * Returns a scheduling function that chooses a subset of the active
+   * threads and randomly chooses a member of the subset as the next
+   * runnable thread.  The subset is chosen with size n, and the choice
+   * is made every m steps.
+   */
+  static std::function<int(int)> uniformSubset(long seed, int n = 2,
+                                               int m = 64);
+
+  /** Obtains permission for the current thread to perform inter-thread
+   *  communication. */
+  static void beforeSharedAccess();
+
+  /** Releases permission for the current thread to perform inter-thread
+   *  communication. */
+  static void afterSharedAccess();
+
+  /** Launches a thread that will participate in the same deterministic
+   *  schedule as the current thread. */
+  template <typename Func, typename... Args>
+  static inline std::thread thread(Func&& func, Args&&... args) {
+    // TODO: maybe future versions of gcc will allow forwarding to thread
+    auto sched = tls_sched;
+    auto sem = sched ? sched->beforeThreadCreate() : nullptr;
+    auto child = std::thread([=](Args... a) {
+      if (sched) sched->afterThreadCreate(sem);
+      SCOPE_EXIT { if (sched) sched->beforeThreadExit(); };
+      func(a...);
+    }, args...);
+    if (sched) {
+      beforeSharedAccess();
+      sched->active_.insert(child.get_id());
+      afterSharedAccess();
+    }
+    return child;
+  }
+
+  /** Calls child.join() as part of a deterministic schedule. */
+  static void join(std::thread& child);
+
+  /** Calls sem_post(sem) as part of a deterministic schedule. */
+  static void post(sem_t* sem);
+
+  /** Calls sem_trywait(sem) as part of a deterministic schedule, returning
+   *  true on success and false on transient failure. */
+  static bool tryWait(sem_t* sem);
+
+  /** Calls sem_wait(sem) as part of a deterministic schedule. */
+  static void wait(sem_t* sem);
+
+  /** Used scheduler_ to get a random number b/w [0, n). If tls_sched is
+   *  not set-up it falls back to std::rand() */
+  static int getRandNumber(int n);
+
+ private:
+  static __thread sem_t* tls_sem;
+  static __thread DeterministicSchedule* tls_sched;
+
+  std::function<int(int)> scheduler_;
+  std::vector<sem_t*> sems_;
+  std::unordered_set<std::thread::id> active_;
+
+  sem_t* beforeThreadCreate();
+  void afterThreadCreate(sem_t*);
+  void beforeThreadExit();
+};
+
+
+/**
+ * DeterministicAtomic<T> is a drop-in replacement std::atomic<T> that
+ * cooperates with DeterministicSchedule.
+ */
+template <typename T>
+struct DeterministicAtomic {
+  std::atomic<T> data;
+
+  DeterministicAtomic() = default;
+  ~DeterministicAtomic() = default;
+  DeterministicAtomic(DeterministicAtomic<T> const &) = delete;
+  DeterministicAtomic<T>& operator= (DeterministicAtomic<T> const &) = delete;
+
+  constexpr /* implicit */ DeterministicAtomic(T v) noexcept : data(v) {}
+
+  bool is_lock_free() const noexcept {
+    return data.is_lock_free();
+  }
+
+  bool compare_exchange_strong(
+          T& v0, T v1,
+          std::memory_order mo = std::memory_order_seq_cst) noexcept {
+    DeterministicSchedule::beforeSharedAccess();
+    bool rv = data.compare_exchange_strong(v0, v1, mo);
+    DeterministicSchedule::afterSharedAccess();
+    return rv;
+  }
+
+  bool compare_exchange_weak(
+          T& v0, T v1,
+          std::memory_order mo = std::memory_order_seq_cst) noexcept {
+    DeterministicSchedule::beforeSharedAccess();
+    bool rv = data.compare_exchange_weak(v0, v1, mo);
+    DeterministicSchedule::afterSharedAccess();
+    return rv;
+  }
+
+  T exchange(T v, std::memory_order mo = std::memory_order_seq_cst) noexcept {
+    DeterministicSchedule::beforeSharedAccess();
+    T rv = data.exchange(v, mo);
+    DeterministicSchedule::afterSharedAccess();
+    return rv;
+  }
+
+  /* implicit */ operator T () const noexcept {
+    DeterministicSchedule::beforeSharedAccess();
+    T rv = data;
+    DeterministicSchedule::afterSharedAccess();
+    return rv;
+  }
+
+  T load(std::memory_order mo = std::memory_order_seq_cst) const noexcept {
+    DeterministicSchedule::beforeSharedAccess();
+    T rv = data.load(mo);
+    DeterministicSchedule::afterSharedAccess();
+    return rv;
+  }
+
+  T operator= (T v) noexcept {
+    DeterministicSchedule::beforeSharedAccess();
+    T rv = (data = v);
+    DeterministicSchedule::afterSharedAccess();
+    return rv;
+  }
+
+  void store(T v, std::memory_order mo = std::memory_order_seq_cst) noexcept {
+    DeterministicSchedule::beforeSharedAccess();
+    data.store(v, mo);
+    DeterministicSchedule::afterSharedAccess();
+  }
+
+  T operator++ () noexcept {
+    DeterministicSchedule::beforeSharedAccess();
+    T rv = ++data;
+    DeterministicSchedule::afterSharedAccess();
+    return rv;
+  }
+
+  T operator++ (int postDummy) noexcept {
+    DeterministicSchedule::beforeSharedAccess();
+    T rv = data++;
+    DeterministicSchedule::afterSharedAccess();
+    return rv;
+  }
+
+  T operator-- () noexcept {
+    DeterministicSchedule::beforeSharedAccess();
+    T rv = --data;
+    DeterministicSchedule::afterSharedAccess();
+    return rv;
+  }
+
+  T operator-- (int postDummy) noexcept {
+    DeterministicSchedule::beforeSharedAccess();
+    T rv = data--;
+    DeterministicSchedule::afterSharedAccess();
+    return rv;
+  }
+
+  T operator+= (T v) noexcept {
+    DeterministicSchedule::beforeSharedAccess();
+    T rv = (data += v);
+    DeterministicSchedule::afterSharedAccess();
+    return rv;
+  }
+
+  T operator-= (T v) noexcept {
+    DeterministicSchedule::beforeSharedAccess();
+    T rv = (data -= v);
+    DeterministicSchedule::afterSharedAccess();
+    return rv;
+  }
+
+  T operator&= (T v) noexcept {
+    DeterministicSchedule::beforeSharedAccess();
+    T rv = (data &= v);
+    DeterministicSchedule::afterSharedAccess();
+    return rv;
+  }
+
+  T operator|= (T v) noexcept {
+    DeterministicSchedule::beforeSharedAccess();
+    T rv = (data |= v);
+    DeterministicSchedule::afterSharedAccess();
+    return rv;
+  }
+};
+
+}}
+
+namespace folly { namespace detail {
+
+template<>
+bool Futex<test::DeterministicAtomic>::futexWait(uint32_t expected,
+                                                 uint32_t waitMask);
+
+/// This function ignores the time bound, and instead pseudo-randomly chooses
+/// whether the timeout was reached. To do otherwise would not be deterministic.
+FutexResult futexWaitUntilImpl(Futex<test::DeterministicAtomic> *futex,
+                               uint32_t expected, uint32_t waitMask);
+
+template<> template<class Clock, class Duration>
+FutexResult
+Futex<test::DeterministicAtomic>::futexWaitUntil(
+          uint32_t expected,
+          const time_point<Clock, Duration>& absTimeUnused,
+          uint32_t waitMask) {
+  return futexWaitUntilImpl(this, expected, waitMask);
+}
+
+template<>
+int Futex<test::DeterministicAtomic>::futexWake(int count, uint32_t wakeMask);
+
+}}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/DeterministicScheduleTest.cpp
@@ -0,0 +1,58 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "DeterministicSchedule.h"
+
+#include <gflags/gflags.h>
+#include <gtest/gtest.h>
+
+using namespace folly::test;
+
+TEST(DeterministicSchedule, uniform) {
+  auto p = DeterministicSchedule::uniform(0);
+  int buckets[10] = {};
+  for (int i = 0; i < 100000; ++i) {
+    buckets[p(10)]++;
+  }
+  for (int i = 0; i < 10; ++i) {
+    EXPECT_TRUE(buckets[i] > 9000);
+  }
+}
+
+TEST(DeterministicSchedule, uniformSubset) {
+  auto ps = DeterministicSchedule::uniformSubset(0, 3, 100);
+  int buckets[10] = {};
+  std::set<int> seen;
+  for (int i = 0; i < 100000; ++i) {
+    if (i > 0 && (i % 100) == 0) {
+      EXPECT_EQ(seen.size(), 3);
+      seen.clear();
+    }
+    int x = ps(10);
+    seen.insert(x);
+    EXPECT_TRUE(seen.size() <= 3);
+    buckets[x]++;
+  }
+  for (int i = 0; i < 10; ++i) {
+    EXPECT_TRUE(buckets[i] > 9000);
+  }
+}
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  return RUN_ALL_TESTS();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/DiscriminatedPtrTest.cpp
@@ -0,0 +1,125 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/DiscriminatedPtr.h"
+
+#include <gtest/gtest.h>
+
+using namespace folly;
+
+TEST(DiscriminatedPtr, Basic) {
+  struct Foo { };
+  struct Bar { };
+  typedef DiscriminatedPtr<void, int, Foo, Bar> Ptr;
+
+  int a = 10;
+  Ptr p;
+  EXPECT_TRUE(p.empty());
+  EXPECT_FALSE(p.hasType<void>());
+  EXPECT_FALSE(p.hasType<int>());
+  EXPECT_FALSE(p.hasType<Foo>());
+  EXPECT_FALSE(p.hasType<Bar>());
+
+  p.set(&a);
+  EXPECT_FALSE(p.empty());
+  EXPECT_FALSE(p.hasType<void>());
+  EXPECT_TRUE(p.hasType<int>());
+  EXPECT_FALSE(p.hasType<Foo>());
+  EXPECT_FALSE(p.hasType<Bar>());
+
+  EXPECT_EQ(&a, p.get_nothrow<int>());
+  EXPECT_EQ(&a, static_cast<const Ptr&>(p).get_nothrow<int>());
+  EXPECT_EQ(&a, p.get<int>());
+  EXPECT_EQ(&a, static_cast<const Ptr&>(p).get<int>());
+  EXPECT_EQ(static_cast<void*>(NULL), p.get_nothrow<void>());
+  EXPECT_THROW({p.get<void>();}, std::invalid_argument);
+
+  Foo foo;
+  p.set(&foo);
+  EXPECT_FALSE(p.empty());
+  EXPECT_FALSE(p.hasType<void>());
+  EXPECT_FALSE(p.hasType<int>());
+  EXPECT_TRUE(p.hasType<Foo>());
+  EXPECT_FALSE(p.hasType<Bar>());
+
+  EXPECT_EQ(static_cast<int*>(NULL), p.get_nothrow<int>());
+
+  p.clear();
+  EXPECT_TRUE(p.empty());
+  EXPECT_FALSE(p.hasType<void>());
+  EXPECT_FALSE(p.hasType<int>());
+  EXPECT_FALSE(p.hasType<Foo>());
+  EXPECT_FALSE(p.hasType<Bar>());
+}
+
+TEST(DiscriminatedPtr, Apply) {
+  struct Foo { };
+  struct Visitor {
+    std::string operator()(int* ptr) { return "int"; }
+    std::string operator()(const int* ptr) { return "const int"; }
+    std::string operator()(Foo* ptr) { return "Foo"; }
+    std::string operator()(const Foo* ptr) { return "const Foo"; }
+  };
+
+  typedef DiscriminatedPtr<int, Foo> Ptr;
+  Ptr p;
+
+  int a = 0;
+  p.set(&a);
+  EXPECT_EQ("int", p.apply(Visitor()));
+  EXPECT_EQ("const int", static_cast<const Ptr&>(p).apply(Visitor()));
+
+  Foo foo;
+  p.set(&foo);
+  EXPECT_EQ("Foo", p.apply(Visitor()));
+  EXPECT_EQ("const Foo", static_cast<const Ptr&>(p).apply(Visitor()));
+
+  p.clear();
+  EXPECT_THROW({p.apply(Visitor());}, std::invalid_argument);
+}
+
+TEST(DiscriminatedPtr, ApplyVoid) {
+  struct Foo { };
+  struct Visitor {
+    void operator()(int* ptr) { result = "int"; }
+    void operator()(const int* ptr) { result = "const int"; }
+    void operator()(Foo* ptr) { result = "Foo"; }
+    void operator()(const Foo* ptr) { result = "const Foo"; }
+
+    std::string result;
+  };
+
+  typedef DiscriminatedPtr<int, Foo> Ptr;
+  Ptr p;
+  Visitor v;
+
+  int a = 0;
+  p.set(&a);
+  p.apply(v);
+  EXPECT_EQ("int", v.result);
+  static_cast<const Ptr&>(p).apply(v);
+  EXPECT_EQ("const int", v.result);
+
+  Foo foo;
+  p.set(&foo);
+  p.apply(v);
+  EXPECT_EQ("Foo", v.result);
+  static_cast<const Ptr&>(p).apply(v);
+  EXPECT_EQ("const Foo", v.result);
+
+  p.clear();
+  EXPECT_THROW({p.apply(v);}, std::invalid_argument);
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/DynamicConverterTest.cpp
@@ -0,0 +1,372 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// @author Nicholas Ormrod <njormrod@fb.com>
+
+#include "folly/DynamicConverter.h"
+
+#include <algorithm>
+#include <gflags/gflags.h>
+#include <gtest/gtest.h>
+#include <map>
+#include <vector>
+
+#include "folly/Benchmark.h"
+
+using namespace folly;
+using namespace folly::dynamicconverter_detail;
+
+TEST(DynamicConverter, template_metaprogramming) {
+  struct A {};
+
+  bool c1f = is_container<int>::value;
+  bool c2f = is_container<std::pair<int, int>>::value;
+  bool c3f = is_container<A>::value;
+
+  bool c1t = is_container<std::vector<int>>::value;
+  bool c2t = is_container<std::set<int>>::value;
+  bool c3t = is_container<std::map<int, int>>::value;
+
+  EXPECT_EQ(c1f, false);
+  EXPECT_EQ(c2f, false);
+  EXPECT_EQ(c3f, false);
+  EXPECT_EQ(c1t, true);
+  EXPECT_EQ(c2t, true);
+  EXPECT_EQ(c3t, true);
+
+
+  bool m1f = is_map<int>::value;
+  bool m2f = is_map<std::set<int>>::value;
+
+  bool m1t = is_map<std::map<int, int>>::value;
+
+  EXPECT_EQ(m1f, false);
+  EXPECT_EQ(m2f, false);
+  EXPECT_EQ(m1t, true);
+
+
+  bool r1f = is_range<int>::value;
+
+  bool r1t = is_range<std::set<int>>::value;
+  bool r2t = is_range<std::vector<int>>::value;
+
+  EXPECT_EQ(r1f, false);
+  EXPECT_EQ(r1t, true);
+  EXPECT_EQ(r2t, true);
+}
+
+TEST(DynamicConverter, arithmetic_types) {
+  dynamic d1 = 12;
+  auto i1 = convertTo<int>(d1);
+  EXPECT_EQ(i1, 12);
+
+  dynamic d2 = 123456789012345;
+  auto i2 = convertTo<int64_t>(d2);
+  EXPECT_EQ(i2, 123456789012345);
+
+  dynamic d4 = 3.141;
+  auto i4 = convertTo<float>(d4);
+  EXPECT_EQ((int)(i4*100), 314);
+
+  dynamic d5 = true;
+  auto i5 = convertTo<bool>(d5);
+  EXPECT_EQ(i5, true);
+
+  dynamic d6 = 15;
+  const auto i6 = convertTo<const int>(d6);
+  EXPECT_EQ(i6, 15);
+
+  dynamic d7 = "87";
+  auto i7 = convertTo<int>(d7);
+  EXPECT_EQ(i7, 87);
+
+  dynamic d8 = "false";
+  auto i8 = convertTo<bool>(d8);
+  EXPECT_EQ(i8, false);
+}
+
+TEST(DynamicConverter, simple_builtins) {
+  dynamic d1 = "Haskell";
+  auto i1 = convertTo<folly::fbstring>(d1);
+  EXPECT_EQ(i1, "Haskell");
+
+  dynamic d2 = 13;
+  auto i2 = convertTo<std::string>(d2);
+  EXPECT_EQ(i2, "13");
+
+  dynamic d3 = { 12, "Scala" };
+  auto i3 = convertTo<std::pair<int, std::string>>(d3);
+  EXPECT_EQ(i3.first, 12);
+  EXPECT_EQ(i3.second, "Scala");
+
+  dynamic d4 = dynamic::object("C", "C++");
+  auto i4 = convertTo<std::pair<std::string, folly::fbstring>>(d4);
+  EXPECT_EQ(i4.first, "C");
+  EXPECT_EQ(i4.second, "C++");
+}
+
+TEST(DynamicConverter, simple_fbvector) {
+  dynamic d1 = { 1, 2, 3 };
+  auto i1 = convertTo<folly::fbvector<int>>(d1);
+  decltype(i1) i1b = { 1, 2, 3 };
+  EXPECT_EQ(i1, i1b);
+}
+
+TEST(DynamicConverter, simple_container) {
+  dynamic d1 = { 1, 2, 3 };
+  auto i1 = convertTo<std::vector<int>>(d1);
+  decltype(i1) i1b = { 1, 2, 3 };
+  EXPECT_EQ(i1, i1b);
+
+  dynamic d2 = { 1, 3, 5, 2, 4 };
+  auto i2 = convertTo<std::set<int>>(d2);
+  decltype(i2) i2b = { 1, 2, 3, 5, 4 };
+  EXPECT_EQ(i2, i2b);
+}
+
+TEST(DynamicConverter, simple_map) {
+  dynamic d1 = dynamic::object(1, "one")(2, "two");
+  auto i1 = convertTo<std::map<int, std::string>>(d1);
+  decltype(i1) i1b = { { 1, "one" }, { 2, "two" } };
+  EXPECT_EQ(i1, i1b);
+
+  dynamic d2 = { { 3, "three" }, { 4, "four" } };
+  auto i2 = convertTo<std::unordered_map<int, std::string>>(d2);
+  decltype(i2) i2b = { { 3, "three" }, { 4, "four" } };
+  EXPECT_EQ(i2, i2b);
+}
+
+TEST(DynamicConverter, map_keyed_by_string) {
+  dynamic d1 = dynamic::object("1", "one")("2", "two");
+  auto i1 = convertTo<std::map<std::string, std::string>>(d1);
+  decltype(i1) i1b = { { "1", "one" }, { "2", "two" } };
+  EXPECT_EQ(i1, i1b);
+
+  dynamic d2 = { { "3", "three" }, { "4", "four" } };
+  auto i2 = convertTo<std::unordered_map<std::string, std::string>>(d2);
+  decltype(i2) i2b = { { "3", "three" }, { "4", "four" } };
+  EXPECT_EQ(i2, i2b);
+}
+
+TEST(DynamicConverter, map_to_vector_of_pairs) {
+  dynamic d1 = dynamic::object("1", "one")("2", "two");
+  auto i1 = convertTo<std::vector<std::pair<std::string, std::string>>>(d1);
+  std::sort(i1.begin(), i1.end());
+  decltype(i1) i1b = { { "1", "one" }, { "2", "two" } };
+  EXPECT_EQ(i1, i1b);
+}
+
+TEST(DynamicConverter, nested_containers) {
+  dynamic d1 = { { 1 }, { }, { 2, 3 } };
+  auto i1 = convertTo<folly::fbvector<std::vector<uint8_t>>>(d1);
+  decltype(i1) i1b = { { 1 }, { }, { 2, 3 } };
+  EXPECT_EQ(i1, i1b);
+
+  dynamic h2a = { "3", ".", "1", "4" };
+  dynamic h2b = { "2", ".", "7", "2" };
+  dynamic d2 = dynamic::object(3.14, h2a)(2.72, h2b);
+  auto i2 = convertTo<std::map<double, std::vector<folly::fbstring>>>(d2);
+  decltype(i2) i2b =
+    { { 3.14, { "3", ".", "1", "4" } },
+      { 2.72, { "2", ".", "7", "2" } } };
+  EXPECT_EQ(i2, i2b);
+}
+
+struct A {
+  int i;
+  bool operator==(const A & o) const { return i == o.i; }
+};
+namespace folly {
+template <> struct DynamicConverter<A> {
+  static A convert(const dynamic & d) {
+    return { convertTo<int>(d["i"]) };
+  }
+};
+}
+TEST(DynamicConverter, custom_class) {
+  dynamic d1 = dynamic::object("i", 17);
+  auto i1 = convertTo<A>(d1);
+  EXPECT_EQ(i1.i, 17);
+
+  dynamic d2 = { dynamic::object("i", 18), dynamic::object("i", 19) };
+  auto i2 = convertTo<std::vector<A>>(d2);
+  decltype(i2) i2b = { { 18 }, { 19 } };
+  EXPECT_EQ(i2, i2b);
+}
+
+TEST(DynamicConverter, crazy) {
+  // we are going to create a vector<unordered_map<bool, T>>
+  // we will construct some of the maps from dynamic objects,
+  //   some from a vector of KV pairs.
+  // T will be vector<set<string>>
+
+  std::set<std::string>
+    s1 = { "a", "e", "i", "o", "u" },
+    s2 = { "2", "3", "5", "7" },
+    s3 = { "Hello", "World" };
+
+  std::vector<std::set<std::string>>
+    v1 = {},
+    v2 = { s1, s2 },
+    v3 = { s3 };
+
+  std::unordered_map<bool, std::vector<std::set<std::string>>>
+    m1 = { { true, v1 }, { false, v2 } },
+    m2 = { { true, v3 } };
+
+  std::vector<std::unordered_map<bool, std::vector<std::set<std::string>>>>
+    f1 = { m1, m2 };
+
+
+  dynamic
+    ds1 = { "a", "e", "i", "o", "u" },
+    ds2 = { "2", "3", "5", "7" },
+    ds3 = { "Hello", "World" };
+
+  dynamic
+    dv1 = {},
+    dv2 = { ds1, ds2 },
+    dv3 = { ds3 };
+
+  dynamic
+    dm1 = dynamic::object(true, dv1)(false, dv2),
+    dm2 = { { true, dv3 } };
+
+  dynamic
+    df1 = { dm1, dm2 };
+
+
+  auto i = convertTo<std::vector<std::unordered_map<bool, std::vector<
+          std::set<std::string>>>>>(df1); // yes, that is 5 close-chevrons
+
+  EXPECT_EQ(f1, i);
+}
+
+TEST(DynamicConverter, consts) {
+  dynamic d1 = 7.5;
+  auto i1 = convertTo<const double>(d1);
+  EXPECT_EQ(7.5, i1);
+
+  dynamic d2 = "Hello";
+  auto i2 = convertTo<const std::string>(d2);
+  decltype(i2) i2b = "Hello";
+  EXPECT_EQ(i2b, i2);
+
+  dynamic d3 = true;
+  auto i3 = convertTo<const bool>(d3);
+  EXPECT_EQ(true, i3);
+
+  dynamic d4 = "true";
+  auto i4 = convertTo<const bool>(d4);
+  EXPECT_EQ(true, i4);
+
+  dynamic d5 = { 1, 2 };
+  auto i5 = convertTo<const std::pair<const int, const int>>(d5);
+  decltype(i5) i5b = { 1, 2 };
+  EXPECT_EQ(i5b, i5);
+}
+
+struct Token {
+  int kind_;
+  fbstring lexeme_;
+
+  explicit Token(int kind, const fbstring& lexeme)
+    : kind_(kind), lexeme_(lexeme) {}
+};
+
+namespace folly {
+template <> struct DynamicConverter<Token> {
+  static Token convert(const dynamic& d) {
+    int k = convertTo<int>(d["KIND"]);
+    fbstring lex = convertTo<fbstring>(d["LEXEME"]);
+    return Token(k, lex);
+  }
+};
+}
+
+TEST(DynamicConverter, example) {
+  dynamic d1 = dynamic::object("KIND", 2)("LEXEME", "a token");
+  auto i1 = convertTo<Token>(d1);
+  EXPECT_EQ(i1.kind_, 2);
+  EXPECT_EQ(i1.lexeme_, "a token");
+}
+
+TEST(DynamicConverter, construct) {
+  using std::vector;
+  using std::map;
+  using std::pair;
+  using std::string;
+  {
+    vector<int> c { 1, 2, 3 };
+    dynamic d = { 1, 2, 3 };
+    EXPECT_EQ(d, toDynamic(c));
+  }
+
+  {
+    map<int, int> c { { 2, 4 }, { 3, 9 } };
+    dynamic d = dynamic::object(2, 4)(3, 9);
+    EXPECT_EQ(d, toDynamic(c));
+  }
+
+  {
+    map<string, string> c { { "a", "b" } };
+    dynamic d = dynamic::object("a", "b");
+    EXPECT_EQ(d, toDynamic(c));
+  }
+
+  {
+    map<string, pair<string, int>> c { { "a", { "b", 3 } } };
+    dynamic d = dynamic::object("a", dynamic { "b", 3 });
+    EXPECT_EQ(d, toDynamic(c));
+  }
+
+  {
+    map<string, pair<string, int>> c { { "a", { "b", 3 } } };
+    dynamic d = dynamic::object("a", dynamic { "b", 3 });
+    EXPECT_EQ(d, toDynamic(c));
+  }
+
+  {
+    vector<int> vi { 2, 3, 4, 5 };
+    auto c = std::make_pair(range(vi.begin(), vi.begin() + 3),
+                            range(vi.begin() + 1, vi.begin() + 4));
+    dynamic d = { { 2, 3, 4 }, { 3, 4, 5 } };
+    EXPECT_EQ(d, toDynamic(c));
+  }
+}
+
+TEST(DynamicConverter, errors) {
+  const auto int32Over =
+    static_cast<int64_t>(std::numeric_limits<int32_t>().max()) + 1;
+  const auto floatOver =
+    static_cast<double>(std::numeric_limits<float>().max()) * 2;
+
+  dynamic d1 = int32Over;
+  EXPECT_THROW(convertTo<int32_t>(d1), std::range_error);
+
+  dynamic d2 = floatOver;
+  EXPECT_THROW(convertTo<float>(d2), std::range_error);
+}
+
+int main(int argc, char ** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  if (FLAGS_benchmark) {
+    folly::runBenchmarks();
+  }
+  return RUN_ALL_TESTS();
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/DynamicTest.cpp
@@ -0,0 +1,324 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <boost/next_prior.hpp>
+#include <gflags/gflags.h>
+#include <gtest/gtest.h>
+
+#include "folly/Benchmark.h"
+#include "folly/dynamic.h"
+#include "folly/gen/Base.h"
+#include "folly/json.h"
+
+using folly::dynamic;
+
+TEST(Dynamic, ObjectBasics) {
+  dynamic obj = dynamic::object("a", false);
+  EXPECT_EQ(obj.at("a"), false);
+  EXPECT_EQ(obj.size(), 1);
+  obj.insert("a", true);
+  EXPECT_EQ(obj.size(), 1);
+  EXPECT_EQ(obj.at("a"), true);
+  obj.at("a") = nullptr;
+  EXPECT_EQ(obj.size(), 1);
+  EXPECT_TRUE(obj.at("a") == nullptr);
+
+  dynamic newObject = dynamic::object;
+
+  newObject["z"] = 12;
+  EXPECT_EQ(newObject.size(), 1);
+  newObject["a"] = true;
+  EXPECT_EQ(newObject.size(), 2);
+
+  EXPECT_EQ(*newObject.keys().begin(), newObject.items().begin()->first);
+  EXPECT_EQ(*newObject.values().begin(), newObject.items().begin()->second);
+  std::vector<std::pair<folly::fbstring, dynamic>> found;
+  found.push_back(std::make_pair(
+     newObject.keys().begin()->asString(),
+     *newObject.values().begin()));
+
+  EXPECT_EQ(*boost::next(newObject.keys().begin()),
+            boost::next(newObject.items().begin())->first);
+  EXPECT_EQ(*boost::next(newObject.values().begin()),
+            boost::next(newObject.items().begin())->second);
+  found.push_back(std::make_pair(
+      boost::next(newObject.keys().begin())->asString(),
+      *boost::next(newObject.values().begin())));
+
+  std::sort(found.begin(), found.end());
+
+  EXPECT_EQ("a", found[0].first);
+  EXPECT_TRUE(found[0].second.asBool());
+
+  EXPECT_EQ("z", found[1].first);
+  EXPECT_EQ(12, found[1].second.asInt());
+
+  dynamic obj2 = dynamic::object;
+  EXPECT_TRUE(obj2.isObject());
+
+  dynamic d3 = nullptr;
+  EXPECT_TRUE(d3 == nullptr);
+  d3 = dynamic::object;
+  EXPECT_TRUE(d3.isObject());
+  d3["foo"] = { 1, 2, 3 };
+  EXPECT_EQ(d3.count("foo"), 1);
+
+  d3[123] = 321;
+  EXPECT_EQ(d3.at(123), 321);
+
+  d3["123"] = 42;
+  EXPECT_EQ(d3.at("123"), 42);
+  EXPECT_EQ(d3.at(123), 321);
+
+  // We don't allow objects as keys in objects.
+  EXPECT_ANY_THROW(newObject[d3] = 12);
+}
+
+TEST(Dynamic, ObjectErase) {
+  dynamic obj = dynamic::object("key1", "val")
+                               ("key2", "val2");
+  EXPECT_EQ(obj.count("key1"), 1);
+  EXPECT_EQ(obj.count("key2"), 1);
+  EXPECT_EQ(obj.erase("key1"), 1);
+  EXPECT_EQ(obj.count("key1"), 0);
+  EXPECT_EQ(obj.count("key2"), 1);
+  EXPECT_EQ(obj.erase("key1"), 0);
+  obj["key1"] = 12;
+  EXPECT_EQ(obj.count("key1"), 1);
+  EXPECT_EQ(obj.count("key2"), 1);
+  auto it = obj.find("key2");
+  obj.erase(it);
+  EXPECT_EQ(obj.count("key1"), 1);
+  EXPECT_EQ(obj.count("key2"), 0);
+
+  obj["asd"] = 42.0;
+  obj["foo"] = 42.0;
+  EXPECT_EQ(obj.size(), 3);
+  auto ret = obj.erase(boost::next(obj.items().begin()), obj.items().end());
+  EXPECT_TRUE(ret == obj.items().end());
+  EXPECT_EQ(obj.size(), 1);
+  obj.erase(obj.items().begin());
+  EXPECT_TRUE(obj.empty());
+}
+
+TEST(Dynamic, ArrayErase) {
+  dynamic arr = { 1, 2, 3, 4, 5, 6 };
+
+  EXPECT_THROW(arr.erase(1), std::exception);
+  EXPECT_EQ(arr.size(), 6);
+  EXPECT_EQ(arr[0], 1);
+  arr.erase(arr.begin());
+  EXPECT_EQ(arr.size(), 5);
+
+  arr.erase(boost::next(arr.begin()), boost::prior(arr.end()));
+  EXPECT_EQ(arr.size(), 2);
+  EXPECT_EQ(arr[0], 2);
+  EXPECT_EQ(arr[1], 6);
+}
+
+TEST(Dynamic, StringBasics) {
+  dynamic str = "hello world";
+  EXPECT_EQ(11, str.size());
+  EXPECT_FALSE(str.empty());
+  str = "";
+  EXPECT_TRUE(str.empty());
+}
+
+TEST(Dynamic, ArrayBasics) {
+  dynamic array = { 1, 2, 3 };
+  EXPECT_EQ(array.size(), 3);
+  EXPECT_EQ(array.at(0), 1);
+  EXPECT_EQ(array.at(1), 2);
+  EXPECT_EQ(array.at(2), 3);
+
+  EXPECT_ANY_THROW(array.at(3));
+
+  array.push_back("foo");
+  EXPECT_EQ(array.size(), 4);
+
+  array.resize(12, "something");
+  EXPECT_EQ(array.size(), 12);
+  EXPECT_EQ(array[11], "something");
+}
+
+TEST(Dynamic, DeepCopy) {
+  dynamic val = { "foo", "bar", { "foo1", "bar1" } };
+  EXPECT_EQ(val.at(2).at(0), "foo1");
+  EXPECT_EQ(val.at(2).at(1), "bar1");
+  dynamic val2 = val;
+  EXPECT_EQ(val2.at(2).at(0), "foo1");
+  EXPECT_EQ(val2.at(2).at(1), "bar1");
+  EXPECT_EQ(val.at(2).at(0), "foo1");
+  EXPECT_EQ(val.at(2).at(1), "bar1");
+  val2.at(2).at(0) = "foo3";
+  val2.at(2).at(1) = "bar3";
+  EXPECT_EQ(val.at(2).at(0), "foo1");
+  EXPECT_EQ(val.at(2).at(1), "bar1");
+  EXPECT_EQ(val2.at(2).at(0), "foo3");
+  EXPECT_EQ(val2.at(2).at(1), "bar3");
+
+  dynamic obj = dynamic::object("a", "b")
+                               ("c", {"d", "e", "f"})
+                               ;
+  EXPECT_EQ(obj.at("a"), "b");
+  dynamic obj2 = obj;
+  obj2.at("a") = {1, 2, 3};
+  EXPECT_EQ(obj.at("a"), "b");
+  dynamic expected = {1, 2, 3};
+  EXPECT_EQ(obj2.at("a"), expected);
+}
+
+TEST(Dynamic, Operator) {
+  bool caught = false;
+  try {
+    dynamic d1 = dynamic::object;
+    dynamic d2 = dynamic::object;
+    auto foo = d1 < d2;
+  } catch (std::exception const& e) {
+    caught = true;
+  }
+  EXPECT_TRUE(caught);
+
+  dynamic foo = "asd";
+  dynamic bar = "bar";
+  dynamic sum = foo + bar;
+  EXPECT_EQ(sum, "asdbar");
+
+  dynamic some = 12;
+  dynamic nums = 4;
+  dynamic math = some / nums;
+  EXPECT_EQ(math, 3);
+}
+
+TEST(Dynamic, Conversions) {
+  dynamic str = "12.0";
+  EXPECT_EQ(str.asDouble(), 12.0);
+  EXPECT_ANY_THROW(str.asInt());
+  EXPECT_ANY_THROW(str.asBool());
+
+  str = "12";
+  EXPECT_EQ(str.asInt(), 12);
+  EXPECT_EQ(str.asDouble(), 12.0);
+  str = "0";
+  EXPECT_EQ(str.asBool(), false);
+  EXPECT_EQ(str.asInt(), 0);
+  EXPECT_EQ(str.asDouble(), 0);
+  EXPECT_EQ(str.asString(), "0");
+
+  dynamic num = 12;
+  EXPECT_EQ("12", num.asString());
+  EXPECT_EQ(12.0, num.asDouble());
+}
+
+TEST(Dynamic, StringPtrs) {
+  dynamic str = "12.0";
+  dynamic num = 12.0;
+
+  EXPECT_EQ(0, strcmp(str.c_str(), "12.0"));
+  EXPECT_EQ(0, strncmp(str.data(), "12.0", str.asString().length()));
+
+  EXPECT_ANY_THROW(num.c_str());
+  EXPECT_ANY_THROW(num.data());
+}
+
+TEST(Dynamic, FormattedIO) {
+  std::ostringstream out;
+  dynamic doubl = 123.33;
+  dynamic dint = 12;
+  out << "0x" << std::hex << ++dint << ' ' << std::setprecision(1)
+      << doubl << '\n';
+  EXPECT_EQ(out.str(), "0xd 1e+02\n");
+
+  out.str("");
+  dynamic arrr = { 1, 2, 3 };
+  out << arrr;
+  EXPECT_EQ(out.str(), "[1,2,3]");
+
+  out.str("");
+  dynamic objy = dynamic::object("a", 12);
+  out << objy;
+  EXPECT_EQ(out.str(), R"({"a":12})");
+
+  out.str("");
+  dynamic objy2 = { objy, dynamic::object(12, "str"),
+                          dynamic::object(true, false) };
+  out << objy2;
+  EXPECT_EQ(out.str(), R"([{"a":12},{12:"str"},{true:false}])");
+}
+
+TEST(Dynamic, GetSetDefaultTest) {
+  dynamic d1 = dynamic::object("foo", "bar");
+  EXPECT_EQ(d1.getDefault("foo", "baz"), "bar");
+  EXPECT_EQ(d1.getDefault("quux", "baz"), "baz");
+
+  dynamic d2 = dynamic::object("foo", "bar");
+  EXPECT_EQ(d2.setDefault("foo", "quux"), "bar");
+  d2.setDefault("bar", dynamic({})).push_back(42);
+  EXPECT_EQ(d2["bar"][0], 42);
+
+  dynamic d3 = dynamic::object, empty = dynamic::object;
+  EXPECT_EQ(d3.getDefault("foo"), empty);
+  d3.setDefault("foo")["bar"] = "baz";
+  EXPECT_EQ(d3["foo"]["bar"], "baz");
+
+  // we do not allow getDefault/setDefault on arrays
+  dynamic d4 = dynamic({});
+  EXPECT_ANY_THROW(d4.getDefault("foo", "bar"));
+  EXPECT_ANY_THROW(d4.setDefault("foo", "bar"));
+}
+
+TEST(Dynamic, ObjectForwarding) {
+  // Make sure dynamic::object can be constructed the same way as any
+  // dynamic.
+  dynamic d = dynamic::object("asd", {"foo", "bar"});
+  dynamic d2 = dynamic::object("key2", {"value", "words"})
+                              ("key", "value1");
+}
+
+TEST(Dynamic, GetPtr) {
+  dynamic array = { 1, 2, "three" };
+  EXPECT_TRUE(array.get_ptr(0));
+  EXPECT_FALSE(array.get_ptr(3));
+  EXPECT_EQ(dynamic("three"), *array.get_ptr(2));
+  const dynamic& carray = array;
+  EXPECT_EQ(dynamic("three"), *carray.get_ptr(2));
+
+  dynamic object = dynamic::object("one", 1)("two", 2);
+  EXPECT_TRUE(object.get_ptr("one"));
+  EXPECT_FALSE(object.get_ptr("three"));
+  EXPECT_EQ(dynamic(2), *object.get_ptr("two"));
+  *object.get_ptr("one") = 11;
+  EXPECT_EQ(dynamic(11), *object.get_ptr("one"));
+  const dynamic& cobject = object;
+  EXPECT_EQ(dynamic(2), *cobject.get_ptr("two"));
+}
+
+TEST(Dynamic, ArrayGenerator) {
+  // Make sure arrays can be used with folly::gen.
+  using namespace folly::gen;
+  dynamic arr { 1, 2, 3, 4 };
+  EXPECT_EQ(from(arr) | take(3) | member(&dynamic::asInt) | sum, 6);
+}
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  if (FLAGS_benchmark) {
+    folly::runBenchmarks();
+  }
+  return RUN_ALL_TESTS();
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/EndianTest.cpp
@@ -0,0 +1,65 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Bits.h"
+
+#include <gtest/gtest.h>
+
+using namespace folly;
+
+TEST(Endian, Basic) {
+  uint8_t v8 = 0x12;
+  uint8_t v8s = 0x12;
+  uint16_t v16 = 0x1234;
+  uint16_t v16s = 0x3412;
+  uint32_t v32 = 0x12345678;
+  uint32_t v32s = 0x78563412;
+  uint64_t v64 = 0x123456789abcdef0ULL;
+  uint64_t v64s = 0xf0debc9a78563412ULL;
+
+#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__
+
+#define GEN1(sz) \
+  EXPECT_EQ(v##sz, Endian::little(v##sz)); \
+  EXPECT_EQ(v##sz, Endian::little##sz(v##sz)); \
+  EXPECT_EQ(v##sz##s, Endian::big(v##sz)); \
+  EXPECT_EQ(v##sz##s, Endian::big##sz(v##sz));
+
+#elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__
+
+#define GEN1(sz) \
+  EXPECT_EQ(v##sz##s, Endian::little(v##sz)); \
+  EXPECT_EQ(v##sz##s, Endian::little##sz(v##sz)); \
+  EXPECT_EQ(v##sz, Endian::big(v##sz)); \
+  EXPECT_EQ(v##sz, Endian::big##sz(v##sz));
+
+#else
+# error Your machine uses a weird endianness!
+#endif  /* __BYTE_ORDER__ */
+
+#define GEN(sz) \
+  EXPECT_EQ(v##sz##s, Endian::swap(v##sz)); \
+  EXPECT_EQ(v##sz##s, Endian::swap##sz(v##sz)); \
+  GEN1(sz);
+
+  GEN(8);
+  GEN(16)
+  GEN(32)
+  GEN(64)
+
+#undef GEN
+#undef GEN1
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/ExceptionTest.cpp
@@ -0,0 +1,97 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Exception.h"
+
+#include <cstdio>
+#include <memory>
+
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+namespace folly { namespace test {
+
+#define EXPECT_SYSTEM_ERROR(statement, err, msg) \
+  try { \
+    statement; \
+    ADD_FAILURE() << "Didn't throw"; \
+  } catch (const std::system_error& e) { \
+    std::system_error expected(err, std::system_category(), msg); \
+    EXPECT_STREQ(expected.what(), e.what()); \
+  } catch (...) { \
+    ADD_FAILURE() << "Threw a different type"; \
+  }
+
+
+TEST(ExceptionTest, Simple) {
+  // Make sure errno isn't used when we don't want it to, set it to something
+  // else than what we set when we call Explicit functions
+  errno = ERANGE;
+  EXPECT_SYSTEM_ERROR({throwSystemErrorExplicit(EIO, "hello");},
+                      EIO, "hello");
+  errno = ERANGE;
+  EXPECT_SYSTEM_ERROR({throwSystemErrorExplicit(EIO, "hello", " world");},
+                      EIO, "hello world");
+  errno = ERANGE;
+  EXPECT_SYSTEM_ERROR({throwSystemError("hello", " world");},
+                      ERANGE, "hello world");
+
+  EXPECT_NO_THROW({checkPosixError(0, "hello", " world");});
+  errno = ERANGE;
+  EXPECT_SYSTEM_ERROR({checkPosixError(EIO, "hello", " world");},
+                      EIO, "hello world");
+
+  EXPECT_NO_THROW({checkKernelError(0, "hello", " world");});
+  EXPECT_NO_THROW({checkKernelError(EIO, "hello", " world");});
+  errno = ERANGE;
+  EXPECT_SYSTEM_ERROR({checkKernelError(-EIO, "hello", " world");},
+                      EIO, "hello world");
+
+  EXPECT_NO_THROW({checkUnixError(0, "hello", " world");});
+  EXPECT_NO_THROW({checkUnixError(1, "hello", " world");});
+  errno = ERANGE;
+  EXPECT_SYSTEM_ERROR({checkUnixError(-1, "hello", " world");},
+                      ERANGE, "hello world");
+
+  EXPECT_NO_THROW({checkUnixErrorExplicit(0, EIO, "hello", " world");});
+  EXPECT_NO_THROW({checkUnixErrorExplicit(1, EIO, "hello", " world");});
+  errno = ERANGE;
+  EXPECT_SYSTEM_ERROR({checkUnixErrorExplicit(-1, EIO, "hello", " world");},
+                      EIO, "hello world");
+
+  std::shared_ptr<FILE> fp(tmpfile(), fclose);
+  ASSERT_TRUE(fp != nullptr);
+
+  EXPECT_NO_THROW({checkFopenError(fp.get(), "hello", " world");});
+  errno = ERANGE;
+  EXPECT_SYSTEM_ERROR({checkFopenError(nullptr, "hello", " world");},
+                      ERANGE, "hello world");
+
+  EXPECT_NO_THROW({checkFopenErrorExplicit(fp.get(), EIO, "hello", " world");});
+  errno = ERANGE;
+  EXPECT_SYSTEM_ERROR({checkFopenErrorExplicit(nullptr, EIO,
+                                               "hello", " world");},
+                      EIO, "hello world");
+}
+
+}}  // namespaces
+
+int main(int argc, char *argv[]) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  return RUN_ALL_TESTS();
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/FBStringBenchmark.cpp
@@ -0,0 +1,153 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+//
+// Author: andrei.alexandrescu@fb.com
+
+#include "folly/FBString.h"
+
+#include <cstdlib>
+
+#include <list>
+#include <fstream>
+#include <boost/algorithm/string.hpp>
+#include <boost/random.hpp>
+
+#include <gflags/gflags.h>
+
+#include "folly/Foreach.h"
+#include "folly/Random.h"
+#include "folly/Benchmark.h"
+
+using namespace std;
+using namespace folly;
+
+static const int seed = folly::randomNumberSeed();
+typedef boost::mt19937 RandomT;
+static RandomT rng(seed);
+static const size_t maxString = 100;
+static const bool avoidAliasing = true;
+
+template <class Integral1, class Integral2>
+Integral2 random(Integral1 low, Integral2 up) {
+  boost::uniform_int<> range(low, up);
+  return range(rng);
+}
+
+template <class String>
+void randomString(String* toFill, unsigned int maxSize = 1000) {
+  assert(toFill);
+  toFill->resize(random(0, maxSize));
+  FOR_EACH (i, *toFill) {
+    *i = random('a', 'z');
+  }
+}
+
+template <class String>
+void randomBinaryString(String* toFill, unsigned int maxSize = 1000) {
+  assert(toFill);
+  toFill->resize(random(0, maxSize));
+  FOR_EACH (i, *toFill) {
+    *i = random('0', '1');
+  }
+}
+
+template <class String, class Integral>
+void Num2String(String& str, Integral n) {
+  str.resize(30, '\0');
+  sprintf(&str[0], "%lu", static_cast<unsigned long>(n));
+  str.resize(strlen(str.c_str()));
+}
+
+std::list<char> RandomList(unsigned int maxSize) {
+  std::list<char> lst(random(0u, maxSize));
+  std::list<char>::iterator i = lst.begin();
+  for (; i != lst.end(); ++i) {
+    *i = random('a', 'z');
+ }
+  return lst;
+}
+
+#define CONCAT(A, B) CONCAT_HELPER(A, B)
+#define CONCAT_HELPER(A, B) A##B
+#define BENCHFUN(F) CONCAT(CONCAT(BM_, F), CONCAT(_, STRING))
+
+#define STRING string
+#include "folly/test/FBStringTestBenchmarks.cpp.h"
+#undef STRING
+#define STRING fbstring
+#include "folly/test/FBStringTestBenchmarks.cpp.h"
+#undef STRING
+
+int main(int argc, char** argv) {
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  folly::runBenchmarks();
+  return 0;
+}
+
+/*
+malloc
+
+BENCHFUN(defaultCtor)                  100000  1.426 s   14.26 us  68.47 k
+BM_copyCtor_string/32k                 100000  63.48 ms  634.8 ns  1.502 M
+BM_ctorFromArray_string/32k            100000  303.3 ms  3.033 us  321.9 k
+BM_ctorFromChar_string/1M              100000  9.915 ms  99.15 ns  9.619 M
+BM_assignmentOp_string/256             100000  69.09 ms  690.9 ns   1.38 M
+BENCHFUN(assignmentFill)               100000  1.775 ms  17.75 ns  53.73 M
+BM_resize_string/512k                  100000  1.667 s   16.67 us  58.58 k
+BM_findSuccessful_string/512k          100000  287.3 ms  2.873 us  339.9 k
+BM_findUnsuccessful_string/512k        100000  320.3 ms  3.203 us  304.9 k
+BM_replace_string/256                  100000  69.68 ms  696.8 ns  1.369 M
+BM_push_back_string/1k                 100000  433.1 ms  4.331 us  225.5 k
+
+BENCHFUN(defaultCtor)                  100000  1.086 s   10.86 us  89.91 k
+BM_copyCtor_fbstring/32k               100000  4.218 ms  42.18 ns  22.61 M
+BM_ctorFromArray_fbstring/32k          100000  145.2 ms  1.452 us  672.7 k
+BM_ctorFromChar_fbstring/1M            100000   9.21 ms   92.1 ns  10.35 M
+BM_assignmentOp_fbstring/256           100000  61.95 ms  619.5 ns   1.54 M
+BENCHFUN(assignmentFill)               100000   1.41 ms   14.1 ns  67.64 M
+BM_resize_fbstring/512k                100000  1.668 s   16.68 us  58.56 k
+BM_findSuccessful_fbstring/512k        100000   20.6 ms    206 ns  4.629 M
+BM_findUnsuccessful_fbstring/512k      100000  141.3 ms  1.413 us  691.1 k
+BM_replace_fbstring/256                100000  77.12 ms  771.2 ns  1.237 M
+BM_push_back_fbstring/1k               100000  1.745 s   17.45 us  55.95 k
+
+jemalloc
+
+BENCHFUN(defaultCtor)                  100000  1.426 s   14.26 us   68.5 k
+BM_copyCtor_string/32k                 100000  275.7 ms  2.757 us  354.2 k
+BM_ctorFromArray_string/32k            100000    270 ms    2.7 us  361.7 k
+BM_ctorFromChar_string/1M              100000  10.36 ms  103.6 ns  9.206 M
+BM_assignmentOp_string/256             100000  70.44 ms  704.3 ns  1.354 M
+BENCHFUN(assignmentFill)               100000  1.766 ms  17.66 ns     54 M
+BM_resize_string/512k                  100000  1.675 s   16.75 us  58.29 k
+BM_findSuccessful_string/512k          100000  90.89 ms  908.9 ns  1.049 M
+BM_findUnsuccessful_string/512k        100000  315.1 ms  3.151 us  309.9 k
+BM_replace_string/256                  100000  71.14 ms  711.4 ns  1.341 M
+BM_push_back_string/1k                 100000  425.1 ms  4.251 us  229.7 k
+
+BENCHFUN(defaultCtor)                  100000  1.082 s   10.82 us  90.23 k
+BM_copyCtor_fbstring/32k               100000  4.213 ms  42.13 ns  22.64 M
+BM_ctorFromArray_fbstring/32k          100000  113.2 ms  1.132 us    863 k
+BM_ctorFromChar_fbstring/1M            100000  9.162 ms  91.62 ns  10.41 M
+BM_assignmentOp_fbstring/256           100000  61.34 ms  613.4 ns  1.555 M
+BENCHFUN(assignmentFill)               100000  1.408 ms  14.08 ns  67.73 M
+BM_resize_fbstring/512k                100000  1.671 s   16.71 us  58.43 k
+BM_findSuccessful_fbstring/512k        100000  8.723 ms  87.23 ns  10.93 M
+BM_findUnsuccessful_fbstring/512k      100000  141.3 ms  1.413 us  691.2 k
+BM_replace_fbstring/256                100000  77.83 ms  778.3 ns  1.225 M
+BM_push_back_fbstring/1k               100000  1.744 s   17.44 us  55.99 k
+*/
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/FBStringTestBenchmarks.cpp.h
@@ -0,0 +1,252 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * This file is supposed to be included from within
+ * FBStringTest. Do not use otherwise.
+ */
+
+void BENCHFUN(initRNG)(int iters, int) {
+  srand(seed);
+}
+BENCHMARK_PARAM(BENCHFUN(initRNG), 0);
+
+void BENCHFUN(defaultCtor)(int iters, int) {
+  FOR_EACH_RANGE (i, 0, iters) {
+    STRING s[4096];
+    doNotOptimizeAway(&s);
+  }
+}
+BENCHMARK_PARAM(BENCHFUN(defaultCtor), 0);
+
+void BENCHFUN(copyCtor)(int iters, int arg) {
+  STRING s;
+  BENCHMARK_SUSPEND {
+    randomString(&s, arg);
+  }
+  FOR_EACH_RANGE (i, 0, iters) {
+    STRING s1 = s;
+    doNotOptimizeAway(&s1);
+  }
+}
+BENCHMARK_PARAM(BENCHFUN(copyCtor), 32768);
+
+void BENCHFUN(ctorFromArray)(int iters, int arg) {
+  STRING s;
+  BENCHMARK_SUSPEND {
+    randomString(&s, arg);
+    if (s.empty()) {
+      s = "This is rare.";
+    }
+  }
+  FOR_EACH_RANGE (i, 0, iters) {
+    STRING s1(s.data(), s.size());
+    doNotOptimizeAway(&s1);
+  }
+}
+BENCHMARK_PARAM(BENCHFUN(ctorFromArray), 32768);
+
+void BENCHFUN(ctorFromTwoPointers)(int iters, int arg) {
+  static STRING s;
+  BENCHMARK_SUSPEND {
+    if (s.size() < arg) s.resize(arg);
+  }
+  FOR_EACH_RANGE (i, 0, iters) {
+    STRING s1(s.begin(), s.end());
+    doNotOptimizeAway(&s1);
+  }
+}
+BENCHMARK_PARAM(BENCHFUN(ctorFromTwoPointers), 0);
+BENCHMARK_PARAM(BENCHFUN(ctorFromTwoPointers), 7);
+BENCHMARK_PARAM(BENCHFUN(ctorFromTwoPointers), 15);
+BENCHMARK_PARAM(BENCHFUN(ctorFromTwoPointers), 23);
+BENCHMARK_PARAM(BENCHFUN(ctorFromTwoPointers), 24);
+
+void BENCHFUN(ctorFromChar)(int iters, int arg) {
+  FOR_EACH_RANGE (i, 0, iters) {
+    STRING s1('a', arg);
+    doNotOptimizeAway(&s1);
+  }
+}
+BENCHMARK_PARAM(BENCHFUN(ctorFromChar), 1048576);
+
+void BENCHFUN(assignmentOp)(int iters, int arg) {
+  STRING s;
+  BENCHMARK_SUSPEND {
+    randomString(&s, arg);
+  }
+  FOR_EACH_RANGE (i, 0, iters) {
+    STRING s1;
+    BENCHMARK_SUSPEND {
+      randomString(&s1, arg);
+      doNotOptimizeAway(&s1);
+    }
+    s1 = s;
+  }
+}
+BENCHMARK_PARAM(BENCHFUN(assignmentOp), 256);
+
+void BENCHFUN(assignmentFill)(int iters, int) {
+  STRING s;
+  FOR_EACH_RANGE (i, 0, iters) {
+    s = static_cast<char>(i);
+    doNotOptimizeAway(&s);
+  }
+}
+BENCHMARK_PARAM(BENCHFUN(assignmentFill), 0);
+
+void BENCHFUN(resize)(int iters, int arg) {
+  STRING s;
+  FOR_EACH_RANGE (i, 0, iters) {
+    s.resize(random(0, arg));
+    doNotOptimizeAway(&s);
+  }
+}
+BENCHMARK_PARAM(BENCHFUN(resize), 524288);
+
+void BENCHFUN(findSuccessful)(int iters, int arg) {
+  size_t pos, len;
+  STRING s;
+
+  BENCHMARK_SUSPEND {
+
+    // Text courtesy (ahem) of
+    // http://www.psychologytoday.com/blog/career-transitions/200906/
+    // the-dreaded-writing-sample
+    s = "\
+Even if you've mastered the art of the cover letter and the resume, \
+another part of the job search process can trip up an otherwise \
+qualified candidate: the writing sample.\n\
+\n\
+Strong writing and communication skills are highly sought after by \
+most employers. Whether crafting short emails or lengthy annual \
+reports, many workers use their writing skills every day. And for an \
+employer seeking proof behind that ubiquitous candidate \
+phrase,\"excellent communication skills\", a required writing sample \
+is invaluable.\n\
+\n\
+Writing samples need the same care and attention given to cover \
+letters and resumes. Candidates with otherwise impeccable credentials \
+are routinely eliminated by a poorly chosen writing sample. Notice I \
+said \"poorly chosen\" not \"poorly written.\" Because that's the rub: \
+a writing sample not only reveals the individual's writing skills, it \
+also offers a peek into what they consider important or relevant for \
+the position. If you miss that mark with your writing sample, don't \
+expect to get a call for an interview.";
+
+    pos = random(0, s.size());
+    len = random(0, s.size() - pos);
+  }
+  FOR_EACH_RANGE (i, 0, iters) {
+    doNotOptimizeAway(s.find(s.data(), pos, len));
+  }
+}
+BENCHMARK_PARAM(BENCHFUN(findSuccessful), 524288);
+
+void BENCHFUN(findUnsuccessful)(int iters, int arg) {
+  STRING s, s1;
+
+  BENCHMARK_SUSPEND {
+    s = "\
+Even if you've mastered the art of the cover letter and the resume, \
+another part of the job search process can trip up an otherwise \
+qualified candidate: the writing sample.\n\
+\n\
+Strong writing and communication skills are highly sought after by \
+most employers. Whether crafting short emails or lengthy annual \
+reports, many workers use their writing skills every day. And for an \
+employer seeking proof behind that ubiquitous candidate \
+phrase,\"excellent communication skills\", a required writing sample \
+is invaluable.\n\
+\n\
+Writing samples need the same care and attention given to cover \
+letters and resumes. Candidates with otherwise impeccable credentials \
+are routinely eliminated by a poorly chosen writing sample. Notice I \
+said \"poorly chosen\" not \"poorly written.\" Because that's the rub: \
+a writing sample not only reveals the individual's writing skills, it \
+also offers a peek into what they consider important or relevant for \
+the position. If you miss that mark with your writing sample, don't \
+expect to get a call for an interview.";
+
+    s1 = "So how do you tackle that writing sample request?";
+  }
+
+  FOR_EACH_RANGE (i, 0, iters) {
+    doNotOptimizeAway(s.find(s1));
+  }
+}
+BENCHMARK_PARAM(BENCHFUN(findUnsuccessful), 524288);
+
+void BENCHFUN(equality)(int iters, int arg) {
+  std::vector<STRING> haystack(arg);
+
+  BENCHMARK_SUSPEND {
+    for (auto& hay : haystack) {
+      randomBinaryString(&hay, 1024);
+    }
+  }
+
+  FOR_EACH_RANGE (i, 0, iters) {
+    STRING needle;
+    randomBinaryString(&needle, 1024);
+    doNotOptimizeAway(std::find(haystack.begin(), haystack.end(), needle));
+  }
+}
+BENCHMARK_PARAM(BENCHFUN(equality), 65536);
+
+void BENCHFUN(replace)(int iters, int arg) {
+  STRING s;
+  BENCHMARK_SUSPEND {
+    randomString(&s, arg);
+  }
+  FOR_EACH_RANGE (i, 0, iters) {
+    BenchmarkSuspender susp;
+    doNotOptimizeAway(&s);
+    auto const pos = random(0, s.size());
+    auto toRemove = random(0, s.size() - pos);
+    auto toInsert = random(0, arg);
+    STRING s1;
+    randomString(&s1, toInsert);
+    susp.dismiss();
+
+   s.replace(pos, toRemove, s1);
+  }
+}
+BENCHMARK_PARAM(BENCHFUN(replace), 256);
+
+void BENCHFUN(push_back)(int iters, int arg) {
+  FOR_EACH_RANGE (i, 0, iters) {
+    STRING s;
+    FOR_EACH_RANGE (j, 0, arg) {
+      s += ' ';
+    }
+  }
+}
+BENCHMARK_PARAM(BENCHFUN(push_back), 1);
+BENCHMARK_PARAM(BENCHFUN(push_back), 23);
+BENCHMARK_PARAM(BENCHFUN(push_back), 127);
+BENCHMARK_PARAM(BENCHFUN(push_back), 1024);
+
+void BENCHFUN(short_append)(int iters, int arg) {
+  FOR_EACH_RANGE (i, 0, iters) {
+    STRING s;
+    FOR_EACH_RANGE (j, 0, arg) {
+      s += "012";
+    }
+  }
+}
+BENCHMARK_PARAM(BENCHFUN(short_append), 23);
+BENCHMARK_PARAM(BENCHFUN(short_append), 1024);
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/FBStringTest.cpp
@@ -0,0 +1,1231 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+//
+// Author: andrei.alexandrescu@fb.com
+
+#include "folly/FBString.h"
+
+#include <cstdlib>
+
+#include <list>
+#include <fstream>
+#include <iomanip>
+#include <boost/algorithm/string.hpp>
+#include <boost/random.hpp>
+#include <gtest/gtest.h>
+
+#include <gflags/gflags.h>
+
+#include "folly/Foreach.h"
+#include "folly/Portability.h"
+#include "folly/Random.h"
+#include "folly/Conv.h"
+
+using namespace std;
+using namespace folly;
+
+static const int seed = folly::randomNumberSeed();
+typedef boost::mt19937 RandomT;
+static RandomT rng(seed);
+static const size_t maxString = 100;
+static const bool avoidAliasing = true;
+
+template <class Integral1, class Integral2>
+Integral2 random(Integral1 low, Integral2 up) {
+  boost::uniform_int<> range(low, up);
+  return range(rng);
+}
+
+template <class String>
+void randomString(String* toFill, unsigned int maxSize = 1000) {
+  assert(toFill);
+  toFill->resize(random(0, maxSize));
+  FOR_EACH (i, *toFill) {
+    *i = random('a', 'z');
+  }
+}
+
+template <class String, class Integral>
+void Num2String(String& str, Integral n) {
+
+  std::string tmp = folly::to<std::string>(n);
+  str = String(tmp.begin(), tmp.end());
+}
+
+std::list<char> RandomList(unsigned int maxSize) {
+  std::list<char> lst(random(0u, maxSize));
+  std::list<char>::iterator i = lst.begin();
+  for (; i != lst.end(); ++i) {
+    *i = random('a', 'z');
+ }
+  return lst;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Tests begin here
+////////////////////////////////////////////////////////////////////////////////
+
+template <class String> void clause11_21_4_2_a(String & test) {
+  test.String::~String();
+  new(&test) String();
+}
+template <class String> void clause11_21_4_2_b(String & test) {
+  String test2(test);
+  assert(test2 == test);
+}
+template <class String> void clause11_21_4_2_c(String & test) {
+  // Test move constructor. There is a more specialized test, see
+  // TEST(FBString, testMoveCtor)
+  String donor(test);
+  String test2(std::move(donor));
+  EXPECT_EQ(test2, test);
+  // Technically not required, but all implementations that actually
+  // support move will move large strings. Make a guess for 128 as the
+  // maximum small string optimization that's reasonable.
+  EXPECT_LE(donor.size(), 128);
+}
+template <class String> void clause11_21_4_2_d(String & test) {
+  // Copy constructor with position and length
+  const size_t pos = random(0, test.size());
+  String s(test, pos, random(0, 9)
+           ? random(0, (size_t)(test.size() - pos))
+           : String::npos); // test for npos, too, in 10% of the cases
+  test = s;
+}
+template <class String> void clause11_21_4_2_e(String & test) {
+  // Constructor from char*, size_t
+  const size_t
+    pos = random(0, test.size()),
+    n = random(0, test.size() - pos);
+  String before(test.data(), test.size());
+  String s(test.c_str() + pos, n);
+  String after(test.data(), test.size());
+  EXPECT_EQ(before, after);
+  test.swap(s);
+}
+template <class String> void clause11_21_4_2_f(String & test) {
+  // Constructor from char*
+  const size_t
+    pos = random(0, test.size()),
+    n = random(0, test.size() - pos);
+  String before(test.data(), test.size());
+  String s(test.c_str() + pos);
+  String after(test.data(), test.size());
+  EXPECT_EQ(before, after);
+  test.swap(s);
+}
+template <class String> void clause11_21_4_2_g(String & test) {
+  // Constructor from size_t, char
+  const size_t n = random(0, test.size());
+  const auto c = test.front();
+  test = String(n, c);
+}
+template <class String> void clause11_21_4_2_h(String & test) {
+  // Constructors from various iterator pairs
+  // Constructor from char*, char*
+  String s1(test.begin(), test.end());
+  EXPECT_EQ(test, s1);
+  String s2(test.data(), test.data() + test.size());
+  EXPECT_EQ(test, s2);
+  // Constructor from other iterators
+  std::list<char> lst;
+  for (auto c : test) lst.push_back(c);
+  String s3(lst.begin(), lst.end());
+  EXPECT_EQ(test, s3);
+  // Constructor from wchar_t iterators
+  std::list<wchar_t> lst1;
+  for (auto c : test) lst1.push_back(c);
+  String s4(lst1.begin(), lst1.end());
+  EXPECT_EQ(test, s4);
+  // Constructor from wchar_t pointers
+  wchar_t t[20];
+  t[0] = 'a';
+  t[1] = 'b';
+  fbstring s5(t, t + 2);;
+  EXPECT_EQ("ab", s5);
+}
+template <class String> void clause11_21_4_2_i(String & test) {
+  // From initializer_list<char>
+  std::initializer_list<typename String::value_type>
+    il = { 'h', 'e', 'l', 'l', 'o' };
+  String s(il);
+  test.swap(s);
+}
+template <class String> void clause11_21_4_2_j(String & test) {
+  // Assignment from const String&
+  auto size = random(0, 2000);
+  String s(size, '\0');
+  EXPECT_EQ(s.size(), size);
+  FOR_EACH_RANGE (i, 0, s.size()) {
+    s[i] = random('a', 'z');
+  }
+  test = s;
+}
+template <class String> void clause11_21_4_2_k(String & test) {
+  // Assignment from String&&
+  auto size = random(0, 2000);
+  String s(size, '\0');
+  EXPECT_EQ(s.size(), size);
+  FOR_EACH_RANGE (i, 0, s.size()) {
+    s[i] = random('a', 'z');
+  }
+  test = std::move(s);
+  if (typeid(String) == typeid(fbstring)) {
+    EXPECT_LE(s.size(), 128);
+  }
+}
+template <class String> void clause11_21_4_2_l(String & test) {
+  // Assignment from char*
+  String s(random(0, 1000), '\0');
+  size_t i = 0;
+  for (; i != s.size(); ++i) {
+    s[i] = random('a', 'z');
+  }
+  test = s.c_str();
+}
+template <class String> void clause11_21_4_2_lprime(String & test) {
+  // Aliased assign
+  const size_t pos = random(0, test.size());
+  if (avoidAliasing) {
+    test = String(test.c_str() + pos);
+  } else {
+    test = test.c_str() + pos;
+  }
+}
+template <class String> void clause11_21_4_2_m(String & test) {
+  // Assignment from char
+  test = random('a', 'z');
+}
+template <class String> void clause11_21_4_2_n(String & test) {
+  // Assignment from initializer_list<char>
+  initializer_list<typename String::value_type>
+    il = { 'h', 'e', 'l', 'l', 'o' };
+  test = il;
+}
+
+template <class String> void clause11_21_4_3(String & test) {
+  // Iterators. The code below should leave test unchanged
+  EXPECT_EQ(test.size(), test.end() - test.begin());
+  EXPECT_EQ(test.size(), test.rend() - test.rbegin());
+  EXPECT_EQ(test.size(), test.cend() - test.cbegin());
+  EXPECT_EQ(test.size(), test.crend() - test.crbegin());
+
+  auto s = test.size();
+  test.resize(test.end() - test.begin());
+  EXPECT_EQ(s, test.size());
+  test.resize(test.rend() - test.rbegin());
+  EXPECT_EQ(s, test.size());
+}
+
+template <class String> void clause11_21_4_4(String & test) {
+  // exercise capacity, size, max_size
+  EXPECT_EQ(test.size(), test.length());
+  EXPECT_LE(test.size(), test.max_size());
+  EXPECT_LE(test.capacity(), test.max_size());
+  EXPECT_LE(test.size(), test.capacity());
+
+  // exercise shrink_to_fit. Nonbinding request so we can't really do
+  // much beyond calling it.
+  auto copy = test;
+  copy.reserve(copy.capacity() * 3);
+  copy.shrink_to_fit();
+  EXPECT_EQ(copy, test);
+
+  // exercise empty
+  string empty("empty");
+  string notempty("not empty");
+  if (test.empty()) test = String(empty.begin(), empty.end());
+  else test = String(notempty.begin(), notempty.end());
+}
+
+template <class String> void clause11_21_4_5(String & test) {
+  // exercise element access
+  if (!test.empty()) {
+    EXPECT_EQ(test[0], test.front());
+    EXPECT_EQ(test[test.size() - 1], test.back());
+    auto const i = random(0, test.size() - 1);
+    EXPECT_EQ(test[i], test.at(i));
+    test = test[i];
+  }
+}
+
+template <class String> void clause11_21_4_6_1(String & test) {
+  // 21.3.5 modifiers (+=)
+  String test1;
+  randomString(&test1);
+  assert(test1.size() == char_traits
+      <typename String::value_type>::length(test1.c_str()));
+  auto len = test.size();
+  test += test1;
+  EXPECT_EQ(test.size(), test1.size() + len);
+  FOR_EACH_RANGE (i, 0, test1.size()) {
+    EXPECT_EQ(test[len + i], test1[i]);
+  }
+  // aliasing modifiers
+  String test2 = test;
+  auto dt = test2.data();
+  auto sz = test.c_str();
+  len = test.size();
+  EXPECT_EQ(memcmp(sz, dt, len), 0);
+  String copy(test.data(), test.size());
+  EXPECT_EQ(char_traits
+      <typename String::value_type>::length(test.c_str()), len);
+  test += test;
+  //test.append(test);
+  EXPECT_EQ(test.size(), 2 * len);
+  EXPECT_EQ(char_traits
+      <typename String::value_type>::length(test.c_str()), 2 * len);
+  FOR_EACH_RANGE (i, 0, len) {
+    EXPECT_EQ(test[i], copy[i]);
+    EXPECT_EQ(test[i], test[len + i]);
+  }
+  len = test.size();
+  EXPECT_EQ(char_traits
+      <typename String::value_type>::length(test.c_str()), len);
+  // more aliasing
+  auto const pos = random(0, test.size());
+  EXPECT_EQ(char_traits
+      <typename String::value_type>::length(test.c_str() + pos), len - pos);
+  if (avoidAliasing) {
+    String addMe(test.c_str() + pos);
+    EXPECT_EQ(addMe.size(), len - pos);
+    test += addMe;
+  } else {
+    test += test.c_str() + pos;
+  }
+  EXPECT_EQ(test.size(), 2 * len - pos);
+  // single char
+  len = test.size();
+  test += random('a', 'z');
+  EXPECT_EQ(test.size(), len + 1);
+  // initializer_list
+  initializer_list<typename String::value_type> il { 'a', 'b', 'c' };
+  test += il;
+}
+
+template <class String> void clause11_21_4_6_2(String & test) {
+  // 21.3.5 modifiers (append, push_back)
+  String s;
+
+  // Test with a small string first
+  char c = random('a', 'z');
+  s.push_back(c);
+  EXPECT_EQ(s[s.size() - 1], c);
+  EXPECT_EQ(s.size(), 1);
+  s.resize(s.size() - 1);
+
+  randomString(&s, maxString);
+  test.append(s);
+  randomString(&s, maxString);
+  test.append(s, random(0, s.size()), random(0, maxString));
+  randomString(&s, maxString);
+  test.append(s.c_str(), random(0, s.size()));
+  randomString(&s, maxString);
+  test.append(s.c_str());
+  test.append(random(0, maxString), random('a', 'z'));
+  std::list<char> lst(RandomList(maxString));
+  test.append(lst.begin(), lst.end());
+  c = random('a', 'z');
+  test.push_back(c);
+  EXPECT_EQ(test[test.size() - 1], c);
+  // initializer_list
+  initializer_list<typename String::value_type> il { 'a', 'b', 'c' };
+  test.append(il);
+}
+
+template <class String> void clause11_21_4_6_3_a(String & test) {
+  // assign
+  String s;
+  randomString(&s);
+  test.assign(s);
+  EXPECT_EQ(test, s);
+  // move assign
+  test.assign(std::move(s));
+  if (typeid(String) == typeid(fbstring)) {
+    EXPECT_LE(s.size(), 128);
+  }
+}
+
+template <class String> void clause11_21_4_6_3_b(String & test) {
+  // assign
+  String s;
+  randomString(&s, maxString);
+  test.assign(s, random(0, s.size()), random(0, maxString));
+}
+
+template <class String> void clause11_21_4_6_3_c(String & test) {
+  // assign
+  String s;
+  randomString(&s, maxString);
+  test.assign(s.c_str(), random(0, s.size()));
+}
+
+template <class String> void clause11_21_4_6_3_d(String & test) {
+  // assign
+  String s;
+  randomString(&s, maxString);
+  test.assign(s.c_str());
+}
+
+template <class String> void clause11_21_4_6_3_e(String & test) {
+  // assign
+  String s;
+  randomString(&s, maxString);
+  test.assign(random(0, maxString), random('a', 'z'));
+}
+
+template <class String> void clause11_21_4_6_3_f(String & test) {
+  // assign from bidirectional iterator
+  std::list<char> lst(RandomList(maxString));
+  test.assign(lst.begin(), lst.end());
+}
+
+template <class String> void clause11_21_4_6_3_g(String & test) {
+  // assign from aliased source
+  test.assign(test);
+}
+
+template <class String> void clause11_21_4_6_3_h(String & test) {
+  // assign from aliased source
+  test.assign(test, random(0, test.size()), random(0, maxString));
+}
+
+template <class String> void clause11_21_4_6_3_i(String & test) {
+  // assign from aliased source
+  test.assign(test.c_str(), random(0, test.size()));
+}
+
+template <class String> void clause11_21_4_6_3_j(String & test) {
+  // assign from aliased source
+  test.assign(test.c_str());
+}
+
+template <class String> void clause11_21_4_6_3_k(String & test) {
+  // assign from initializer_list
+  initializer_list<typename String::value_type> il { 'a', 'b', 'c' };
+  test.assign(il);
+}
+
+template <class String> void clause11_21_4_6_4(String & test) {
+  // insert
+  String s;
+  randomString(&s, maxString);
+  test.insert(random(0, test.size()), s);
+  randomString(&s, maxString);
+  test.insert(random(0, test.size()),
+              s, random(0, s.size()),
+              random(0, maxString));
+  randomString(&s, maxString);
+  test.insert(random(0, test.size()),
+              s.c_str(), random(0, s.size()));
+  randomString(&s, maxString);
+  test.insert(random(0, test.size()), s.c_str());
+  test.insert(random(0, test.size()),
+              random(0, maxString), random('a', 'z'));
+  typename String::size_type pos = random(0, test.size());
+  typename String::iterator res =
+    test.insert(test.begin() + pos, random('a', 'z'));
+  EXPECT_EQ(res - test.begin(), pos);
+  std::list<char> lst(RandomList(maxString));
+  pos = random(0, test.size());
+  // Uncomment below to see a bug in gcc
+  /*res = */test.insert(test.begin() + pos, lst.begin(), lst.end());
+  // insert from initializer_list
+  initializer_list<typename String::value_type> il { 'a', 'b', 'c' };
+  pos = random(0, test.size());
+  // Uncomment below to see a bug in gcc
+  /*res = */test.insert(test.begin() + pos, il);
+
+  // Test with actual input iterators
+  stringstream ss;
+  ss << "hello cruel world";
+  auto i = istream_iterator<char>(ss);
+  test.insert(test.begin(), i, istream_iterator<char>());
+}
+
+template <class String> void clause11_21_4_6_5(String & test) {
+  // erase and pop_back
+  if (!test.empty()) {
+    test.erase(random(0, test.size()), random(0, maxString));
+  }
+  if (!test.empty()) {
+    // TODO: is erase(end()) allowed?
+    test.erase(test.begin() + random(0, test.size() - 1));
+  }
+  if (!test.empty()) {
+    auto const i = test.begin() + random(0, test.size());
+    if (i != test.end()) {
+      test.erase(i, i + random(0, size_t(test.end() - i)));
+    }
+  }
+  if (!test.empty()) {
+    // Can't test pop_back with std::string, doesn't support it yet.
+    //test.pop_back();
+  }
+}
+
+template <class String> void clause11_21_4_6_6(String & test) {
+  auto pos = random(0, test.size());
+  if (avoidAliasing) {
+    test.replace(pos, random(0, test.size() - pos),
+                 String(test));
+  } else {
+    test.replace(pos, random(0, test.size() - pos), test);
+  }
+  pos = random(0, test.size());
+  String s;
+  randomString(&s, maxString);
+  test.replace(pos, pos + random(0, test.size() - pos), s);
+  auto pos1 = random(0, test.size());
+  auto pos2 = random(0, test.size());
+  if (avoidAliasing) {
+    test.replace(pos1, pos1 + random(0, test.size() - pos1),
+                 String(test),
+                 pos2, pos2 + random(0, test.size() - pos2));
+  } else {
+    test.replace(pos1, pos1 + random(0, test.size() - pos1),
+                 test, pos2, pos2 + random(0, test.size() - pos2));
+  }
+  pos1 = random(0, test.size());
+  String str;
+  randomString(&str, maxString);
+  pos2 = random(0, str.size());
+  test.replace(pos1, pos1 + random(0, test.size() - pos1),
+               str, pos2, pos2 + random(0, str.size() - pos2));
+  pos = random(0, test.size());
+  if (avoidAliasing) {
+    test.replace(pos, random(0, test.size() - pos),
+                 String(test).c_str(), test.size());
+  } else {
+    test.replace(pos, random(0, test.size() - pos),
+                 test.c_str(), test.size());
+  }
+  pos = random(0, test.size());
+  randomString(&str, maxString);
+  test.replace(pos, pos + random(0, test.size() - pos),
+               str.c_str(), str.size());
+  pos = random(0, test.size());
+  randomString(&str, maxString);
+  test.replace(pos, pos + random(0, test.size() - pos),
+               str.c_str());
+  pos = random(0, test.size());
+  test.replace(pos, random(0, test.size() - pos),
+               random(0, maxString), random('a', 'z'));
+  pos = random(0, test.size());
+  if (avoidAliasing) {
+    auto newString = String(test);
+    test.replace(
+      test.begin() + pos,
+      test.begin() + pos + random(0, test.size() - pos),
+      newString);
+  } else {
+    test.replace(
+      test.begin() + pos,
+      test.begin() + pos + random(0, test.size() - pos),
+      test);
+  }
+  pos = random(0, test.size());
+  if (avoidAliasing) {
+    auto newString = String(test);
+    test.replace(
+      test.begin() + pos,
+      test.begin() + pos + random(0, test.size() - pos),
+      newString.c_str(),
+      test.size() - random(0, test.size()));
+  } else {
+    test.replace(
+      test.begin() + pos,
+      test.begin() + pos + random(0, test.size() - pos),
+      test.c_str(),
+      test.size() - random(0, test.size()));
+  }
+  pos = random(0, test.size());
+  auto const n = random(0, test.size() - pos);
+  typename String::iterator b = test.begin();
+  String str1;
+  randomString(&str1, maxString);
+  const String & str3 = str1;
+  const typename String::value_type* ss = str3.c_str();
+  test.replace(
+    b + pos,
+    b + pos + n,
+    ss);
+  pos = random(0, test.size());
+  test.replace(
+    test.begin() + pos,
+    test.begin() + pos + random(0, test.size() - pos),
+    random(0, maxString), random('a', 'z'));
+}
+
+template <class String> void clause11_21_4_6_7(String & test) {
+  std::vector<typename String::value_type>
+    vec(random(0, maxString));
+  test.copy(
+    &vec[0],
+    vec.size(),
+    random(0, test.size()));
+}
+
+template <class String> void clause11_21_4_6_8(String & test) {
+  String s;
+  randomString(&s, maxString);
+  s.swap(test);
+}
+
+template <class String> void clause11_21_4_7_1(String & test) {
+  // 21.3.6 string operations
+  // exercise c_str() and data()
+  assert(test.c_str() == test.data());
+  // exercise get_allocator()
+  String s;
+  randomString(&s, maxString);
+  assert(test.get_allocator() == s.get_allocator());
+}
+
+template <class String> void clause11_21_4_7_2_a(String & test) {
+  String str = test.substr(
+    random(0, test.size()),
+    random(0, test.size()));
+  Num2String(test, test.find(str, random(0, test.size())));
+}
+
+template <class String> void clause11_21_4_7_2_b(String & test) {
+  auto from = random(0, test.size());
+  auto length = random(0, test.size() - from);
+  String str = test.substr(from, length);
+  Num2String(test, test.find(str.c_str(),
+                             random(0, test.size()),
+                             random(0, str.size())));
+}
+
+template <class String> void clause11_21_4_7_2_c(String & test) {
+  String str = test.substr(
+    random(0, test.size()),
+    random(0, test.size()));
+  Num2String(test, test.find(str.c_str(),
+                             random(0, test.size())));
+}
+
+template <class String> void clause11_21_4_7_2_d(String & test) {
+  Num2String(test, test.find(
+               random('a', 'z'),
+               random(0, test.size())));
+}
+
+template <class String> void clause11_21_4_7_3_a(String & test) {
+  String str = test.substr(
+    random(0, test.size()),
+    random(0, test.size()));
+  Num2String(test, test.rfind(str, random(0, test.size())));
+}
+
+template <class String> void clause11_21_4_7_3_b(String & test) {
+  String str = test.substr(
+    random(0, test.size()),
+    random(0, test.size()));
+  Num2String(test, test.rfind(str.c_str(),
+                              random(0, test.size()),
+                              random(0, str.size())));
+}
+
+template <class String> void clause11_21_4_7_3_c(String & test) {
+  String str = test.substr(
+    random(0, test.size()),
+    random(0, test.size()));
+  Num2String(test, test.rfind(str.c_str(),
+                              random(0, test.size())));
+}
+
+template <class String> void clause11_21_4_7_3_d(String & test) {
+  Num2String(test, test.rfind(
+               random('a', 'z'),
+               random(0, test.size())));
+}
+
+template <class String> void clause11_21_4_7_4_a(String & test) {
+  String str;
+  randomString(&str, maxString);
+  Num2String(test, test.find_first_of(str,
+                                      random(0, test.size())));
+}
+
+template <class String> void clause11_21_4_7_4_b(String & test) {
+  String str;
+  randomString(&str, maxString);
+  Num2String(test, test.find_first_of(str.c_str(),
+                                      random(0, test.size()),
+                                      random(0, str.size())));
+}
+
+template <class String> void clause11_21_4_7_4_c(String & test) {
+  String str;
+  randomString(&str, maxString);
+  Num2String(test, test.find_first_of(str.c_str(),
+                                      random(0, test.size())));
+}
+
+template <class String> void clause11_21_4_7_4_d(String & test) {
+  Num2String(test, test.find_first_of(
+               random('a', 'z'),
+               random(0, test.size())));
+}
+
+template <class String> void clause11_21_4_7_5_a(String & test) {
+  String str;
+  randomString(&str, maxString);
+  Num2String(test, test.find_last_of(str,
+                                     random(0, test.size())));
+}
+
+template <class String> void clause11_21_4_7_5_b(String & test) {
+  String str;
+  randomString(&str, maxString);
+  Num2String(test, test.find_last_of(str.c_str(),
+                                     random(0, test.size()),
+                                     random(0, str.size())));
+}
+
+template <class String> void clause11_21_4_7_5_c(String & test) {
+  String str;
+  randomString(&str, maxString);
+  Num2String(test, test.find_last_of(str.c_str(),
+                                     random(0, test.size())));
+}
+
+template <class String> void clause11_21_4_7_5_d(String & test) {
+  Num2String(test, test.find_last_of(
+               random('a', 'z'),
+               random(0, test.size())));
+}
+
+template <class String> void clause11_21_4_7_6_a(String & test) {
+  String str;
+  randomString(&str, maxString);
+  Num2String(test, test.find_first_not_of(str,
+                                          random(0, test.size())));
+}
+
+template <class String> void clause11_21_4_7_6_b(String & test) {
+  String str;
+  randomString(&str, maxString);
+  Num2String(test, test.find_first_not_of(str.c_str(),
+                                          random(0, test.size()),
+                                          random(0, str.size())));
+}
+
+template <class String> void clause11_21_4_7_6_c(String & test) {
+  String str;
+  randomString(&str, maxString);
+  Num2String(test, test.find_first_not_of(str.c_str(),
+                                          random(0, test.size())));
+}
+
+template <class String> void clause11_21_4_7_6_d(String & test) {
+  Num2String(test, test.find_first_not_of(
+               random('a', 'z'),
+               random(0, test.size())));
+}
+
+template <class String> void clause11_21_4_7_7_a(String & test) {
+  String str;
+  randomString(&str, maxString);
+  Num2String(test, test.find_last_not_of(str,
+                                         random(0, test.size())));
+}
+
+template <class String> void clause11_21_4_7_7_b(String & test) {
+  String str;
+  randomString(&str, maxString);
+  Num2String(test, test.find_last_not_of(str.c_str(),
+                                         random(0, test.size()),
+                                         random(0, str.size())));
+}
+
+template <class String> void clause11_21_4_7_7_c(String & test) {
+  String str;
+  randomString(&str, maxString);
+  Num2String(test, test.find_last_not_of(str.c_str(),
+                                         random(0, test.size())));
+}
+
+template <class String> void clause11_21_4_7_7_d(String & test) {
+  Num2String(test, test.find_last_not_of(
+               random('a', 'z'),
+               random(0, test.size())));
+}
+
+template <class String> void clause11_21_4_7_8(String & test) {
+  test = test.substr(random(0, test.size()), random(0, test.size()));
+}
+
+template <class String> void clause11_21_4_7_9_a(String & test) {
+  String s;
+  randomString(&s, maxString);
+  int tristate = test.compare(s);
+  if (tristate > 0) tristate = 1;
+  else if (tristate < 0) tristate = 2;
+  Num2String(test, tristate);
+}
+
+template <class String> void clause11_21_4_7_9_b(String & test) {
+  String s;
+  randomString(&s, maxString);
+  int tristate = test.compare(
+    random(0, test.size()),
+    random(0, test.size()),
+    s);
+  if (tristate > 0) tristate = 1;
+  else if (tristate < 0) tristate = 2;
+  Num2String(test, tristate);
+}
+
+template <class String> void clause11_21_4_7_9_c(String & test) {
+  String str;
+  randomString(&str, maxString);
+  int tristate = test.compare(
+    random(0, test.size()),
+    random(0, test.size()),
+    str,
+    random(0, str.size()),
+    random(0, str.size()));
+  if (tristate > 0) tristate = 1;
+  else if (tristate < 0) tristate = 2;
+  Num2String(test, tristate);
+}
+
+template <class String> void clause11_21_4_7_9_d(String & test) {
+  String s;
+  randomString(&s, maxString);
+  int tristate = test.compare(s.c_str());
+  if (tristate > 0) tristate = 1;
+  else if (tristate < 0) tristate = 2;
+                Num2String(test, tristate);
+}
+
+template <class String> void clause11_21_4_7_9_e(String & test) {
+  String str;
+  randomString(&str, maxString);
+  int tristate = test.compare(
+    random(0, test.size()),
+    random(0, test.size()),
+    str.c_str(),
+    random(0, str.size()));
+  if (tristate > 0) tristate = 1;
+  else if (tristate < 0) tristate = 2;
+  Num2String(test, tristate);
+}
+
+template <class String> void clause11_21_4_8_1_a(String & test) {
+  String s1;
+  randomString(&s1, maxString);
+  String s2;
+  randomString(&s2, maxString);
+  test = s1 + s2;
+}
+
+template <class String> void clause11_21_4_8_1_b(String & test) {
+  String s;
+  randomString(&s, maxString);
+  String s1;
+  randomString(&s1, maxString);
+  test = s.c_str() + s1;
+}
+
+template <class String> void clause11_21_4_8_1_c(String & test) {
+  String s;
+  randomString(&s, maxString);
+  test = typename String::value_type(random('a', 'z')) + s;
+}
+
+template <class String> void clause11_21_4_8_1_d(String & test) {
+  String s;
+  randomString(&s, maxString);
+  String s1;
+  randomString(&s1, maxString);
+  test = s + s1.c_str();
+}
+
+template <class String> void clause11_21_4_8_1_e(String & test) {
+  String s;
+  randomString(&s, maxString);
+  String s1;
+  randomString(&s1, maxString);
+  test = s + s1.c_str();
+}
+
+template <class String> void clause11_21_4_8_1_f(String & test) {
+  String s;
+  randomString(&s, maxString);
+  test = s + typename String::value_type(random('a', 'z'));
+}
+
+// Numbering here is from C++11
+template <class String> void clause11_21_4_8_9_a(String & test) {
+  basic_stringstream<typename String::value_type> stst(test.c_str());
+  String str;
+  while (stst) {
+    stst >> str;
+    test += str + test;
+  }
+}
+
+TEST(FBString, testAllClauses) {
+  EXPECT_TRUE(1) << "Starting with seed: " << seed;
+  std::string r;
+  std::wstring wr;
+  folly::fbstring c;
+  folly::basic_fbstring<wchar_t> wc;
+#define TEST_CLAUSE(x)                                              \
+  do {                                                              \
+      if (1) {} else EXPECT_TRUE(1) << "Testing clause " << #x;     \
+      randomString(&r);                                             \
+      c = r;                                                        \
+      EXPECT_EQ(c, r);                                              \
+      wr = std::wstring(r.begin(), r.end());                        \
+      wc = folly::basic_fbstring<wchar_t>(wr.c_str());              \
+      auto localSeed = seed + count;                                \
+      rng = RandomT(localSeed);                                     \
+      clause11_##x(r);                                                \
+      rng = RandomT(localSeed);                                     \
+      clause11_##x(c);                                                \
+      EXPECT_EQ(r, c)                                               \
+        << "Lengths: " << r.size() << " vs. " << c.size()           \
+        << "\nReference: '" << r << "'"                             \
+        << "\nActual:    '" << c.data()[0] << "'";                  \
+      rng = RandomT(localSeed);                                     \
+      clause11_##x(wc);                                               \
+      int wret = wcslen(wc.c_str());                                \
+      char mb[wret+1];                                              \
+      int ret = wcstombs(mb, wc.c_str(), sizeof(mb));               \
+      if (ret == wret) mb[wret] = '\0';                             \
+      const char *mc = c.c_str();                                   \
+      std::string one(mb);                                          \
+      std::string two(mc);                                          \
+      EXPECT_EQ(one, two);                                          \
+    } while (++count % 100 != 0)
+
+  int count = 0;
+  TEST_CLAUSE(21_4_2_a);
+  TEST_CLAUSE(21_4_2_b);
+  TEST_CLAUSE(21_4_2_c);
+  TEST_CLAUSE(21_4_2_d);
+  TEST_CLAUSE(21_4_2_e);
+  TEST_CLAUSE(21_4_2_f);
+  TEST_CLAUSE(21_4_2_g);
+  TEST_CLAUSE(21_4_2_h);
+  TEST_CLAUSE(21_4_2_i);
+  TEST_CLAUSE(21_4_2_j);
+  TEST_CLAUSE(21_4_2_k);
+  TEST_CLAUSE(21_4_2_l);
+  TEST_CLAUSE(21_4_2_lprime);
+  TEST_CLAUSE(21_4_2_m);
+  TEST_CLAUSE(21_4_2_n);
+  TEST_CLAUSE(21_4_3);
+  TEST_CLAUSE(21_4_4);
+  TEST_CLAUSE(21_4_5);
+  TEST_CLAUSE(21_4_6_1);
+  TEST_CLAUSE(21_4_6_2);
+  TEST_CLAUSE(21_4_6_3_a);
+  TEST_CLAUSE(21_4_6_3_b);
+  TEST_CLAUSE(21_4_6_3_c);
+  TEST_CLAUSE(21_4_6_3_d);
+  TEST_CLAUSE(21_4_6_3_e);
+  TEST_CLAUSE(21_4_6_3_f);
+  TEST_CLAUSE(21_4_6_3_g);
+  TEST_CLAUSE(21_4_6_3_h);
+  TEST_CLAUSE(21_4_6_3_i);
+  TEST_CLAUSE(21_4_6_3_j);
+  TEST_CLAUSE(21_4_6_3_k);
+  TEST_CLAUSE(21_4_6_4);
+  TEST_CLAUSE(21_4_6_5);
+  TEST_CLAUSE(21_4_6_6);
+  TEST_CLAUSE(21_4_6_7);
+  TEST_CLAUSE(21_4_6_8);
+  TEST_CLAUSE(21_4_7_1);
+
+  TEST_CLAUSE(21_4_7_2_a);
+  TEST_CLAUSE(21_4_7_2_b);
+  TEST_CLAUSE(21_4_7_2_c);
+  TEST_CLAUSE(21_4_7_2_d);
+  TEST_CLAUSE(21_4_7_3_a);
+  TEST_CLAUSE(21_4_7_3_b);
+  TEST_CLAUSE(21_4_7_3_c);
+  TEST_CLAUSE(21_4_7_3_d);
+  TEST_CLAUSE(21_4_7_4_a);
+  TEST_CLAUSE(21_4_7_4_b);
+  TEST_CLAUSE(21_4_7_4_c);
+  TEST_CLAUSE(21_4_7_4_d);
+  TEST_CLAUSE(21_4_7_5_a);
+  TEST_CLAUSE(21_4_7_5_b);
+  TEST_CLAUSE(21_4_7_5_c);
+  TEST_CLAUSE(21_4_7_5_d);
+  TEST_CLAUSE(21_4_7_6_a);
+  TEST_CLAUSE(21_4_7_6_b);
+  TEST_CLAUSE(21_4_7_6_c);
+  TEST_CLAUSE(21_4_7_6_d);
+  TEST_CLAUSE(21_4_7_7_a);
+  TEST_CLAUSE(21_4_7_7_b);
+  TEST_CLAUSE(21_4_7_7_c);
+  TEST_CLAUSE(21_4_7_7_d);
+  TEST_CLAUSE(21_4_7_8);
+  TEST_CLAUSE(21_4_7_9_a);
+  TEST_CLAUSE(21_4_7_9_b);
+  TEST_CLAUSE(21_4_7_9_c);
+  TEST_CLAUSE(21_4_7_9_d);
+  TEST_CLAUSE(21_4_7_9_e);
+  TEST_CLAUSE(21_4_8_1_a);
+  TEST_CLAUSE(21_4_8_1_b);
+  TEST_CLAUSE(21_4_8_1_c);
+  TEST_CLAUSE(21_4_8_1_d);
+  TEST_CLAUSE(21_4_8_1_e);
+  TEST_CLAUSE(21_4_8_1_f);
+  TEST_CLAUSE(21_4_8_9_a);
+}
+
+TEST(FBString, testGetline) {
+  fbstring s1 = "\
+Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras accumsan \n\
+elit ut urna consectetur in sagittis mi auctor. Nulla facilisi. In nec \n\
+dolor leo, vitae imperdiet neque. Donec ut erat mauris, a faucibus \n\
+elit. Integer consectetur gravida augue, sit amet mattis mauris auctor \n\
+sed. Morbi congue libero eu nunc sodales adipiscing. In lectus nunc, \n\
+vulputate a fringilla at, venenatis quis justo. Proin eu velit \n\
+nibh. Maecenas vitae tellus eros. Pellentesque habitant morbi \n\
+tristique senectus et netus et malesuada fames ac turpis \n\
+egestas. Vivamus faucibus feugiat consequat. Donec fermentum neque sit \n\
+amet ligula suscipit porta. Phasellus facilisis felis in purus luctus \n\
+quis posuere leo tempor. Nam nunc purus, luctus a pharetra ut, \n\
+placerat at dui. Donec imperdiet, diam quis convallis pulvinar, dui \n\
+est commodo lorem, ut tincidunt diam nibh et nibh. Maecenas nec velit \n\
+massa, ut accumsan magna. Donec imperdiet tempor nisi et \n\
+laoreet. Phasellus lectus quam, ultricies ut tincidunt in, dignissim \n\
+id eros. Mauris vulputate tortor nec neque pellentesque sagittis quis \n\
+sed nisl. In diam lacus, lobortis ut posuere nec, ornare id quam.";
+  char f[] = "/tmp/fbstring_testing.XXXXXX";
+  int fd = mkstemp(f);
+  EXPECT_TRUE(fd > 0);
+  if (fd > 0) {
+    close(fd);  // Yeah
+    std::ofstream out(f);
+    if (!(out << s1)) {
+      EXPECT_TRUE(0) << "Couldn't write to temp file.";
+      return;
+    }
+  }
+  vector<fbstring> v;
+  boost::split(v, s1, boost::is_any_of("\n"));
+  {
+    ifstream input(f);
+    fbstring line;
+    FOR_EACH (i, v) {
+      EXPECT_TRUE(!getline(input, line).fail());
+      EXPECT_EQ(line, *i);
+    }
+  }
+  unlink(f);
+}
+
+TEST(FBString, testMoveCtor) {
+  // Move constructor. Make sure we allocate a large string, so the
+  // small string optimization doesn't kick in.
+  auto size = random(100, 2000);
+  fbstring s(size, 'a');
+  fbstring test = std::move(s);
+  EXPECT_TRUE(s.empty());
+  EXPECT_EQ(size, test.size());
+}
+
+TEST(FBString, testMoveAssign) {
+  // Move constructor. Make sure we allocate a large string, so the
+  // small string optimization doesn't kick in.
+  auto size = random(100, 2000);
+  fbstring s(size, 'a');
+  fbstring test;
+  test = std::move(s);
+  EXPECT_TRUE(s.empty());
+  EXPECT_EQ(size, test.size());
+}
+
+TEST(FBString, testMoveOperatorPlusLhs) {
+  // Make sure we allocate a large string, so the
+  // small string optimization doesn't kick in.
+  auto size1 = random(100, 2000);
+  auto size2 = random(100, 2000);
+  fbstring s1(size1, 'a');
+  fbstring s2(size2, 'b');
+  fbstring test;
+  test = std::move(s1) + s2;
+  EXPECT_TRUE(s1.empty());
+  EXPECT_EQ(size1 + size2, test.size());
+}
+
+TEST(FBString, testMoveOperatorPlusRhs) {
+  // Make sure we allocate a large string, so the
+  // small string optimization doesn't kick in.
+  auto size1 = random(100, 2000);
+  auto size2 = random(100, 2000);
+  fbstring s1(size1, 'a');
+  fbstring s2(size2, 'b');
+  fbstring test;
+  test = s1 + std::move(s2);
+  EXPECT_EQ(size1 + size2, test.size());
+}
+
+// The GNU C++ standard library throws an std::logic_error when an std::string
+// is constructed with a null pointer. Verify that we mirror this behavior.
+//
+// N.B. We behave this way even if the C++ library being used is something
+//      other than libstdc++. Someday if we deem it important to present
+//      identical undefined behavior for other platforms, we can re-visit this.
+TEST(FBString, testConstructionFromLiteralZero) {
+  EXPECT_THROW(fbstring s(0), std::logic_error);
+}
+
+TEST(FBString, testFixedBugs) {
+  { // D479397
+    fbstring str(1337, 'f');
+    fbstring cp = str;
+    cp.clear();
+    cp.c_str();
+    EXPECT_EQ(str.front(), 'f');
+  }
+  { // D481173, --extra-cxxflags=-DFBSTRING_CONSERVATIVE
+    fbstring str(1337, 'f');
+    for (int i = 0; i < 2; ++i) {
+      fbstring cp = str;
+      cp[1] = 'b';
+      EXPECT_EQ(cp.c_str()[cp.size()], '\0');
+      cp.push_back('?');
+    }
+  }
+  { // D580267
+    {
+      fbstring str(1337, 'f');
+      fbstring cp = str;
+      cp.push_back('f');
+    }
+    {
+      fbstring str(1337, 'f');
+      fbstring cp = str;
+      cp += "bb";
+    }
+  }
+  { // D661622
+    folly::basic_fbstring<wchar_t> s;
+    EXPECT_EQ(0, s.size());
+  }
+  { // D785057
+    fbstring str(1337, 'f');
+    std::swap(str, str);
+    EXPECT_EQ(1337, str.size());
+  }
+  { // D1012196, --allocator=malloc
+    fbstring str(128, 'f');
+    str.clear();  // Empty medium string.
+    fbstring copy(str);  // Medium string of 0 capacity.
+    copy.push_back('b');
+    EXPECT_GE(copy.capacity(), 1);
+  }
+}
+
+TEST(FBString, findWithNpos) {
+  fbstring fbstr("localhost:80");
+  EXPECT_EQ(fbstring::npos, fbstr.find(":", fbstring::npos));
+}
+
+TEST(FBString, testHash) {
+  fbstring a;
+  fbstring b;
+  a.push_back(0);
+  a.push_back(1);
+  b.push_back(0);
+  b.push_back(2);
+  std::hash<fbstring> hashfunc;
+  EXPECT_NE(hashfunc(a), hashfunc(b));
+}
+
+TEST(FBString, testFrontBack) {
+  fbstring str("hello");
+  EXPECT_EQ(str.front(), 'h');
+  EXPECT_EQ(str.back(), 'o');
+  str.front() = 'H';
+  EXPECT_EQ(str.front(), 'H');
+  str.back() = 'O';
+  EXPECT_EQ(str.back(), 'O');
+  EXPECT_EQ(str, "HellO");
+}
+
+TEST(FBString, noexcept) {
+  EXPECT_TRUE(noexcept(fbstring()));
+  // std::move is not marked noexcept in gcc 4.6, sigh
+#if __GNUC_PREREQ(4, 7)
+  fbstring x;
+  EXPECT_FALSE(noexcept(fbstring(x)));
+  EXPECT_TRUE(noexcept(fbstring(std::move(x))));
+  fbstring y;
+  EXPECT_FALSE(noexcept(y = x));
+  EXPECT_TRUE(noexcept(y = std::move(x)));
+#endif
+}
+
+TEST(FBString, iomanip) {
+  stringstream ss;
+  fbstring fbstr("Hello");
+
+  ss << setw(6) << fbstr;
+  EXPECT_EQ(ss.str(), " Hello");
+  ss.str("");
+
+  ss << left << setw(6) << fbstr;
+  EXPECT_EQ(ss.str(), "Hello ");
+  ss.str("");
+
+  ss << right << setw(6) << fbstr;
+  EXPECT_EQ(ss.str(), " Hello");
+  ss.str("");
+
+  ss << setw(4) << fbstr;
+  EXPECT_EQ(ss.str(), "Hello");
+  ss.str("");
+
+  ss << setfill('^') << setw(6) << fbstr;
+  EXPECT_EQ(ss.str(), "^Hello");
+  ss.str("");
+}
+
+TEST(FBString, rvalueIterators) {
+  // you cannot take &* of a move-iterator, so use that for testing
+  fbstring s = "base";
+  fbstring r = "hello";
+  r.replace(r.begin(), r.end(),
+      make_move_iterator(s.begin()), make_move_iterator(s.end()));
+  EXPECT_EQ("base", r);
+
+  // The following test is probably not required by the standard.
+  // i.e. this could be in the realm of undefined behavior.
+  fbstring b = "123abcXYZ";
+  auto ait = b.begin() + 3;
+  auto Xit = b.begin() + 6;
+  b.replace(ait, b.end(), b.begin(), Xit);
+  EXPECT_EQ("123123abc", b); // if things go wrong, you'd get "123123123"
+}
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  return RUN_ALL_TESTS();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/FBVectorBenchmark.cpp
@@ -0,0 +1,112 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+//
+// Author: andrei.alexandrescu@fb.com
+
+#include "folly/Traits.h"
+#include "folly/Random.h"
+#include "folly/FBString.h"
+#include "folly/FBVector.h"
+#include "folly/Benchmark.h"
+
+#include <gflags/gflags.h>
+
+#include <gtest/gtest.h>
+#include <list>
+#include <memory>
+#include <boost/random.hpp>
+
+using namespace std;
+using namespace folly;
+
+auto static const seed = randomNumberSeed();
+typedef boost::mt19937 RandomT;
+static RandomT rng(seed);
+static const size_t maxString = 100;
+static const bool avoidAliasing = true;
+
+template <class Integral1, class Integral2>
+Integral2 random(Integral1 low, Integral2 up) {
+  boost::uniform_int<> range(low, up);
+  return range(rng);
+}
+
+template <class String>
+void randomString(String* toFill, unsigned int maxSize = 1000) {
+  assert(toFill);
+  toFill->resize(random(0, maxSize));
+  FOR_EACH (i, *toFill) {
+    *i = random('a', 'z');
+  }
+}
+
+template <class String, class Integral>
+void Num2String(String& str, Integral n) {
+  str.resize(10, '\0');
+  sprintf(&str[0], "%ul", 10);
+  str.resize(strlen(str.c_str()));
+}
+
+std::list<char> RandomList(unsigned int maxSize) {
+  std::list<char> lst(random(0u, maxSize));
+  std::list<char>::iterator i = lst.begin();
+  for (; i != lst.end(); ++i) {
+    *i = random('a', 'z');
+  }
+  return lst;
+}
+
+template<class T> T randomObject();
+
+template<> int randomObject<int>() {
+  return random(0, 1024);
+}
+
+template<> folly::fbstring randomObject<folly::fbstring>() {
+  folly::fbstring result;
+  randomString(&result);
+  return result;
+}
+
+#define CONCAT(A, B) CONCAT_HELPER(A, B)
+#define CONCAT_HELPER(A, B) A##B
+#define BENCHFUN(F) CONCAT(CONCAT(BM_, F), CONCAT(_, VECTOR))
+#define TESTFUN(F) TEST(fbvector, CONCAT(F, VECTOR))
+
+typedef vector<int> IntVector;
+typedef fbvector<int> IntFBVector;
+typedef vector<folly::fbstring> FBStringVector;
+typedef fbvector<folly::fbstring> FBStringFBVector;
+
+#define VECTOR IntVector
+#include "folly/test/FBVectorTestBenchmarks.cpp.h"
+#undef VECTOR
+#define VECTOR IntFBVector
+#include "folly/test/FBVectorTestBenchmarks.cpp.h"
+#undef VECTOR
+#define VECTOR FBStringVector
+#include "folly/test/FBVectorTestBenchmarks.cpp.h"
+#undef VECTOR
+#define VECTOR FBStringFBVector
+#include "folly/test/FBVectorTestBenchmarks.cpp.h"
+#undef VECTOR
+
+int main(int argc, char** argv) {
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  folly::runBenchmarks();
+  return 0;
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/FBVectorTestBenchmarks.cpp.h
@@ -0,0 +1,379 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * This file is supposed to be included from within
+ * FBVectorTest. Do not use otherwise.
+ */
+
+TESTFUN(clause_23_3_6_1_1) {
+  VECTOR v;
+  EXPECT_TRUE(v.empty());
+  VECTOR::allocator_type a;
+  VECTOR v1(a);
+  EXPECT_TRUE(v1.empty());
+}
+
+TESTFUN(clause_23_3_6_1_3) {
+  auto const n = random(0U, 10000U);
+  VECTOR v(n);
+  EXPECT_EQ(v.size(), n);
+  FOR_EACH (i, v) {
+    EXPECT_EQ(*i, VECTOR::value_type());
+  }
+}
+
+TESTFUN(clause_23_3_6_1_9) {
+  // Insert with iterators
+  list<VECTOR::value_type> lst;
+  auto const n = random(0U, 10000U);
+  FOR_EACH_RANGE (i, 0, n) {
+    lst.push_back(randomObject<VECTOR::value_type>());
+  }
+  VECTOR v(lst.begin(), lst.end());
+  EXPECT_EQ(v.size(), lst.size());
+  size_t j = 0;
+  FOR_EACH (i, lst) {
+    EXPECT_EQ(v[j++], *i);
+  }
+}
+
+TESTFUN(clause_23_3_6_1_11) {
+  // assign with iterators
+  list<VECTOR::value_type> lst;
+  auto const n = random(0U, 10000U);
+  FOR_EACH_RANGE (i, 0, n) {
+    lst.push_back(randomObject<VECTOR::value_type>());
+  }
+  VECTOR v;
+  v.assign(lst.begin(), lst.end());
+  EXPECT_EQ(v.size(), lst.size());
+  size_t j = 0;
+  FOR_EACH (i, lst) {
+    EXPECT_EQ(v[j++], *i);
+  }
+
+  // aliased assign
+  v.assign(v.begin(), v.begin() + v.size() / 2);
+  EXPECT_EQ(v.size(), lst.size() / 2);
+  j = 0;
+  FOR_EACH (i, lst) {
+    if (j == v.size()) break;
+    EXPECT_EQ(v[j++], *i);
+  }
+}
+
+TESTFUN(clause_23_3_6_1_12) {
+  VECTOR v;
+  auto const n = random(0U, 10000U);
+  auto const obj = randomObject<VECTOR::value_type>();
+  v.assign(n, obj);
+  EXPECT_EQ(v.size(), n);
+  FOR_EACH (i, v) {
+    EXPECT_EQ(*i, obj);
+  }
+}
+
+TESTFUN(clause_23_3_6_2_1) {
+  VECTOR v;
+  auto const n = random(0U, 10000U);
+  v.reserve(n);
+  EXPECT_GE(v.capacity(), n);
+}
+
+TESTFUN(clause_23_3_6_2_7) {
+  auto const n1 = random(0U, 10000U);
+  auto const n2 = random(0U, 10000U);
+  auto const obj1 = randomObject<VECTOR::value_type>();
+  auto const obj2 = randomObject<VECTOR::value_type>();
+  VECTOR v1(n1, obj1), v2(n2, obj2);
+  v1.swap(v2);
+  EXPECT_EQ(v1.size(), n2);
+  EXPECT_EQ(v2.size(), n1);
+  FOR_EACH (i, v1) {
+    EXPECT_EQ(*i, obj2);
+  }
+  FOR_EACH (i, v2) {
+    EXPECT_EQ(*i, obj1);
+  }
+}
+
+TESTFUN(clause_23_3_6_2_9) {
+  VECTOR v;
+  auto const n1 = random(0U, 10000U);
+  v.resize(n1);
+  FOR_EACH (i, v) {
+    EXPECT_EQ(*i, VECTOR::value_type());
+  }
+  auto const n2 = random(0U, 10000U);
+  FOR_EACH (i, v) {
+    EXPECT_EQ(*i, VECTOR::value_type());
+  }
+}
+
+TESTFUN(clause_23_3_6_2_11) {
+  VECTOR v;
+  auto const n1 = random(0U, 10000U);
+  auto const obj1 = randomObject<VECTOR::value_type>();
+  v.resize(n1, obj1);
+  FOR_EACH (i, v) {
+    EXPECT_EQ(*i, obj1);
+  }
+  auto const n2 = random(0U, 10000U);
+  auto const obj2 = randomObject<VECTOR::value_type>();
+  v.resize(n2, obj2);
+  if (n1 < n2) {
+    FOR_EACH_RANGE (i, n1, n2) {
+      EXPECT_EQ(v[i], obj2);
+    }
+  }
+}
+
+TESTFUN(clause_absent_element_access) {
+  VECTOR v;
+  auto const n1 = random(1U, 10000U);
+  auto const obj1 = randomObject<VECTOR::value_type>();
+  v.resize(n1, obj1);
+  auto const n = random(0U, v.size() - 1);
+  EXPECT_EQ(v[n], v.at(n));
+  auto const obj2 = randomObject<VECTOR::value_type>();
+  v[n] = obj2;
+  EXPECT_EQ(v[n], v.at(n));
+  EXPECT_EQ(v[n], obj2);
+  auto const obj3 = randomObject<VECTOR::value_type>();
+  v.at(n) = obj3;
+  EXPECT_EQ(v[n], v.at(n));
+  EXPECT_EQ(v[n], obj3);
+}
+
+TESTFUN(clause_23_3_6_3_1) {
+  VECTOR v;
+  auto const n1 = random(1U, 10000U);
+  auto const obj1 = randomObject<VECTOR::value_type>();
+  v.resize(n1, obj1);
+  EXPECT_EQ(v.data(), &v.front());
+}
+
+TESTFUN(clause_23_3_6_4_1_a) {
+  VECTOR v, w;
+  auto const n1 = random(1U, 10000U);
+  FOR_EACH_RANGE (i, 0, n1) {
+    auto const obj1 = randomObject<VECTOR::value_type>();
+    v.push_back(obj1);
+    w.push_back(obj1);
+  }
+  auto const n2 = random(0U, n1 - 1);
+  auto pos = v.begin() + n2;
+  auto const obj2 = randomObject<VECTOR::value_type>();
+
+  auto r = v.insert(pos, obj2);
+
+  EXPECT_EQ(v.size(), w.size() + 1);
+  EXPECT_EQ(r - v.begin(), n2);
+  EXPECT_EQ(*r, obj2);
+  FOR_EACH_RANGE (i, 0, r - v.begin()) {
+    EXPECT_EQ(v[i], w[i]);
+  }
+  FOR_EACH_RANGE (i, r - v.begin() + 1, v.size()) {
+    EXPECT_EQ(v[i], w[i - 1]);
+  }
+}
+
+TESTFUN(clause_23_3_6_4_1_c) {
+  // This test only works for fbvector
+  fbvector<VECTOR::value_type> v, w;
+  auto const n1 = random(1U, 10000U);
+  FOR_EACH_RANGE (i, 0, n1) {
+    auto const obj1 = randomObject<VECTOR::value_type>();
+    v.push_back(obj1);
+    w.push_back(obj1);
+  }
+  auto const n2 = random(0U, n1-1);
+  auto pos = v.begin() + n2;
+  auto const obj2 = randomObject<VECTOR::value_type>();
+  auto const n3 = random(0U, 10000U);
+
+  auto r = v.insert(pos, n3, obj2);
+
+  EXPECT_EQ(v.size(), w.size() + n3);
+  EXPECT_EQ(r - v.begin(), n2);
+  FOR_EACH_RANGE (i, 0, r - v.begin()) {
+    EXPECT_EQ(v[i], w[i]);
+  }
+  FOR_EACH_RANGE (i, r - v.begin(), r - v.begin() + n3) {
+    EXPECT_EQ(v[i], obj2);
+  }
+  FOR_EACH_RANGE (i, r - v.begin() + n3, v.size()) {
+    EXPECT_EQ(v[i], w[i - n3]);
+  }
+}
+
+TESTFUN(clause_23_3_6_4_1_d) {
+  VECTOR v, w;
+  auto const n1 = random(0U, 10000U);
+  FOR_EACH_RANGE (i, 0, n1) {
+    auto const obj1 = randomObject<VECTOR::value_type>();
+    v.push_back(obj1);
+    w.push_back(obj1);
+  }
+  EXPECT_EQ(v.size(), n1);
+
+  auto const obj2 = randomObject<VECTOR::value_type>();
+  v.push_back(obj2);
+  EXPECT_EQ(v.back(), obj2);
+  EXPECT_EQ(v.size(), w.size() + 1);
+
+  FOR_EACH_RANGE (i, 0, w.size()) {
+    EXPECT_EQ(v[i], w[i]);
+  }
+}
+
+TESTFUN(clause_23_3_6_4_3) {
+  VECTOR v, w;
+  auto const n1 = random(1U, 10000U);
+  FOR_EACH_RANGE (i, 0, n1) {
+    auto const obj1 = randomObject<VECTOR::value_type>();
+    v.push_back(obj1);
+    w.push_back(obj1);
+  }
+  EXPECT_EQ(v.size(), n1);
+
+  auto const n2 = random(0U, n1 - 1);
+  auto it = v.erase(v.begin() + n2);
+  EXPECT_EQ(v.size() + 1, w.size());
+
+  FOR_EACH_RANGE (i, 0, it - v.begin()) {
+    EXPECT_EQ(v[i], w[i]);
+  }
+
+  FOR_EACH_RANGE (i, it - v.begin(), v.size()) {
+    EXPECT_EQ(v[i], w[i + 1]);
+  }
+}
+
+TESTFUN(clause_23_3_6_4_4) {
+  VECTOR v, w;
+  auto const n1 = random(1U, 10000U);
+  FOR_EACH_RANGE (i, 0, n1) {
+    auto const obj1 = randomObject<VECTOR::value_type>();
+    v.push_back(obj1);
+    w.push_back(obj1);
+  }
+  EXPECT_EQ(v.size(), n1);
+
+  auto const n2 = random(0U, n1 - 1);
+  auto const n3 = random(n2, n1 - 1);
+  auto it = v.erase(v.begin() + n2, v.begin() + n3);
+  EXPECT_EQ(v.size() + (n3 - n2), w.size());
+
+  FOR_EACH_RANGE (i, 0, it - v.begin()) {
+    EXPECT_EQ(v[i], w[i]);
+  }
+
+  FOR_EACH_RANGE (i, it - v.begin(), v.size()) {
+    EXPECT_EQ(v[i], w[i + (n3 - n2)]);
+  }
+}
+
+TESTFUN(clause_23_3_6_4_clear) {
+  VECTOR v;
+  v.clear();
+  EXPECT_TRUE(v.empty());
+  v.resize(random(0U, 10000U));
+  auto c = v.capacity();
+  v.clear();
+  EXPECT_TRUE(v.empty());
+  EXPECT_EQ(v.capacity(), c);
+}
+
+BENCHMARK(BENCHFUN(zzInitRNG), iters) {
+  //LOG(INFO) << "\nTesting with type " << typeid(VECTOR).name() << "\n";
+  srand(seed);
+}
+
+BENCHMARK(BENCHFUN(defaultCtor), iters) {
+  FOR_EACH_RANGE (i, 0, iters) {
+    VECTOR v[4096];
+    doNotOptimizeAway(&v);
+  }
+}
+
+void BENCHFUN(sizeCtor)(int iters, int size) {
+  FOR_EACH_RANGE (i, 0, iters) {
+    VECTOR v(size);
+    doNotOptimizeAway(&v);
+  }
+}
+BENCHMARK_PARAM(BENCHFUN(sizeCtor), 128);
+BENCHMARK_PARAM(BENCHFUN(sizeCtor), 1024);
+BENCHMARK_PARAM(BENCHFUN(sizeCtor), 1048576);
+
+void BENCHFUN(fillCtor)(int iters, int size) {
+  FOR_EACH_RANGE (i, 0, iters) {
+    VECTOR v(size_t(size), randomObject<VECTOR::value_type>());
+    doNotOptimizeAway(&v);
+  }
+}
+BENCHMARK_PARAM(BENCHFUN(fillCtor), 128);
+BENCHMARK_PARAM(BENCHFUN(fillCtor), 1024);
+BENCHMARK_PARAM(BENCHFUN(fillCtor), 10240);
+
+void BENCHFUN(pushBack)(int iters, int size) {
+  auto const obj = randomObject<VECTOR::value_type>();
+  FOR_EACH_RANGE (i, 0, iters) {
+    VECTOR v;
+    FOR_EACH_RANGE (j, 0, size) {
+      v.push_back(obj);
+    }
+  }
+}
+BENCHMARK_PARAM(BENCHFUN(pushBack), 128);
+BENCHMARK_PARAM(BENCHFUN(pushBack), 1024);
+BENCHMARK_PARAM(BENCHFUN(pushBack), 10240);
+BENCHMARK_PARAM(BENCHFUN(pushBack), 102400);
+BENCHMARK_PARAM(BENCHFUN(pushBack), 512000);
+
+void BENCHFUN(reserve)(int iters, int size) {
+  auto const obj = randomObject<VECTOR::value_type>();
+  VECTOR v(random(0U, 10000U), obj);
+  FOR_EACH_RANGE (i, 0, iters) {
+    v.reserve(random(0U, 100000U));
+  }
+}
+BENCHMARK_PARAM(BENCHFUN(reserve), 128);
+BENCHMARK_PARAM(BENCHFUN(reserve), 1024);
+BENCHMARK_PARAM(BENCHFUN(reserve), 10240);
+
+void BENCHFUN(insert)(int iters, int size) {
+  auto const obj1 = randomObject<VECTOR::value_type>();
+  auto const obj2 = randomObject<VECTOR::value_type>();
+  VECTOR v(random(0U, 1U), obj1);
+  FOR_EACH_RANGE (i, 0, iters / 100) {
+    v.insert(v.begin(), obj2);
+  }
+}
+BENCHMARK_PARAM(BENCHFUN(insert), 100);
+
+void BENCHFUN(erase)(int iters, int size) {
+  auto const obj1 = randomObject<VECTOR::value_type>();
+  VECTOR v(random(0U, 100U), obj1);
+  FOR_EACH_RANGE (i, 0, iters) {
+    if (v.empty()) continue;
+    v.erase(v.begin());
+  }
+}
+BENCHMARK_PARAM(BENCHFUN(erase), 1024);
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/FBVectorTest.cpp
@@ -0,0 +1,281 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+//
+// Author: andrei.alexandrescu@fb.com
+
+#include "folly/Traits.h"
+#include "folly/Random.h"
+#include "folly/FBString.h"
+#include "folly/FBVector.h"
+
+#include <gflags/gflags.h>
+
+#include <gtest/gtest.h>
+#include <list>
+#include <map>
+#include <memory>
+#include <boost/random.hpp>
+
+using namespace std;
+using namespace folly;
+
+auto static const seed = randomNumberSeed();
+typedef boost::mt19937 RandomT;
+static RandomT rng(seed);
+static const size_t maxString = 100;
+static const bool avoidAliasing = true;
+
+template <class Integral1, class Integral2>
+Integral2 random(Integral1 low, Integral2 up) {
+  boost::uniform_int<> range(low, up);
+  return range(rng);
+}
+
+template <class String>
+void randomString(String* toFill, unsigned int maxSize = 1000) {
+  assert(toFill);
+  toFill->resize(random(0, maxSize));
+  FOR_EACH (i, *toFill) {
+    *i = random('a', 'z');
+  }
+}
+
+template <class String, class Integral>
+void Num2String(String& str, Integral n) {
+  str.resize(10, '\0');
+  sprintf(&str[0], "%ul", 10);
+  str.resize(strlen(str.c_str()));
+}
+
+std::list<char> RandomList(unsigned int maxSize) {
+  std::list<char> lst(random(0u, maxSize));
+  std::list<char>::iterator i = lst.begin();
+  for (; i != lst.end(); ++i) {
+    *i = random('a', 'z');
+  }
+  return lst;
+}
+
+template<class T> T randomObject();
+
+template<> int randomObject<int>() {
+  return random(0, 1024);
+}
+
+template<> folly::fbstring randomObject<folly::fbstring>() {
+  folly::fbstring result;
+  randomString(&result);
+  return result;
+}
+
+////////////////////////////////////////////////////////////////////////////////
+// Tests begin here
+////////////////////////////////////////////////////////////////////////////////
+
+TEST(fbvector, clause_23_3_6_1_3_ambiguity) {
+  fbvector<int> v(10, 20);
+  EXPECT_EQ(v.size(), 10);
+  FOR_EACH (i, v) {
+    EXPECT_EQ(*i, 20);
+  }
+}
+
+TEST(fbvector, clause_23_3_6_1_11_ambiguity) {
+  fbvector<int> v;
+  v.assign(10, 20);
+  EXPECT_EQ(v.size(), 10);
+  FOR_EACH (i, v) {
+    EXPECT_EQ(*i, 20);
+  }
+}
+
+TEST(fbvector, clause_23_3_6_2_6) {
+  fbvector<int> v;
+  auto const n = random(0U, 10000U);
+  v.reserve(n);
+  auto const n1 = random(0U, 10000U);
+  auto const obj = randomObject<int>();
+  v.assign(n1, obj);
+  v.shrink_to_fit();
+  // Nothing to verify except that the call made it through
+}
+
+TEST(fbvector, clause_23_3_6_4_ambiguity) {
+  fbvector<int> v;
+  fbvector<int>::const_iterator i = v.end();
+  v.insert(i, 10, 20);
+  EXPECT_EQ(v.size(), 10);
+  FOR_EACH (i, v) {
+    EXPECT_EQ(*i, 20);
+  }
+}
+
+TEST(fbvector, composition) {
+  fbvector< fbvector<double> > matrix(100, fbvector<double>(100));
+}
+
+TEST(fbvector, works_with_std_string) {
+  fbvector<std::string> v(10, "hello");
+  EXPECT_EQ(v.size(), 10);
+  v.push_back("world");
+}
+
+namespace {
+class UserDefinedType { int whatevs_; };
+}
+
+FOLLY_ASSUME_FBVECTOR_COMPATIBLE(UserDefinedType);
+
+TEST(fbvector, works_with_user_defined_type) {
+  fbvector<UserDefinedType> v(10);
+  EXPECT_EQ(v.size(), 10);
+  v.push_back(UserDefinedType());
+}
+
+TEST(fbvector, move_construction) {
+  fbvector<int> v1(100, 100);
+  fbvector<int> v2;
+  EXPECT_EQ(v1.size(), 100);
+  EXPECT_EQ(v1.front(), 100);
+  EXPECT_EQ(v2.size(), 0);
+  v2 = std::move(v1);
+  EXPECT_EQ(v1.size(), 0);
+  EXPECT_EQ(v2.size(), 100);
+  EXPECT_EQ(v2.front(), 100);
+
+  v1.assign(100, 100);
+  auto other = std::move(v1);
+  EXPECT_EQ(v1.size(), 0);
+  EXPECT_EQ(other.size(), 100);
+  EXPECT_EQ(other.front(), 100);
+}
+
+TEST(fbvector, emplace) {
+  fbvector<std::string> s(12, "asd");
+  EXPECT_EQ(s.size(), 12);
+  EXPECT_EQ(s.front(), "asd");
+  s.emplace_back("funk");
+  EXPECT_EQ(s.back(), "funk");
+}
+
+TEST(fbvector, initializer_lists) {
+  fbvector<int> vec = { 1, 2, 3 };
+  EXPECT_EQ(vec.size(), 3);
+  EXPECT_EQ(vec[0], 1);
+  EXPECT_EQ(vec[1], 2);
+  EXPECT_EQ(vec[2], 3);
+
+  vec = { 0, 0, 12, 16 };
+  EXPECT_EQ(vec.size(), 4);
+  EXPECT_EQ(vec[0], 0);
+  EXPECT_EQ(vec[1], 0);
+  EXPECT_EQ(vec[2], 12);
+  EXPECT_EQ(vec[3], 16);
+
+  vec.insert(vec.begin() + 1, { 23, 23 });
+  EXPECT_EQ(vec.size(), 6);
+  EXPECT_EQ(vec[0], 0);
+  EXPECT_EQ(vec[1], 23);
+  EXPECT_EQ(vec[2], 23);
+  EXPECT_EQ(vec[3], 0);
+  EXPECT_EQ(vec[4], 12);
+  EXPECT_EQ(vec[5], 16);
+}
+
+TEST(fbvector, unique_ptr) {
+  fbvector<std::unique_ptr<int> > v(12);
+  std::unique_ptr<int> p(new int(12));
+  v.push_back(std::move(p));
+  EXPECT_EQ(*v.back(), 12);
+
+  v[0] = std::move(p);
+  EXPECT_FALSE(v[0].get());
+  v[0].reset(new int(32));
+  std::unique_ptr<int> somePtr;
+  v.insert(v.begin(), std::move(somePtr));
+  EXPECT_EQ(*v[1], 32);
+}
+
+TEST(FBVector, task858056) {
+  fbvector<fbstring> cycle;
+  cycle.push_back("foo");
+  cycle.push_back("bar");
+  cycle.push_back("baz");
+  fbstring message("Cycle detected: ");
+  FOR_EACH_R (node_name, cycle) {
+    message += "[";
+    message += *node_name;
+    message += "] ";
+  }
+  EXPECT_EQ("Cycle detected: [baz] [bar] [foo] ", message);
+}
+
+TEST(FBVector, move_iterator) {
+  fbvector<int> base = { 0, 1, 2 };
+
+  auto cp1 = base;
+  fbvector<int> fbvi1(std::make_move_iterator(cp1.begin()),
+                      std::make_move_iterator(cp1.end()));
+  EXPECT_EQ(fbvi1, base);
+
+  auto cp2 = base;
+  fbvector<int> fbvi2;
+  fbvi2.assign(std::make_move_iterator(cp2.begin()),
+               std::make_move_iterator(cp2.end()));
+  EXPECT_EQ(fbvi2, base);
+
+  auto cp3 = base;
+  fbvector<int> fbvi3;
+  fbvi3.insert(fbvi3.end(),
+               std::make_move_iterator(cp3.begin()),
+               std::make_move_iterator(cp3.end()));
+  EXPECT_EQ(fbvi3, base);
+}
+
+TEST(FBVector, reserve_consistency) {
+  struct S { int64_t a, b, c, d; };
+
+  fbvector<S> fb1;
+  for (size_t i = 0; i < 1000; ++i) {
+    fb1.reserve(1);
+    EXPECT_EQ(fb1.size(), 0);
+    fb1.shrink_to_fit();
+  }
+}
+
+TEST(FBVector, vector_of_maps) {
+  fbvector<std::map<std::string, std::string>> v;
+
+  v.push_back(std::map<std::string, std::string>());
+  v.push_back(std::map<std::string, std::string>());
+
+  EXPECT_EQ(2, v.size());
+
+  v[1]["hello"] = "world";
+  EXPECT_EQ(0, v[0].size());
+  EXPECT_EQ(1, v[1].size());
+
+  v[0]["foo"] = "bar";
+  EXPECT_EQ(1, v[0].size());
+  EXPECT_EQ(1, v[1].size());
+}
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  return RUN_ALL_TESTS();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/FileTest.cpp
@@ -0,0 +1,212 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/File.h"
+
+#include <mutex>
+
+#include <boost/thread/locks.hpp>
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+#include "folly/Benchmark.h"
+#include "folly/String.h"
+#include "folly/Subprocess.h"
+#include "folly/experimental/io/FsUtil.h"
+#include "folly/experimental/TestUtil.h"
+
+using namespace folly;
+using namespace folly::test;
+
+namespace {
+void expectWouldBlock(ssize_t r) {
+  int savedErrno = errno;
+  EXPECT_EQ(-1, r);
+  EXPECT_EQ(EAGAIN, savedErrno) << errnoStr(errno);
+}
+void expectOK(ssize_t r) {
+  int savedErrno = errno;
+  EXPECT_LE(0, r) << ": errno=" << errnoStr(errno);
+}
+}  // namespace
+
+TEST(File, Simple) {
+  // Open a file, ensure it's indeed open for reading
+  char buf = 'x';
+  {
+    File f("/etc/hosts");
+    EXPECT_NE(-1, f.fd());
+    EXPECT_EQ(1, ::read(f.fd(), &buf, 1));
+    f.close();
+    EXPECT_EQ(-1, f.fd());
+  }
+}
+
+TEST(File, OwnsFd) {
+  // Wrap a file descriptor, make sure that ownsFd works
+  // We'll test that the file descriptor is closed by closing the writing
+  // end of a pipe and making sure that a non-blocking read from the reading
+  // end returns 0.
+
+  char buf = 'x';
+  int p[2];
+  expectOK(::pipe(p));
+  int flags = ::fcntl(p[0], F_GETFL);
+  expectOK(flags);
+  expectOK(::fcntl(p[0], F_SETFL, flags | O_NONBLOCK));
+  expectWouldBlock(::read(p[0], &buf, 1));
+  {
+    File f(p[1]);
+    EXPECT_EQ(p[1], f.fd());
+  }
+  // Ensure that moving the file doesn't close it
+  {
+    File f(p[1]);
+    EXPECT_EQ(p[1], f.fd());
+    File f1(std::move(f));
+    EXPECT_EQ(-1, f.fd());
+    EXPECT_EQ(p[1], f1.fd());
+  }
+  expectWouldBlock(::read(p[0], &buf, 1));  // not closed
+  {
+    File f(p[1], true);
+    EXPECT_EQ(p[1], f.fd());
+  }
+  ssize_t r = ::read(p[0], &buf, 1);  // eof
+  expectOK(r);
+  EXPECT_EQ(0, r);
+  ::close(p[0]);
+}
+
+TEST(File, Release) {
+  File in(STDOUT_FILENO, false);
+  CHECK_EQ(STDOUT_FILENO, in.release());
+  CHECK_EQ(-1, in.release());
+}
+
+#define EXPECT_CONTAINS(haystack, needle) \
+  EXPECT_NE(::std::string::npos, ::folly::StringPiece(haystack).find(needle)) \
+    << "Haystack: '" << haystack << "'\nNeedle: '" << needle << "'";
+
+TEST(File, UsefulError) {
+  try {
+    File("does_not_exist.txt", 0, 0666);
+  } catch (const std::runtime_error& e) {
+    EXPECT_CONTAINS(e.what(), "does_not_exist.txt");
+    EXPECT_CONTAINS(e.what(), "0666");
+  }
+}
+
+TEST(File, Truthy) {
+  File temp = File::temporary();
+
+  EXPECT_TRUE(bool(temp));
+
+  if (temp) {
+    ;
+  } else {
+    EXPECT_FALSE(true);
+  }
+
+  if (File file = File::temporary()) {
+    ;
+  } else {
+    EXPECT_FALSE(true);
+  }
+
+  EXPECT_FALSE(bool(File()));
+  if (File()) {
+    EXPECT_TRUE(false);
+  }
+  if (File notOpened = File()) {
+    EXPECT_TRUE(false);
+  }
+}
+
+TEST(File, Locks) {
+  typedef std::unique_lock<File> Lock;
+  typedef boost::shared_lock<File> SharedLock;
+
+  // Find out where we are.
+  static constexpr size_t pathLength = 2048;
+  char buf[pathLength + 1];
+  int r = readlink("/proc/self/exe", buf, pathLength);
+  CHECK_ERR(r);
+  buf[r] = '\0';
+
+  fs::path helper(buf);
+  helper.remove_filename();
+  helper /= "file_test_lock_helper";
+
+  TemporaryFile tempFile;
+  File f(tempFile.fd());
+
+  enum LockMode { EXCLUSIVE, SHARED };
+  auto testLock = [&] (LockMode mode, bool expectedSuccess) {
+    auto ret =
+      Subprocess({helper.native(),
+                  mode == SHARED ? "-s" : "-x",
+                  tempFile.path().native()}).wait();
+    EXPECT_TRUE(ret.exited());
+    if (ret.exited()) {
+      EXPECT_EQ(expectedSuccess ? 0 : 42, ret.exitStatus());
+    }
+  };
+
+  // Make sure nothing breaks and things compile.
+  {
+    Lock lock(f);
+  }
+
+  {
+    SharedLock lock(f);
+  }
+
+  {
+    Lock lock(f, std::defer_lock);
+    EXPECT_TRUE(lock.try_lock());
+  }
+
+  {
+    SharedLock lock(f, boost::defer_lock);
+    EXPECT_TRUE(lock.try_lock());
+  }
+
+  // X blocks X
+  {
+    Lock lock(f);
+    testLock(EXCLUSIVE, false);
+  }
+
+  // X blocks S
+  {
+    Lock lock(f);
+    testLock(SHARED, false);
+  }
+
+  // S blocks X
+  {
+    SharedLock lock(f);
+    testLock(EXCLUSIVE, false);
+  }
+
+  // S does not block S
+  {
+    SharedLock lock(f);
+    testLock(SHARED, true);
+  }
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/FileTestLockHelper.cpp
@@ -0,0 +1,39 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <gflags/gflags.h>
+#include <glog/logging.h>
+
+#include "folly/File.h"
+
+DEFINE_bool(s, false, "get shared lock");
+DEFINE_bool(x, false, "get exclusive lock");
+
+int main(int argc, char *argv[]) {
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  google::InitGoogleLogging(argv[0]);
+  CHECK_EQ(FLAGS_s + FLAGS_x, 1)
+    << "exactly one of -s and -x must be specified";
+  CHECK_EQ(argc, 2);
+  folly::File f(argv[1], O_RDWR);
+  bool r;
+  if (FLAGS_s) {
+    r = f.try_lock_shared();
+  } else {
+    r = f.try_lock();
+  }
+  return r ? 0 : 42;
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/FileUtilTest.cpp
@@ -0,0 +1,292 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/FileUtil.h"
+#include "folly/detail/FileUtilDetail.h"
+
+#include <deque>
+
+#include <glog/logging.h>
+#include <gflags/gflags.h>
+#include <gtest/gtest.h>
+
+#include "folly/Benchmark.h"
+#include "folly/Range.h"
+#include "folly/String.h"
+
+namespace folly { namespace test {
+
+using namespace fileutil_detail;
+using namespace std;
+
+namespace {
+
+class Reader {
+ public:
+  Reader(off_t offset, StringPiece data, std::deque<ssize_t> spec);
+
+  // write-like
+  ssize_t operator()(int fd, void* buf, size_t count);
+
+  // pwrite-like
+  ssize_t operator()(int fd, void* buf, size_t count, off_t offset);
+
+  // writev-like
+  ssize_t operator()(int fd, const iovec* iov, int count);
+
+  // pwritev-like
+  ssize_t operator()(int fd, const iovec* iov, int count, off_t offset);
+
+  const std::deque<ssize_t> spec() const { return spec_; }
+
+ private:
+  ssize_t nextSize();
+
+  off_t offset_;
+  StringPiece data_;
+  std::deque<ssize_t> spec_;
+};
+
+Reader::Reader(off_t offset, StringPiece data, std::deque<ssize_t> spec)
+  : offset_(offset),
+    data_(data),
+    spec_(std::move(spec)) {
+}
+
+ssize_t Reader::nextSize() {
+  if (spec_.empty()) {
+    throw std::runtime_error("spec empty");
+  }
+  ssize_t n = spec_.front();
+  spec_.pop_front();
+  if (n <= 0) {
+    if (n == -1) {
+      errno = EIO;
+    }
+    spec_.clear();  // so we fail if called again
+  } else {
+    offset_ += n;
+  }
+  return n;
+}
+
+ssize_t Reader::operator()(int fd, void* buf, size_t count) {
+  ssize_t n = nextSize();
+  if (n <= 0) {
+    return n;
+  }
+  if (n > count) {
+    throw std::runtime_error("requested count too small");
+  }
+  memcpy(buf, data_.data(), n);
+  data_.advance(n);
+  return n;
+}
+
+ssize_t Reader::operator()(int fd, void* buf, size_t count, off_t offset) {
+  EXPECT_EQ(offset_, offset);
+  return operator()(fd, buf, count);
+}
+
+ssize_t Reader::operator()(int fd, const iovec* iov, int count) {
+  ssize_t n = nextSize();
+  if (n <= 0) {
+    return n;
+  }
+  ssize_t remaining = n;
+  for (; count != 0 && remaining != 0; ++iov, --count) {
+    ssize_t len = std::min(remaining, ssize_t(iov->iov_len));
+    memcpy(iov->iov_base, data_.data(), len);
+    data_.advance(len);
+    remaining -= len;
+  }
+  if (remaining != 0) {
+    throw std::runtime_error("requested total size too small");
+  }
+  return n;
+}
+
+ssize_t Reader::operator()(int fd, const iovec* iov, int count, off_t offset) {
+  EXPECT_EQ(offset_, offset);
+  return operator()(fd, iov, count);
+}
+
+}  // namespace
+
+class FileUtilTest : public ::testing::Test {
+ protected:
+  FileUtilTest();
+
+  Reader reader(std::deque<ssize_t> spec);
+
+  std::string in_;
+  std::vector<std::pair<size_t, Reader>> readers_;
+};
+
+FileUtilTest::FileUtilTest()
+  : in_("1234567890abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ") {
+  CHECK_EQ(62, in_.size());
+
+  readers_.emplace_back(0, reader({0}));
+  readers_.emplace_back(62, reader({62}));
+  readers_.emplace_back(62, reader({62, -1}));  // error after end (not called)
+  readers_.emplace_back(61, reader({61, 0}));
+  readers_.emplace_back(-1, reader({61, -1}));  // error before end
+  readers_.emplace_back(62, reader({31, 31}));
+  readers_.emplace_back(62, reader({1, 10, 20, 10, 1, 20}));
+  readers_.emplace_back(61, reader({1, 10, 20, 10, 20, 0}));
+  readers_.emplace_back(41, reader({1, 10, 20, 10, 0}));
+  readers_.emplace_back(-1, reader({1, 10, 20, 10, 20, -1}));
+}
+
+Reader FileUtilTest::reader(std::deque<ssize_t> spec) {
+  return Reader(42, in_, std::move(spec));
+}
+
+TEST_F(FileUtilTest, read) {
+  for (auto& p : readers_) {
+    std::string out(in_.size(), '\0');
+    EXPECT_EQ(p.first, wrapFull(p.second, 0, &out[0], out.size()));
+    if (p.first != -1) {
+      EXPECT_EQ(in_.substr(0, p.first), out.substr(0, p.first));
+    }
+  }
+}
+
+TEST_F(FileUtilTest, pread) {
+  for (auto& p : readers_) {
+    std::string out(in_.size(), '\0');
+    EXPECT_EQ(p.first, wrapFull(p.second, 0, &out[0], out.size(), off_t(42)));
+    if (p.first != -1) {
+      EXPECT_EQ(in_.substr(0, p.first), out.substr(0, p.first));
+    }
+  }
+}
+
+class IovecBuffers {
+ public:
+  explicit IovecBuffers(std::initializer_list<size_t> sizes);
+
+  std::vector<iovec> iov() const { return iov_; }  // yes, make a copy
+  std::string join() const { return folly::join("", buffers_); }
+  size_t size() const;
+
+ private:
+  std::vector<std::string> buffers_;
+  std::vector<iovec> iov_;
+};
+
+IovecBuffers::IovecBuffers(std::initializer_list<size_t> sizes) {
+  iov_.reserve(sizes.size());
+  for (auto& s : sizes) {
+    buffers_.push_back(std::string(s, '\0'));
+  }
+  for (auto& b : buffers_) {
+    iovec iov;
+    iov.iov_base = &b[0];
+    iov.iov_len = b.size();
+    iov_.push_back(iov);
+  }
+}
+
+size_t IovecBuffers::size() const {
+  size_t s = 0;
+  for (auto& b : buffers_) {
+    s += b.size();
+  }
+  return s;
+}
+
+TEST_F(FileUtilTest, readv) {
+  for (auto& p : readers_) {
+    IovecBuffers buf({12, 19, 31});
+    ASSERT_EQ(62, buf.size());
+
+    auto iov = buf.iov();
+    EXPECT_EQ(p.first, wrapvFull(p.second, 0, iov.data(), iov.size()));
+    if (p.first != -1) {
+      EXPECT_EQ(in_.substr(0, p.first), buf.join().substr(0, p.first));
+    }
+  }
+}
+
+#if FOLLY_HAVE_PREADV
+TEST_F(FileUtilTest, preadv) {
+  for (auto& p : readers_) {
+    IovecBuffers buf({12, 19, 31});
+    ASSERT_EQ(62, buf.size());
+
+    auto iov = buf.iov();
+    EXPECT_EQ(p.first,
+              wrapvFull(p.second, 0, iov.data(), iov.size(), off_t(42)));
+    if (p.first != -1) {
+      EXPECT_EQ(in_.substr(0, p.first), buf.join().substr(0, p.first));
+    }
+  }
+}
+#endif
+
+TEST(String, readFile) {
+  srand(time(nullptr));
+  const string tmpPrefix = to<string>("/tmp/folly-file-util-test-",
+                                      getpid(), "-", rand(), "-");
+  const string afile = tmpPrefix + "myfile";
+  const string emptyFile = tmpPrefix + "myfile2";
+
+  SCOPE_EXIT {
+    unlink(afile.c_str());
+    unlink(emptyFile.c_str());
+  };
+
+  auto f = fopen(emptyFile.c_str(), "wb");
+  EXPECT_NE(nullptr, f);
+  EXPECT_EQ(0, fclose(f));
+  f = fopen(afile.c_str(), "wb");
+  EXPECT_NE(nullptr, f);
+  EXPECT_EQ(3, fwrite("bar", 1, 3, f));
+  EXPECT_EQ(0, fclose(f));
+
+  {
+    string contents;
+    EXPECT_TRUE(readFile(emptyFile.c_str(), contents));
+    EXPECT_EQ(contents, "");
+    EXPECT_TRUE(readFile(afile.c_str(), contents, 0));
+    EXPECT_EQ("", contents);
+    EXPECT_TRUE(readFile(afile.c_str(), contents, 2));
+    EXPECT_EQ("ba", contents);
+    EXPECT_TRUE(readFile(afile.c_str(), contents));
+    EXPECT_EQ("bar", contents);
+  }
+  {
+    vector<unsigned char> contents;
+    EXPECT_TRUE(readFile(emptyFile.c_str(), contents));
+    EXPECT_EQ(vector<unsigned char>(), contents);
+    EXPECT_TRUE(readFile(afile.c_str(), contents, 0));
+    EXPECT_EQ(vector<unsigned char>(), contents);
+    EXPECT_TRUE(readFile(afile.c_str(), contents, 2));
+    EXPECT_EQ(vector<unsigned char>({'b', 'a'}), contents);
+    EXPECT_TRUE(readFile(afile.c_str(), contents));
+    EXPECT_EQ(vector<unsigned char>({'b', 'a', 'r'}), contents);
+  }
+}
+
+}}  // namespaces
+
+int main(int argc, char *argv[]) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  return RUN_ALL_TESTS();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/FingerprintTest.cpp
@@ -0,0 +1,175 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Fingerprint.h"
+
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+#include "folly/detail/SlowFingerprint.h"
+#include "folly/Benchmark.h"
+
+using namespace folly;
+using namespace folly::detail;
+
+TEST(Fingerprint, BroderOptimization) {
+  // Test that the Broder optimization produces the same result as
+  // the default (slow) implementation that processes one bit at a time.
+  uint64_t val_a = 0xfaceb00cdeadbeefUL;
+  uint64_t val_b = 0x1234567890abcdefUL;
+
+  uint64_t slow[2];
+  uint64_t fast[2];
+
+  SlowFingerprint<64>().update64(val_a).update64(val_b).write(slow);
+  Fingerprint<64>().update64(val_a).update64(val_b).write(fast);
+  EXPECT_EQ(slow[0], fast[0]);
+
+  SlowFingerprint<96>().update64(val_a).update64(val_b).write(slow);
+  Fingerprint<96>().update64(val_a).update64(val_b).write(fast);
+  EXPECT_EQ(slow[0], fast[0]);
+  EXPECT_EQ(slow[1], fast[1]);
+
+  SlowFingerprint<128>().update64(val_a).update64(val_b).write(slow);
+  Fingerprint<128>().update64(val_a).update64(val_b).write(fast);
+  EXPECT_EQ(slow[0], fast[0]);
+  EXPECT_EQ(slow[1], fast[1]);
+}
+
+TEST(Fingerprint, MultiByteUpdate) {
+  // Test that the multi-byte update functions (update32, update64,
+  // update(StringPiece)) produce the same result as calling update8
+  // repeatedly.
+  uint64_t val_a = 0xfaceb00cdeadbeefUL;
+  uint64_t val_b = 0x1234567890abcdefUL;
+  uint8_t bytes[16];
+  for (int i = 0; i < 8; i++) {
+    bytes[i] = (val_a >> (8*(7-i))) & 0xff;
+  }
+  for (int i = 0; i < 8; i++) {
+    bytes[i+8] = (val_b >> (8*(7-i))) & 0xff;
+  }
+  StringPiece sp((const char*)bytes, 16);
+
+  uint64_t u8[2];      // updating 8 bits at a time
+  uint64_t u32[2];     // updating 32 bits at a time
+  uint64_t u64[2];     // updating 64 bits at a time
+  uint64_t usp[2];     // update(StringPiece)
+  uint64_t uconv[2];   // convenience function (fingerprint*(StringPiece))
+
+  {
+    Fingerprint<64> fp;
+    for (int i = 0; i < 16; i++) {
+      fp.update8(bytes[i]);
+    }
+    fp.write(u8);
+  }
+  Fingerprint<64>().update32(val_a >> 32).update32(val_a & 0xffffffff).
+    update32(val_b >> 32).update32(val_b & 0xffffffff).write(u32);
+  Fingerprint<64>().update64(val_a).update64(val_b).write(u64);
+  Fingerprint<64>().update(sp).write(usp);
+  uconv[0] = fingerprint64(sp);
+
+  EXPECT_EQ(u8[0], u32[0]);
+  EXPECT_EQ(u8[0], u64[0]);
+  EXPECT_EQ(u8[0], usp[0]);
+  EXPECT_EQ(u8[0], uconv[0]);
+
+  {
+    Fingerprint<96> fp;
+    for (int i = 0; i < 16; i++) {
+      fp.update8(bytes[i]);
+    }
+    fp.write(u8);
+  }
+  Fingerprint<96>().update32(val_a >> 32).update32(val_a & 0xffffffff).
+    update32(val_b >> 32).update32(val_b & 0xffffffff).write(u32);
+  Fingerprint<96>().update64(val_a).update64(val_b).write(u64);
+  Fingerprint<96>().update(sp).write(usp);
+  uint32_t uconv_lsb;
+  fingerprint96(sp, &(uconv[0]), &uconv_lsb);
+  uconv[1] = (uint64_t)uconv_lsb << 32;
+
+  EXPECT_EQ(u8[0], u32[0]);
+  EXPECT_EQ(u8[1], u32[1]);
+  EXPECT_EQ(u8[0], u64[0]);
+  EXPECT_EQ(u8[1], u64[1]);
+  EXPECT_EQ(u8[0], usp[0]);
+  EXPECT_EQ(u8[1], usp[1]);
+  EXPECT_EQ(u8[0], uconv[0]);
+  EXPECT_EQ(u8[1], uconv[1]);
+
+  {
+    Fingerprint<128> fp;
+    for (int i = 0; i < 16; i++) {
+      fp.update8(bytes[i]);
+    }
+    fp.write(u8);
+  }
+  Fingerprint<128>().update32(val_a >> 32).update32(val_a & 0xffffffff).
+    update32(val_b >> 32).update32(val_b & 0xffffffff).write(u32);
+  Fingerprint<128>().update64(val_a).update64(val_b).write(u64);
+  Fingerprint<128>().update(sp).write(usp);
+  fingerprint128(sp, &(uconv[0]), &(uconv[1]));
+
+  EXPECT_EQ(u8[0], u32[0]);
+  EXPECT_EQ(u8[1], u32[1]);
+  EXPECT_EQ(u8[0], u64[0]);
+  EXPECT_EQ(u8[1], u64[1]);
+  EXPECT_EQ(u8[0], usp[0]);
+  EXPECT_EQ(u8[1], usp[1]);
+  EXPECT_EQ(u8[0], uconv[0]);
+  EXPECT_EQ(u8[1], uconv[1]);
+}
+
+TEST(Fingerprint, Alignment) {
+  // Test that update() gives the same result regardless of string alignment
+  const char test_str[] = "hello world 12345";
+  int len = sizeof(test_str)-1;
+  std::unique_ptr<char[]> str(new char[len+8]);
+  uint64_t ref_fp;
+  SlowFingerprint<64>().update(StringPiece(test_str, len)).write(&ref_fp);
+  for (int i = 0; i < 8; i++) {
+    char* p = str.get();
+    char* q;
+    // Fill the string as !!hello??????
+    for (int j = 0; j < i; j++) {
+      *p++ = '!';
+    }
+    q = p;
+    for (int j = 0; j < len; j++) {
+      *p++ = test_str[j];
+    }
+    for (int j = i; j < 8; j++) {
+      *p++ = '?';
+    }
+
+    uint64_t fp;
+    Fingerprint<64>().update(StringPiece(q, len)).write(&fp);
+    EXPECT_EQ(ref_fp, fp);
+  }
+}
+
+int main(int argc, char *argv[]) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  auto ret = RUN_ALL_TESTS();
+  if (!ret) {
+    folly::runBenchmarksOnFlag();
+  }
+  return ret;
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/ForeachTest.cpp
@@ -0,0 +1,283 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Foreach.h"
+
+#include "folly/Benchmark.h"
+#include <gtest/gtest.h>
+#include <map>
+#include <string>
+#include <vector>
+#include <list>
+
+using namespace folly;
+using namespace folly::detail;
+
+TEST(Foreach, ForEachRvalue) {
+  const char* const hello = "hello";
+  int n = 0;
+  FOR_EACH(it, std::string(hello)) {
+    ++n;
+  }
+  EXPECT_EQ(strlen(hello), n);
+  FOR_EACH_R(it, std::string(hello)) {
+    --n;
+    EXPECT_EQ(hello[n], *it);
+  }
+  EXPECT_EQ(0, n);
+}
+
+TEST(Foreach, ForEachKV) {
+  std::map<std::string, int> testMap;
+  testMap["abc"] = 1;
+  testMap["def"] = 2;
+  std::string keys = "";
+  int values = 0;
+  int numEntries = 0;
+  FOR_EACH_KV (key, value, testMap) {
+    keys += key;
+    values += value;
+    ++numEntries;
+  }
+  EXPECT_EQ("abcdef", keys);
+  EXPECT_EQ(3, values);
+  EXPECT_EQ(2, numEntries);
+}
+
+TEST(Foreach, ForEachKVBreak) {
+  std::map<std::string, int> testMap;
+  testMap["abc"] = 1;
+  testMap["def"] = 2;
+  std::string keys = "";
+  int values = 0;
+  int numEntries = 0;
+  FOR_EACH_KV (key, value, testMap) {
+    keys += key;
+    values += value;
+    ++numEntries;
+    break;
+  }
+  EXPECT_EQ("abc", keys);
+  EXPECT_EQ(1, values);
+  EXPECT_EQ(1, numEntries);
+}
+
+TEST(Foreach, ForEachKvWithMultiMap) {
+  std::multimap<std::string, int> testMap;
+  testMap.insert(std::make_pair("abc", 1));
+  testMap.insert(std::make_pair("abc", 2));
+  testMap.insert(std::make_pair("def", 3));
+  std::string keys = "";
+  int values = 0;
+  int numEntries = 0;
+  FOR_EACH_KV (key, value, testMap) {
+    keys += key;
+    values += value;
+    ++numEntries;
+  }
+  EXPECT_EQ("abcabcdef", keys);
+  EXPECT_EQ(6, values);
+  EXPECT_EQ(3, numEntries);
+}
+
+TEST(Foreach, ForEachEnumerate) {
+  std::vector<int> vv;
+  int sumAA = 0;
+  int sumIter = 0;
+  int numIterations = 0;
+  FOR_EACH_ENUMERATE(aa, iter, vv) {
+    sumAA += aa;
+    sumIter += *iter;
+    ++numIterations;
+  }
+  EXPECT_EQ(sumAA, 0);
+  EXPECT_EQ(sumIter, 0);
+  EXPECT_EQ(numIterations, 0);
+
+  vv.push_back(1);
+  vv.push_back(3);
+  vv.push_back(5);
+  FOR_EACH_ENUMERATE(aa, iter, vv) {
+    sumAA += aa;
+    sumIter += *iter;
+    ++numIterations;
+  }
+  EXPECT_EQ(sumAA, 3);   // 0 + 1 + 2
+  EXPECT_EQ(sumIter, 9); // 1 + 3 + 5
+  EXPECT_EQ(numIterations, 3);
+}
+
+TEST(Foreach, ForEachEnumerateBreak) {
+  std::vector<int> vv;
+  int sumAA = 0;
+  int sumIter = 0;
+  int numIterations = 0;
+  vv.push_back(1);
+  vv.push_back(2);
+  vv.push_back(4);
+  vv.push_back(8);
+  FOR_EACH_ENUMERATE(aa, iter, vv) {
+    sumAA += aa;
+    sumIter += *iter;
+    ++numIterations;
+    if (aa == 1) break;
+  }
+  EXPECT_EQ(sumAA, 1);   // 0 + 1
+  EXPECT_EQ(sumIter, 3); // 1 + 2
+  EXPECT_EQ(numIterations, 2);
+}
+
+TEST(Foreach, ForEachRangeR) {
+  int sum = 0;
+
+  FOR_EACH_RANGE_R (i, 0, 0) {
+    sum += i;
+  }
+  EXPECT_EQ(0, sum);
+
+  FOR_EACH_RANGE_R (i, 0, -1) {
+    sum += i;
+  }
+  EXPECT_EQ(0, sum);
+
+  FOR_EACH_RANGE_R (i, 0, 5) {
+    sum += i;
+  }
+  EXPECT_EQ(10, sum);
+
+  std::list<int> lst = { 0, 1, 2, 3, 4 };
+  sum = 0;
+  FOR_EACH_RANGE_R (i, lst.begin(), lst.end()) {
+    sum += *i;
+  }
+  EXPECT_EQ(10, sum);
+}
+
+// Benchmarks:
+// 1. Benchmark iterating through the man with FOR_EACH, and also assign
+//    iter->first and iter->second to local vars inside the FOR_EACH loop.
+// 2. Benchmark iterating through the man with FOR_EACH, but use iter->first and
+//    iter->second as is, without assigning to local variables.
+// 3. Use FOR_EACH_KV loop to iterate through the map.
+
+std::map<int, std::string> bmMap;  // For use in benchmarks below.
+
+void setupBenchmark(int iters) {
+  bmMap.clear();
+  for (int i = 0; i < iters; ++i) {
+    bmMap[i] = "teststring";
+  }
+}
+
+BENCHMARK(ForEachKVNoMacroAssign, iters) {
+  int sumKeys = 0;
+  std::string sumValues;
+
+  BENCHMARK_SUSPEND {
+    setupBenchmark(iters);
+    int sumKeys = 0;
+    std::string sumValues = "";
+  }
+
+  FOR_EACH (iter, bmMap) {
+    const int k = iter->first;
+    const std::string v = iter->second;
+    sumKeys += k;
+    sumValues += v;
+  }
+}
+
+BENCHMARK(ForEachKVNoMacroNoAssign, iters) {
+  int sumKeys = 0;
+  std::string sumValues;
+
+  BENCHMARK_SUSPEND {
+    setupBenchmark(iters);
+  }
+
+  FOR_EACH (iter, bmMap) {
+    sumKeys += iter->first;
+    sumValues += iter->second;
+  }
+}
+
+BENCHMARK(ManualLoopNoAssign, iters) {
+  BENCHMARK_SUSPEND {
+    setupBenchmark(iters);
+  }
+  int sumKeys = 0;
+  std::string sumValues;
+
+  for (auto iter = bmMap.begin(); iter != bmMap.end(); ++iter) {
+    sumKeys += iter->first;
+    sumValues += iter->second;
+  }
+}
+
+BENCHMARK(ForEachKVMacro, iters) {
+  BENCHMARK_SUSPEND {
+    setupBenchmark(iters);
+  }
+  int sumKeys = 0;
+  std::string sumValues;
+
+  FOR_EACH_KV (k, v, bmMap) {
+    sumKeys += k;
+    sumValues += v;
+  }
+}
+
+BENCHMARK(ForEachManual, iters) {
+  int sum = 1;
+  for (auto i = 1; i < iters; ++i) {
+    sum *= i;
+  }
+  doNotOptimizeAway(sum);
+}
+
+BENCHMARK(ForEachRange, iters) {
+  int sum = 1;
+  FOR_EACH_RANGE (i, 1, iters) {
+    sum *= i;
+  }
+  doNotOptimizeAway(sum);
+}
+
+BENCHMARK(ForEachDescendingManual, iters) {
+  int sum = 1;
+  for (auto i = iters; i-- > 1; ) {
+    sum *= i;
+  }
+  doNotOptimizeAway(sum);
+}
+
+BENCHMARK(ForEachRangeR, iters) {
+  int sum = 1;
+  FOR_EACH_RANGE_R (i, 1, iters) {
+    sum *= i;
+  }
+  doNotOptimizeAway(sum);
+}
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  auto r = RUN_ALL_TESTS();
+  if (r) {
+    return r;
+  }
+  runBenchmarks();
+  return 0;
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/FormatBenchmark.cpp
@@ -0,0 +1,187 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Format.h"
+
+#include <glog/logging.h>
+
+#include "folly/FBVector.h"
+#include "folly/Benchmark.h"
+#include "folly/dynamic.h"
+#include "folly/json.h"
+
+using namespace folly;
+
+namespace {
+
+char bigBuf[300];
+
+}  // namespace
+
+BENCHMARK(octal_sprintf, iters) {
+  while (iters--) {
+    sprintf(bigBuf, "%o", static_cast<unsigned int>(iters));
+  }
+}
+
+BENCHMARK_RELATIVE(octal_uintToOctal, iters) {
+  while (iters--) {
+    detail::uintToOctal(bigBuf, detail::kMaxOctalLength,
+                        static_cast<unsigned int>(iters));
+  }
+}
+
+BENCHMARK_DRAW_LINE()
+
+BENCHMARK(hex_sprintf, iters) {
+  while (iters--) {
+    sprintf(bigBuf, "%x", static_cast<unsigned int>(iters));
+  }
+}
+
+BENCHMARK_RELATIVE(hex_uintToHex, iters) {
+  while (iters--) {
+    detail::uintToHexLower(bigBuf, detail::kMaxHexLength,
+                           static_cast<unsigned int>(iters));
+  }
+}
+
+BENCHMARK_DRAW_LINE()
+
+BENCHMARK(intAppend_sprintf) {
+  fbstring out;
+  for (int i = -1000; i < 1000; i++) {
+    sprintf(bigBuf, "%d", i);
+    out.append(bigBuf);
+  }
+}
+
+BENCHMARK_RELATIVE(intAppend_to) {
+  fbstring out;
+  for (int i = -1000; i < 1000; i++) {
+    toAppend(i, &out);
+  }
+}
+
+BENCHMARK_RELATIVE(intAppend_format) {
+  fbstring out;
+  for (int i = -1000; i < 1000; i++) {
+    format(&out, "{}", i);
+  }
+}
+
+BENCHMARK_DRAW_LINE()
+
+BENCHMARK(bigFormat_sprintf, iters) {
+  while (iters--) {
+    for (int i = -100; i < 100; i++) {
+      sprintf(bigBuf,
+              "%d %d %d %d %d"
+              "%d %d %d %d %d"
+              "%d %d %d %d %d"
+              "%d %d %d %d %d",
+              i, i+1, i+2, i+3, i+4,
+              i+5, i+6, i+7, i+8, i+9,
+              i+10, i+11, i+12, i+13, i+14,
+              i+15, i+16, i+17, i+18, i+19);
+    }
+  }
+}
+
+BENCHMARK_RELATIVE(bigFormat_format, iters) {
+  char* p;
+  auto writeToBuf = [&p] (StringPiece sp) mutable {
+    memcpy(p, sp.data(), sp.size());
+    p += sp.size();
+  };
+
+  while (iters--) {
+    for (int i = -100; i < 100; i++) {
+      p = bigBuf;
+      format("{} {} {} {} {}"
+             "{} {} {} {} {}"
+             "{} {} {} {} {}"
+             "{} {} {} {} {}",
+              i, i+1, i+2, i+3, i+4,
+              i+5, i+6, i+7, i+8, i+9,
+              i+10, i+11, i+12, i+13, i+14,
+              i+15, i+16, i+17, i+18, i+19)(writeToBuf);
+    }
+  }
+}
+
+BENCHMARK_DRAW_LINE()
+
+BENCHMARK(format_nested_strings, iters) {
+  while (iters--) {
+    fbstring out;
+    for (int i = 0; i < 1000; ++i) {
+      out.clear();
+      format(&out, "{} {}",
+             format("{} {}", i, i + 1).str(),
+             format("{} {}", -i, -i - 1).str());
+    }
+  }
+}
+
+BENCHMARK_RELATIVE(format_nested_fbstrings, iters) {
+  while (iters--) {
+    fbstring out;
+    for (int i = 0; i < 1000; ++i) {
+      out.clear();
+      format(&out, "{} {}",
+             format("{} {}", i, i + 1).fbstr(),
+             format("{} {}", -i, -i - 1).fbstr());
+    }
+  }
+}
+
+BENCHMARK_RELATIVE(format_nested_direct, iters) {
+  while (iters--) {
+    fbstring out;
+    for (int i = 0; i < 1000; ++i) {
+      out.clear();
+      format(&out, "{} {}",
+             format("{} {}", i, i + 1),
+             format("{} {}", -i, -i - 1));
+    }
+  }
+}
+
+// Benchmark results on my dev server (dual-CPU Xeon L5520 @ 2.7GHz)
+//
+// ============================================================================
+// folly/test/FormatTest.cpp                         relative  ns/iter  iters/s
+// ============================================================================
+// octal_sprintf                                               100.57     9.94M
+// octal_uintToOctal                                 2599.47%    3.87   258.46M
+// ----------------------------------------------------------------------------
+// hex_sprintf                                                 100.13     9.99M
+// hex_uintToHex                                     3331.75%    3.01   332.73M
+// ----------------------------------------------------------------------------
+// intAppend_sprintf                                           406.07K    2.46K
+// intAppend_to                                       166.03%  244.58K    4.09K
+// intAppend_format                                   147.57%  275.17K    3.63K
+// ----------------------------------------------------------------------------
+// bigFormat_sprintf                                           255.40K    3.92K
+// bigFormat_format                                   102.18%  249.94K    4.00K
+// ============================================================================
+
+int main(int argc, char *argv[]) {
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  runBenchmarks();
+  return 0;
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/FormatTest.cpp
@@ -0,0 +1,376 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Format.h"
+
+#include <glog/logging.h>
+#include <gflags/gflags.h>
+#include <gtest/gtest.h>
+
+#include "folly/FBVector.h"
+#include "folly/FileUtil.h"
+#include "folly/dynamic.h"
+#include "folly/json.h"
+
+using namespace folly;
+
+template <class... Args>
+std::string fstr(StringPiece fmt, Args&&... args) {
+  return format(fmt, std::forward<Args>(args)...).str();
+}
+
+template <class C>
+std::string vstr(StringPiece fmt, const C& c) {
+  return vformat(fmt, c).str();
+}
+
+template <class... Args>
+std::string fstrChecked(StringPiece fmt, Args&&... args) {
+  return formatChecked(fmt, std::forward<Args>(args)...).str();
+}
+
+template <class C>
+std::string vstrChecked(StringPiece fmt, const C& c) {
+  return vformatChecked(fmt, c).str();
+}
+
+template <class Uint>
+void compareOctal(Uint u) {
+  char buf1[detail::kMaxOctalLength + 1];
+  buf1[detail::kMaxOctalLength] = '\0';
+  char* p = buf1 + detail::uintToOctal(buf1, detail::kMaxOctalLength, u);
+
+  char buf2[detail::kMaxOctalLength + 1];
+  sprintf(buf2, "%jo", static_cast<uintmax_t>(u));
+
+  EXPECT_EQ(std::string(buf2), std::string(p));
+}
+
+template <class Uint>
+void compareHex(Uint u) {
+  char buf1[detail::kMaxHexLength + 1];
+  buf1[detail::kMaxHexLength] = '\0';
+  char* p = buf1 + detail::uintToHexLower(buf1, detail::kMaxHexLength, u);
+
+  char buf2[detail::kMaxHexLength + 1];
+  sprintf(buf2, "%jx", static_cast<uintmax_t>(u));
+
+  EXPECT_EQ(std::string(buf2), std::string(p));
+}
+
+template <class Uint>
+void compareBinary(Uint u) {
+  char buf[detail::kMaxBinaryLength + 1];
+  buf[detail::kMaxBinaryLength] = '\0';
+  char* p = buf + detail::uintToBinary(buf, detail::kMaxBinaryLength, u);
+
+  std::string repr;
+  if (u == 0) {
+    repr = '0';
+  } else {
+    std::string tmp;
+    for (; u; u >>= 1) {
+      tmp.push_back(u & 1 ? '1' : '0');
+    }
+    repr.assign(tmp.rbegin(), tmp.rend());
+  }
+
+  EXPECT_EQ(repr, std::string(p));
+}
+
+TEST(Format, uintToOctal) {
+  for (unsigned i = 0; i < (1u << 16) + 2; i++) {
+    compareOctal(i);
+  }
+}
+
+TEST(Format, uintToHex) {
+  for (unsigned i = 0; i < (1u << 16) + 2; i++) {
+    compareHex(i);
+  }
+}
+
+TEST(Format, uintToBinary) {
+  for (unsigned i = 0; i < (1u << 16) + 2; i++) {
+    compareBinary(i);
+  }
+}
+
+TEST(Format, Simple) {
+  EXPECT_EQ("hello", fstr("hello"));
+  EXPECT_EQ("42", fstr("{}", 42));
+  EXPECT_EQ("42 42", fstr("{0} {0}", 42));
+  EXPECT_EQ("00042  23   42", fstr("{0:05} {1:3} {0:4}", 42, 23));
+  EXPECT_EQ("hello world hello 42",
+            fstr("{0} {1} {0} {2}", "hello", "world", 42));
+  EXPECT_EQ("XXhelloXX", fstr("{:X^9}", "hello"));
+  EXPECT_EQ("XXX42XXXX", fstr("{:X^9}", 42));
+  EXPECT_EQ("-0xYYYY2a", fstr("{:Y=#9x}", -42));
+  EXPECT_EQ("*", fstr("{}", '*'));
+  EXPECT_EQ("42", fstr("{}", 42));
+  EXPECT_EQ("0042", fstr("{:04}", 42));
+
+  EXPECT_EQ("hello  ", fstr("{:7}", "hello"));
+  EXPECT_EQ("hello  ", fstr("{:<7}", "hello"));
+  EXPECT_EQ("  hello", fstr("{:>7}", "hello"));
+
+  std::vector<int> v1 {10, 20, 30};
+  EXPECT_EQ("0020", fstr("{0[1]:04}", v1));
+  EXPECT_EQ("0020", vstr("{1:04}", v1));
+  EXPECT_EQ("10 20", vstr("{} {}", v1));
+
+  const std::vector<int> v2 = v1;
+  EXPECT_EQ("0020", fstr("{0[1]:04}", v2));
+  EXPECT_EQ("0020", vstr("{1:04}", v2));
+
+  const int p[] = {10, 20, 30};
+  const int* q = p;
+  EXPECT_EQ("0020", fstr("{0[1]:04}", p));
+  EXPECT_EQ("0020", vstr("{1:04}", p));
+  EXPECT_EQ("0020", fstr("{0[1]:04}", q));
+  EXPECT_EQ("0020", vstr("{1:04}", q));
+  EXPECT_NE("", fstr("{}", q));
+
+  EXPECT_EQ("0x", fstr("{}", p).substr(0, 2));
+  EXPECT_EQ("10", vstr("{}", p));
+  EXPECT_EQ("0x", fstr("{}", q).substr(0, 2));
+  EXPECT_EQ("10", vstr("{}", q));
+  q = nullptr;
+  EXPECT_EQ("(null)", fstr("{}", q));
+
+  std::map<int, std::string> m { {10, "hello"}, {20, "world"} };
+  EXPECT_EQ("worldXX", fstr("{[20]:X<7}", m));
+  EXPECT_EQ("worldXX", vstr("{20:X<7}", m));
+
+  std::map<std::string, std::string> m2 { {"hello", "world"} };
+  EXPECT_EQ("worldXX", fstr("{[hello]:X<7}", m2));
+  EXPECT_EQ("worldXX", vstr("{hello:X<7}", m2));
+
+  // Test indexing in strings
+  EXPECT_EQ("61 62", fstr("{0[0]:x} {0[1]:x}", "abcde"));
+  EXPECT_EQ("61 62", vstr("{0:x} {1:x}", "abcde"));
+  EXPECT_EQ("61 62", fstr("{0[0]:x} {0[1]:x}", std::string("abcde")));
+  EXPECT_EQ("61 62", vstr("{0:x} {1:x}", std::string("abcde")));
+
+  // Test booleans
+  EXPECT_EQ("true", fstr("{}", true));
+  EXPECT_EQ("1", fstr("{:d}", true));
+  EXPECT_EQ("false", fstr("{}", false));
+  EXPECT_EQ("0", fstr("{:d}", false));
+
+  // Test pairs
+  {
+    std::pair<int, std::string> p {42, "hello"};
+    EXPECT_EQ("    42 hello ", fstr("{0[0]:6} {0[1]:6}", p));
+    EXPECT_EQ("    42 hello ", vstr("{:6} {:6}", p));
+  }
+
+  // Test tuples
+  {
+    std::tuple<int, std::string, int> t { 42, "hello", 23 };
+    EXPECT_EQ("    42 hello      23", fstr("{0[0]:6} {0[1]:6} {0[2]:6}", t));
+    EXPECT_EQ("    42 hello      23", vstr("{:6} {:6} {:6}", t));
+  }
+
+  // Test writing to stream
+  std::ostringstream os;
+  os << format("{} {}", 42, 23);
+  EXPECT_EQ("42 23", os.str());
+
+  // Test appending to string
+  std::string s;
+  format(&s, "{} {}", 42, 23);
+  format(&s, " hello {:X<7}", "world");
+  EXPECT_EQ("42 23 hello worldXX", s);
+
+  // Test writing to FILE. I'd use open_memstream but that's not available
+  // outside of Linux (even though it's in POSIX.1-2008).
+  {
+    int fds[2];
+    CHECK_ERR(pipe(fds));
+    SCOPE_EXIT { closeNoInt(fds[1]); };
+    {
+      FILE* fp = fdopen(fds[1], "wb");
+      PCHECK(fp);
+      SCOPE_EXIT { fclose(fp); };
+      writeTo(fp, format("{} {}", 42, 23));  // <= 512 bytes (PIPE_BUF)
+    }
+
+    char buf[512];
+    ssize_t n = readFull(fds[0], buf, sizeof(buf));
+    CHECK_GE(n, 0);
+
+    EXPECT_EQ("42 23", std::string(buf, n));
+  }
+}
+
+TEST(Format, Float) {
+  double d = 1;
+  EXPECT_EQ("1", fstr("{}", 1.0));
+  EXPECT_EQ("0.1", fstr("{}", 0.1));
+  EXPECT_EQ("0.01", fstr("{}", 0.01));
+  EXPECT_EQ("0.001", fstr("{}", 0.001));
+  EXPECT_EQ("0.0001", fstr("{}", 0.0001));
+  EXPECT_EQ("1e-5", fstr("{}", 0.00001));
+  EXPECT_EQ("1e-6", fstr("{}", 0.000001));
+
+  EXPECT_EQ("10", fstr("{}", 10.0));
+  EXPECT_EQ("100", fstr("{}", 100.0));
+  EXPECT_EQ("1000", fstr("{}", 1000.0));
+  EXPECT_EQ("10000", fstr("{}", 10000.0));
+  EXPECT_EQ("100000", fstr("{}", 100000.0));
+  EXPECT_EQ("1e+6", fstr("{}", 1000000.0));
+  EXPECT_EQ("1e+7", fstr("{}", 10000000.0));
+
+  EXPECT_EQ("1.00", fstr("{:.2f}", 1.0));
+  EXPECT_EQ("0.10", fstr("{:.2f}", 0.1));
+  EXPECT_EQ("0.01", fstr("{:.2f}", 0.01));
+  EXPECT_EQ("0.00", fstr("{:.2f}", 0.001));
+}
+
+TEST(Format, MultiLevel) {
+  std::vector<std::map<std::string, std::string>> v = {
+    {
+      {"hello", "world"},
+    },
+  };
+
+  EXPECT_EQ("world", fstr("{[0.hello]}", v));
+}
+
+TEST(Format, dynamic) {
+  auto dyn = parseJson(
+      "{\n"
+      "  \"hello\": \"world\",\n"
+      "  \"x\": [20, 30],\n"
+      "  \"y\": {\"a\" : 42}\n"
+      "}");
+
+  EXPECT_EQ("world", fstr("{0[hello]}", dyn));
+  EXPECT_EQ("20", fstr("{0[x.0]}", dyn));
+  EXPECT_EQ("42", fstr("{0[y.a]}", dyn));
+
+  EXPECT_EQ("(null)", fstr("{}", dynamic(nullptr)));
+}
+
+namespace {
+
+struct KeyValue {
+  std::string key;
+  int value;
+};
+
+}  // namespace
+
+namespace folly {
+
+template <> class FormatValue<KeyValue> {
+ public:
+  explicit FormatValue(const KeyValue& kv) : kv_(kv) { }
+
+  template <class FormatCallback>
+  void format(FormatArg& arg, FormatCallback& cb) const {
+    format_value::formatFormatter(
+        folly::format("<key={}, value={}>", kv_.key, kv_.value),
+        arg, cb);
+  }
+
+ private:
+  const KeyValue& kv_;
+};
+
+}  // namespace
+
+TEST(Format, Custom) {
+  KeyValue kv { "hello", 42 };
+
+  EXPECT_EQ("<key=hello, value=42>", fstr("{}", kv));
+  EXPECT_EQ("<key=hello, value=42>", fstr("{:10}", kv));
+  EXPECT_EQ("<key=hello", fstr("{:.10}", kv));
+  EXPECT_EQ("<key=hello, value=42>XX", fstr("{:X<23}", kv));
+  EXPECT_EQ("XX<key=hello, value=42>", fstr("{:X>23}", kv));
+  EXPECT_EQ("<key=hello, value=42>", fstr("{0[0]}", &kv));
+  EXPECT_NE("", fstr("{}", &kv));
+}
+
+namespace {
+
+struct Opaque {
+  int k;
+};
+
+} // namespace
+
+TEST(Format, Unformatted) {
+  Opaque o;
+  EXPECT_NE("", fstr("{}", &o));
+  EXPECT_DEATH(fstr("{0[0]}", &o), "No formatter available for this type");
+  EXPECT_THROW(fstrChecked("{0[0]}", &o), std::invalid_argument);
+}
+
+TEST(Format, Nested) {
+  EXPECT_EQ("1 2 3 4", fstr("{} {} {}", 1, 2, format("{} {}", 3, 4)));
+  //
+  // not copyable, must hold temporary in scope instead.
+  auto&& saved = format("{} {}", 3, 4);
+  EXPECT_EQ("1 2 3 4", fstr("{} {} {}", 1, 2, saved));
+}
+
+TEST(Format, OutOfBounds) {
+  std::vector<int> ints{1, 2, 3, 4, 5};
+  EXPECT_EQ("1 3 5", fstr("{0[0]} {0[2]} {0[4]}", ints));
+  EXPECT_THROW(fstr("{[5]}", ints), std::out_of_range);
+  EXPECT_THROW(fstrChecked("{[5]}", ints), std::out_of_range);
+
+  std::map<std::string, int> map{{"hello", 0}, {"world", 1}};
+  EXPECT_EQ("hello = 0", fstr("hello = {[hello]}", map));
+  EXPECT_THROW(fstr("{[nope]}", map), std::out_of_range);
+  EXPECT_THROW(vstr("{nope}", map), std::out_of_range);
+  EXPECT_THROW(vstrChecked("{nope}", map), std::out_of_range);
+}
+
+TEST(Format, BogusFormatString) {
+  // format() will crash the program if the format string is invalid.
+  EXPECT_DEATH(fstr("}"), "single '}' in format string");
+  EXPECT_DEATH(fstr("foo}bar"), "single '}' in format string");
+  EXPECT_DEATH(fstr("foo{bar"), "missing ending '}'");
+  EXPECT_DEATH(fstr("{[test]"), "missing ending '}'");
+  EXPECT_DEATH(fstr("{-1.3}"), "argument index must be non-negative");
+  EXPECT_DEATH(fstr("{1.3}", 0, 1, 2), "index not allowed");
+  EXPECT_DEATH(fstr("{0} {} {1}", 0, 1, 2),
+               "may not have both default and explicit arg indexes");
+
+  // formatChecked() should throw exceptions rather than crashing the program
+  EXPECT_THROW(fstrChecked("}"), std::invalid_argument);
+  EXPECT_THROW(fstrChecked("foo}bar"), std::invalid_argument);
+  EXPECT_THROW(fstrChecked("foo{bar"), std::invalid_argument);
+  EXPECT_THROW(fstrChecked("{[test]"), std::invalid_argument);
+  EXPECT_THROW(fstrChecked("{-1.3}"), std::invalid_argument);
+  EXPECT_THROW(fstrChecked("{1.3}", 0, 1, 2), std::invalid_argument);
+  EXPECT_THROW(fstrChecked("{0} {} {1}", 0, 1, 2), std::invalid_argument);
+
+  // This one fails in detail::enforceWhitespace(), which throws
+  // std::range_error
+  EXPECT_DEATH(fstr("{0[test}"), "Non-whitespace: \\[");
+  EXPECT_THROW(fstrChecked("{0[test}"), std::exception);
+}
+
+int main(int argc, char *argv[]) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  return RUN_ALL_TESTS();
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/function_benchmark/benchmark_impl.cpp
@@ -0,0 +1,36 @@
+// Copyright 2004-present Facebook.  All rights reserved.
+#include "folly/test/function_benchmark/benchmark_impl.h"
+
+#include "folly/test/function_benchmark/test_functions.h"
+
+/*
+ * These functions are defined in a separate file so that gcc won't be able to
+ * inline them and optimize away the indirect calls.
+ */
+
+void BM_fn_ptr_invoke_impl(int iters, void (*fn)()) {
+  for (int n = 0; n < iters; ++n) {
+    fn();
+  }
+}
+
+void BM_std_function_invoke_impl(int iters,
+                                 const std::function<void()>& fn) {
+  for (int n = 0; n < iters; ++n) {
+    fn();
+  }
+}
+
+void BM_mem_fn_invoke_impl(int iters,
+                           TestClass* tc,
+                           void (TestClass::*memfn)()) {
+  for (int n = 0; n < iters; ++n) {
+    (tc->*memfn)();
+  }
+}
+
+void BM_virtual_fn_invoke_impl(int iters, VirtualClass* vc) {
+  for (int n = 0; n < iters; ++n) {
+    vc->doNothing();
+  }
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/function_benchmark/benchmark_impl.h
@@ -0,0 +1,35 @@
+// Copyright 2004-present Facebook.  All rights reserved.
+#ifndef BENCHMARK_IMPL_H_
+#define BENCHMARK_IMPL_H_
+
+#include <functional>
+
+class TestClass;
+class VirtualClass;
+
+void BM_fn_ptr_invoke_impl(int iters, void (*fn)());
+void BM_std_function_invoke_impl(int iters, const std::function<void()>& fn);
+void BM_mem_fn_invoke_impl(int iters,
+                           TestClass* tc,
+                           void (TestClass::*memfn)());
+void BM_virtual_fn_invoke_impl(int iters, VirtualClass* vc);
+
+// Inlined version of BM_fn_ptr_invoke_impl().
+// The compiler could potentially even optimize the call to the function
+// pointer if it is a constexpr.
+inline void BM_fn_ptr_invoke_inlined_impl(int iters, void (*fn)()) {
+  for (int n = 0; n < iters; ++n) {
+    fn();
+  }
+}
+
+// Invoke a function object as a template parameter.
+// This can be used to directly invoke lambda functions
+template<typename T>
+void BM_invoke_fn_template_impl(int iters, const T& fn) {
+  for (int n = 0; n < iters; ++n) {
+    fn();
+  }
+}
+
+#endif // BENCHMARK_IMPL_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/function_benchmark/main.cpp
@@ -0,0 +1,268 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/test/function_benchmark/benchmark_impl.h"
+#include "folly/test/function_benchmark/test_functions.h"
+
+#include "folly/Benchmark.h"
+#include "folly/ScopeGuard.h"
+#include <gflags/gflags.h>
+#include <glog/logging.h>
+
+using folly::ScopeGuard;
+using folly::makeGuard;
+
+// Declare the bm_max_iters flag from folly/Benchmark.cpp
+DECLARE_int32(bm_max_iters);
+
+// Directly invoking a function
+BENCHMARK(fn_invoke, iters) {
+  for (int n = 0; n < iters; ++n) {
+    doNothing();
+  }
+}
+
+// Invoking a function through a function pointer
+BENCHMARK(fn_ptr_invoke, iters) {
+  BM_fn_ptr_invoke_impl(iters, doNothing);
+}
+
+// Invoking a function through a std::function object
+BENCHMARK(std_function_invoke, iters) {
+  BM_std_function_invoke_impl(iters, doNothing);
+}
+
+// Invoking a member function through a member function pointer
+BENCHMARK(mem_fn_invoke, iters) {
+  TestClass tc;
+  BM_mem_fn_invoke_impl(iters, &tc, &TestClass::doNothing);
+}
+
+// Invoke a function pointer through an inlined wrapper function
+BENCHMARK(fn_ptr_invoke_through_inline, iters) {
+  BM_fn_ptr_invoke_inlined_impl(iters, doNothing);
+}
+
+// Invoke a lambda that calls doNothing() through an inlined wrapper function
+BENCHMARK(lambda_invoke_fn, iters) {
+  BM_invoke_fn_template_impl(iters, [] { doNothing(); });
+}
+
+// Invoke a lambda that does nothing
+BENCHMARK(lambda_noop, iters) {
+  BM_invoke_fn_template_impl(iters, [] {});
+}
+
+// Invoke a lambda that modifies a local variable
+BENCHMARK(lambda_local_var, iters) {
+  uint32_t count1 = 0;
+  uint32_t count2 = 0;
+  BM_invoke_fn_template_impl(iters, [&] {
+    // Do something slightly more complicated than just incrementing a
+    // variable.  Otherwise gcc is smart enough to optimize the loop away.
+    if (count1 & 0x1) {
+      ++count2;
+    }
+    ++count1;
+  });
+
+  // Use the values we computed, so gcc won't optimize the loop away
+  CHECK_EQ(iters, count1);
+  CHECK_EQ(iters / 2, count2);
+}
+
+// Invoke a function pointer through the same wrapper used for lambdas
+BENCHMARK(fn_ptr_invoke_through_template, iters) {
+  BM_invoke_fn_template_impl(iters, doNothing);
+}
+
+// Invoking a virtual method
+BENCHMARK(virtual_fn_invoke, iters) {
+  VirtualClass vc;
+  BM_virtual_fn_invoke_impl(iters, &vc);
+}
+
+// Creating a function pointer and invoking it
+BENCHMARK(fn_ptr_create_invoke, iters) {
+  for (int n = 0; n < iters; ++n) {
+    void (*fn)() = doNothing;
+    fn();
+  }
+}
+
+// Creating a std::function object from a function pointer, and invoking it
+BENCHMARK(std_function_create_invoke, iters) {
+  for (int n = 0; n < iters; ++n) {
+    std::function<void()> fn = doNothing;
+    fn();
+  }
+}
+
+// Creating a pointer-to-member and invoking it
+BENCHMARK(mem_fn_create_invoke, iters) {
+  TestClass tc;
+  for (int n = 0; n < iters; ++n) {
+    void (TestClass::*memfn)() = &TestClass::doNothing;
+    (tc.*memfn)();
+  }
+}
+
+// Using std::bind to create a std::function from a member function,
+// and invoking it
+BENCHMARK(std_bind_create_invoke, iters) {
+  TestClass tc;
+  for (int n = 0; n < iters; ++n) {
+    std::function<void()> fn = std::bind(&TestClass::doNothing, &tc);
+    fn();
+  }
+}
+
+// Using std::bind directly to invoke a member function
+BENCHMARK(std_bind_direct_invoke, iters) {
+  TestClass tc;
+  for (int n = 0; n < iters; ++n) {
+    auto fn = std::bind(&TestClass::doNothing, &tc);
+    fn();
+  }
+}
+
+// Using ScopeGuard to invoke a std::function
+BENCHMARK(scope_guard_std_function, iters) {
+  std::function<void()> fn(doNothing);
+  for (int n = 0; n < iters; ++n) {
+    ScopeGuard g = makeGuard(fn);
+  }
+}
+
+// Using ScopeGuard to invoke a std::function,
+// but create the ScopeGuard with an rvalue to a std::function
+BENCHMARK(scope_guard_std_function_rvalue, iters) {
+  for (int n = 0; n < iters; ++n) {
+    ScopeGuard g = makeGuard(std::function<void()>(doNothing));
+  }
+}
+
+// Using ScopeGuard to invoke a function pointer
+BENCHMARK(scope_guard_fn_ptr, iters) {
+  for (int n = 0; n < iters; ++n) {
+    ScopeGuard g = makeGuard(doNothing);
+  }
+}
+
+// Using ScopeGuard to invoke a lambda that does nothing
+BENCHMARK(scope_guard_lambda_noop, iters) {
+  for (int n = 0; n < iters; ++n) {
+    ScopeGuard g = makeGuard([] {});
+  }
+}
+
+// Using ScopeGuard to invoke a lambda that invokes a function
+BENCHMARK(scope_guard_lambda_function, iters) {
+  for (int n = 0; n < iters; ++n) {
+    ScopeGuard g = makeGuard([] { doNothing(); });
+  }
+}
+
+// Using ScopeGuard to invoke a lambda that modifies a local variable
+BENCHMARK(scope_guard_lambda_local_var, iters) {
+  uint32_t count = 0;
+  for (int n = 0; n < iters; ++n) {
+    ScopeGuard g = makeGuard([&] {
+      // Increment count if n is odd.  Without this conditional check
+      // (i.e., if we just increment count each time through the loop),
+      // gcc is smart enough to optimize the entire loop away, and just set
+      // count = iters.
+      if (n & 0x1) {
+        ++count;
+      }
+    });
+  }
+
+  // Check that the value of count is what we expect.
+  // This check is necessary: if we don't use count, gcc detects that count is
+  // unused and optimizes the entire loop away.
+  CHECK_EQ(iters / 2, count);
+}
+
+BENCHMARK_DRAW_LINE()
+
+BENCHMARK(throw_exception, iters) {
+  for (int n = 0; n < iters; ++n) {
+    try {
+      throwException();
+    } catch (const std::exception& ex) {
+    }
+  }
+}
+
+BENCHMARK(catch_no_exception, iters) {
+  for (int n = 0; n < iters; ++n) {
+    try {
+      doNothing();
+    } catch (const std::exception& ex) {
+    }
+  }
+}
+
+BENCHMARK(return_exc_ptr, iters) {
+  for (int n = 0; n < iters; ++n) {
+    returnExceptionPtr();
+  }
+}
+
+BENCHMARK(exc_ptr_param_return, iters) {
+  for (int n = 0; n < iters; ++n) {
+    std::exception_ptr ex;
+    exceptionPtrReturnParam(&ex);
+  }
+}
+
+BENCHMARK(exc_ptr_param_return_null, iters) {
+  for (int n = 0; n < iters; ++n) {
+    exceptionPtrReturnParam(nullptr);
+  }
+}
+
+BENCHMARK(return_string, iters) {
+  for (int n = 0; n < iters; ++n) {
+    returnString();
+  }
+}
+
+BENCHMARK(return_string_noexcept, iters) {
+  for (int n = 0; n < iters; ++n) {
+    returnStringNoExcept();
+  }
+}
+
+BENCHMARK(return_code, iters) {
+  for (int n = 0; n < iters; ++n) {
+    returnCode(false);
+  }
+}
+
+BENCHMARK(return_code_noexcept, iters) {
+  for (int n = 0; n < iters; ++n) {
+    returnCodeNoExcept(false);
+  }
+}
+
+// main()
+
+int main(int argc, char** argv) {
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  folly::runBenchmarks();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/function_benchmark/Makefile.am
@@ -0,0 +1,11 @@
+ACLOCAL_AMFLAGS = -I m4
+
+# depends on libfollybenchmark
+
+# TESTS = function_benchmark
+
+# check_PROGRAMS = $(TESTS)
+
+# noinst_HEADERS = test_functions.h benchmark_impl.h
+
+# function_benchmark_SOURCES = benchmark_impl.cpp main.cpp test_functions.cpp
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/function_benchmark/test_functions.cpp
@@ -0,0 +1,80 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/test/function_benchmark/test_functions.h"
+
+/*
+ * These functions are defined in a separate file so that
+ * gcc won't be able to inline them.
+ */
+
+
+class Exception : public std::exception {
+ public:
+  explicit Exception(const std::string& value) : value_(value) {}
+  virtual ~Exception(void) throw() {}
+
+  virtual const char *what(void) const throw() {
+    return value_.c_str();
+  }
+
+ private:
+  std::string value_;
+};
+
+void doNothing() {
+}
+
+void throwException() {
+  throw Exception("this is a test");
+}
+
+std::exception_ptr returnExceptionPtr() {
+  Exception ex("this is a test");
+  return std::make_exception_ptr(ex);
+}
+
+void exceptionPtrReturnParam(std::exception_ptr* excReturn) {
+  if (excReturn) {
+    Exception ex("this is a test");
+    *excReturn = std::make_exception_ptr(ex);
+  }
+}
+
+std::string returnString() {
+  return "this is a test";
+}
+
+std::string returnStringNoExcept() noexcept {
+  return "this is a test";
+}
+
+int returnCode(int value) {
+  return value;
+}
+
+int returnCodeNoExcept(int value) noexcept {
+  return value;
+}
+
+void TestClass::doNothing() {
+}
+
+VirtualClass::~VirtualClass() {
+}
+
+void VirtualClass::doNothing() {
+};
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/function_benchmark/test_functions.h
@@ -0,0 +1,44 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef TEST_FUNCTIONS_H_
+#define TEST_FUNCTIONS_H_
+
+#include <exception>
+#include <string>
+
+void doNothing();
+
+void throwException();
+std::exception_ptr returnExceptionPtr();
+void exceptionPtrReturnParam(std::exception_ptr* excReturn);
+std::string returnString();
+std::string returnStringNoExcept() noexcept;
+int returnCode(int value);
+int returnCodeNoExcept(int value) noexcept;
+
+class TestClass {
+ public:
+  void doNothing();
+};
+
+class VirtualClass {
+ public:
+  virtual ~VirtualClass();
+  virtual void doNothing();
+};
+
+#endif // TEST_FUNCTIONS_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/FutexTest.cpp
@@ -0,0 +1,186 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/detail/Futex.h"
+#include "folly/test/DeterministicSchedule.h"
+
+#include <chrono>
+#include <thread>
+
+#include <gflags/gflags.h>
+#include <gtest/gtest.h>
+#include <common/logging/logging.h>
+#include <time.h>
+
+using namespace folly::detail;
+using namespace folly::test;
+using namespace std::chrono;
+
+typedef DeterministicSchedule DSched;
+
+template <template<typename> class Atom>
+void run_basic_tests() {
+  Futex<Atom> f(0);
+
+  EXPECT_FALSE(f.futexWait(1));
+  EXPECT_EQ(f.futexWake(), 0);
+
+  auto thr = DSched::thread([&]{
+    EXPECT_TRUE(f.futexWait(0));
+  });
+
+  while (f.futexWake() != 1) {
+    std::this_thread::yield();
+  }
+
+  DSched::join(thr);
+}
+
+template<template<typename> class Atom>
+void run_wait_until_tests();
+
+template <typename Clock>
+void stdAtomicWaitUntilTests() {
+  Futex<std::atomic> f(0);
+
+  auto thrA = DSched::thread([&]{
+    while (true) {
+      typename Clock::time_point nowPlus2s = Clock::now() + seconds(2);
+      auto res = f.futexWaitUntil(0, nowPlus2s);
+      EXPECT_TRUE(res == FutexResult::TIMEDOUT || res == FutexResult::AWOKEN);
+      if (res == FutexResult::AWOKEN) {
+        break;
+      }
+    }
+  });
+
+  while (f.futexWake() != 1) {
+    std::this_thread::yield();
+  }
+
+  DSched::join(thrA);
+
+  auto start = Clock::now();
+  EXPECT_EQ(f.futexWaitUntil(0, start + milliseconds(100)),
+            FutexResult::TIMEDOUT);
+  LOG(INFO) << "Futex wait timed out after waiting for "
+            << duration_cast<milliseconds>(Clock::now() - start).count()
+            << "ms";
+}
+
+template <typename Clock>
+void deterministicAtomicWaitUntilTests() {
+  Futex<DeterministicAtomic> f(0);
+
+  // Futex wait must eventually fail with either FutexResult::TIMEDOUT or
+  // FutexResult::INTERRUPTED
+  auto res = f.futexWaitUntil(0, Clock::now() + milliseconds(100));
+  EXPECT_TRUE(res == FutexResult::TIMEDOUT || res == FutexResult::INTERRUPTED);
+}
+
+template <>
+void run_wait_until_tests<std::atomic>() {
+  stdAtomicWaitUntilTests<system_clock>();
+  stdAtomicWaitUntilTests<steady_clock>();
+}
+
+template <>
+void run_wait_until_tests<DeterministicAtomic>() {
+  deterministicAtomicWaitUntilTests<system_clock>();
+  deterministicAtomicWaitUntilTests<steady_clock>();
+}
+
+uint64_t diff(uint64_t a, uint64_t b) {
+  return a > b ? a - b : b - a;
+}
+
+void run_system_clock_test() {
+  /* Test to verify that system_clock uses clock_gettime(CLOCK_REALTIME, ...)
+   * for the time_points */
+  struct timespec ts;
+  const int maxIters = 1000;
+  int iter = 0;
+  uint64_t delta = 10000000 /* 10 ms */;
+
+  /** The following loop is only to make the test more robust in the presence of
+   * clock adjustments that can occur. We just run the loop maxIter times and
+   * expect with very high probability that there will be atleast one iteration
+   * of the test during which clock adjustments > delta have not occurred. */
+  while (iter < maxIters) {
+    uint64_t a = duration_cast<nanoseconds>(system_clock::now()
+                                            .time_since_epoch()).count();
+
+    clock_gettime(CLOCK_REALTIME, &ts);
+    uint64_t b = ts.tv_sec * 1000000000ULL + ts.tv_nsec;
+
+    uint64_t c = duration_cast<nanoseconds>(system_clock::now()
+                                            .time_since_epoch()).count();
+
+    if (diff(a, b) <= delta &&
+        diff(b, c) <= delta &&
+        diff(a, c) <= 2 * delta) {
+      /* Success! system_clock uses CLOCK_REALTIME for time_points */
+      break;
+    }
+    iter++;
+  }
+  EXPECT_TRUE(iter < maxIters);
+}
+
+void run_steady_clock_test() {
+  /* Test to verify that steady_clock uses clock_gettime(CLOCK_MONOTONIC, ...)
+   * for the time_points */
+  EXPECT_TRUE(steady_clock::is_steady);
+
+  uint64_t A = duration_cast<nanoseconds>(steady_clock::now()
+                                          .time_since_epoch()).count();
+
+  struct timespec ts;
+  clock_gettime(CLOCK_MONOTONIC, &ts);
+  uint64_t B = ts.tv_sec * 1000000000ULL + ts.tv_nsec;
+
+  uint64_t C = duration_cast<nanoseconds>(steady_clock::now()
+                                          .time_since_epoch()).count();
+  EXPECT_TRUE(A <= B && B <= C);
+}
+
+TEST(Futex, clock_source) {
+  run_system_clock_test();
+
+  /* On some systems steady_clock is just an alias for system_clock. So,
+   * we must skip run_steady_clock_test if the two clocks are the same. */
+  if (!std::is_same<system_clock,steady_clock>::value) {
+    run_steady_clock_test();
+  }
+}
+
+TEST(Futex, basic_live) {
+  run_basic_tests<std::atomic>();
+  run_wait_until_tests<std::atomic>();
+}
+
+TEST(Futex, basic_deterministic) {
+  DSched sched(DSched::uniform(0));
+  run_basic_tests<DeterministicAtomic>();
+  run_wait_until_tests<DeterministicAtomic>();
+}
+
+int main(int argc, char ** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  return RUN_ALL_TESTS();
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/GroupVarintTest.cpp
@@ -0,0 +1,265 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <stdarg.h>
+#include "folly/GroupVarint.h"
+
+// On platforms where it's not supported, GroupVarint will be compiled out.
+#if HAVE_GROUP_VARINT
+
+#include <gtest/gtest.h>
+
+using namespace folly;
+
+namespace {
+
+class StringAppender {
+ public:
+  /* implicit */ StringAppender(std::string& s) : s_(s) { }
+  void operator()(StringPiece sp) {
+    s_.append(sp.data(), sp.size());
+  }
+ private:
+  std::string& s_;
+};
+
+typedef GroupVarintEncoder<uint32_t, StringAppender> GroupVarint32Encoder;
+typedef GroupVarintEncoder<uint64_t, StringAppender> GroupVarint64Encoder;
+typedef GroupVarintDecoder<uint32_t> GroupVarint32Decoder;
+typedef GroupVarintDecoder<uint32_t> GroupVarint64Decoder;
+
+// Expected bytes follow, terminate with -1
+void testGroupVarint32(uint32_t a, uint32_t b, uint32_t c, uint32_t d, ...) {
+  va_list ap;
+  va_start(ap, d);
+  std::vector<char> expectedBytes;
+  int byte;
+  while ((byte = va_arg(ap, int)) != -1) {
+    expectedBytes.push_back(byte);
+  }
+  va_end(ap);
+
+  size_t size = GroupVarint32::size(a, b, c, d);
+  EXPECT_EQ(expectedBytes.size(), size);
+
+  std::vector<char> foundBytes;
+  foundBytes.resize(size + 4);
+  char* start = &(foundBytes.front());
+  char* p = GroupVarint32::encode(start, a, b, c, d);
+  EXPECT_EQ((void*)(start + size), (void*)p);
+
+  for (size_t i = 0; i < size; i++) {
+    EXPECT_EQ(0xff & expectedBytes[i], 0xff & foundBytes[i]);
+  }
+
+  // Test decoding
+  EXPECT_EQ(size, GroupVarint32::encodedSize(start));
+
+  uint32_t fa, fb, fc, fd;
+  const char* r = GroupVarint32::decode(start, &fa, &fb, &fc, &fd);
+  EXPECT_EQ((void*)(start + size), (void*)r);
+
+  EXPECT_EQ(a, fa);
+  EXPECT_EQ(b, fb);
+  EXPECT_EQ(c, fc);
+  EXPECT_EQ(d, fd);
+}
+
+void testGroupVarint64(uint64_t a, uint64_t b, uint64_t c, uint64_t d,
+                       uint64_t e, ...) {
+  va_list ap;
+  va_start(ap, e);
+  std::vector<char> expectedBytes;
+  int byte;
+  while ((byte = va_arg(ap, int)) != -1) {
+    expectedBytes.push_back(byte);
+  }
+  va_end(ap);
+
+  size_t size = GroupVarint64::size(a, b, c, d, e);
+  EXPECT_EQ(expectedBytes.size(), size);
+
+  std::vector<char> foundBytes;
+  foundBytes.resize(size + 8);
+  char* start = &(foundBytes.front());
+  char* p = GroupVarint64::encode(start, a, b, c, d, e);
+  EXPECT_EQ((void*)(start + size), (void*)p);
+
+  for (size_t i = 0; i < size; i++) {
+    EXPECT_EQ(0xff & expectedBytes[i], 0xff & foundBytes[i]);
+  }
+
+  // Test decoding
+  EXPECT_EQ(size, GroupVarint64::encodedSize(start));
+
+  uint64_t fa, fb, fc, fd, fe;
+  const char* r = GroupVarint64::decode(start, &fa, &fb, &fc, &fd, &fe);
+  EXPECT_EQ((void*)(start + size), (void*)r);
+
+  EXPECT_EQ(a, fa);
+  EXPECT_EQ(b, fb);
+  EXPECT_EQ(c, fc);
+  EXPECT_EQ(d, fd);
+  EXPECT_EQ(e, fe);
+}
+
+}  // namespace
+
+TEST(GroupVarint, GroupVarint32) {
+  EXPECT_EQ(0, GroupVarint32::maxSize(0));
+  EXPECT_EQ(5, GroupVarint32::maxSize(1));
+  EXPECT_EQ(9, GroupVarint32::maxSize(2));
+  EXPECT_EQ(13, GroupVarint32::maxSize(3));
+  EXPECT_EQ(17, GroupVarint32::maxSize(4));
+  EXPECT_EQ(22, GroupVarint32::maxSize(5));
+  EXPECT_EQ(26, GroupVarint32::maxSize(6));
+  testGroupVarint32(0, 0, 0, 0,
+                    0, 0, 0, 0, 0, -1);
+  testGroupVarint32(1, 2, 3, 4,
+                    0, 1, 2, 3, 4, -1);
+  testGroupVarint32(1 << 8, (2 << 16) + 3, (4 << 24) + (5 << 8) + 6, 7,
+                    0x39, 0, 1, 3, 0, 2, 6, 5, 0, 4, 7, -1);
+}
+
+TEST(GroupVarint, GroupVarint64) {
+  EXPECT_EQ(0, GroupVarint64::maxSize(0));
+  EXPECT_EQ(10, GroupVarint64::maxSize(1));
+  EXPECT_EQ(18, GroupVarint64::maxSize(2));
+  EXPECT_EQ(26, GroupVarint64::maxSize(3));
+  EXPECT_EQ(34, GroupVarint64::maxSize(4));
+  EXPECT_EQ(42, GroupVarint64::maxSize(5));
+  EXPECT_EQ(52, GroupVarint64::maxSize(6));
+  testGroupVarint64(0, 0, 0, 0, 0,
+                    0, 0, 0, 0, 0, 0, 0, -1);
+  testGroupVarint64(1, 2, 3, 4, 5,
+                    0, 0, 1, 2, 3, 4, 5, -1);
+  testGroupVarint64(1 << 8, (2 << 16) + 3, (4 << 24) + (5 << 8) + 6,
+                    (7ULL << 32) + (8 << 16),
+                    (9ULL << 56) + (10ULL << 40) + 11,
+                    0xd1, 0x78,
+                    0, 1,
+                    3, 0, 2,
+                    6, 5, 0, 4,
+                    0, 0, 8, 0, 7,
+                    11, 0, 0, 0, 0, 10, 0, 9,
+                    -1);
+}
+
+TEST(GroupVarint, GroupVarintEncoder) {
+  std::string s;
+  {
+    GroupVarint32Encoder gv(s);
+    gv.add(0);
+    gv.finish();
+  }
+  EXPECT_EQ(2, s.size());
+  EXPECT_EQ(std::string("\x00\x00", 2), s);
+  s.clear();
+  {
+    GroupVarint32Encoder gv(s);
+    gv.add(1);
+    gv.add(2);
+    gv.add(3);
+    gv.add(4);
+    gv.finish();
+  }
+  EXPECT_EQ(5, s.size());
+  EXPECT_EQ(std::string("\x00\x01\x02\x03\x04", 5), s);
+}
+
+
+TEST(GroupVarint, GroupVarintDecoder) {
+  // Make sure we don't read out of bounds
+  std::string padding(17, 'X');
+
+  {
+    std::string s("\x00\x00", 2);
+    s += padding;
+    StringPiece p(s.data(), 2);
+
+    GroupVarint32Decoder gv(p);
+    uint32_t v;
+    EXPECT_TRUE(gv.next(&v));
+    EXPECT_EQ(0, v);
+    EXPECT_FALSE(gv.next(&v));
+    EXPECT_TRUE(gv.rest().empty());
+  }
+
+  {
+    std::string s("\x00\x01\x02\x03\x04\x01\x02\x03\x04", 9);
+    s += padding;
+    StringPiece p(s.data(), 9);
+
+    GroupVarint32Decoder gv(p);
+    uint32_t v;
+    EXPECT_TRUE(gv.next(&v));
+    EXPECT_EQ(1, v);
+    EXPECT_TRUE(gv.next(&v));
+    EXPECT_EQ(2, v);
+    EXPECT_TRUE(gv.next(&v));
+    EXPECT_EQ(3, v);
+    EXPECT_TRUE(gv.next(&v));
+    EXPECT_EQ(4, v);
+    EXPECT_TRUE(gv.next(&v));
+    EXPECT_EQ(0x0302, v);
+    EXPECT_TRUE(gv.next(&v));
+    EXPECT_EQ(4, v);
+    EXPECT_FALSE(gv.next(&v));
+    EXPECT_TRUE(gv.rest().empty());
+  }
+
+  {
+    // Limit max count when reading a full block
+    std::string s("\x00\x01\x02\x03\x04\x01\x02\x03\x04", 9);
+    s += padding;
+    StringPiece p(s.data(), 9);
+
+    GroupVarint32Decoder gv(p, 3);
+    uint32_t v;
+    EXPECT_TRUE(gv.next(&v));
+    EXPECT_EQ(1, v);
+    EXPECT_TRUE(gv.next(&v));
+    EXPECT_EQ(2, v);
+    EXPECT_TRUE(gv.next(&v));
+    EXPECT_EQ(3, v);
+    EXPECT_FALSE(gv.next(&v));
+    EXPECT_EQ(std::string("\x04\x01\x02\x03\x04", 5), gv.rest().toString());
+  }
+
+  {
+    // Limit max count when reading a partial block
+    std::string s("\x00\x01\x02\x03\x04\x01\x02\x03\x04", 9);
+    s += padding;
+    StringPiece p(s.data(), 9);
+
+    GroupVarint32Decoder gv(p, 5);
+    uint32_t v;
+    EXPECT_TRUE(gv.next(&v));
+    EXPECT_EQ(1, v);
+    EXPECT_TRUE(gv.next(&v));
+    EXPECT_EQ(2, v);
+    EXPECT_TRUE(gv.next(&v));
+    EXPECT_EQ(3, v);
+    EXPECT_TRUE(gv.next(&v));
+    EXPECT_EQ(4, v);
+    EXPECT_TRUE(gv.next(&v));
+    EXPECT_EQ(0x0302, v);
+    EXPECT_FALSE(gv.next(&v));
+    EXPECT_EQ(std::string("\x04", 1), gv.rest().toString());
+  }
+}
+
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/HashTest.cpp
@@ -0,0 +1,251 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Hash.h"
+#include "folly/MapUtil.h"
+#include <gtest/gtest.h>
+#include <stdint.h>
+#include <unordered_map>
+#include <utility>
+
+using namespace folly::hash;
+
+TEST(Hash, Fnv32) {
+  const char* s1 = "hello, world!";
+  const uint32_t s1_res = 3605494790UL;
+  EXPECT_EQ(fnv32(s1), s1_res);
+  EXPECT_EQ(fnv32(s1), fnv32_buf(s1, strlen(s1)));
+
+  const char* s2 = "monkeys! m0nk3yz! ev3ry \\/\\/here~~~~";
+  const uint32_t s2_res = 1270448334UL;
+  EXPECT_EQ(fnv32(s2), s2_res);
+  EXPECT_EQ(fnv32(s2), fnv32_buf(s2, strlen(s2)));
+
+  const char* s3 = "";
+  const uint32_t s3_res = 2166136261UL;
+  EXPECT_EQ(fnv32(s3), s3_res);
+  EXPECT_EQ(fnv32(s3), fnv32_buf(s3, strlen(s3)));
+}
+
+TEST(Hash, Fnv64) {
+  const char* s1 = "hello, world!";
+  const uint64_t s1_res = 13991426986746681734ULL;
+  EXPECT_EQ(fnv64(s1), s1_res);
+  EXPECT_EQ(fnv64(s1), fnv64_buf(s1, strlen(s1)));
+
+  const char* s2 = "monkeys! m0nk3yz! ev3ry \\/\\/here~~~~";
+  const uint64_t s2_res = 6091394665637302478ULL;
+  EXPECT_EQ(fnv64(s2), s2_res);
+  EXPECT_EQ(fnv64(s2), fnv64_buf(s2, strlen(s2)));
+
+  const char* s3 = "";
+  const uint64_t s3_res = 14695981039346656037ULL;
+  EXPECT_EQ(fnv64(s3), s3_res);
+  EXPECT_EQ(fnv64(s3), fnv64_buf(s3, strlen(s3)));
+
+  // note: Use fnv64_buf to make a single hash value from multiple
+  // fields/datatypes.
+  const char* t4_a = "E Pluribus";
+  int64_t t4_b = 0xF1E2D3C4B5A69788;
+  int32_t t4_c = 0xAB12CD34;
+  const char* t4_d = "Unum";
+  uint64_t t4_res = 15571330457339273965ULL;
+  uint64_t t4_hash1 = fnv64_buf(t4_a,
+                                strlen(t4_a));
+  uint64_t t4_hash2 = fnv64_buf(reinterpret_cast<void*>(&t4_b),
+                                sizeof(int64_t),
+                                t4_hash1);
+  uint64_t t4_hash3 = fnv64_buf(reinterpret_cast<void*>(&t4_c),
+                                sizeof(int32_t),
+                                t4_hash2);
+  uint64_t t4_hash4 = fnv64_buf(t4_d,
+                                strlen(t4_d),
+                                t4_hash3);
+  EXPECT_EQ(t4_hash4, t4_res);
+  // note: These are probabalistic, not determinate, but c'mon.
+  // These hash values should be different, or something's not
+  // working.
+  EXPECT_NE(t4_hash1, t4_hash4);
+  EXPECT_NE(t4_hash2, t4_hash4);
+  EXPECT_NE(t4_hash3, t4_hash4);
+}
+
+TEST(Hash, Hsieh32) {
+  const char* s1 = "hello, world!";
+  const uint32_t s1_res = 2918802987ul;
+  EXPECT_EQ(hsieh_hash32(s1), s1_res);
+  EXPECT_EQ(hsieh_hash32(s1), hsieh_hash32_buf(s1, strlen(s1)));
+
+  const char* s2 = "monkeys! m0nk3yz! ev3ry \\/\\/here~~~~";
+  const uint32_t s2_res = 47373213ul;
+  EXPECT_EQ(hsieh_hash32(s2), s2_res);
+  EXPECT_EQ(hsieh_hash32(s2), hsieh_hash32_buf(s2, strlen(s2)));
+
+  const char* s3 = "";
+  const uint32_t s3_res = 0;
+  EXPECT_EQ(hsieh_hash32(s3), s3_res);
+  EXPECT_EQ(hsieh_hash32(s3), hsieh_hash32_buf(s3, strlen(s3)));
+}
+
+TEST(Hash, TWang_Mix64) {
+  uint64_t i1 = 0x78a87873e2d31dafULL;
+  uint64_t i1_res = 3389151152926383528ULL;
+  EXPECT_EQ(i1_res, twang_mix64(i1));
+  EXPECT_EQ(i1, twang_unmix64(i1_res));
+
+  uint64_t i2 = 0x0123456789abcdefULL;
+  uint64_t i2_res = 3061460455458984563ull;
+  EXPECT_EQ(i2_res, twang_mix64(i2));
+  EXPECT_EQ(i2, twang_unmix64(i2_res));
+}
+
+namespace {
+void checkTWang(uint64_t r) {
+  uint64_t result = twang_mix64(r);
+  EXPECT_EQ(r, twang_unmix64(result));
+}
+}  // namespace
+
+TEST(Hash, TWang_Unmix64) {
+  // We'll try (1 << i), (1 << i) + 1, (1 << i) - 1
+  for (int i = 1; i < 64; i++) {
+    checkTWang((1U << i) - 1);
+    checkTWang(1U << i);
+    checkTWang((1U << i) + 1);
+  }
+}
+
+TEST(Hash, TWang_32From64) {
+  uint64_t i1 = 0x78a87873e2d31dafULL;
+  uint32_t i1_res = 1525586863ul;
+  EXPECT_EQ(twang_32from64(i1), i1_res);
+
+  uint64_t i2 = 0x0123456789abcdefULL;
+  uint32_t i2_res = 2918899159ul;
+  EXPECT_EQ(twang_32from64(i2), i2_res);
+}
+
+TEST(Hash, Jenkins_Rev_Mix32) {
+  uint32_t i1 = 3805486511ul;
+  uint32_t i1_res = 381808021ul;
+  EXPECT_EQ(i1_res, jenkins_rev_mix32(i1));
+  EXPECT_EQ(i1, jenkins_rev_unmix32(i1_res));
+
+  uint32_t i2 = 2309737967ul;
+  uint32_t i2_res = 1834777923ul;
+  EXPECT_EQ(i2_res, jenkins_rev_mix32(i2));
+  EXPECT_EQ(i2, jenkins_rev_unmix32(i2_res));
+}
+
+namespace {
+void checkJenkins(uint32_t r) {
+  uint32_t result = jenkins_rev_mix32(r);
+  EXPECT_EQ(r, jenkins_rev_unmix32(result));
+}
+}  // namespace
+
+TEST(Hash, Jenkins_Rev_Unmix32) {
+  // We'll try (1 << i), (1 << i) + 1, (1 << i) - 1
+  for (int i = 1; i < 32; i++) {
+    checkJenkins((1U << i) - 1);
+    checkJenkins(1U << i);
+    checkJenkins((1U << i) + 1);
+  }
+}
+
+TEST(Hash, hasher) {
+  // Basically just confirms that things compile ok.
+  std::unordered_map<int32_t,int32_t,folly::hasher<int32_t>> m;
+  m.insert(std::make_pair(4, 5));
+  EXPECT_EQ(get_default(m, 4), 5);
+}
+
+// Not a full hasher since only handles one type
+class TestHasher {
+ public:
+  static size_t hash(const std::pair<int, int>& p) {
+    return p.first + p.second;
+  }
+};
+
+template <typename T, typename... Ts>
+size_t hash_combine_test(const T& t, const Ts&... ts) {
+  return hash_combine_generic<TestHasher>(t, ts...);
+}
+
+TEST(Hash, pair) {
+  auto a = std::make_pair(1, 2);
+  auto b = std::make_pair(3, 4);
+  auto c = std::make_pair(1, 2);
+  auto d = std::make_pair(2, 1);
+  EXPECT_EQ(hash_combine(a),
+            hash_combine(c));
+  EXPECT_NE(hash_combine(b),
+            hash_combine(c));
+  EXPECT_NE(hash_combine(d),
+            hash_combine(c));
+
+  // With composition
+  EXPECT_EQ(hash_combine(a, b),
+            hash_combine(c, b));
+  // Test order dependence
+  EXPECT_NE(hash_combine(a, b),
+            hash_combine(b, a));
+
+  // Test with custom hasher
+  EXPECT_EQ(hash_combine_test(a),
+            hash_combine_test(c));
+  // 3 + 4 != 1 + 2
+  EXPECT_NE(hash_combine_test(b),
+            hash_combine_test(c));
+  // This time, thanks to a terrible hash function, these are equal
+  EXPECT_EQ(hash_combine_test(d),
+            hash_combine_test(c));
+  // With composition
+  EXPECT_EQ(hash_combine_test(a, b),
+            hash_combine_test(c, b));
+  // Test order dependence
+  EXPECT_NE(hash_combine_test(a, b),
+            hash_combine_test(b, a));
+  // Again, 1 + 2 == 2 + 1
+  EXPECT_EQ(hash_combine_test(a, b),
+            hash_combine_test(d, b));
+}
+
+TEST(Hash, hash_combine) {
+  EXPECT_NE(hash_combine(1, 2), hash_combine(2, 1));
+}
+
+TEST(Hash, std_tuple) {
+  typedef std::tuple<int64_t, std::string, int32_t> tuple3;
+  tuple3 t(42, "foo", 1);
+
+  std::unordered_map<tuple3, std::string> m;
+  m[t] = "bar";
+  EXPECT_EQ("bar", m[t]);
+}
+
+TEST(Hash, std_tuple_different_hash) {
+  typedef std::tuple<int64_t, std::string, int32_t> tuple3;
+  tuple3 t1(42, "foo", 1);
+  tuple3 t2(9, "bar", 3);
+  tuple3 t3(42, "foo", 3);
+
+  EXPECT_NE(std::hash<tuple3>()(t1),
+            std::hash<tuple3>()(t2));
+  EXPECT_NE(std::hash<tuple3>()(t1),
+            std::hash<tuple3>()(t3));
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/HasMemberFnTraitsTest.cpp
@@ -0,0 +1,124 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+ * @author: Marcelo Juchem <marcelo@fb.com>
+ */
+
+#include "folly/Traits.h"
+
+#include <gtest/gtest.h>
+#include <glog/logging.h>
+
+#include <string>
+
+using namespace std;
+using namespace folly;
+
+FOLLY_CREATE_HAS_MEMBER_FN_TRAITS(has_test, test);
+
+struct Foo {
+  int test();
+  int test() const;
+  string test(const string&) const;
+};
+
+struct Bar {
+  int test();
+  double test(int,long);
+  long test(int) const;
+};
+
+struct Gaz {
+  void test();
+  void test() const;
+  void test() volatile;
+  void test() const volatile;
+};
+
+struct NoCV {
+  void test();
+};
+
+struct Const {
+  void test() const;
+};
+
+struct Volatile {
+  void test() volatile;
+};
+
+struct CV {
+  void test() const volatile;
+};
+
+bool log_value(const char* what, bool result) {
+  LOG(INFO) << what << ": " << boolalpha << result;
+  return result;
+}
+
+#define LOG_VALUE(x) log_value(#x, x)
+
+TEST(HasMemberFnTraits, DirectMembers) {
+  EXPECT_TRUE(LOG_VALUE((has_test<Foo, int()>::value)));
+  EXPECT_TRUE(LOG_VALUE((has_test<Foo, int() const>::value)));
+  EXPECT_FALSE(LOG_VALUE((has_test<Foo, double(int, long)>::value)));
+  EXPECT_TRUE(LOG_VALUE((has_test<Foo, string(const string&) const>::value)));
+  EXPECT_FALSE(LOG_VALUE((has_test<Foo, long(int) const>::value)));
+  EXPECT_FALSE(LOG_VALUE((has_test<Foo, string(string) const>::value)));
+
+  EXPECT_TRUE(LOG_VALUE((has_test<Bar, int()>::value)));
+  EXPECT_FALSE(LOG_VALUE((has_test<Bar, int() const>::value)));
+  EXPECT_TRUE(LOG_VALUE((has_test<Bar, double(int, long)>::value)));
+  EXPECT_FALSE(LOG_VALUE((has_test<Bar, string(const string&) const>::value)));
+  EXPECT_TRUE(LOG_VALUE((has_test<Bar, long(int) const>::value)));
+  EXPECT_FALSE(LOG_VALUE((has_test<Bar, string(string) const>::value)));
+
+  EXPECT_TRUE(LOG_VALUE((has_test<Gaz, void()>::value)));
+  EXPECT_TRUE(LOG_VALUE((has_test<Gaz, void() const>::value)));
+  EXPECT_TRUE(LOG_VALUE((has_test<Gaz, void() volatile>::value)));
+  EXPECT_TRUE(LOG_VALUE((has_test<Gaz, void() const volatile>::value)));
+  EXPECT_TRUE(LOG_VALUE((has_test<Gaz, void() volatile const>::value)));
+
+  EXPECT_TRUE(LOG_VALUE((has_test<NoCV, void()>::value)));
+  EXPECT_FALSE(LOG_VALUE((has_test<NoCV, void() const>::value)));
+  EXPECT_FALSE(LOG_VALUE((has_test<NoCV, void() volatile>::value)));
+  EXPECT_FALSE(LOG_VALUE((has_test<NoCV, void() const volatile>::value)));
+  EXPECT_FALSE(LOG_VALUE((has_test<NoCV, void() volatile const>::value)));
+
+  EXPECT_FALSE(LOG_VALUE((has_test<Const, void()>::value)));
+  EXPECT_TRUE(LOG_VALUE((has_test<Const, void() const>::value)));
+  EXPECT_FALSE(LOG_VALUE((has_test<Const, void() volatile>::value)));
+  EXPECT_FALSE(LOG_VALUE((has_test<Const, void() const volatile>::value)));
+  EXPECT_FALSE(LOG_VALUE((has_test<Const, void() volatile const>::value)));
+
+  EXPECT_FALSE(LOG_VALUE((has_test<Volatile, void()>::value)));
+  EXPECT_FALSE(LOG_VALUE((has_test<Volatile, void() const>::value)));
+  EXPECT_TRUE(LOG_VALUE((has_test<Volatile, void() volatile>::value)));
+  EXPECT_FALSE(LOG_VALUE((has_test<Volatile, void() const volatile>::value)));
+  EXPECT_FALSE(LOG_VALUE((has_test<Volatile, void() volatile const>::value)));
+
+  EXPECT_FALSE(LOG_VALUE((has_test<CV, void()>::value)));
+  EXPECT_FALSE(LOG_VALUE((has_test<CV, void() const>::value)));
+  EXPECT_FALSE(LOG_VALUE((has_test<CV, void() volatile>::value)));
+  EXPECT_TRUE(LOG_VALUE((has_test<CV, void() const volatile>::value)));
+  EXPECT_TRUE(LOG_VALUE((has_test<CV, void() volatile const>::value)));
+}
+
+int main(int argc, char *argv[]) {
+  testing::InitGoogleTest(&argc, argv);
+  return RUN_ALL_TESTS();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/HistogramBenchmark.cpp
@@ -0,0 +1,43 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#include "folly/stats/Histogram.h"
+
+#include <gflags/gflags.h>
+
+#include "folly/Benchmark.h"
+#include "folly/Foreach.h"
+
+using folly::Histogram;
+
+void addValue(uint n, int64_t bucketSize, int64_t min, int64_t max) {
+  Histogram<int64_t> hist(bucketSize, min, max);
+  int64_t num = min;
+  FOR_EACH_RANGE (i, 0, n) {
+    hist.addValue(num);
+    ++num;
+    if (num > max) { num = min; }
+  }
+}
+
+BENCHMARK_NAMED_PARAM(addValue, 0_to_100, 1, 0, 100);
+BENCHMARK_NAMED_PARAM(addValue, 0_to_1000, 10, 0, 1000);
+BENCHMARK_NAMED_PARAM(addValue, 5k_to_20k, 250, 5000, 20000);
+
+int main(int argc, char *argv[]) {
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  folly::runBenchmarks();
+  return 0;
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/HistogramTest.cpp
@@ -0,0 +1,205 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/stats/Histogram.h"
+#include "folly/stats/Histogram-defs.h"
+
+#include <gflags/gflags.h>
+#include <gtest/gtest.h>
+
+using folly::Histogram;
+
+// Insert 100 evenly distributed values into a histogram with 100 buckets
+TEST(Histogram, Test100) {
+  Histogram<int64_t> h(1, 0, 100);
+
+  for (unsigned int n = 0; n < 100; ++n) {
+    h.addValue(n);
+  }
+
+  // 100 buckets, plus 1 for below min, and 1 for above max
+  EXPECT_EQ(h.getNumBuckets(), 102);
+
+  double epsilon = 1e-6;
+  for (unsigned int n = 0; n <= 100; ++n) {
+    double pct = n / 100.0;
+
+    // Floating point arithmetic isn't 100% accurate, and if we just divide
+    // (n / 100) the value should be exactly on a bucket boundary.  Add espilon
+    // to ensure we fall in the upper bucket.
+    if (n < 100) {
+      double lowPct = -1.0;
+      double highPct = -1.0;
+      unsigned int bucketIdx = h.getPercentileBucketIdx(pct + epsilon,
+                                                        &lowPct, &highPct);
+      EXPECT_EQ(n + 1, bucketIdx);
+      EXPECT_FLOAT_EQ(n / 100.0, lowPct);
+      EXPECT_FLOAT_EQ((n + 1) / 100.0, highPct);
+    }
+
+    // Also test n - epsilon, to test falling in the lower bucket.
+    if (n > 0) {
+      double lowPct = -1.0;
+      double highPct = -1.0;
+      unsigned int bucketIdx = h.getPercentileBucketIdx(pct - epsilon,
+                                                        &lowPct, &highPct);
+      EXPECT_EQ(n, bucketIdx);
+      EXPECT_FLOAT_EQ((n - 1) / 100.0, lowPct);
+      EXPECT_FLOAT_EQ(n / 100.0, highPct);
+    }
+
+    // Check getPercentileEstimate()
+    EXPECT_EQ(n, h.getPercentileEstimate(pct));
+  }
+}
+
+// Test calling getPercentileBucketIdx() and getPercentileEstimate() on an
+// empty histogram
+TEST(Histogram, TestEmpty) {
+  Histogram<int64_t> h(1, 0, 100);
+
+  for (unsigned int n = 0; n <= 100; ++n) {
+    double pct = n / 100.0;
+
+    double lowPct = -1.0;
+    double highPct = -1.0;
+    unsigned int bucketIdx = h.getPercentileBucketIdx(pct, &lowPct, &highPct);
+    EXPECT_EQ(1, bucketIdx);
+    EXPECT_FLOAT_EQ(0.0, lowPct);
+    EXPECT_FLOAT_EQ(0.0, highPct);
+
+    EXPECT_EQ(0, h.getPercentileEstimate(pct));
+  }
+}
+
+// Test calling getPercentileBucketIdx() and getPercentileEstimate() on a
+// histogram with just a single value.
+TEST(Histogram, Test1) {
+  Histogram<int64_t> h(1, 0, 100);
+  h.addValue(42);
+
+  for (unsigned int n = 0; n < 100; ++n) {
+    double pct = n / 100.0;
+
+    double lowPct = -1.0;
+    double highPct = -1.0;
+    unsigned int bucketIdx = h.getPercentileBucketIdx(pct, &lowPct, &highPct);
+    EXPECT_EQ(43, bucketIdx);
+    EXPECT_FLOAT_EQ(0.0, lowPct);
+    EXPECT_FLOAT_EQ(1.0, highPct);
+
+    EXPECT_EQ(42, h.getPercentileEstimate(pct));
+  }
+}
+
+// Test adding enough numbers to make the sum value overflow in the
+// "below min" bucket
+TEST(Histogram, TestOverflowMin) {
+  Histogram<int64_t> h(1, 0, 100);
+
+  for (unsigned int n = 0; n < 9; ++n) {
+    h.addValue(-0x0fffffffffffffff);
+  }
+
+  // Compute a percentile estimate.  We only added values to the "below min"
+  // bucket, so this should check that bucket.  We're mainly verifying that the
+  // code doesn't crash here when the bucket average is larger than the max
+  // value that is supposed to be in the bucket.
+  int64_t estimate = h.getPercentileEstimate(0.05);
+  // The code will return the smallest possible value when it detects an
+  // overflow beyond the minimum value.
+  EXPECT_EQ(std::numeric_limits<int64_t>::min(), estimate);
+}
+
+// Test adding enough numbers to make the sum value overflow in the
+// "above max" bucket
+TEST(Histogram, TestOverflowMax) {
+  Histogram<int64_t> h(1, 0, 100);
+
+  for (unsigned int n = 0; n < 9; ++n) {
+    h.addValue(0x0fffffffffffffff);
+  }
+
+  // The code will return the maximum possible value when it detects an
+  // overflow beyond the max value.
+  int64_t estimate = h.getPercentileEstimate(0.95);
+  EXPECT_EQ(std::numeric_limits<int64_t>::max(), estimate);
+}
+
+// Test adding enough numbers to make the sum value overflow in one of the
+// normal buckets
+TEST(Histogram, TestOverflowBucket) {
+  Histogram<int64_t> h(0x0100000000000000, 0, 0x1000000000000000);
+
+  for (unsigned int n = 0; n < 9; ++n) {
+    h.addValue(0x0fffffffffffffff);
+  }
+
+  // The histogram code should return the bucket midpoint
+  // when it detects overflow.
+  int64_t estimate = h.getPercentileEstimate(0.95);
+  EXPECT_EQ(0x0f80000000000000, estimate);
+}
+
+TEST(Histogram, TestDouble) {
+  // Insert 100 evenly spaced values into a histogram
+  Histogram<double> h(100.0, 0.0, 5000.0);
+  for (double n = 50; n < 5000; n += 100) {
+    h.addValue(n);
+  }
+  EXPECT_EQ(52, h.getNumBuckets());
+  EXPECT_EQ(2500.0, h.getPercentileEstimate(0.5));
+  EXPECT_EQ(4500.0, h.getPercentileEstimate(0.9));
+}
+
+// Test where the bucket width is not an even multiple of the histogram range
+TEST(Histogram, TestDoubleInexactWidth) {
+  Histogram<double> h(100.0, 0.0, 4970.0);
+  for (double n = 50; n < 5000; n += 100) {
+    h.addValue(n);
+  }
+  EXPECT_EQ(52, h.getNumBuckets());
+  EXPECT_EQ(2500.0, h.getPercentileEstimate(0.5));
+  EXPECT_EQ(4500.0, h.getPercentileEstimate(0.9));
+
+  EXPECT_EQ(0, h.getBucketByIndex(51).count);
+  h.addValue(4990);
+  h.addValue(5100);
+  EXPECT_EQ(2, h.getBucketByIndex(51).count);
+  EXPECT_EQ(2600.0, h.getPercentileEstimate(0.5));
+}
+
+// Test where the bucket width is larger than the histogram range
+// (There isn't really much point to defining a histogram this way,
+// but we want to ensure that it still works just in case.)
+TEST(Histogram, TestDoubleWidthTooBig) {
+  Histogram<double> h(100.0, 0.0, 7.0);
+  EXPECT_EQ(3, h.getNumBuckets());
+
+  for (double n = 0; n < 7; n += 1) {
+    h.addValue(n);
+  }
+  EXPECT_EQ(0, h.getBucketByIndex(0).count);
+  EXPECT_EQ(7, h.getBucketByIndex(1).count);
+  EXPECT_EQ(0, h.getBucketByIndex(2).count);
+  EXPECT_EQ(3.0, h.getPercentileEstimate(0.5));
+
+  h.addValue(-1.0);
+  EXPECT_EQ(1, h.getBucketByIndex(0).count);
+  h.addValue(7.5);
+  EXPECT_EQ(1, h.getBucketByIndex(2).count);
+  EXPECT_EQ(3.0, h.getPercentileEstimate(0.5));
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/IndexedMemPoolTest.cpp
@@ -0,0 +1,162 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <folly/IndexedMemPool.h>
+#include <folly/test/DeterministicSchedule.h>
+#include <thread>
+#include <unistd.h>
+#include <semaphore.h>
+#include <gflags/gflags.h>
+#include <gtest/gtest.h>
+
+using namespace folly;
+using namespace folly::test;
+
+TEST(IndexedMemPool, unique_ptr) {
+  typedef IndexedMemPool<size_t> Pool;
+  Pool pool(100);
+
+  for (size_t i = 0; i < 100000; ++i) {
+    auto ptr = pool.allocElem();
+    EXPECT_TRUE(!!ptr);
+    *ptr = i;
+  }
+
+  std::vector<Pool::UniquePtr> leak;
+  while (true) {
+    auto ptr = pool.allocElem();
+    if (!ptr) {
+      // good, we finally ran out
+      break;
+    }
+    leak.emplace_back(std::move(ptr));
+    EXPECT_LT(leak.size(), 10000);
+  }
+}
+
+TEST(IndexedMemPool, no_starvation) {
+  const int count = 1000;
+  const int poolSize = 100;
+
+  typedef DeterministicSchedule Sched;
+  Sched sched(Sched::uniform(0));
+
+  typedef IndexedMemPool<int,8,8,DeterministicAtomic> Pool;
+  Pool pool(poolSize);
+
+  for (auto pass = 0; pass < 10; ++pass) {
+    int fd[2];
+    EXPECT_EQ(pipe(fd), 0);
+
+    // makes sure we wait for available nodes, rather than fail allocIndex
+    sem_t allocSem;
+    sem_init(&allocSem, 0, poolSize);
+
+    // this semaphore is only needed for deterministic replay, so that we
+    // always block in an Sched:: operation rather than in a read() syscall
+    sem_t readSem;
+    sem_init(&readSem, 0, 0);
+
+    std::thread produce = Sched::thread([&]() {
+      for (auto i = 0; i < count; ++i) {
+        Sched::wait(&allocSem);
+        uint32_t idx = pool.allocIndex();
+        EXPECT_NE(idx, 0);
+        EXPECT_LE(idx,
+            poolSize + (pool.NumLocalLists - 1) * pool.LocalListLimit);
+        pool[idx] = i;
+        EXPECT_EQ(write(fd[1], &idx, sizeof(idx)), sizeof(idx));
+        Sched::post(&readSem);
+      }
+    });
+
+    std::thread consume = Sched::thread([&]() {
+      for (auto i = 0; i < count; ++i) {
+        uint32_t idx;
+        Sched::wait(&readSem);
+        EXPECT_EQ(read(fd[0], &idx, sizeof(idx)), sizeof(idx));
+        EXPECT_NE(idx, 0);
+        EXPECT_GE(idx, 1);
+        EXPECT_LE(idx,
+            poolSize + (Pool::NumLocalLists - 1) * Pool::LocalListLimit);
+        EXPECT_EQ(pool[idx], i);
+        pool.recycleIndex(idx);
+        Sched::post(&allocSem);
+      }
+    });
+
+    Sched::join(produce);
+    Sched::join(consume);
+    close(fd[0]);
+    close(fd[1]);
+  }
+}
+
+TEST(IndexedMemPool, st_capacity) {
+  // only one local list => capacity is exact
+  typedef IndexedMemPool<int,1,32> Pool;
+  Pool pool(10);
+
+  EXPECT_EQ(pool.capacity(), 10);
+  EXPECT_EQ(Pool::maxIndexForCapacity(10), 10);
+  for (auto i = 0; i < 10; ++i) {
+    EXPECT_NE(pool.allocIndex(), 0);
+  }
+  EXPECT_EQ(pool.allocIndex(), 0);
+}
+
+TEST(IndexedMemPool, mt_capacity) {
+  typedef IndexedMemPool<int,16,32> Pool;
+  Pool pool(1000);
+
+  std::thread threads[10];
+  for (auto i = 0; i < 10; ++i) {
+    threads[i] = std::thread([&]() {
+      for (auto j = 0; j < 100; ++j) {
+        uint32_t idx = pool.allocIndex();
+        EXPECT_NE(idx, 0);
+      }
+    });
+  }
+
+  for (auto i = 0; i < 10; ++i) {
+    threads[i].join();
+  }
+
+  for (auto i = 0; i < 16 * 32; ++i) {
+    pool.allocIndex();
+  }
+  EXPECT_EQ(pool.allocIndex(), 0);
+}
+
+TEST(IndexedMemPool, locate_elem) {
+  IndexedMemPool<int> pool(1000);
+
+  for (auto i = 0; i < 1000; ++i) {
+    auto idx = pool.allocIndex();
+    EXPECT_FALSE(idx == 0);
+    int* elem = &pool[idx];
+    EXPECT_TRUE(idx == pool.locateElem(elem));
+  }
+
+  EXPECT_EQ(pool.locateElem(nullptr), 0);
+}
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  return RUN_ALL_TESTS();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/JsonTest.cpp
@@ -0,0 +1,487 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/json.h"
+#include <gtest/gtest.h>
+#include <gflags/gflags.h>
+#include <cmath>
+#include <limits>
+#include <iostream>
+#include <boost/next_prior.hpp>
+#include "folly/Benchmark.h"
+
+using folly::dynamic;
+using folly::parseJson;
+using folly::toJson;
+
+TEST(Json, Unicode) {
+  auto val = parseJson("\"I \u2665 UTF-8\"");
+  EXPECT_EQ("I \u2665 UTF-8", val.asString());
+  val = parseJson("\"I \\u2665 UTF-8\"");
+  EXPECT_EQ("I \u2665 UTF-8", val.asString());
+  val = parseJson("\"I \U0001D11E playing in G-clef\"");
+  EXPECT_EQ("I \U0001D11E playing in G-clef", val.asString());
+
+  val = parseJson("\"I \\uD834\\uDD1E playing in G-clef\"");
+  EXPECT_EQ("I \U0001D11E playing in G-clef", val.asString());
+}
+
+TEST(Json, Parse) {
+  auto num = parseJson("12");
+  EXPECT_TRUE(num.isInt());
+  EXPECT_EQ(num, 12);
+  num = parseJson("12e5");
+  EXPECT_TRUE(num.isDouble());
+  EXPECT_EQ(num, 12e5);
+  auto numAs1 = num.asDouble();
+  EXPECT_EQ(numAs1, 12e5);
+  EXPECT_EQ(num, 12e5);
+  EXPECT_EQ(num, 1200000);
+
+  auto largeNumber = parseJson("4611686018427387904");
+  EXPECT_TRUE(largeNumber.isInt());
+  EXPECT_EQ(largeNumber, 4611686018427387904L);
+
+  auto negative = parseJson("-123");
+  EXPECT_EQ(negative, -123);
+
+  auto bfalse = parseJson("false");
+  auto btrue = parseJson("true");
+  EXPECT_EQ(bfalse, false);
+  EXPECT_EQ(btrue, true);
+
+  auto null = parseJson("null");
+  EXPECT_TRUE(null == nullptr);
+
+  auto doub1 = parseJson("12.0");
+  auto doub2 = parseJson("12e2");
+  EXPECT_EQ(doub1, 12.0);
+  EXPECT_EQ(doub2, 12e2);
+  EXPECT_EQ(std::numeric_limits<double>::infinity(),
+            parseJson("Infinity").asDouble());
+  EXPECT_EQ(-std::numeric_limits<double>::infinity(),
+            parseJson("-Infinity").asDouble());
+  EXPECT_TRUE(std::isnan(parseJson("NaN").asDouble()));
+
+  // case matters
+  EXPECT_THROW(parseJson("infinity"), std::runtime_error);
+  EXPECT_THROW(parseJson("inf"), std::runtime_error);
+  EXPECT_THROW(parseJson("nan"), std::runtime_error);
+
+  auto array = parseJson(
+    "[12,false, false  , null , [12e4,32, [], 12]]");
+  EXPECT_EQ(array.size(), 5);
+  if (array.size() == 5) {
+    EXPECT_EQ(boost::prior(array.end())->size(), 4);
+  }
+
+  EXPECT_THROW(parseJson("\n[12,\n\nnotvalidjson"),
+               std::runtime_error);
+
+  EXPECT_THROW(parseJson("12e2e2"),
+               std::runtime_error);
+
+  EXPECT_THROW(parseJson("{\"foo\":12,\"bar\":42} \"something\""),
+               std::runtime_error);
+
+  dynamic value = dynamic::object
+    ("foo", "bar")
+    ("junk", 12)
+    ("another", 32.2)
+    ("a",
+      {
+        dynamic::object("a", "b")
+                       ("c", "d"),
+        12.5,
+        "Yo Dawg",
+        { "heh" },
+        nullptr
+      }
+    )
+    ;
+
+  // Print then parse and get the same thing, hopefully.
+  EXPECT_EQ(value, parseJson(toJson(value)));
+
+
+  // Test an object with non-string values.
+  dynamic something = parseJson(
+    "{\"old_value\":40,\"changed\":true,\"opened\":false}");
+  dynamic expected = dynamic::object
+    ("old_value", 40)
+    ("changed", true)
+    ("opened", false);
+  EXPECT_EQ(something, expected);
+}
+
+TEST(Json, ParseTrailingComma) {
+  folly::json::serialization_opts on, off;
+  on.allow_trailing_comma = true;
+  off.allow_trailing_comma = false;
+
+  dynamic arr { 1, 2 };
+  EXPECT_EQ(arr, parseJson("[1, 2]", on));
+  EXPECT_EQ(arr, parseJson("[1, 2,]", on));
+  EXPECT_EQ(arr, parseJson("[1, 2, ]", on));
+  EXPECT_EQ(arr, parseJson("[1, 2 , ]", on));
+  EXPECT_EQ(arr, parseJson("[1, 2 ,]", on));
+  EXPECT_THROW(parseJson("[1, 2,]", off), std::runtime_error);
+
+  dynamic obj = dynamic::object("a", 1);
+  EXPECT_EQ(obj, parseJson("{\"a\": 1}", on));
+  EXPECT_EQ(obj, parseJson("{\"a\": 1,}", on));
+  EXPECT_EQ(obj, parseJson("{\"a\": 1, }", on));
+  EXPECT_EQ(obj, parseJson("{\"a\": 1 , }", on));
+  EXPECT_EQ(obj, parseJson("{\"a\": 1 ,}", on));
+  EXPECT_THROW(parseJson("{\"a\":1,}", off), std::runtime_error);
+}
+
+TEST(Json, JavascriptSafe) {
+  auto badDouble = (1ll << 63ll) + 1;
+  dynamic badDyn = badDouble;
+  EXPECT_EQ(folly::toJson(badDouble), folly::to<folly::fbstring>(badDouble));
+  folly::json::serialization_opts opts;
+  opts.javascript_safe = true;
+  EXPECT_ANY_THROW(folly::json::serialize(badDouble, opts));
+
+  auto okDouble = 1ll << 63ll;
+  dynamic okDyn = okDouble;
+  EXPECT_EQ(folly::toJson(okDouble), folly::to<folly::fbstring>(okDouble));
+}
+
+TEST(Json, Produce) {
+  auto value = parseJson(R"( "f\"oo" )");
+  EXPECT_EQ(toJson(value), R"("f\"oo")");
+  value = parseJson("\"Control code: \001 \002 \x1f\"");
+  EXPECT_EQ(toJson(value), R"("Control code: \u0001 \u0002 \u001f")");
+
+  // We're not allowed to have non-string keys in json.
+  EXPECT_THROW(toJson(dynamic::object("abc", "xyz")(42.33, "asd")),
+               std::runtime_error);
+}
+
+TEST(Json, JsonEscape) {
+  folly::json::serialization_opts opts;
+  EXPECT_EQ(
+    folly::json::serialize("\b\f\n\r\x01\t\\\"/\v\a", opts),
+    R"("\b\f\n\r\u0001\t\\\"/\u000b\u0007")");
+}
+
+TEST(Json, JsonNonAsciiEncoding) {
+  folly::json::serialization_opts opts;
+  opts.encode_non_ascii = true;
+
+  // simple tests
+  EXPECT_EQ(folly::json::serialize("\x1f", opts), R"("\u001f")");
+  EXPECT_EQ(folly::json::serialize("\xc2\xa2", opts), R"("\u00a2")");
+  EXPECT_EQ(folly::json::serialize("\xe2\x82\xac", opts), R"("\u20ac")");
+
+  // multiple unicode encodings
+  EXPECT_EQ(
+    folly::json::serialize("\x1f\xe2\x82\xac", opts),
+    R"("\u001f\u20ac")");
+  EXPECT_EQ(
+    folly::json::serialize("\x1f\xc2\xa2\xe2\x82\xac", opts),
+    R"("\u001f\u00a2\u20ac")");
+  EXPECT_EQ(
+    folly::json::serialize("\xc2\x80\xef\xbf\xbf", opts),
+    R"("\u0080\uffff")");
+  EXPECT_EQ(
+    folly::json::serialize("\xe0\xa0\x80\xdf\xbf", opts),
+    R"("\u0800\u07ff")");
+
+  // first possible sequence of a certain length
+  EXPECT_EQ(folly::json::serialize("\xc2\x80", opts), R"("\u0080")");
+  EXPECT_EQ(folly::json::serialize("\xe0\xa0\x80", opts), R"("\u0800")");
+
+  // last possible sequence of a certain length
+  EXPECT_EQ(folly::json::serialize("\xdf\xbf", opts), R"("\u07ff")");
+  EXPECT_EQ(folly::json::serialize("\xef\xbf\xbf", opts), R"("\uffff")");
+
+  // other boundary conditions
+  EXPECT_EQ(folly::json::serialize("\xed\x9f\xbf", opts), R"("\ud7ff")");
+  EXPECT_EQ(folly::json::serialize("\xee\x80\x80", opts), R"("\ue000")");
+  EXPECT_EQ(folly::json::serialize("\xef\xbf\xbd", opts), R"("\ufffd")");
+
+  // incomplete sequences
+  EXPECT_ANY_THROW(folly::json::serialize("a\xed\x9f", opts));
+  EXPECT_ANY_THROW(folly::json::serialize("b\xee\x80", opts));
+  EXPECT_ANY_THROW(folly::json::serialize("c\xef\xbf", opts));
+
+  // impossible bytes
+  EXPECT_ANY_THROW(folly::json::serialize("\xfe", opts));
+  EXPECT_ANY_THROW(folly::json::serialize("\xff", opts));
+
+  // Sample overlong sequences
+  EXPECT_ANY_THROW(folly::json::serialize("\xc0\xaf", opts));
+  EXPECT_ANY_THROW(folly::json::serialize("\xe0\x80\xaf", opts));
+
+  // Maximum overlong sequences
+  EXPECT_ANY_THROW(folly::json::serialize("\xc1\xbf", opts));
+  EXPECT_ANY_THROW(folly::json::serialize("\x30\x9f\xbf", opts));
+
+  // illegal code positions
+  EXPECT_ANY_THROW(folly::json::serialize("\xed\xa0\x80", opts));
+  EXPECT_ANY_THROW(folly::json::serialize("\xed\xbf\xbf", opts));
+
+  // Overlong representation of NUL character
+  EXPECT_ANY_THROW(folly::json::serialize("\xc0\x80", opts));
+  EXPECT_ANY_THROW(folly::json::serialize("\xe0\x80\x80", opts));
+
+  // Longer than 3 byte encodings
+  EXPECT_ANY_THROW(folly::json::serialize("\xf4\x8f\xbf\xbf", opts));
+  EXPECT_ANY_THROW(folly::json::serialize("\xed\xaf\xbf\xed\xbf\xbf", opts));
+}
+
+TEST(Json, UTF8Retention) {
+
+  // test retention with valid utf8 strings
+  folly::fbstring input = "\u2665";
+  folly::fbstring jsonInput = folly::toJson(input);
+  folly::fbstring output = folly::parseJson(jsonInput).asString();
+  folly::fbstring jsonOutput = folly::toJson(output);
+
+  EXPECT_EQ(input, output);
+  EXPECT_EQ(jsonInput, jsonOutput);
+
+  // test retention with invalid utf8 - note that non-ascii chars are retained
+  // as is, and no unicode encoding is attempted so no exception is thrown.
+  EXPECT_EQ(
+    folly::toJson("a\xe0\xa0\x80z\xc0\x80"),
+    "\"a\xe0\xa0\x80z\xc0\x80\""
+  );
+}
+
+TEST(Json, UTF8EncodeNonAsciiRetention) {
+
+  folly::json::serialization_opts opts;
+  opts.encode_non_ascii = true;
+
+  // test encode_non_ascii valid utf8 strings
+  folly::fbstring input = "\u2665";
+  folly::fbstring jsonInput = folly::json::serialize(input, opts);
+  folly::fbstring output = folly::parseJson(jsonInput).asString();
+  folly::fbstring jsonOutput = folly::json::serialize(output, opts);
+
+  EXPECT_EQ(input, output);
+  EXPECT_EQ(jsonInput, jsonOutput);
+
+  // test encode_non_ascii with invalid utf8 - note that an attempt to encode
+  // non-ascii to unicode will result is a utf8 validation and throw exceptions.
+  EXPECT_ANY_THROW(folly::json::serialize("a\xe0\xa0\x80z\xc0\x80", opts));
+  EXPECT_ANY_THROW(folly::json::serialize("a\xe0\xa0\x80z\xe0\x80\x80", opts));
+}
+
+TEST(Json, UTF8Validation) {
+  folly::json::serialization_opts opts;
+  opts.validate_utf8 = true;
+
+  // test validate_utf8 valid utf8 strings - note that we only validate the
+  // for utf8 but don't encode non-ascii to unicode so they are retained as is.
+  EXPECT_EQ(folly::json::serialize("a\xc2\x80z", opts), "\"a\xc2\x80z\"");
+  EXPECT_EQ(
+    folly::json::serialize("a\xe0\xa0\x80z", opts),
+    "\"a\xe0\xa0\x80z\"");
+  EXPECT_EQ(
+    folly::json::serialize("a\xe0\xa0\x80m\xc2\x80z", opts),
+    "\"a\xe0\xa0\x80m\xc2\x80z\"");
+
+  // test validate_utf8 with invalid utf8
+  EXPECT_ANY_THROW(folly::json::serialize("a\xe0\xa0\x80z\xc0\x80", opts));
+  EXPECT_ANY_THROW(folly::json::serialize("a\xe0\xa0\x80z\xe0\x80\x80", opts));
+
+  opts.skip_invalid_utf8 = true;
+  EXPECT_EQ(folly::json::serialize("a\xe0\xa0\x80z\xc0\x80", opts),
+            "\"a\xe0\xa0\x80z\ufffd\ufffd\"");
+  EXPECT_EQ(folly::json::serialize("a\xe0\xa0\x80z\xc0\x80\x80", opts),
+            "\"a\xe0\xa0\x80z\ufffd\ufffd\ufffd\"");
+  EXPECT_EQ(folly::json::serialize("z\xc0\x80z\xe0\xa0\x80", opts),
+            "\"z\ufffd\ufffdz\xe0\xa0\x80\"");
+
+  opts.encode_non_ascii = true;
+  EXPECT_EQ(folly::json::serialize("a\xe0\xa0\x80z\xc0\x80", opts),
+            "\"a\\u0800z\\ufffd\\ufffd\"");
+  EXPECT_EQ(folly::json::serialize("a\xe0\xa0\x80z\xc0\x80\x80", opts),
+            "\"a\\u0800z\\ufffd\\ufffd\\ufffd\"");
+  EXPECT_EQ(folly::json::serialize("z\xc0\x80z\xe0\xa0\x80", opts),
+            "\"z\\ufffd\\ufffdz\\u0800\"");
+
+}
+
+
+TEST(Json, ParseNonStringKeys) {
+  // test string keys
+  EXPECT_EQ("a", parseJson("{\"a\":[]}").items().begin()->first.asString());
+
+  // check that we don't allow non-string keys as this violates the
+  // strict JSON spec (though it is emitted by the output of
+  // folly::dynamic with operator <<).
+  EXPECT_THROW(parseJson("{1:[]}"), std::runtime_error);
+
+  // check that we can parse colloquial JSON if the option is set
+  folly::json::serialization_opts opts;
+  opts.allow_non_string_keys = true;
+
+  auto val = parseJson("{1:[]}", opts);
+  EXPECT_EQ(1, val.items().begin()->first.asInt());
+
+
+  // test we can still read in strings
+  auto sval = parseJson("{\"a\":[]}", opts);
+  EXPECT_EQ("a", sval.items().begin()->first.asString());
+
+  // test we can read in doubles
+  auto dval = parseJson("{1.5:[]}", opts);
+  EXPECT_EQ(1.5, dval.items().begin()->first.asDouble());
+}
+
+TEST(Json, SortKeys) {
+  folly::json::serialization_opts opts_on, opts_off;
+  opts_on.sort_keys = true;
+  opts_off.sort_keys = false;
+
+  dynamic value = dynamic::object
+    ("foo", "bar")
+    ("junk", 12)
+    ("another", 32.2)
+    ("a",
+      {
+        dynamic::object("a", "b")
+                       ("c", "d"),
+        12.5,
+        "Yo Dawg",
+        { "heh" },
+        nullptr
+      }
+    )
+    ;
+
+  std::string sorted_keys =
+    R"({"a":[{"a":"b","c":"d"},12.5,"Yo Dawg",["heh"],null],)"
+    R"("another":32.2,"foo":"bar","junk":12})";
+
+  EXPECT_EQ(value, parseJson(folly::json::serialize(value, opts_on)));
+  EXPECT_EQ(value, parseJson(folly::json::serialize(value, opts_off)));
+
+  EXPECT_EQ(sorted_keys, folly::json::serialize(value, opts_on));
+}
+
+BENCHMARK(jsonSerialize, iters) {
+  folly::json::serialization_opts opts;
+  for (int i = 0; i < iters; ++i) {
+    folly::json::serialize(
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy",
+      opts);
+  }
+}
+
+BENCHMARK(jsonSerializeWithNonAsciiEncoding, iters) {
+  folly::json::serialization_opts opts;
+  opts.encode_non_ascii = true;
+
+  for (int i = 0; i < iters; ++i) {
+    folly::json::serialize(
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy",
+      opts);
+  }
+}
+
+BENCHMARK(jsonSerializeWithUtf8Validation, iters) {
+  folly::json::serialization_opts opts;
+  opts.validate_utf8 = true;
+
+  for (int i = 0; i < iters; ++i) {
+    folly::json::serialize(
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy"
+      "qwerty \xc2\x80 \xef\xbf\xbf poiuy",
+      opts);
+  }
+}
+
+BENCHMARK(parseSmallStringWithUtf, iters) {
+  for (int i = 0; i < iters << 4; ++i) {
+    parseJson("\"I \\u2665 UTF-8 thjasdhkjh blah blah blah\"");
+  }
+}
+
+BENCHMARK(parseNormalString, iters) {
+  for (int i = 0; i < iters << 4; ++i) {
+    parseJson("\"akjhfk jhkjlakjhfk jhkjlakjhfk jhkjl akjhfk\"");
+  }
+}
+
+BENCHMARK(parseBigString, iters) {
+  for (int i = 0; i < iters; ++i) {
+    parseJson("\""
+      "akjhfk jhkjlakjhfk jhkjlakjhfk jhkjl akjhfk"
+      "akjhfk jhkjlakjhfk jhkjlakjhfk jhkjl akjhfk"
+      "akjhfk jhkjlakjhfk jhkjlakjhfk jhkjl akjhfk"
+      "akjhfk jhkjlakjhfk jhkjlakjhfk jhkjl akjhfk"
+      "akjhfk jhkjlakjhfk jhkjlakjhfk jhkjl akjhfk"
+      "akjhfk jhkjlakjhfk jhkjlakjhfk jhkjl akjhfk"
+      "akjhfk jhkjlakjhfk jhkjlakjhfk jhkjl akjhfk"
+      "akjhfk jhkjlakjhfk jhkjlakjhfk jhkjl akjhfk"
+      "akjhfk jhkjlakjhfk jhkjlakjhfk jhkjl akjhfk"
+      "akjhfk jhkjlakjhfk jhkjlakjhfk jhkjl akjhfk"
+      "akjhfk jhkjlakjhfk jhkjlakjhfk jhkjl akjhfk"
+      "\"");
+  }
+}
+
+BENCHMARK(toJson, iters) {
+  dynamic something = parseJson(
+    "{\"old_value\":40,\"changed\":true,\"opened\":false,\"foo\":[1,2,3,4,5,6]}"
+  );
+
+  for (int i = 0; i < iters; i++) {
+    toJson(something);
+  }
+}
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  if (FLAGS_benchmark) {
+    folly::runBenchmarks();
+  }
+  return RUN_ALL_TESTS();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/LazyTest.cpp
@@ -0,0 +1,108 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#include "folly/Lazy.h"
+
+#include <map>
+#include <functional>
+#include <iostream>
+
+#include <gtest/gtest.h>
+
+namespace folly {
+
+TEST(Lazy, Simple) {
+  int computeCount = 0;
+
+  auto const val = folly::lazy([&]() -> int {
+    EXPECT_EQ(++computeCount, 1);
+    return 12;
+  });
+  EXPECT_EQ(computeCount, 0);
+
+  for (int i = 0; i < 100; ++i) {
+    if (i > 50) {
+      EXPECT_EQ(val(), 12);
+      EXPECT_EQ(computeCount, 1);
+    } else {
+      EXPECT_EQ(computeCount, 0);
+    }
+  }
+  EXPECT_EQ(val(), 12);
+  EXPECT_EQ(computeCount, 1);
+}
+
+auto globalCount = folly::lazy([]{ return 0; });
+auto const foo = folly::lazy([]() -> std::string {
+  EXPECT_EQ(++globalCount(), 1);
+  return std::string("YEP");
+});
+
+TEST(Lazy, Global) {
+  EXPECT_EQ(globalCount(), 0);
+  EXPECT_EQ(foo(), "YEP");
+  EXPECT_EQ(globalCount(), 1);
+}
+
+TEST(Lazy, Map) {
+  auto lazyMap = folly::lazy([]() -> std::map<std::string,std::string> {
+    return {
+      { "foo", "bar" },
+      { "baz", "quux" }
+    };
+  });
+
+  EXPECT_EQ(lazyMap().size(), 2);
+  lazyMap()["blah"] = "asd";
+  EXPECT_EQ(lazyMap().size(), 3);
+}
+
+struct CopyCount {
+  CopyCount() {}
+  CopyCount(const CopyCount&) { ++count; }
+  CopyCount(CopyCount&&)      {}
+
+  static int count;
+
+  bool operator()() const { return true ; }
+};
+
+int CopyCount::count = 0;
+
+TEST(Lazy, NonLambda) {
+  auto const rval = folly::lazy(CopyCount());
+  EXPECT_EQ(CopyCount::count, 0);
+  EXPECT_EQ(rval(), true);
+  EXPECT_EQ(CopyCount::count, 0);
+
+  CopyCount cpy;
+  auto const lval = folly::lazy(cpy);
+  EXPECT_EQ(CopyCount::count, 1);
+  EXPECT_EQ(lval(), true);
+  EXPECT_EQ(CopyCount::count, 1);
+
+  std::function<bool()> f = [&]{ return 12; };
+  auto const lazyF = folly::lazy(f);
+  EXPECT_EQ(lazyF(), true);
+}
+
+TEST(Lazy, Consty) {
+  std::function<int ()> const f = [&] { return 12; };
+  auto lz = folly::lazy(f);
+  EXPECT_EQ(lz(), 12);
+}
+
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/LoggingTest.cpp
@@ -0,0 +1,81 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <folly/Logging.h>
+#include <gflags/gflags.h>
+#include <gtest/gtest.h>
+#include <folly/Benchmark.h>
+#include <vector>
+
+TEST(LogEveryMs, basic) {
+  std::vector<std::chrono::steady_clock::time_point> hist;
+
+  while (hist.size() < 10) {
+    FB_LOG_EVERY_MS(INFO, 10)
+      << "test msg " << (hist.push_back(std::chrono::steady_clock::now()),
+                         hist.size());
+  }
+
+  bool atLeastOneIsGood = false;
+  for (int i = 0; i < hist.size() - 1; ++i) {
+    auto delta = hist[i + 1] - hist[i];
+    if (delta > std::chrono::milliseconds(5) &&
+        delta < std::chrono::milliseconds(15)) {
+      atLeastOneIsGood = true;
+    }
+  }
+  EXPECT_TRUE(atLeastOneIsGood);
+}
+
+BENCHMARK(skip_overhead, iter) {
+  auto prev = FLAGS_minloglevel;
+  FLAGS_minloglevel = 2;
+
+  for (unsigned i = 0; i < iter; ++i) {
+    FB_LOG_EVERY_MS(INFO, 1000) << "every 1s";
+  }
+
+  FLAGS_minloglevel = prev;
+}
+
+BENCHMARK(dev_null_log_overhead, iter) {
+  auto prev = FLAGS_minloglevel;
+  FLAGS_minloglevel = 2;
+
+  for (unsigned i = 0; i < iter; ++i) {
+    FB_LOG_EVERY_MS(INFO, -1) << "every -1ms";
+  }
+
+  FLAGS_minloglevel = prev;
+}
+
+// ============================================================================
+// folly/test/LoggingTest.cpp                      relative  time/iter  iters/s
+// ============================================================================
+// skip_overhead                                               36.37ns   27.49M
+// dev_null_log_overhead                                        2.61us  382.57K
+// ============================================================================
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+
+  auto rv = RUN_ALL_TESTS();
+  if (!rv && FLAGS_benchmark) {
+    folly::runBenchmarks();
+  }
+  return rv;
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/Makefile.am
@@ -0,0 +1,187 @@
+SUBDIRS = . function_benchmark
+
+ACLOCAL_AMFLAGS = -I m4
+
+CPPFLAGS += -Igtest-1.6.0/include
+
+TESTS= \
+	sorted_vector_types_test \
+	foreach_test \
+	hash_test \
+	timeout_queue_test \
+	conv_test \
+	range_test \
+	bits_test \
+	bit_iterator_test
+
+lib_LTLIBRARIES = libgtestmain.la libgtest.la
+
+libgtestmain_la_CPPFLAGS = -Igtest-1.6.0 -Igtest-1.6.0/src
+libgtestmain_la_SOURCES = gtest-1.6.0/src/gtest-all.cc gtest-1.6.0/src/gtest_main.cc
+
+libgtest_la_CPPFLAGS = -Igtest-1.6.0 -Igtest-1.6.0/src
+libgtest_la_SOURCES = gtest-1.6.0/src/gtest-all.cc
+
+noinst_HEADERS = FBStringTestBenchmarks.cpp.h \
+		 FBVectorTestBenchmarks.cpp.h
+
+noinst_PROGRAMS=benchmark_test
+
+if HAVE_X86_64
+small_locks_test_SOURCES = SmallLocksTest.cpp
+small_locks_test_LDADD = libgtestmain.la $(top_builddir)/libfolly.la
+TESTS += small_locks_test
+
+# Fails with WARNING: Logging before InitGoogleLogging() is written to STDERR
+packed_sync_ptr_test_SOURCES = PackedSyncPtrTest.cpp
+packed_sync_ptr_test_LDADD = libgtestmain.la $(top_builddir)/libfolly.la
+TESTS += packed_sync_ptr_test
+
+small_vector_test_SOURCES = small_vector_test.cpp
+small_vector_test_LDADD = libgtestmain.la $(top_builddir)/libfolly.la
+TESTS += small_vector_test
+
+discriminated_ptr_test_SOURCES = DiscriminatedPtrTest.cpp
+discriminated_ptr_test_LDADD = libgtestmain.la $(top_builddir)/libfolly.la
+TESTS += discriminated_ptr_test
+endif
+
+sorted_vector_types_test_SOURCES = sorted_vector_test.cpp
+sorted_vector_types_test_LDADD = libgtestmain.la $(top_builddir)/libfolly.la
+
+
+foreach_test_SOURCES = ForeachTest.cpp
+foreach_test_LDADD = libgtestmain.la $(top_builddir)/libfollybenchmark.la $(top_builddir)/libfolly.la
+
+hash_test_SOURCES = HashTest.cpp
+hash_test_LDADD = libgtestmain.la $(top_builddir)/libfolly.la
+
+
+fbstring_test_using_jemalloc_SOURCES = FBStringTest.cpp
+fbstring_test_using_jemalloc_LDADD = libgtestmain.la $(top_builddir)/libfollybenchmark.la $(top_builddir)/libfolly.la
+TESTS += fbstring_test_using_jemalloc
+
+if HAVE_LINUX
+eventfd_test_SOURCES = EventFDTest.cpp
+eventfd_test_LDADD = libgtestmain.la $(top_builddir)/libfollybenchmark.la $(top_builddir)/libfolly.la
+TESTS += eventfd_test
+endif
+
+thread_cached_int_test_SOURCES = ThreadCachedIntTest.cpp
+thread_cached_int_test_LDADD = libgtestmain.la $(top_builddir)/libfollybenchmark.la $(top_builddir)/libfolly.la
+
+thread_local_test_SOURCES = ThreadLocalTest.cpp
+thread_local_test_LDADD = libgtestmain.la $(top_builddir)/libfollybenchmark.la $(top_builddir)/libfolly.la
+
+TESTS += thread_cached_int_test thread_local_test
+
+fbvector_test_SOURCES = FBVectorTest.cpp
+fbvector_test_LDADD = libgtestmain.la $(top_builddir)/libfollybenchmark.la $(top_builddir)/libfolly.la
+TESTS += fbvector_test
+
+# fails due to cout
+dynamic_test_SOURCES = DynamicTest.cpp
+dynamic_test_LDADD = libgtestmain.la $(top_builddir)/libfollybenchmark.la $(top_builddir)/libfolly.la $(top_builddir)/libfolly.la
+TESTS += dynamic_test
+
+# fails due to cout
+json_test_SOURCES = JsonTest.cpp
+json_test_LDADD = libgtestmain.la $(top_builddir)/libfollybenchmark.la $(top_builddir)/libfolly.la  $(top_builddir)/libfolly.la
+TESTS += json_test
+
+benchmark_test_SOURCES = BenchmarkTest.cpp
+benchmark_test_LDADD = libgtestmain.la $(top_builddir)/libfollybenchmark.la $(top_builddir)/libfolly.la
+
+# fails due to destructor
+scope_guard_test_SOURCES = ScopeGuardTest.cpp
+scope_guard_test_LDADD = libgtestmain.la $(top_builddir)/libfolly.la
+TESTS += scope_guard_test
+
+timeout_queue_test_SOURCES = TimeoutQueueTest.cpp
+timeout_queue_test_LDADD = libgtestmain.la $(top_builddir)/libfollytimeout_queue.la
+
+conv_test_SOURCES = ConvTest.cpp
+conv_test_LDADD = libgtestmain.la $(top_builddir)/libfollybenchmark.la $(top_builddir)/libfolly.la
+
+range_test_SOURCES = RangeTest.cpp
+range_test_LDADD = libgtestmain.la $(top_builddir)/libfolly.la
+
+bits_test_SOURCES = BitsTest.cpp
+bits_test_LDADD = libgtestmain.la $(top_builddir)/libfollybenchmark.la $(top_builddir)/libfolly.la
+
+bit_iterator_test_SOURCES = BitIteratorTest.cpp
+bit_iterator_test_LDADD = libgtestmain.la $(top_builddir)/libfollybenchmark.la $(top_builddir)/libfolly.la
+
+endian_test_SOURCES = EndianTest.cpp
+endian_test_LDADD = libgtestmain.la $(top_builddir)/libfolly.la
+TESTS += endian_test
+
+rw_spinlock_test_SOURCES = RWSpinLockTest.cpp
+rw_spinlock_test_LDADD = libgtestmain.la $(top_builddir)/libfollybenchmark.la $(top_builddir)/libfolly.la
+TESTS += rw_spinlock_test
+
+synchronized_test_SOURCES = SynchronizedTest.cpp
+synchronized_test_LDADD = libgtestmain.la $(top_builddir)/libfolly.la
+TESTS += synchronized_test
+
+concurrent_skiplist_test_SOURCES = ConcurrentSkipListTest.cpp
+concurrent_skiplist_test_LDADD = libgtest.la $(top_builddir)/libfolly.la
+TESTS += concurrent_skiplist_test
+
+concurrent_skiplist_benchmark_SOURCES = ConcurrentSkipListBenchmark.cpp
+concurrent_skiplist_benchmark_LDADD = $(top_builddir)/libfollybenchmark.la $(top_builddir)/libfolly.la
+noinst_PROGRAMS += concurrent_skiplist_benchmark
+
+histogram_test_SOURCES = HistogramTest.cpp
+histogram_test_LDADD = libgtestmain.la $(top_builddir)/libfolly.la
+TESTS += histogram_test
+
+group_varint_test_SOURCES = GroupVarintTest.cpp
+group_varint_test_LDADD = libgtestmain.la $(top_builddir)/libfolly.la
+TESTS += group_varint_test
+
+map_util_test_SOURCES = MapUtilTest.cpp
+map_util_test_LDADD = libgtestmain.la $(top_builddir)/libfolly.la
+TESTS += map_util_test
+
+string_test_SOURCES = StringTest.cpp
+string_test_LDADD = libgtest.la $(top_builddir)/libfolly.la $(top_builddir)/libfollybenchmark.la
+TESTS += string_test
+
+producer_consumer_queue_test_SOURCES = ProducerConsumerQueueTest.cpp
+producer_consumer_queue_test_LDADD = libgtestmain.la $(top_builddir)/libfolly.la
+TESTS += producer_consumer_queue_test
+
+atomic_hash_array_test_SOURCES = AtomicHashArrayTest.cpp
+atomic_hash_array_test_LDADD = libgtestmain.la $(top_builddir)/libfolly.la
+TESTS += atomic_hash_array_test
+
+atomic_hash_map_test_SOURCES = AtomicHashMapTest.cpp
+atomic_hash_map_test_LDADD = libgtest.la $(top_builddir)/libfolly.la $(top_builddir)/libfollybenchmark.la
+TESTS += atomic_hash_map_test
+
+format_test_SOURCES = FormatTest.cpp
+format_test_LDADD = libgtest.la $(top_builddir)/libfolly.la $(top_builddir)/libfollybenchmark.la
+TESTS += format_test
+
+fingerprint_test_SOURCES = FingerprintTest.cpp
+fingerprint_test_LDADD = libgtest.la $(top_builddir)/libfolly.la $(top_builddir)/libfollyfingerprint.la $(top_builddir)/libfollybenchmark.la
+TESTS += fingerprint_test
+
+portability_test_SOURCES = PortabilityTest.cpp
+portability_test_LDADD = libgtestmain.la $(top_builddir)/libfolly.la
+TESTS += portability_test
+
+cpuid_test_SOURCES = CpuIdTest.cpp
+cpuid_test_LDADD = libgtestmain.la $(top_builddir)/libfolly.la
+TESTS += cpuid_test
+
+spooky_hash_v1_test_SOURCES = SpookyHashV1Test.cpp
+spooky_hash_v1_test_LDADD = $(top_builddir)/libfolly.la $(top_builddir)/libfollybenchmark.la
+TESTS += spooky_hash_v1_test
+
+spooky_hash_v2_test_SOURCES = SpookyHashV2Test.cpp
+spooky_hash_v2_test_LDADD = $(top_builddir)/libfolly.la $(top_builddir)/libfollybenchmark.la
+TESTS += spooky_hash_v2_test
+
+check_PROGRAMS= $(TESTS)
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/MapUtilTest.cpp
@@ -0,0 +1,34 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/MapUtil.h"
+
+#include <map>
+#include <gtest/gtest.h>
+
+using namespace folly;
+
+TEST(MapUtil, Simple) {
+  std::map<int, int> m;
+  m[1] = 2;
+  EXPECT_EQ(2, get_default(m, 1, 42));
+  EXPECT_EQ(42, get_default(m, 2, 42));
+  EXPECT_EQ(0, get_default(m, 3));
+  EXPECT_EQ(2, *get_ptr(m, 1));
+  EXPECT_TRUE(get_ptr(m, 2) == nullptr);
+  *get_ptr(m, 1) = 4;
+  EXPECT_EQ(4, m.at(1));
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/MemoryIdlerTest.cpp
@@ -0,0 +1,208 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <folly/detail/MemoryIdler.h>
+#include <folly/Baton.h>
+#include <memory>
+#include <thread>
+#include <assert.h>
+#include <semaphore.h>
+#include <gflags/gflags.h>
+#include <gmock/gmock.h>
+#include <gtest/gtest.h>
+#include <folly/Benchmark.h>
+
+using namespace folly;
+using namespace folly::detail;
+using namespace testing;
+
+TEST(MemoryIdler, releaseStack) {
+  MemoryIdler::unmapUnusedStack();
+}
+
+TEST(MemoryIdler, releaseStackMinExtra) {
+  MemoryIdler::unmapUnusedStack(0);
+}
+
+TEST(MemoryIdler, releaseStackLargeExtra) {
+  MemoryIdler::unmapUnusedStack(30000000);
+}
+
+TEST(MemoryIdler, releaseMallocTLS) {
+  auto p = new int[4];
+  MemoryIdler::flushLocalMallocCaches();
+  delete[] p;
+  MemoryIdler::flushLocalMallocCaches();
+  p = new int[4];
+  MemoryIdler::flushLocalMallocCaches();
+  delete[] p;
+}
+
+
+/// MockedAtom gives us a way to select a mocked Futex implementation
+/// inside Baton, even though the atom itself isn't exercised by the
+/// mocked futex
+template <typename T>
+struct MockAtom : public std::atomic<T> {
+  explicit MockAtom(T init = 0) : std::atomic<T>(init) {}
+};
+
+
+/// MockClock is a bit tricky because we are mocking a static function
+/// (now()), so we need to find the corresponding mock instance without
+/// extending its scope beyond that of the test.  I generally avoid
+/// shared_ptr, but a weak_ptr is just the ticket here
+struct MockClock {
+  typedef std::chrono::steady_clock::duration duration;
+  typedef std::chrono::steady_clock::time_point time_point;
+
+  MOCK_METHOD0(nowImpl, time_point(void));
+
+  /// Hold on to the returned shared_ptr until the end of the test
+  static std::shared_ptr<StrictMock<MockClock>> setup() {
+    auto rv = std::make_shared<StrictMock<MockClock>>();
+    s_mockClockInstance = rv;
+    return rv;
+  }
+
+  static time_point now() {
+    return s_mockClockInstance.lock()->nowImpl();
+  }
+
+  static std::weak_ptr<StrictMock<MockClock>> s_mockClockInstance;
+};
+
+std::weak_ptr<StrictMock<MockClock>> MockClock::s_mockClockInstance;
+
+
+
+namespace folly { namespace detail {
+
+/// Futex<MockAtom> is our mocked futex implementation.  Note that the
+/// method signatures differ from the real Futex because we have elided
+/// unused default params and collapsed templated methods into the
+/// used type
+template<>
+struct Futex<MockAtom> {
+  MOCK_METHOD2(futexWait, bool(uint32_t, uint32_t));
+  MOCK_METHOD3(futexWaitUntil,
+               FutexResult(uint32_t, const MockClock::time_point&, uint32_t));
+};
+
+}}
+
+TEST(MemoryIdler, futexWaitValueChangedEarly) {
+  StrictMock<Futex<MockAtom>> fut;
+  auto clock = MockClock::setup();
+  auto begin = MockClock::time_point(std::chrono::seconds(100));
+  auto idleTimeout = MemoryIdler::defaultIdleTimeout.load();
+
+  EXPECT_CALL(*clock, nowImpl())
+      .WillOnce(Return(begin));
+  EXPECT_CALL(fut, futexWaitUntil(1, AllOf(Ge(begin + idleTimeout),
+                                           Lt(begin + 2 * idleTimeout)), -1))
+      .WillOnce(Return(FutexResult::VALUE_CHANGED));
+  EXPECT_FALSE((MemoryIdler::futexWait<MockAtom, MockClock>(fut, 1)));
+}
+
+TEST(MemoryIdler, futexWaitValueChangedLate) {
+  StrictMock<Futex<MockAtom>> fut;
+  auto clock = MockClock::setup();
+  auto begin = MockClock::time_point(std::chrono::seconds(100));
+  auto idleTimeout = MemoryIdler::defaultIdleTimeout.load();
+
+  EXPECT_CALL(*clock, nowImpl())
+      .WillOnce(Return(begin));
+  EXPECT_CALL(fut, futexWaitUntil(1, AllOf(Ge(begin + idleTimeout),
+                                           Lt(begin + 2 * idleTimeout)), -1))
+      .WillOnce(Return(FutexResult::TIMEDOUT));
+  EXPECT_CALL(fut, futexWait(1, -1))
+      .WillOnce(Return(false));
+  EXPECT_FALSE((MemoryIdler::futexWait<MockAtom, MockClock>(fut, 1)));
+}
+
+TEST(MemoryIdler, futexWaitAwokenEarly) {
+  StrictMock<Futex<MockAtom>> fut;
+  auto clock = MockClock::setup();
+  auto begin = MockClock::time_point(std::chrono::seconds(100));
+  auto idleTimeout = MemoryIdler::defaultIdleTimeout.load();
+
+  EXPECT_CALL(*clock, nowImpl())
+      .WillOnce(Return(begin));
+  EXPECT_CALL(fut, futexWaitUntil(1, Ge(begin + idleTimeout), -1))
+      .WillOnce(Return(FutexResult::AWOKEN));
+  EXPECT_TRUE((MemoryIdler::futexWait<MockAtom, MockClock>(fut, 1)));
+}
+
+TEST(MemoryIdler, futexWaitAwokenLate) {
+  StrictMock<Futex<MockAtom>> fut;
+  auto clock = MockClock::setup();
+  auto begin = MockClock::time_point(std::chrono::seconds(100));
+  auto idleTimeout = MemoryIdler::defaultIdleTimeout.load();
+
+  EXPECT_CALL(*clock, nowImpl())
+      .WillOnce(Return(begin));
+  EXPECT_CALL(fut, futexWaitUntil(1, begin + idleTimeout, -1))
+      .WillOnce(Return(FutexResult::TIMEDOUT));
+  EXPECT_CALL(fut, futexWait(1, -1))
+      .WillOnce(Return(true));
+  EXPECT_TRUE((MemoryIdler::futexWait<MockAtom, MockClock>(
+      fut, 1, -1, idleTimeout, 100, 0.0f)));
+}
+
+TEST(MemoryIdler, futexWaitImmediateFlush) {
+  StrictMock<Futex<MockAtom>> fut;
+  auto clock = MockClock::setup();
+
+  EXPECT_CALL(fut, futexWait(2, 0xff))
+      .WillOnce(Return(true));
+  EXPECT_TRUE((MemoryIdler::futexWait<MockAtom, MockClock>(
+      fut, 2, 0xff, std::chrono::seconds(0))));
+}
+
+TEST(MemoryIdler, futexWaitNeverFlush) {
+  StrictMock<Futex<MockAtom>> fut;
+  auto clock = MockClock::setup();
+
+  EXPECT_CALL(fut, futexWait(1, -1))
+      .WillOnce(Return(true));
+  EXPECT_TRUE((MemoryIdler::futexWait<MockAtom, MockClock>(
+      fut, 1, -1, MockClock::duration::max())));
+}
+
+
+BENCHMARK(releaseStack, iters) {
+  for (size_t i = 0; i < iters; ++i) {
+    MemoryIdler::unmapUnusedStack();
+  }
+}
+
+BENCHMARK(releaseMallocTLS, iters) {
+  for (size_t i = 0; i < iters; ++i) {
+    MemoryIdler::flushLocalMallocCaches();
+  }
+}
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+
+  auto rv = RUN_ALL_TESTS();
+  if (!rv && FLAGS_benchmark) {
+    folly::runBenchmarks();
+  }
+  return rv;
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/MemoryMappingTest.cpp
@@ -0,0 +1,127 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <cstdlib>
+#include <gtest/gtest.h>
+#include "folly/MemoryMapping.h"
+
+namespace folly {
+
+TEST(MemoryMapping, Basic) {
+  File f = File::temporary();
+  {
+    WritableMemoryMapping m(File(f.fd()), 0, sizeof(double));
+    double volatile* d = m.asWritableRange<double>().data();
+    *d = 37 * M_PI;
+  }
+  {
+    MemoryMapping m(File(f.fd()), 0, 3);
+    EXPECT_EQ(0, m.asRange<int>().size()); //not big enough
+  }
+  {
+    MemoryMapping m(File(f.fd()), 0, sizeof(double));
+    const double volatile* d = m.asRange<double>().data();
+    EXPECT_EQ(*d, 37 * M_PI);
+  }
+}
+
+TEST(MemoryMapping, DoublyMapped) {
+  File f = File::temporary();
+  // two mappings of the same memory, different addresses.
+  WritableMemoryMapping mw(File(f.fd()), 0, sizeof(double));
+  MemoryMapping mr(File(f.fd()), 0, sizeof(double));
+
+  double volatile* dw = mw.asWritableRange<double>().data();
+  const double volatile* dr = mr.asRange<double>().data();
+
+  // Show that it's truly the same value, even though the pointers differ
+  EXPECT_NE(dw, dr);
+  *dw = 42 * M_PI;
+  EXPECT_EQ(*dr, 42 * M_PI);
+  *dw = 43 * M_PI;
+  EXPECT_EQ(*dr, 43 * M_PI);
+}
+
+namespace {
+
+void writeStringToFileOrDie(const std::string& str, int fd) {
+  const char* b = str.c_str();
+  size_t count = str.size();
+  ssize_t total_bytes = 0;
+  ssize_t r;
+  do {
+    r = write(fd, b, count);
+    if (r == -1) {
+      if (errno == EINTR) {
+        continue;
+      }
+      PCHECK(r) << "write";
+    }
+
+    total_bytes += r;
+    b += r;
+    count -= r;
+  } while (r != 0 && count);
+}
+
+}  // anonymous namespace
+
+TEST(MemoryMapping, Simple) {
+  File f = File::temporary();
+  writeStringToFileOrDie("hello", f.fd());
+
+  {
+    MemoryMapping m(File(f.fd()));
+    EXPECT_EQ("hello", m.data());
+  }
+  {
+    MemoryMapping m(File(f.fd()), 1, 2);
+    EXPECT_EQ("el", m.data());
+  }
+}
+
+TEST(MemoryMapping, LargeFile) {
+  std::string fileData;
+  size_t fileSize = sysconf(_SC_PAGESIZE) * 3 + 10;
+  fileData.reserve(fileSize);
+  for (size_t i = 0; i < fileSize; i++) {
+    fileData.push_back(0xff & random());
+  }
+
+  File f = File::temporary();
+  writeStringToFileOrDie(fileData, f.fd());
+
+  {
+    MemoryMapping m(File(f.fd()));
+    EXPECT_EQ(fileData, m.data());
+  }
+  {
+    size_t size = sysconf(_SC_PAGESIZE) * 2;
+    StringPiece s(fileData.data() + 9, size - 9);
+    MemoryMapping m(File(f.fd()), 9, size - 9);
+    EXPECT_EQ(s.toString(), m.data());
+  }
+}
+
+TEST(MemoryMapping, ZeroLength) {
+  File f = File::temporary();
+  MemoryMapping m(File(f.fd()));
+  EXPECT_TRUE(m.mlock(MemoryMapping::LockMode::MUST_LOCK));
+  EXPECT_TRUE(m.mlocked());
+  EXPECT_EQ(0, m.data().size());
+}
+
+} // namespace folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/MemoryTest.cpp
@@ -0,0 +1,95 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Memory.h"
+#include "folly/Arena.h"
+#include "folly/String.h"
+
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+#include <type_traits>
+
+using namespace folly;
+
+template <std::size_t> struct T {};
+template <std::size_t> struct S {};
+template <std::size_t> struct P {};
+
+TEST(as_stl_allocator, sanity_check) {
+  typedef StlAllocator<SysArena, int> stl_arena_alloc;
+
+  EXPECT_TRUE((std::is_same<
+    as_stl_allocator<int, SysArena>::type,
+    stl_arena_alloc
+  >::value));
+
+  EXPECT_TRUE((std::is_same<
+    as_stl_allocator<int, stl_arena_alloc>::type,
+    stl_arena_alloc
+  >::value));
+}
+
+TEST(StlAllocator, void_allocator) {
+  typedef StlAllocator<SysArena, void> void_allocator;
+  SysArena arena;
+  void_allocator valloc(&arena);
+
+  typedef void_allocator::rebind<int>::other int_allocator;
+  int_allocator ialloc(valloc);
+
+  auto i = std::allocate_shared<int>(ialloc, 10);
+  ASSERT_NE(nullptr, i.get());
+  EXPECT_EQ(10, *i);
+  i.reset();
+  ASSERT_EQ(nullptr, i.get());
+}
+
+TEST(rebind_allocator, sanity_check) {
+  std::allocator<long> alloc;
+
+  auto i = std::allocate_shared<int>(
+    rebind_allocator<int, decltype(alloc)>(alloc), 10
+  );
+  ASSERT_NE(nullptr, i.get());
+  EXPECT_EQ(10, *i);
+  i.reset();
+  ASSERT_EQ(nullptr, i.get());
+
+  auto d = std::allocate_shared<double>(
+    rebind_allocator<double>(alloc), 5.6
+  );
+  ASSERT_NE(nullptr, d.get());
+  EXPECT_EQ(5.6, *d);
+  d.reset();
+  ASSERT_EQ(nullptr, d.get());
+
+  auto s = std::allocate_shared<std::string>(
+    rebind_allocator<std::string>(alloc), "HELLO, WORLD"
+  );
+  ASSERT_NE(nullptr, s.get());
+  EXPECT_EQ("HELLO, WORLD", *s);
+  s.reset();
+  ASSERT_EQ(nullptr, s.get());
+}
+
+int main(int argc, char **argv) {
+  FLAGS_logtostderr = true;
+  google::InitGoogleLogging(argv[0]);
+  testing::InitGoogleTest(&argc, argv);
+
+  return RUN_ALL_TESTS();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/MergeTest.cpp
@@ -0,0 +1,68 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Merge.h"
+#include <gtest/gtest.h>
+#include <map>
+#include <vector>
+
+TEST(MergeTest, NonOverlapping) {
+  std::vector<int> a = {0, 2, 4, 6};
+  std::vector<int> b = {1, 3, 5, 7};
+  std::vector<int> c;
+
+  folly::merge(a.begin(), a.end(),
+               b.begin(), b.end(),
+               std::back_inserter(c));
+  EXPECT_EQ(8, c.size());
+  for (int i = 0; i < 8; ++i) {
+    EXPECT_EQ(i, c[i]);
+  }
+}
+
+TEST(MergeTest, OverlappingInSingleInputRange) {
+  std::vector<std::pair<int, int>> a = {{0, 0}, {0, 1}};
+  std::vector<std::pair<int, int>> b = {{2, 2}, {3, 3}};
+  std::map<int, int> c;
+
+  folly::merge(a.begin(), a.end(),
+               b.begin(), b.end(),
+               std::inserter(c, c.begin()));
+  EXPECT_EQ(3, c.size());
+
+  // First value is inserted, second is not
+  EXPECT_EQ(c[0], 0);
+
+  EXPECT_EQ(c[2], 2);
+  EXPECT_EQ(c[3], 3);
+}
+
+TEST(MergeTest, OverlappingInDifferentInputRange) {
+  std::vector<std::pair<int, int>> a = {{0, 0}, {1, 1}};
+  std::vector<std::pair<int, int>> b = {{0, 2}, {3, 3}};
+  std::map<int, int> c;
+
+  folly::merge(a.begin(), a.end(),
+               b.begin(), b.end(),
+               std::inserter(c, c.begin()));
+  EXPECT_EQ(3, c.size());
+
+  // Value from a is inserted, value from b is not.
+  EXPECT_EQ(c[0], 0);
+
+  EXPECT_EQ(c[1], 1);
+  EXPECT_EQ(c[3], 3);
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/MoveWrapperTest.cpp
@@ -0,0 +1,37 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <gtest/gtest.h>
+
+#include "folly/MoveWrapper.h"
+#include <memory>
+
+namespace folly {
+
+TEST(makeMoveWrapper, Empty) {
+  // checks for crashes
+  auto p = makeMoveWrapper(std::unique_ptr<int>());
+}
+
+TEST(makeMoveWrapper, NonEmpty) {
+  auto u = std::unique_ptr<int>(new int(5));
+  EXPECT_EQ(*u, 5);
+  auto p = makeMoveWrapper(std::move(u));
+  EXPECT_TRUE(!u);
+  EXPECT_EQ(**p, 5);
+}
+
+} // namespace
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/MPMCPipelineTest.cpp
@@ -0,0 +1,166 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/MPMCPipeline.h"
+
+#include <thread>
+#include <vector>
+
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+#include "folly/Conv.h"
+
+namespace folly { namespace test {
+
+TEST(MPMCPipeline, Trivial) {
+  MPMCPipeline<int, std::string> a(2, 2);
+  EXPECT_EQ(0, a.sizeGuess());
+  a.blockingWrite(42);
+  EXPECT_EQ(1, a.sizeGuess());
+
+  int val;
+  auto ticket = a.blockingReadStage<0>(val);
+  EXPECT_EQ(42, val);
+  EXPECT_EQ(1, a.sizeGuess());
+
+  a.blockingWriteStage<0>(ticket, "hello world");
+  EXPECT_EQ(1, a.sizeGuess());
+
+  std::string s;
+
+  a.blockingRead(s);
+  EXPECT_EQ("hello world", s);
+  EXPECT_EQ(0, a.sizeGuess());
+}
+
+TEST(MPMCPipeline, TrivialAmplification) {
+  MPMCPipeline<int, MPMCPipelineStage<std::string, 2>> a(2, 2);
+  EXPECT_EQ(0, a.sizeGuess());
+  a.blockingWrite(42);
+  EXPECT_EQ(2, a.sizeGuess());
+
+  int val;
+  auto ticket = a.blockingReadStage<0>(val);
+  EXPECT_EQ(42, val);
+  EXPECT_EQ(2, a.sizeGuess());
+
+  a.blockingWriteStage<0>(ticket, "hello world");
+  EXPECT_EQ(2, a.sizeGuess());
+  a.blockingWriteStage<0>(ticket, "goodbye");
+  EXPECT_EQ(2, a.sizeGuess());
+
+  std::string s;
+
+  a.blockingRead(s);
+  EXPECT_EQ("hello world", s);
+  EXPECT_EQ(1, a.sizeGuess());
+
+  a.blockingRead(s);
+  EXPECT_EQ("goodbye", s);
+  EXPECT_EQ(0, a.sizeGuess());
+}
+
+TEST(MPMCPipeline, MultiThreaded) {
+  constexpr size_t numThreadsPerStage = 6;
+  MPMCPipeline<int, std::string, std::string> a(5, 5, 5);
+
+  std::vector<std::thread> threads;
+  threads.reserve(numThreadsPerStage * 2 + 1);
+  for (size_t i = 0; i < numThreadsPerStage; ++i) {
+    threads.emplace_back([&a, i] () {
+      for (;;) {
+        int val;
+        auto ticket = a.blockingReadStage<0>(val);
+        if (val == -1) {  // stop
+          // We still need to propagate
+          a.blockingWriteStage<0>(ticket, "");
+          break;
+        }
+        a.blockingWriteStage<0>(
+            ticket, folly::to<std::string>(val, " hello"));
+      }
+    });
+  }
+
+  for (size_t i = 0; i < numThreadsPerStage; ++i) {
+    threads.emplace_back([&a, i] () {
+      for (;;) {
+        std::string val;
+        auto ticket = a.blockingReadStage<1>(val);
+        if (val.empty()) {  // stop
+          // We still need to propagate
+          a.blockingWriteStage<1>(ticket, "");
+          break;
+        }
+        a.blockingWriteStage<1>(
+            ticket, folly::to<std::string>(val, " world"));
+      }
+    });
+  }
+
+  std::vector<std::string> results;
+  threads.emplace_back([&a, &results] () {
+    for (;;) {
+      std::string val;
+      a.blockingRead(val);
+      if (val.empty()) {
+        break;
+      }
+      results.push_back(val);
+    }
+  });
+
+  constexpr size_t numValues = 1000;
+  for (size_t i = 0; i < numValues; ++i) {
+    a.blockingWrite(i);
+  }
+  for (size_t i = 0; i < numThreadsPerStage; ++i) {
+    a.blockingWrite(-1);
+  }
+
+  for (auto& t : threads) {
+    t.join();
+  }
+
+  // The consumer thread dequeued the first empty string, there should be
+  // numThreadsPerStage - 1 left.
+  EXPECT_EQ(numThreadsPerStage - 1, a.sizeGuess());
+  for (size_t i = 0; i < numThreadsPerStage - 1; ++i) {
+    std::string val;
+    a.blockingRead(val);
+    EXPECT_TRUE(val.empty());
+  }
+  {
+    std::string tmp;
+    EXPECT_FALSE(a.read(tmp));
+  }
+  EXPECT_EQ(0, a.sizeGuess());
+
+  EXPECT_EQ(numValues, results.size());
+  for (size_t i = 0; i < results.size(); ++i) {
+    EXPECT_EQ(folly::to<std::string>(i, " hello world"), results[i]);
+  }
+}
+
+}}  // namespaces
+
+int main(int argc, char *argv[]) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  return RUN_ALL_TESTS();
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/MPMCQueueTest.cpp
@@ -0,0 +1,638 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <folly/MPMCQueue.h>
+#include <folly/Format.h>
+#include <folly/Memory.h>
+#include <folly/test/DeterministicSchedule.h>
+
+#include <boost/intrusive_ptr.hpp>
+#include <memory>
+#include <functional>
+#include <thread>
+#include <utility>
+#include <unistd.h>
+#include <sys/time.h>
+#include <sys/resource.h>
+
+#include <gflags/gflags.h>
+#include <gtest/gtest.h>
+
+FOLLY_ASSUME_FBVECTOR_COMPATIBLE_1(boost::intrusive_ptr);
+
+using namespace folly;
+using namespace detail;
+using namespace test;
+
+typedef DeterministicSchedule DSched;
+
+
+template <template<typename> class Atom>
+void run_mt_sequencer_test(int numThreads, int numOps, uint32_t init) {
+  TurnSequencer<Atom> seq(init);
+  Atom<int> spinThreshold(0);
+
+  int prev = -1;
+  std::vector<std::thread> threads(numThreads);
+  for (int i = 0; i < numThreads; ++i) {
+    threads[i] = DSched::thread([&, i]{
+      for (int op = i; op < numOps; op += numThreads) {
+        seq.waitForTurn(init + op, spinThreshold, (op % 32) == 0);
+        EXPECT_EQ(prev, op - 1);
+        prev = op;
+        seq.completeTurn(init + op);
+      }
+    });
+  }
+
+  for (auto& thr : threads) {
+    DSched::join(thr);
+  }
+
+  EXPECT_EQ(prev, numOps - 1);
+}
+
+TEST(MPMCQueue, sequencer) {
+  run_mt_sequencer_test<std::atomic>(1, 100, 0);
+  run_mt_sequencer_test<std::atomic>(2, 100000, -100);
+  run_mt_sequencer_test<std::atomic>(100, 10000, -100);
+}
+
+TEST(MPMCQueue, sequencer_deterministic) {
+  DSched sched(DSched::uniform(0));
+  run_mt_sequencer_test<DeterministicAtomic>(1, 100, -50);
+  run_mt_sequencer_test<DeterministicAtomic>(2, 10000, (1 << 29) - 100);
+  run_mt_sequencer_test<DeterministicAtomic>(10, 1000, -100);
+}
+
+template <typename T>
+void runElementTypeTest(T&& src) {
+  MPMCQueue<T> cq(10);
+  cq.blockingWrite(std::move(src));
+  T dest;
+  cq.blockingRead(dest);
+  EXPECT_TRUE(cq.write(std::move(dest)));
+  EXPECT_TRUE(cq.read(dest));
+}
+
+struct RefCounted {
+  mutable std::atomic<int> rc;
+
+  RefCounted() : rc(0) {}
+};
+
+void intrusive_ptr_add_ref(RefCounted const* p) {
+  p->rc++;
+}
+
+void intrusive_ptr_release(RefCounted const* p) {
+  if (--(p->rc)) {
+    delete p;
+  }
+}
+
+TEST(MPMCQueue, lots_of_element_types) {
+  runElementTypeTest(10);
+  runElementTypeTest(std::string("abc"));
+  runElementTypeTest(std::make_pair(10, std::string("def")));
+  runElementTypeTest(std::vector<std::string>{ { "abc" } });
+  runElementTypeTest(std::make_shared<char>('a'));
+  runElementTypeTest(folly::make_unique<char>('a'));
+  runElementTypeTest(boost::intrusive_ptr<RefCounted>(new RefCounted));
+}
+
+TEST(MPMCQueue, single_thread_enqdeq) {
+  MPMCQueue<int> cq(10);
+
+  for (int pass = 0; pass < 10; ++pass) {
+    for (int i = 0; i < 10; ++i) {
+      EXPECT_TRUE(cq.write(i));
+    }
+    EXPECT_FALSE(cq.write(-1));
+    EXPECT_FALSE(cq.isEmpty());
+    EXPECT_EQ(cq.size(), 10);
+
+    for (int i = 0; i < 5; ++i) {
+      int dest = -1;
+      EXPECT_TRUE(cq.read(dest));
+      EXPECT_EQ(dest, i);
+    }
+    for (int i = 5; i < 10; ++i) {
+      int dest = -1;
+      cq.blockingRead(dest);
+      EXPECT_EQ(dest, i);
+    }
+    int dest = -1;
+    EXPECT_FALSE(cq.read(dest));
+    EXPECT_EQ(dest, -1);
+
+    EXPECT_TRUE(cq.isEmpty());
+    EXPECT_EQ(cq.size(), 0);
+  }
+}
+
+TEST(MPMCQueue, tryenq_capacity_test) {
+  for (size_t cap = 1; cap < 100; ++cap) {
+    MPMCQueue<int> cq(cap);
+    for (int i = 0; i < cap; ++i) {
+      EXPECT_TRUE(cq.write(i));
+    }
+    EXPECT_FALSE(cq.write(100));
+  }
+}
+
+TEST(MPMCQueue, enq_capacity_test) {
+  for (auto cap : { 1, 100, 10000 }) {
+    MPMCQueue<int> cq(cap);
+    for (int i = 0; i < cap; ++i) {
+      cq.blockingWrite(i);
+    }
+    int t = 0;
+    int when;
+    auto thr = std::thread([&]{
+      cq.blockingWrite(100);
+      when = t;
+    });
+    usleep(2000);
+    t = 1;
+    int dummy;
+    cq.blockingRead(dummy);
+    thr.join();
+    EXPECT_EQ(when, 1);
+  }
+}
+
+template <template<typename> class Atom>
+void runTryEnqDeqTest(int numThreads, int numOps) {
+  // write and read aren't linearizable, so we don't have
+  // hard guarantees on their individual behavior.  We can still test
+  // correctness in aggregate
+  MPMCQueue<int,Atom> cq(numThreads);
+
+  uint64_t n = numOps;
+  std::vector<std::thread> threads(numThreads);
+  std::atomic<uint64_t> sum(0);
+  for (int t = 0; t < numThreads; ++t) {
+    threads[t] = DSched::thread([&,t]{
+      uint64_t threadSum = 0;
+      int src = t;
+      // received doesn't reflect any actual values, we just start with
+      // t and increment by numThreads to get the rounding of termination
+      // correct if numThreads doesn't evenly divide numOps
+      int received = t;
+      while (src < n || received < n) {
+        if (src < n && cq.write(src)) {
+          src += numThreads;
+        }
+
+        int dst;
+        if (received < n && cq.read(dst)) {
+          received += numThreads;
+          threadSum += dst;
+        }
+      }
+      sum += threadSum;
+    });
+  }
+  for (auto& t : threads) {
+    DSched::join(t);
+  }
+  EXPECT_TRUE(cq.isEmpty());
+  EXPECT_EQ(n * (n - 1) / 2 - sum, 0);
+}
+
+TEST(MPMCQueue, mt_try_enq_deq) {
+  int nts[] = { 1, 3, 100 };
+
+  int n = 100000;
+  for (int nt : nts) {
+    runTryEnqDeqTest<std::atomic>(nt, n);
+  }
+}
+
+TEST(MPMCQueue, mt_try_enq_deq_deterministic) {
+  int nts[] = { 3, 10 };
+
+  long seed = 0;
+  LOG(INFO) << "using seed " << seed;
+
+  int n = 1000;
+  for (int nt : nts) {
+    {
+      DSched sched(DSched::uniform(seed));
+      runTryEnqDeqTest<DeterministicAtomic>(nt, n);
+    }
+    {
+      DSched sched(DSched::uniformSubset(seed, 2));
+      runTryEnqDeqTest<DeterministicAtomic>(nt, n);
+    }
+  }
+}
+
+uint64_t nowMicro() {
+  timeval tv;
+  gettimeofday(&tv, 0);
+  return static_cast<uint64_t>(tv.tv_sec) * 1000000 + tv.tv_usec;
+}
+
+template <typename Q>
+std::string producerConsumerBench(Q&& queue, std::string qName,
+                                  int numProducers, int numConsumers,
+                                  int numOps, bool ignoreContents = false) {
+  Q& q = queue;
+
+  struct rusage beginUsage;
+  getrusage(RUSAGE_SELF, &beginUsage);
+
+  auto beginMicro = nowMicro();
+
+  uint64_t n = numOps;
+  std::atomic<uint64_t> sum(0);
+
+  std::vector<std::thread> producers(numProducers);
+  for (int t = 0; t < numProducers; ++t) {
+    producers[t] = DSched::thread([&,t]{
+      for (int i = t; i < numOps; i += numProducers) {
+        q.blockingWrite(i);
+      }
+    });
+  }
+
+  std::vector<std::thread> consumers(numConsumers);
+  for (int t = 0; t < numConsumers; ++t) {
+    consumers[t] = DSched::thread([&,t]{
+      uint64_t localSum = 0;
+      for (int i = t; i < numOps; i += numConsumers) {
+        int dest = -1;
+        q.blockingRead(dest);
+        EXPECT_FALSE(dest == -1);
+        localSum += dest;
+      }
+      sum += localSum;
+    });
+  }
+
+  for (auto& t : producers) {
+    DSched::join(t);
+  }
+  for (auto& t : consumers) {
+    DSched::join(t);
+  }
+  if (!ignoreContents) {
+    EXPECT_EQ(n * (n - 1) / 2 - sum, 0);
+  }
+
+  auto endMicro = nowMicro();
+
+  struct rusage endUsage;
+  getrusage(RUSAGE_SELF, &endUsage);
+
+  uint64_t nanosPer = (1000 * (endMicro - beginMicro)) / n;
+  long csw = endUsage.ru_nvcsw + endUsage.ru_nivcsw -
+      (beginUsage.ru_nvcsw + beginUsage.ru_nivcsw);
+
+  return folly::format(
+      "{}, {} producers, {} consumers => {} nanos/handoff, {} csw / {} handoff",
+      qName, numProducers, numConsumers, nanosPer, csw, n).str();
+}
+
+
+TEST(MPMCQueue, mt_prod_cons_deterministic) {
+  // we use the Bench method, but perf results are meaningless under DSched
+  DSched sched(DSched::uniform(0));
+
+  producerConsumerBench(MPMCQueue<int,DeterministicAtomic>(10),
+          "", 1, 1, 1000);
+  producerConsumerBench(MPMCQueue<int,DeterministicAtomic>(100),
+          "", 10, 10, 1000);
+  producerConsumerBench(MPMCQueue<int,DeterministicAtomic>(10),
+          "", 1, 1, 1000);
+  producerConsumerBench(MPMCQueue<int,DeterministicAtomic>(100),
+          "", 10, 10, 1000);
+  producerConsumerBench(MPMCQueue<int,DeterministicAtomic>(1),
+          "", 10, 10, 1000);
+}
+
+#define PC_BENCH(q, np, nc, ...) \
+    producerConsumerBench(q, #q, (np), (nc), __VA_ARGS__)
+
+TEST(MPMCQueue, mt_prod_cons) {
+  int n = 100000;
+  LOG(INFO) << PC_BENCH(MPMCQueue<int>(10), 1, 1, n);
+  LOG(INFO) << PC_BENCH(MPMCQueue<int>(10), 10, 1, n);
+  LOG(INFO) << PC_BENCH(MPMCQueue<int>(10), 1, 10, n);
+  LOG(INFO) << PC_BENCH(MPMCQueue<int>(10), 10, 10, n);
+  LOG(INFO) << PC_BENCH(MPMCQueue<int>(10000), 1, 1, n);
+  LOG(INFO) << PC_BENCH(MPMCQueue<int>(10000), 10, 1, n);
+  LOG(INFO) << PC_BENCH(MPMCQueue<int>(10000), 1, 10, n);
+  LOG(INFO) << PC_BENCH(MPMCQueue<int>(10000), 10, 10, n);
+  LOG(INFO) << PC_BENCH(MPMCQueue<int>(100000), 32, 100, n);
+}
+
+template <template<typename> class Atom>
+uint64_t runNeverFailTest(int numThreads, int numOps) {
+  // always #enq >= #deq
+  MPMCQueue<int,Atom> cq(numThreads);
+
+  uint64_t n = numOps;
+  auto beginMicro = nowMicro();
+
+  std::vector<std::thread> threads(numThreads);
+  std::atomic<uint64_t> sum(0);
+  for (int t = 0; t < numThreads; ++t) {
+    threads[t] = DSched::thread([&,t]{
+      uint64_t threadSum = 0;
+      for (int i = t; i < n; i += numThreads) {
+        // enq + deq
+        EXPECT_TRUE(cq.writeIfNotFull(i));
+
+        int dest = -1;
+        EXPECT_TRUE(cq.readIfNotEmpty(dest));
+        EXPECT_TRUE(dest >= 0);
+        threadSum += dest;
+      }
+      sum += threadSum;
+    });
+  }
+  for (auto& t : threads) {
+    DSched::join(t);
+  }
+  EXPECT_TRUE(cq.isEmpty());
+  EXPECT_EQ(n * (n - 1) / 2 - sum, 0);
+
+  return nowMicro() - beginMicro;
+}
+
+TEST(MPMCQueue, mt_never_fail) {
+  int nts[] = { 1, 3, 100 };
+
+  int n = 100000;
+  for (int nt : nts) {
+    uint64_t elapsed = runNeverFailTest<std::atomic>(nt, n);
+    LOG(INFO) << (elapsed * 1000.0) / (n * 2) << " nanos per op with "
+              << nt << " threads";
+  }
+}
+
+TEST(MPMCQueue, mt_never_fail_deterministic) {
+  int nts[] = { 3, 10 };
+
+  long seed = 0; // nowMicro() % 10000;
+  LOG(INFO) << "using seed " << seed;
+
+  int n = 1000;
+  for (int nt : nts) {
+    {
+      DSched sched(DSched::uniform(seed));
+      runNeverFailTest<DeterministicAtomic>(nt, n);
+    }
+    {
+      DSched sched(DSched::uniformSubset(seed, 2));
+      runNeverFailTest<DeterministicAtomic>(nt, n);
+    }
+  }
+}
+
+enum LifecycleEvent {
+  NOTHING = -1,
+  DEFAULT_CONSTRUCTOR,
+  COPY_CONSTRUCTOR,
+  MOVE_CONSTRUCTOR,
+  TWO_ARG_CONSTRUCTOR,
+  COPY_OPERATOR,
+  MOVE_OPERATOR,
+  DESTRUCTOR,
+  MAX_LIFECYCLE_EVENT
+};
+
+static __thread int lc_counts[MAX_LIFECYCLE_EVENT];
+static __thread int lc_prev[MAX_LIFECYCLE_EVENT];
+
+static int lc_outstanding() {
+  return lc_counts[DEFAULT_CONSTRUCTOR] + lc_counts[COPY_CONSTRUCTOR] +
+      lc_counts[MOVE_CONSTRUCTOR] + lc_counts[TWO_ARG_CONSTRUCTOR] -
+      lc_counts[DESTRUCTOR];
+}
+
+static void lc_snap() {
+  for (int i = 0; i < MAX_LIFECYCLE_EVENT; ++i) {
+    lc_prev[i] = lc_counts[i];
+  }
+}
+
+#define LIFECYCLE_STEP(...) lc_step(__LINE__, __VA_ARGS__)
+
+static void lc_step(int lineno, int what = NOTHING, int what2 = NOTHING) {
+  for (int i = 0; i < MAX_LIFECYCLE_EVENT; ++i) {
+    int delta = i == what || i == what2 ? 1 : 0;
+    EXPECT_EQ(lc_counts[i] - lc_prev[i], delta)
+        << "lc_counts[" << i << "] - lc_prev[" << i << "] was "
+        << (lc_counts[i] - lc_prev[i]) << ", expected " << delta
+        << ", from line " << lineno;
+  }
+  lc_snap();
+}
+
+template <typename R>
+struct Lifecycle {
+  typedef R IsRelocatable;
+
+  bool constructed;
+
+  Lifecycle() noexcept : constructed(true) {
+    ++lc_counts[DEFAULT_CONSTRUCTOR];
+  }
+
+  explicit Lifecycle(int n, char const* s) noexcept : constructed(true) {
+    ++lc_counts[TWO_ARG_CONSTRUCTOR];
+  }
+
+  Lifecycle(const Lifecycle& rhs) noexcept : constructed(true) {
+    ++lc_counts[COPY_CONSTRUCTOR];
+  }
+
+  Lifecycle(Lifecycle&& rhs) noexcept : constructed(true) {
+    ++lc_counts[MOVE_CONSTRUCTOR];
+  }
+
+  Lifecycle& operator= (const Lifecycle& rhs) noexcept {
+    ++lc_counts[COPY_OPERATOR];
+    return *this;
+  }
+
+  Lifecycle& operator= (Lifecycle&& rhs) noexcept {
+    ++lc_counts[MOVE_OPERATOR];
+    return *this;
+  }
+
+  ~Lifecycle() noexcept {
+    ++lc_counts[DESTRUCTOR];
+    assert(lc_outstanding() >= 0);
+    assert(constructed);
+    constructed = false;
+  }
+};
+
+template <typename R>
+void runPerfectForwardingTest() {
+  lc_snap();
+  EXPECT_EQ(lc_outstanding(), 0);
+
+  {
+    MPMCQueue<Lifecycle<R>> queue(50);
+    LIFECYCLE_STEP(NOTHING);
+
+    for (int pass = 0; pass < 10; ++pass) {
+      for (int i = 0; i < 10; ++i) {
+        queue.blockingWrite();
+        LIFECYCLE_STEP(DEFAULT_CONSTRUCTOR);
+
+        queue.blockingWrite(1, "one");
+        LIFECYCLE_STEP(TWO_ARG_CONSTRUCTOR);
+
+        {
+          Lifecycle<R> src;
+          LIFECYCLE_STEP(DEFAULT_CONSTRUCTOR);
+          queue.blockingWrite(std::move(src));
+          LIFECYCLE_STEP(MOVE_CONSTRUCTOR);
+        }
+        LIFECYCLE_STEP(DESTRUCTOR);
+
+        {
+          Lifecycle<R> src;
+          LIFECYCLE_STEP(DEFAULT_CONSTRUCTOR);
+          queue.blockingWrite(src);
+          LIFECYCLE_STEP(COPY_CONSTRUCTOR);
+        }
+        LIFECYCLE_STEP(DESTRUCTOR);
+
+        EXPECT_TRUE(queue.write());
+        LIFECYCLE_STEP(DEFAULT_CONSTRUCTOR);
+      }
+
+      EXPECT_EQ(queue.size(), 50);
+      EXPECT_FALSE(queue.write(2, "two"));
+      LIFECYCLE_STEP(NOTHING);
+
+      for (int i = 0; i < 50; ++i) {
+        {
+          Lifecycle<R> node;
+          LIFECYCLE_STEP(DEFAULT_CONSTRUCTOR);
+
+          queue.blockingRead(node);
+          if (R::value) {
+            // relocatable, moved via memcpy
+            LIFECYCLE_STEP(DESTRUCTOR);
+          } else {
+            LIFECYCLE_STEP(DESTRUCTOR, MOVE_OPERATOR);
+          }
+        }
+        LIFECYCLE_STEP(DESTRUCTOR);
+      }
+
+      EXPECT_EQ(queue.size(), 0);
+    }
+
+    // put one element back before destruction
+    {
+      Lifecycle<R> src(3, "three");
+      LIFECYCLE_STEP(TWO_ARG_CONSTRUCTOR);
+      queue.write(std::move(src));
+      LIFECYCLE_STEP(MOVE_CONSTRUCTOR);
+    }
+    LIFECYCLE_STEP(DESTRUCTOR); // destroy src
+  }
+  LIFECYCLE_STEP(DESTRUCTOR); // destroy queue
+
+  EXPECT_EQ(lc_outstanding(), 0);
+}
+
+TEST(MPMCQueue, perfect_forwarding) {
+  runPerfectForwardingTest<std::false_type>();
+}
+
+TEST(MPMCQueue, perfect_forwarding_relocatable) {
+  runPerfectForwardingTest<std::true_type>();
+}
+
+TEST(MPMCQueue, queue_moving) {
+  lc_snap();
+  EXPECT_EQ(lc_outstanding(), 0);
+
+  {
+    MPMCQueue<Lifecycle<std::false_type>> a(50);
+    LIFECYCLE_STEP(NOTHING);
+
+    a.blockingWrite();
+    LIFECYCLE_STEP(DEFAULT_CONSTRUCTOR);
+
+    // move constructor
+    MPMCQueue<Lifecycle<std::false_type>> b = std::move(a);
+    LIFECYCLE_STEP(NOTHING);
+    EXPECT_EQ(a.capacity(), 0);
+    EXPECT_EQ(a.size(), 0);
+    EXPECT_EQ(b.capacity(), 50);
+    EXPECT_EQ(b.size(), 1);
+
+    b.blockingWrite();
+    LIFECYCLE_STEP(DEFAULT_CONSTRUCTOR);
+
+    // move operator
+    MPMCQueue<Lifecycle<std::false_type>> c;
+    LIFECYCLE_STEP(NOTHING);
+    c = std::move(b);
+    LIFECYCLE_STEP(NOTHING);
+    EXPECT_EQ(c.capacity(), 50);
+    EXPECT_EQ(c.size(), 2);
+
+    {
+      Lifecycle<std::false_type> dst;
+      LIFECYCLE_STEP(DEFAULT_CONSTRUCTOR);
+      c.blockingRead(dst);
+      LIFECYCLE_STEP(DESTRUCTOR, MOVE_OPERATOR);
+
+      {
+        // swap
+        MPMCQueue<Lifecycle<std::false_type>> d(10);
+        LIFECYCLE_STEP(NOTHING);
+        std::swap(c, d);
+        LIFECYCLE_STEP(NOTHING);
+        EXPECT_EQ(c.capacity(), 10);
+        EXPECT_TRUE(c.isEmpty());
+        EXPECT_EQ(d.capacity(), 50);
+        EXPECT_EQ(d.size(), 1);
+
+        d.blockingRead(dst);
+        LIFECYCLE_STEP(DESTRUCTOR, MOVE_OPERATOR);
+
+        c.blockingWrite(dst);
+        LIFECYCLE_STEP(COPY_CONSTRUCTOR);
+
+        d.blockingWrite(std::move(dst));
+        LIFECYCLE_STEP(MOVE_CONSTRUCTOR);
+      } // d goes out of scope
+      LIFECYCLE_STEP(DESTRUCTOR);
+    } // dst goes out of scope
+    LIFECYCLE_STEP(DESTRUCTOR);
+  } // c goes out of scope
+  LIFECYCLE_STEP(DESTRUCTOR);
+}
+
+int main(int argc, char ** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  return RUN_ALL_TESTS();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/OptionalTest.cpp
@@ -0,0 +1,426 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Optional.h"
+
+#include <memory>
+#include <vector>
+#include <algorithm>
+#include <iomanip>
+#include <string>
+
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+#include <boost/optional.hpp>
+
+using std::unique_ptr;
+using std::shared_ptr;
+
+namespace folly {
+
+template<class V>
+std::ostream& operator<<(std::ostream& os, const Optional<V>& v) {
+  if (v) {
+    os << "Optional(" << v.value() << ')';
+  } else {
+    os << "None";
+  }
+  return os;
+}
+
+struct NoDefault {
+  NoDefault(int, int) {}
+  char a, b, c;
+};
+
+static_assert(sizeof(Optional<char>) == 2, "");
+static_assert(sizeof(Optional<int>) == 8, "");
+static_assert(sizeof(Optional<NoDefault>) == 4, "");
+static_assert(sizeof(Optional<char>) == sizeof(boost::optional<char>), "");
+static_assert(sizeof(Optional<short>) == sizeof(boost::optional<short>), "");
+static_assert(sizeof(Optional<int>) == sizeof(boost::optional<int>), "");
+static_assert(sizeof(Optional<double>) == sizeof(boost::optional<double>), "");
+
+TEST(Optional, NoDefault) {
+  Optional<NoDefault> x;
+  EXPECT_FALSE(x);
+  x.emplace(4, 5);
+  EXPECT_TRUE(bool(x));
+  x.clear();
+  EXPECT_FALSE(x);
+}
+
+TEST(Optional, String) {
+  Optional<std::string> maybeString;
+  EXPECT_FALSE(maybeString);
+  maybeString = "hello";
+  EXPECT_TRUE(bool(maybeString));
+}
+
+TEST(Optional, Const) {
+  { // default construct
+    Optional<const int> opt;
+    EXPECT_FALSE(bool(opt));
+    opt.emplace(4);
+    EXPECT_EQ(*opt, 4);
+    opt.emplace(5);
+    EXPECT_EQ(*opt, 5);
+    opt.clear();
+    EXPECT_FALSE(bool(opt));
+  }
+  { // copy-constructed
+    const int x = 6;
+    Optional<const int> opt(x);
+    EXPECT_EQ(*opt, 6);
+  }
+  { // move-constructed
+    const int x = 7;
+    Optional<const int> opt(std::move(x));
+    EXPECT_EQ(*opt, 7);
+  }
+  // no assignment allowed
+}
+
+TEST(Optional, Simple) {
+  Optional<int> opt;
+  EXPECT_FALSE(bool(opt));
+  opt = 4;
+  EXPECT_TRUE(bool(opt));
+  EXPECT_EQ(4, *opt);
+  opt = 5;
+  EXPECT_EQ(5, *opt);
+  opt.clear();
+  EXPECT_FALSE(bool(opt));
+}
+
+TEST(Optional, EmptyConstruct) {
+  Optional<int> opt;
+  EXPECT_FALSE(bool(opt));
+  Optional<int> test1(opt);
+  EXPECT_FALSE(bool(test1));
+  Optional<int> test2(std::move(opt));
+  EXPECT_FALSE(bool(test2));
+}
+
+TEST(Optional, Unique) {
+  Optional<unique_ptr<int>> opt;
+
+  opt.clear();
+  EXPECT_FALSE(bool(opt));
+  // empty->emplaced
+  opt.emplace(new int(5));
+  EXPECT_TRUE(bool(opt));
+  EXPECT_EQ(5, **opt);
+
+  opt.clear();
+  // empty->moved
+  opt = unique_ptr<int>(new int(6));
+  EXPECT_EQ(6, **opt);
+  // full->moved
+  opt = unique_ptr<int>(new int(7));
+  EXPECT_EQ(7, **opt);
+
+  // move it out by move construct
+  Optional<unique_ptr<int>> moved(std::move(opt));
+  EXPECT_TRUE(bool(moved));
+  EXPECT_FALSE(bool(opt));
+  EXPECT_EQ(7, **moved);
+
+  EXPECT_TRUE(bool(moved));
+  opt = std::move(moved); // move it back by move assign
+  EXPECT_FALSE(bool(moved));
+  EXPECT_TRUE(bool(opt));
+  EXPECT_EQ(7, **opt);
+}
+
+TEST(Optional, Shared) {
+  shared_ptr<int> ptr;
+  Optional<shared_ptr<int>> opt;
+  EXPECT_FALSE(bool(opt));
+  // empty->emplaced
+  opt.emplace(new int(5));
+  EXPECT_TRUE(bool(opt));
+  ptr = opt.value();
+  EXPECT_EQ(ptr.get(), opt->get());
+  EXPECT_EQ(2, ptr.use_count());
+  opt.clear();
+  EXPECT_EQ(1, ptr.use_count());
+  // full->copied
+  opt = ptr;
+  EXPECT_EQ(2, ptr.use_count());
+  EXPECT_EQ(ptr.get(), opt->get());
+  opt.clear();
+  EXPECT_EQ(1, ptr.use_count());
+  // full->moved
+  opt = std::move(ptr);
+  EXPECT_EQ(1, opt->use_count());
+  EXPECT_EQ(nullptr, ptr.get());
+  {
+    Optional<shared_ptr<int>> copied(opt);
+    EXPECT_EQ(2, opt->use_count());
+    Optional<shared_ptr<int>> moved(std::move(opt));
+    EXPECT_EQ(2, moved->use_count());
+    moved.emplace(new int(6));
+    EXPECT_EQ(1, moved->use_count());
+    copied = moved;
+    EXPECT_EQ(2, moved->use_count());
+  }
+}
+
+TEST(Optional, Order) {
+  std::vector<Optional<int>> vect{
+    { none },
+    { 3 },
+    { 1 },
+    { none },
+    { 2 },
+  };
+  std::vector<Optional<int>> expected {
+    { none },
+    { none },
+    { 1 },
+    { 2 },
+    { 3 },
+  };
+  std::sort(vect.begin(), vect.end());
+  EXPECT_EQ(vect, expected);
+}
+
+TEST(Optional, Swap) {
+  Optional<std::string> a;
+  Optional<std::string> b;
+
+  swap(a, b);
+  EXPECT_FALSE(a.hasValue());
+  EXPECT_FALSE(b.hasValue());
+
+  a = "hello";
+  EXPECT_TRUE(a.hasValue());
+  EXPECT_FALSE(b.hasValue());
+  EXPECT_EQ("hello", a.value());
+
+  swap(a, b);
+  EXPECT_FALSE(a.hasValue());
+  EXPECT_TRUE(b.hasValue());
+  EXPECT_EQ("hello", b.value());
+
+  a = "bye";
+  EXPECT_TRUE(a.hasValue());
+  EXPECT_EQ("bye", a.value());
+
+  swap(a, b);
+}
+
+TEST(Optional, Comparisons) {
+  Optional<int> o_;
+  Optional<int> o1(1);
+  Optional<int> o2(2);
+
+  EXPECT_TRUE(o_ <= o_);
+  EXPECT_TRUE(o_ == o_);
+  EXPECT_TRUE(o_ >= o_);
+
+  EXPECT_TRUE(o1 < o2);
+  EXPECT_TRUE(o1 <= o2);
+  EXPECT_TRUE(o1 <= o1);
+  EXPECT_TRUE(o1 == o1);
+  EXPECT_TRUE(o1 != o2);
+  EXPECT_TRUE(o1 >= o1);
+  EXPECT_TRUE(o2 >= o1);
+  EXPECT_TRUE(o2 > o1);
+
+  EXPECT_FALSE(o2 < o1);
+  EXPECT_FALSE(o2 <= o1);
+  EXPECT_FALSE(o2 <= o1);
+  EXPECT_FALSE(o2 == o1);
+  EXPECT_FALSE(o1 != o1);
+  EXPECT_FALSE(o1 >= o2);
+  EXPECT_FALSE(o1 >= o2);
+  EXPECT_FALSE(o1 > o2);
+
+  /* folly::Optional explicitly doesn't support comparisons with contained value
+  EXPECT_TRUE(1 < o2);
+  EXPECT_TRUE(1 <= o2);
+  EXPECT_TRUE(1 <= o1);
+  EXPECT_TRUE(1 == o1);
+  EXPECT_TRUE(2 != o1);
+  EXPECT_TRUE(1 >= o1);
+  EXPECT_TRUE(2 >= o1);
+  EXPECT_TRUE(2 > o1);
+
+  EXPECT_FALSE(o2 < 1);
+  EXPECT_FALSE(o2 <= 1);
+  EXPECT_FALSE(o2 <= 1);
+  EXPECT_FALSE(o2 == 1);
+  EXPECT_FALSE(o2 != 2);
+  EXPECT_FALSE(o1 >= 2);
+  EXPECT_FALSE(o1 >= 2);
+  EXPECT_FALSE(o1 > 2);
+  */
+
+  // boost::optional does support comparison with contained value, which can
+  // lead to confusion when a bool is contained
+  boost::optional<int> boi(3);
+  EXPECT_TRUE(boi < 5);
+  EXPECT_TRUE(boi <= 4);
+  EXPECT_TRUE(boi == 3);
+  EXPECT_TRUE(boi != 2);
+  EXPECT_TRUE(boi >= 1);
+  EXPECT_TRUE(boi > 0);
+  EXPECT_TRUE(1 <  boi);
+  EXPECT_TRUE(2 <= boi);
+  EXPECT_TRUE(3 == boi);
+  EXPECT_TRUE(4 != boi);
+  EXPECT_TRUE(5 >= boi);
+  EXPECT_TRUE(6 >  boi);
+
+  boost::optional<bool> bob(false);
+  EXPECT_TRUE(bob);
+  EXPECT_TRUE(bob == false); // well that was confusing
+  EXPECT_FALSE(bob != false);
+}
+
+TEST(Optional, Conversions) {
+  Optional<bool> mbool;
+  Optional<short> mshort;
+  Optional<char*> mstr;
+  Optional<int> mint;
+
+  //These don't compile
+  //bool b = mbool;
+  //short s = mshort;
+  //char* c = mstr;
+  //int x = mint;
+  //char* c(mstr);
+  //short s(mshort);
+  //int x(mint);
+
+  // intended explicit operator bool, for if (opt).
+  bool b(mbool);
+
+  // Truthy tests work and are not ambiguous
+  if (mbool && mshort && mstr && mint) { // only checks not-empty
+    if (*mbool && *mshort && *mstr && *mint) { // only checks value
+      ;
+    }
+  }
+
+  mbool = false;
+  EXPECT_TRUE(bool(mbool));
+  EXPECT_FALSE(*mbool);
+
+  mbool = true;
+  EXPECT_TRUE(bool(mbool));
+  EXPECT_TRUE(*mbool);
+
+  mbool = none;
+  EXPECT_FALSE(bool(mbool));
+
+  // No conversion allowed; does not compile
+  // EXPECT_TRUE(mbool == false);
+}
+
+TEST(Optional, Pointee) {
+  Optional<int> x;
+  EXPECT_FALSE(get_pointer(x));
+  x = 1;
+  EXPECT_TRUE(get_pointer(x));
+  *get_pointer(x) = 2;
+  EXPECT_TRUE(*x == 2);
+  x = none;
+  EXPECT_FALSE(get_pointer(x));
+}
+
+TEST(Optional, MakeOptional) {
+  // const L-value version
+  const std::string s("abc");
+  auto optStr = make_optional(s);
+  ASSERT_TRUE(optStr.hasValue());
+  EXPECT_EQ(*optStr, "abc");
+  *optStr = "cde";
+  EXPECT_EQ(s, "abc");
+  EXPECT_EQ(*optStr, "cde");
+
+  // L-value version
+  std::string s2("abc");
+  auto optStr2 = make_optional(s2);
+  ASSERT_TRUE(optStr2.hasValue());
+  EXPECT_EQ(*optStr2, "abc");
+  *optStr2 = "cde";
+  // it's vital to check that s2 wasn't clobbered
+  EXPECT_EQ(s2, "abc");
+
+  // L-value reference version
+  std::string& s3(s2);
+  auto optStr3 = make_optional(s3);
+  ASSERT_TRUE(optStr3.hasValue());
+  EXPECT_EQ(*optStr3, "abc");
+  *optStr3 = "cde";
+  EXPECT_EQ(s3, "abc");
+
+  // R-value ref version
+  unique_ptr<int> pInt(new int(3));
+  auto optIntPtr = make_optional(std::move(pInt));
+  EXPECT_TRUE(pInt.get() == nullptr);
+  ASSERT_TRUE(optIntPtr.hasValue());
+  EXPECT_EQ(**optIntPtr, 3);
+}
+
+class ContainsOptional {
+ public:
+  ContainsOptional() { }
+  explicit ContainsOptional(int x) : opt_(x) { }
+  bool hasValue() const { return opt_.hasValue(); }
+  int value() const { return opt_.value(); }
+
+  ContainsOptional(const ContainsOptional &other) = default;
+  ContainsOptional& operator=(const ContainsOptional &other) = default;
+  ContainsOptional(ContainsOptional &&other) = default;
+  ContainsOptional& operator=(ContainsOptional &&other) = default;
+
+ private:
+  Optional<int> opt_;
+};
+
+/**
+ * Test that a class containing an Optional can be copy and move assigned.
+ * This was broken under gcc 4.7 until assignment operators were explicitly
+ * defined.
+ */
+TEST(Optional, AssignmentContained) {
+  {
+    ContainsOptional source(5), target;
+    target = source;
+    EXPECT_TRUE(target.hasValue());
+    EXPECT_EQ(5, target.value());
+  }
+
+  {
+    ContainsOptional source(5), target;
+    target = std::move(source);
+    EXPECT_TRUE(target.hasValue());
+    EXPECT_EQ(5, target.value());
+    EXPECT_FALSE(source.hasValue());
+  }
+
+  {
+    ContainsOptional opt_uninit, target(10);
+    target = opt_uninit;
+    EXPECT_FALSE(target.hasValue());
+  }
+}
+
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/PackedSyncPtrTest.cpp
@@ -0,0 +1,133 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/PackedSyncPtr.h"
+
+#include <cinttypes>
+#include <gtest/gtest.h>
+#include <thread>
+#include <unordered_map>
+#include <utility>
+
+using folly::PackedSyncPtr;
+
+namespace {
+
+// Compile time check for packability.  This requires that
+// PackedSyncPtr is a POD struct on gcc.
+struct ignore { PackedSyncPtr<int> foo; char c; } __attribute__((packed));
+static_assert(sizeof(ignore) == 9, "PackedSyncPtr wasn't packable");
+
+}
+
+TEST(PackedSyncPtr, Basic) {
+  PackedSyncPtr<std::pair<int,int>> sp;
+  sp.init(new std::pair<int,int>[2]);
+  EXPECT_EQ(sizeof(sp), 8);
+  sp->first = 5;
+  EXPECT_EQ(sp[0].first, 5);
+  sp[1].second = 7;
+  EXPECT_EQ(sp[1].second, 7);
+  sp.lock();
+  EXPECT_EQ(sp[1].second, 7);
+  sp[0].first = 9;
+  EXPECT_EQ(sp->first, 9);
+  sp.unlock();
+  EXPECT_EQ((sp.get() + 1)->second, 7);
+
+  sp.lock();
+  EXPECT_EQ(sp.extra(), 0);
+  sp.setExtra(0x13);
+  EXPECT_EQ(sp.extra(), 0x13);
+  EXPECT_EQ((sp.get() + 1)->second, 7);
+  delete[] sp.get();
+  auto newP = new std::pair<int,int>();
+  sp.set(newP);
+  EXPECT_EQ(sp.extra(), 0x13);
+  EXPECT_EQ(sp.get(), newP);
+  sp.unlock();
+}
+
+// Here we use the PackedSyncPtr to lock the whole SyncVec (base, *base, and sz)
+template<typename T>
+struct SyncVec {
+  PackedSyncPtr<T> base;
+  SyncVec() { base.init(); }
+  void push_back(const T& t) {
+    base.set((T*) realloc(base.get(),
+      (base.extra() + 1) * sizeof(T)));
+    base[base.extra()] = t;
+    base.setExtra(base.extra() + 1);
+  }
+  void lock() {
+    base.lock();
+  }
+  void unlock() {
+    base.unlock();
+  }
+
+  T* begin() const { return base.get(); }
+  T* end() const { return base.get() + base.extra(); }
+};
+typedef SyncVec<intptr_t> VecT;
+typedef std::unordered_map<int64_t, VecT> Map;
+const int mapCap = 1317;
+const int nthrs = 297;
+static Map map(mapCap);
+
+// Each app thread inserts it's ID into every vec in map
+// map is read only, so doesn't need any additional locking
+void appThread(intptr_t id) {
+  for (auto& kv : map) {
+    kv.second.lock();
+    kv.second.push_back(id);
+    kv.second.unlock();
+  }
+}
+
+TEST(PackedSyncPtr, Application) {
+  for (int64_t i = 0; i < mapCap / 2; ++i) {
+    map.insert(std::make_pair(i, VecT()));
+  }
+  std::vector<std::thread> thrs;
+  for (intptr_t i = 0; i < nthrs; i++) {
+    thrs.push_back(std::thread(appThread, i));
+  }
+  for (auto& t : thrs) {
+    t.join();
+  }
+
+  for (auto& kv : map) {
+    // Make sure every thread successfully inserted it's ID into every vec
+    std::set<intptr_t> idsFound;
+    for (auto& elem : kv.second) {
+      EXPECT_TRUE(idsFound.insert(elem).second);  // check for dups
+    }
+    EXPECT_EQ(idsFound.size(), nthrs); // check they are all there
+  }
+}
+
+TEST(PackedSyncPtr, extraData) {
+  PackedSyncPtr<int> p;
+  p.init();
+  int* unaligned = reinterpret_cast<int*>(0xf003);
+  p.lock();
+  p.set(unaligned);
+  uintptr_t* bytes = reinterpret_cast<uintptr_t*>(&p);
+  LOG(INFO) << "Bytes integer is: 0x" << std::hex << *bytes;
+  EXPECT_EQ(p.get(), unaligned);
+  p.unlock();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/PaddedTest.cpp
@@ -0,0 +1,242 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Padded.h"
+
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+using namespace folly;
+
+TEST(NodeTest, Padding) {
+  typedef padded::Node<int32_t, 64> IntNode;
+  EXPECT_EQ(16, IntNode::kElementCount);
+  EXPECT_EQ(0, IntNode::kPaddingBytes);
+  EXPECT_EQ(alignof(int32_t), alignof(IntNode));
+  EXPECT_EQ(64, sizeof(IntNode));
+  EXPECT_EQ(0, IntNode::nodeCount(0));
+  EXPECT_EQ(0, IntNode::paddedByteSize(0));
+  EXPECT_EQ(0, IntNode::unpaddedByteSize(0));
+  EXPECT_EQ(1, IntNode::nodeCount(1));
+  EXPECT_EQ(64, IntNode::paddedByteSize(1));
+  EXPECT_EQ(4, IntNode::unpaddedByteSize(1));
+  EXPECT_EQ(1, IntNode::nodeCount(16));
+  EXPECT_EQ(64, IntNode::paddedByteSize(16));
+  EXPECT_EQ(64, IntNode::unpaddedByteSize(16));
+  EXPECT_EQ(2, IntNode::nodeCount(17));
+  EXPECT_EQ(128, IntNode::paddedByteSize(17));
+  EXPECT_EQ(68, IntNode::unpaddedByteSize(17));
+  EXPECT_EQ(128, IntNode::paddedByteSize(32));
+  EXPECT_EQ(128, IntNode::unpaddedByteSize(32));
+  EXPECT_EQ(3, IntNode::nodeCount(33));
+  EXPECT_EQ(192, IntNode::paddedByteSize(33));
+  EXPECT_EQ(132, IntNode::unpaddedByteSize(33));
+
+  struct SevenBytes {
+    char c[7];
+  };
+  EXPECT_EQ(1, alignof(SevenBytes));
+  typedef padded::Node<SevenBytes, 64> SevenByteNode;
+  EXPECT_EQ(9, SevenByteNode::kElementCount);  // 64 / 7
+  EXPECT_EQ(1, SevenByteNode::kPaddingBytes);  // 64 % 7
+  EXPECT_EQ(1, alignof(SevenByteNode));
+  EXPECT_EQ(64, sizeof(SevenByteNode));
+  EXPECT_EQ(0, SevenByteNode::nodeCount(0));
+  EXPECT_EQ(0, SevenByteNode::paddedByteSize(0));
+  EXPECT_EQ(0, SevenByteNode::unpaddedByteSize(0));
+  EXPECT_EQ(1, SevenByteNode::nodeCount(1));
+  EXPECT_EQ(64, SevenByteNode::paddedByteSize(1));
+  EXPECT_EQ(7, SevenByteNode::unpaddedByteSize(1));
+  EXPECT_EQ(1, SevenByteNode::nodeCount(9));
+  EXPECT_EQ(64, SevenByteNode::paddedByteSize(9));
+  EXPECT_EQ(63, SevenByteNode::unpaddedByteSize(9));
+  EXPECT_EQ(2, SevenByteNode::nodeCount(10));
+  EXPECT_EQ(128, SevenByteNode::paddedByteSize(10));
+  EXPECT_EQ(71, SevenByteNode::unpaddedByteSize(10));
+  EXPECT_EQ(2, SevenByteNode::nodeCount(18));
+  EXPECT_EQ(128, SevenByteNode::paddedByteSize(18));
+  EXPECT_EQ(127, SevenByteNode::unpaddedByteSize(18));
+  EXPECT_EQ(3, SevenByteNode::nodeCount(19));
+  EXPECT_EQ(192, SevenByteNode::paddedByteSize(19));
+  EXPECT_EQ(135, SevenByteNode::unpaddedByteSize(19));
+}
+
+class IntPaddedTestBase : public ::testing::Test {
+ protected:
+  typedef padded::Node<uint32_t, 64> IntNode;
+  typedef std::vector<IntNode> IntNodeVec;
+  IntNodeVec v_;
+  int n_;
+};
+
+class IntPaddedConstTest : public IntPaddedTestBase {
+ protected:
+  void SetUp() {
+    v_.resize(4);
+    n_ = 0;
+    for (int i = 0; i < 4; i++) {
+      for (int j = 0; j < IntNode::kElementCount; ++j, ++n_) {
+        v_[i].data()[j] = n_;
+      }
+    }
+  }
+};
+
+TEST_F(IntPaddedConstTest, Iteration) {
+  int k = 0;
+  for (auto it = padded::cbegin(v_); it != padded::cend(v_); ++it, ++k) {
+    EXPECT_EQ(k, *it);
+  }
+  EXPECT_EQ(n_, k);
+}
+
+TEST_F(IntPaddedConstTest, Arithmetic) {
+  EXPECT_EQ(64, padded::cend(v_) - padded::cbegin(v_));
+  // Play around block boundaries
+  auto it = padded::cbegin(v_);
+  EXPECT_EQ(0, *it);
+  {
+    auto i2 = it;
+    EXPECT_EQ(0, i2 - it);
+    i2 += 1;
+    EXPECT_EQ(1, *i2);
+    EXPECT_EQ(1, i2 - it);
+    EXPECT_EQ(-1, it - i2);
+  }
+  it += 15;
+  EXPECT_EQ(15, *it);
+  {
+    auto i2 = it;
+    i2 += 1;
+    EXPECT_EQ(16, *i2);
+    EXPECT_EQ(1, i2 - it);
+    EXPECT_EQ(-1, it - i2);
+  }
+  ++it;
+  EXPECT_EQ(16, *it);
+  {
+    auto i2 = it;
+    i2 -= 1;
+    EXPECT_EQ(15, *i2);
+    EXPECT_EQ(-1, i2 - it);
+    EXPECT_EQ(1, it - i2);
+  }
+  --it;
+  EXPECT_EQ(15, *it);
+  {
+    auto i2 = it;
+    i2 -= 1;
+    EXPECT_EQ(14, *i2);
+    EXPECT_EQ(-1, i2 - it);
+    EXPECT_EQ(1, it - i2);
+  }
+}
+
+class IntPaddedNonConstTest : public IntPaddedTestBase {
+};
+
+TEST_F(IntPaddedNonConstTest, Iteration) {
+  v_.resize(4);
+  n_ = 64;
+
+  int k = 0;
+  for (auto it = padded::begin(v_); it != padded::end(v_); ++it, ++k) {
+    *it = k;
+  }
+  EXPECT_EQ(n_, k);
+
+  k = 0;
+  for (int i = 0; i < 4; i++) {
+    for (int j = 0; j < IntNode::kElementCount; ++j, ++k) {
+      EXPECT_EQ(k, v_[i].data()[j]);
+    }
+  }
+}
+
+class StructPaddedTestBase : public ::testing::Test {
+ protected:
+  struct Point {
+    uint8_t x;
+    uint8_t y;
+    uint8_t z;
+  };
+  typedef padded::Node<Point, 64> PointNode;
+  typedef std::vector<PointNode> PointNodeVec;
+  PointNodeVec v_;
+  int n_;
+};
+
+class StructPaddedConstTest : public StructPaddedTestBase {
+ protected:
+  void SetUp() {
+    v_.resize(4);
+    n_ = 0;
+    for (int i = 0; i < 4; i++) {
+      for (int j = 0; j < PointNode::kElementCount; ++j, ++n_) {
+        auto& point = v_[i].data()[j];
+        point.x = n_;
+        point.y = n_ + 1;
+        point.z = n_ + 2;
+      }
+    }
+  }
+};
+
+TEST_F(StructPaddedConstTest, Iteration) {
+  int k = 0;
+  for (auto it = padded::cbegin(v_); it != padded::cend(v_); ++it, ++k) {
+    EXPECT_EQ(k, it->x);
+    EXPECT_EQ(k + 1, it->y);
+    EXPECT_EQ(k + 2, it->z);
+  }
+  EXPECT_EQ(n_, k);
+}
+
+class IntAdaptorTest : public IntPaddedConstTest {
+ protected:
+  typedef padded::Adaptor<IntNodeVec> IntAdaptor;
+  IntAdaptor a_;
+};
+
+TEST_F(IntAdaptorTest, Simple) {
+  for (int i = 0; i < n_; ++i) {
+    EXPECT_EQ((i == 0), a_.empty());
+    EXPECT_EQ(i, a_.size());
+    a_.push_back(i);
+  }
+  EXPECT_EQ(n_, a_.size());
+
+  int k = 0;
+  for (auto it = a_.begin(); it != a_.end(); ++it, ++k) {
+    EXPECT_EQ(k, a_[k]);
+    EXPECT_EQ(k, *it);
+  }
+  EXPECT_EQ(n_, k);
+
+  auto p = a_.move();
+  EXPECT_TRUE(a_.empty());
+  EXPECT_EQ(16, p.second);
+  EXPECT_TRUE(v_ == p.first);
+}
+
+TEST_F(IntAdaptorTest, ResizeConstructor) {
+  IntAdaptor a(n_, 42);
+  EXPECT_EQ(n_, a.size());
+  for (int i = 0; i < n_; ++i) {
+    EXPECT_EQ(42, a[i]);
+  }
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/PortabilityTest.cpp
@@ -0,0 +1,45 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Portability.h"
+
+#include <memory>
+
+#include <gtest/gtest.h>
+
+class Base {
+ public:
+  virtual ~Base() { }
+  virtual int foo() const { return 1; }
+};
+
+class Derived : public Base {
+ public:
+  virtual int foo() const FOLLY_FINAL { return 2; }
+};
+
+// A compiler that supports final will likely inline the call to p->foo()
+// in fooDerived (but not in fooBase) as it knows that Derived::foo() can
+// no longer be overridden.
+int fooBase(const Base* p) { return p->foo() + 1; }
+int fooDerived(const Derived* p) { return p->foo() + 1; }
+
+TEST(Portability, Final) {
+  std::unique_ptr<Derived> p(new Derived);
+  EXPECT_EQ(3, fooBase(p.get()));
+  EXPECT_EQ(3, fooDerived(p.get()));
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/ProducerConsumerQueueBenchmark.cpp
@@ -0,0 +1,265 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// @author: Bert Maher <bertrand@fb.com>
+
+#include <thread>
+#include <iostream>
+#include <stdio.h>
+#include <pthread.h>
+
+#include <gflags/gflags.h>
+#include <glog/logging.h>
+#include "folly/Benchmark.h"
+#include "folly/stats/Histogram.h"
+#include "folly/stats/Histogram-defs.h"
+#include "folly/ProducerConsumerQueue.h"
+
+namespace {
+
+using namespace folly;
+
+typedef int ThroughputType;
+typedef ProducerConsumerQueue<ThroughputType> ThroughputQueueType;
+
+typedef long LatencyType;
+typedef ProducerConsumerQueue<LatencyType> LatencyQueueType;
+
+template<class QueueType>
+struct ThroughputTest {
+  explicit ThroughputTest(size_t size, int iters, int cpu0, int cpu1)
+  : queue_(size),
+    done_(false),
+    iters_(iters),
+    cpu0_(cpu0),
+    cpu1_(cpu1)
+    { }
+
+  void producer() {
+    if (cpu0_ > -1) {
+      cpu_set_t cpuset;
+      CPU_ZERO(&cpuset);
+      CPU_SET(cpu0_, &cpuset);
+      pthread_setaffinity_np(pthread_self(), sizeof(cpu_set_t), &cpuset);
+    }
+    for (int i = 0; i < iters_; ++i) {
+      ThroughputType item = i;
+      while (!queue_.write((ThroughputType) item)) {
+      }
+    }
+  }
+
+  void consumer() {
+    if (cpu1_ > -1) {
+      cpu_set_t cpuset;
+      CPU_ZERO(&cpuset);
+      CPU_SET(cpu1_, &cpuset);
+      pthread_setaffinity_np(pthread_self(), sizeof(cpu_set_t), &cpuset);
+    }
+    for (int i = 0; i < iters_; ++i) {
+      ThroughputType item = 0;
+      while (!queue_.read(item)) {
+        doNotOptimizeAway(item);
+      }
+    }
+  }
+
+  QueueType queue_;
+  std::atomic<bool> done_;
+  int iters_;
+  int cpu0_;
+  int cpu1_;
+};
+
+template<class QueueType>
+struct LatencyTest {
+  explicit LatencyTest(size_t size, int iters, int cpu0, int cpu1)
+  : queue_(size),
+    done_(false),
+    iters_(iters),
+    cpu0_(cpu0),
+    cpu1_(cpu1),
+    hist_(1, 0, 30)
+    {
+      computeTimeCost();
+    }
+
+  void computeTimeCost() {
+    int iterations = 1000;
+    timespec start, end;
+    clock_gettime(CLOCK_REALTIME, &start);
+    for (int i = 0; i < iterations; ++i) {
+      timespec tv;
+      clock_gettime(CLOCK_REALTIME, &tv);
+    }
+    clock_gettime(CLOCK_REALTIME, &end);
+    time_cost_ = 2 * detail::timespecDiff(end, start) / iterations;
+  }
+
+  void producer() {
+    if (cpu0_ > -1) {
+      cpu_set_t cpuset;
+      CPU_ZERO(&cpuset);
+      CPU_SET(cpu0_, &cpuset);
+      pthread_setaffinity_np(pthread_self(), sizeof(cpu_set_t), &cpuset);
+    }
+    for (int i = 0; i < iters_; ++i) {
+      timespec sleeptime, sleepstart;
+      clock_gettime(CLOCK_REALTIME, &sleepstart);
+      do {
+        clock_gettime(CLOCK_REALTIME, &sleeptime);
+      } while (detail::timespecDiff(sleeptime, sleepstart) < 1000000);
+
+      timespec tv;
+      clock_gettime(CLOCK_REALTIME, &tv);
+      while (!queue_.write((LatencyType) tv.tv_nsec)) {
+      }
+    }
+  }
+
+  void consumer() {
+    if (cpu1_ > -1) {
+      cpu_set_t cpuset;
+      CPU_ZERO(&cpuset);
+      CPU_SET(cpu1_, &cpuset);
+      pthread_setaffinity_np(pthread_self(), sizeof(cpu_set_t), &cpuset);
+    }
+    for (int i = 0; i < iters_; ++i) {
+      long enqueue_nsec;
+      while (!queue_.read(enqueue_nsec)) {
+      }
+
+      timespec tv;
+      clock_gettime(CLOCK_REALTIME, &tv);
+      int diff = tv.tv_nsec - enqueue_nsec - time_cost_;
+      if (diff < 0) {
+        continue;
+      }
+
+      // Naive log-scale bucketing.
+      int bucket;
+      for (bucket = 0;
+           bucket <= 30 && (1 << bucket) <= diff;
+           ++bucket) {
+      }
+      hist_.addValue(bucket - 1);
+    }
+  }
+
+  void printHistogram() {
+    hist_.toTSV(std::cout);
+  }
+
+  QueueType queue_;
+  std::atomic<bool> done_;
+  int time_cost_;
+  int iters_;
+  int cpu0_;
+  int cpu1_;
+  Histogram<int> hist_;
+};
+
+void BM_ProducerConsumer(int iters, int size) {
+  BenchmarkSuspender susp;
+  CHECK_GT(size, 0);
+  ThroughputTest<ThroughputQueueType> *test =
+    new ThroughputTest<ThroughputQueueType>(size, iters, -1, -1);
+  susp.dismiss();
+
+  std::thread producer( [test] { test->producer(); } );
+  std::thread consumer( [test] { test->consumer(); } );
+
+  producer.join();
+  test->done_ = true;
+  consumer.join();
+  delete test;
+}
+
+void BM_ProducerConsumerAffinity(int iters, int size) {
+  BenchmarkSuspender susp;
+  CHECK_GT(size, 0);
+  ThroughputTest<ThroughputQueueType> *test =
+    new ThroughputTest<ThroughputQueueType>(size, iters, 0, 1);
+  susp.dismiss();
+
+  std::thread producer( [test] { test->producer(); } );
+  std::thread consumer( [test] { test->consumer(); } );
+
+  producer.join();
+  test->done_ = true;
+  consumer.join();
+  delete test;
+}
+
+void BM_ProducerConsumerLatency(int iters, int size) {
+  BenchmarkSuspender susp;
+  CHECK_GT(size, 0);
+  LatencyTest<LatencyQueueType> *test =
+    new LatencyTest<LatencyQueueType>(size, 100000, 0, 1);
+  susp.dismiss();
+
+  std::thread producer( [test] { test->producer(); } );
+  std::thread consumer( [test] { test->consumer(); } );
+
+  producer.join();
+  test->done_ = true;
+  consumer.join();
+  test->printHistogram();
+  delete test;
+}
+
+
+BENCHMARK_DRAW_LINE();
+
+BENCHMARK_PARAM(BM_ProducerConsumer, 1048574);
+BENCHMARK_PARAM(BM_ProducerConsumerAffinity, 1048574);
+BENCHMARK_PARAM(BM_ProducerConsumerLatency, 1048574);
+
+}
+
+int main(int argc, char** argv) {
+  google::InitGoogleLogging(argv[0]);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+
+  runBenchmarks();
+  return 0;
+}
+
+#if 0
+/*
+Benchmark on Intel(R) Xeon(R) CPU E5-2660 0 @ 2.20GHz
+Latency histogram:
+  log(nsec)
+  min  max     count
+  6    7       5124
+  7    8       4799
+  8    9       49
+  9    10      2
+  10   11      1
+  11   12      5
+  12   13      3
+  13   14      9
+  14   15      8
+============================================================================
+folly/test/ProducerConsumerQueueBenchmark.cpp   relative  time/iter  iters/s
+============================================================================
+----------------------------------------------------------------------------
+BM_ProducerConsumer(1048574)                                 7.52ns  132.90M
+BM_ProducerConsumerAffinity(1048574)                         8.28ns  120.75M
+BM_ProducerConsumerLatency(1048574)                          10.00s   99.98m
+============================================================================
+*/
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/ProducerConsumerQueueTest.cpp
@@ -0,0 +1,285 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/ProducerConsumerQueue.h"
+
+#include <gtest/gtest.h>
+#include <vector>
+#include <atomic>
+#include <chrono>
+#include <memory>
+#include <thread>
+#include <glog/logging.h>
+
+//////////////////////////////////////////////////////////////////////
+
+namespace {
+
+template<class T> struct TestTraits {
+  T limit() const { return 1 << 24; }
+  T generate() const { return rand() % 26; }
+};
+
+template<> struct TestTraits<std::string> {
+  int limit() const { return 1 << 22; }
+  std::string generate() const { return std::string(12, ' '); }
+};
+
+template<class QueueType, size_t Size, bool Pop = false>
+struct PerfTest {
+  typedef typename QueueType::value_type T;
+
+  explicit PerfTest() : queue_(Size), done_(false) {}
+
+  void operator()() {
+    using namespace std::chrono;
+    auto const startTime = system_clock::now();
+
+    std::thread producer([this] { this->producer(); });
+    std::thread consumer([this] { this->consumer(); });
+
+    producer.join();
+    done_ = true;
+    consumer.join();
+
+    auto duration = duration_cast<milliseconds>(
+      system_clock::now() - startTime);
+    LOG(INFO) << "     done: " << duration.count() << "ms";
+  }
+
+  void producer() {
+    for (int i = 0; i < traits_.limit(); ++i) {
+      while (!queue_.write(traits_.generate())) {
+      }
+    }
+  }
+
+  void consumer() {
+    /*static*/ if (Pop) {
+      while (!done_) {
+        if (queue_.frontPtr()) {
+          queue_.popFront();
+        }
+      }
+    } else {
+      while (!done_) {
+        T data;
+        queue_.read(data);
+      }
+    }
+  }
+
+  QueueType queue_;
+  std::atomic<bool> done_;
+  TestTraits<T> traits_;
+};
+
+template<class TestType> void doTest(const char* name) {
+  LOG(INFO) << "  testing: " << name;
+  std::unique_ptr<TestType> const t(new TestType());
+  (*t)();
+}
+
+template<class T, bool Pop = false>
+void perfTestType(const char* type) {
+  const size_t size = 0xfffe;
+
+  LOG(INFO) << "Type: " << type;
+  doTest<PerfTest<folly::ProducerConsumerQueue<T>,size,Pop> >(
+    "ProducerConsumerQueue");
+}
+
+template<class QueueType, size_t Size, bool Pop>
+struct CorrectnessTest {
+  typedef typename QueueType::value_type T;
+
+  explicit CorrectnessTest()
+    : queue_(Size)
+    , done_(false)
+  {
+    const size_t testSize = traits_.limit();
+    testData_.reserve(testSize);
+    for (int i = 0; i < testSize; ++i) {
+      testData_.push_back(traits_.generate());
+    }
+  }
+
+  void operator()() {
+    std::thread producer([this] { this->producer(); });
+    std::thread consumer([this] { this->consumer(); });
+
+    producer.join();
+    done_ = true;
+    consumer.join();
+  }
+
+  void producer() {
+    for (auto& data : testData_) {
+      while (!queue_.write(data)) {
+      }
+    }
+  }
+
+  void consumer() {
+    if (Pop) {
+      consumerPop();
+    } else {
+      consumerRead();
+    }
+  }
+
+  void consumerPop() {
+    for (auto expect : testData_) {
+    again:
+      T* data;
+      if (!(data = queue_.frontPtr())) {
+        if (done_) {
+          // Try one more read; unless there's a bug in the queue class
+          // there should still be more data sitting in the queue even
+          // though the producer thread exited.
+          if (!(data = queue_.frontPtr())) {
+            EXPECT_TRUE(0 && "Finished too early ...");
+            return;
+          }
+        } else {
+          goto again;
+        }
+      } else {
+        queue_.popFront();
+      }
+
+      EXPECT_EQ(*data, expect);
+    }
+  }
+
+  void consumerRead() {
+    for (auto expect : testData_) {
+    again:
+      T data;
+      if (!queue_.read(data)) {
+        if (done_) {
+          // Try one more read; unless there's a bug in the queue class
+          // there should still be more data sitting in the queue even
+          // though the producer thread exited.
+          if (!queue_.read(data)) {
+            EXPECT_TRUE(0 && "Finished too early ...");
+            return;
+          }
+        } else {
+          goto again;
+        }
+      }
+      EXPECT_EQ(data, expect);
+    }
+  }
+
+  std::vector<T> testData_;
+  QueueType queue_;
+  TestTraits<T> traits_;
+  std::atomic<bool> done_;
+};
+
+template<class T, bool Pop = false>
+void correctnessTestType(const std::string& type) {
+  LOG(INFO) << "Type: " << type;
+  doTest<CorrectnessTest<folly::ProducerConsumerQueue<T>,0xfffe,Pop> >(
+    "ProducerConsumerQueue");
+}
+
+struct DtorChecker {
+  static int numInstances;
+  DtorChecker() { ++numInstances; }
+  DtorChecker(const DtorChecker& o) { ++numInstances; }
+  ~DtorChecker() { --numInstances; }
+};
+
+int DtorChecker::numInstances = 0;
+
+}
+
+//////////////////////////////////////////////////////////////////////
+
+TEST(PCQ, QueueCorrectness) {
+  correctnessTestType<std::string,true>("string (front+pop)");
+  correctnessTestType<std::string>("string");
+  correctnessTestType<int>("int");
+  correctnessTestType<unsigned long long>("unsigned long long");
+}
+
+TEST(PCQ, PerfTest) {
+  perfTestType<std::string,true>("string (front+pop)");
+  perfTestType<std::string>("string");
+  perfTestType<int>("int");
+  perfTestType<unsigned long long>("unsigned long long");
+}
+
+TEST(PCQ, Destructor) {
+  // Test that orphaned elements in a ProducerConsumerQueue are
+  // destroyed.
+  {
+    folly::ProducerConsumerQueue<DtorChecker> queue(1024);
+    for (int i = 0; i < 10; ++i) {
+      EXPECT_TRUE(queue.write(DtorChecker()));
+    }
+
+    EXPECT_EQ(DtorChecker::numInstances, 10);
+
+    {
+      DtorChecker ignore;
+      EXPECT_TRUE(queue.read(ignore));
+      EXPECT_TRUE(queue.read(ignore));
+    }
+
+    EXPECT_EQ(DtorChecker::numInstances, 8);
+  }
+
+  EXPECT_EQ(DtorChecker::numInstances, 0);
+
+  // Test the same thing in the case that the queue write pointer has
+  // wrapped, but the read one hasn't.
+  {
+    folly::ProducerConsumerQueue<DtorChecker> queue(4);
+    for (int i = 0; i < 3; ++i) {
+      EXPECT_TRUE(queue.write(DtorChecker()));
+    }
+    EXPECT_EQ(DtorChecker::numInstances, 3);
+    {
+      DtorChecker ignore;
+      EXPECT_TRUE(queue.read(ignore));
+    }
+    EXPECT_EQ(DtorChecker::numInstances, 2);
+    EXPECT_TRUE(queue.write(DtorChecker()));
+    EXPECT_EQ(DtorChecker::numInstances, 3);
+  }
+  EXPECT_EQ(DtorChecker::numInstances, 0);
+}
+
+TEST(PCQ, EmptyFull) {
+  folly::ProducerConsumerQueue<int> queue(3);
+  EXPECT_TRUE(queue.isEmpty());
+  EXPECT_FALSE(queue.isFull());
+
+  EXPECT_TRUE(queue.write(1));
+  EXPECT_FALSE(queue.isEmpty());
+  EXPECT_FALSE(queue.isFull());
+
+  EXPECT_TRUE(queue.write(2));
+  EXPECT_FALSE(queue.isEmpty());
+  EXPECT_TRUE(queue.isFull());  // Tricky: full after 2 writes, not 3.
+
+  EXPECT_FALSE(queue.write(3));
+  EXPECT_EQ(queue.sizeGuess(), 2);
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/RandomTest.cpp
@@ -0,0 +1,109 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Random.h"
+#include "folly/Range.h"
+#include "folly/Benchmark.h"
+#include "folly/Foreach.h"
+
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+#include <algorithm>
+#include <thread>
+#include <vector>
+#include <random>
+
+using namespace folly;
+
+TEST(Random, Simple) {
+  uint32_t prev = 0, seed = 0;
+  for (int i = 0; i < 1024; ++i) {
+    EXPECT_NE(seed = randomNumberSeed(), prev);
+    prev = seed;
+  }
+}
+
+TEST(Random, MultiThreaded) {
+  const int n = 1024;
+  std::vector<uint32_t> seeds(n);
+  std::vector<std::thread> threads;
+  for (int i = 0; i < n; ++i) {
+    threads.push_back(std::thread([i, &seeds] {
+      seeds[i] = randomNumberSeed();
+    }));
+  }
+  for (auto& t : threads) {
+    t.join();
+  }
+  std::sort(seeds.begin(), seeds.end());
+  for (int i = 0; i < n-1; ++i) {
+    EXPECT_LT(seeds[i], seeds[i+1]);
+  }
+}
+
+BENCHMARK(minstdrand, n) {
+  BenchmarkSuspender braces;
+  std::random_device rd;
+  std::minstd_rand rng(rd());
+
+  braces.dismiss();
+
+  FOR_EACH_RANGE (i, 0, n) {
+    doNotOptimizeAway(rng());
+  }
+}
+
+BENCHMARK(mt19937, n) {
+  BenchmarkSuspender braces;
+  std::random_device rd;
+  std::mt19937 rng(rd());
+
+  braces.dismiss();
+
+  FOR_EACH_RANGE (i, 0, n) {
+    doNotOptimizeAway(rng());
+  }
+}
+
+BENCHMARK(threadprng, n) {
+  BenchmarkSuspender braces;
+  ThreadLocalPRNG tprng;
+  tprng();
+
+  braces.dismiss();
+
+  FOR_EACH_RANGE (i, 0, n) {
+    doNotOptimizeAway(tprng());
+  }
+}
+
+BENCHMARK(RandomDouble) { doNotOptimizeAway(Random::randDouble01()); }
+BENCHMARK(Random32) { doNotOptimizeAway(Random::rand32()); }
+BENCHMARK(Random32Num) { doNotOptimizeAway(Random::rand32(100)); }
+BENCHMARK(Random64) { doNotOptimizeAway(Random::rand64()); }
+BENCHMARK(Random64Num) { doNotOptimizeAway(Random::rand64(100ul << 32)); }
+BENCHMARK(Random64OneIn) { doNotOptimizeAway(Random::oneIn(100)); }
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+
+  if (FLAGS_benchmark) {
+    folly::runBenchmarks();
+  }
+  return RUN_ALL_TESTS();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/RangeFindBenchmark.cpp
@@ -0,0 +1,348 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Range.h"
+#include "folly/Benchmark.h"
+#include "folly/Foreach.h"
+#include <algorithm>
+#include <iostream>
+#include <random>
+#include <string>
+
+namespace folly { namespace detail {
+// declaration of functions in Range.cpp
+size_t qfind_first_byte_of_memchr(const StringPiece& haystack,
+                                  const StringPiece& needles);
+
+size_t qfind_first_byte_of_byteset(const StringPiece& haystack,
+                                   const StringPiece& needles);
+
+size_t qfind_first_byte_of_nosse(const StringPiece& haystack,
+                                 const StringPiece& needles);
+}}
+
+using namespace folly;
+using namespace std;
+
+namespace {
+
+std::string str;
+constexpr int kVstrSize = 16;
+std::vector<std::string> vstr;
+std::vector<StringPiece> vstrp;
+
+void initStr(int len) {
+  cout << "string length " << len << ':' << endl;
+  str.clear();
+  str.reserve(len + 1);
+  str.append(len, 'a');
+  str.append(1, 'b');
+
+  // create 16 copies of str, each with a different 16byte alignment.
+  // Useful because some implementations of find_first_of have different
+  // behaviors based on byte alignment.
+  for (int i = 0; i < kVstrSize; ++i) {
+    string s(i, '$');
+    s += str;
+    if (i % 2) {
+      // some find_first_of implementations have special (page-safe) logic
+      // for handling the end of a string.  Flex that logic only sometimes.
+      s += string(32, '$');
+    }
+    vstr.push_back(s);
+    StringPiece sp(vstr.back());
+    sp.advance(i);
+    vstrp.push_back(sp);
+  }
+}
+
+std::mt19937 rnd;
+string ffoTestString;
+const size_t ffoDelimSize = 128;
+vector<string> ffoDelim;
+
+string generateString(int len) {
+  std::uniform_int_distribution<uint32_t> validChar(1, 255);  // no null-char
+  string ret;
+  while (len--) {
+    ret.push_back(validChar(rnd));
+  }
+  return ret;
+}
+
+void initDelims(int len) {
+  ffoDelim.clear();
+
+  string s(len - 1, '\0');  // find_first_of won't finish until last char
+  s.push_back('a');
+  ffoTestString = s;
+
+  for (int i = 0; i < ffoDelimSize; ++i) {
+    // most delimiter sets are pretty small, but occasionally there could
+    // be a big one.
+    auto n = rnd() % 8 + 1;
+    if (n == 8) {
+      n = 32;
+    }
+    auto s = generateString(n);
+    if (rnd() % 2) {
+      // ~half of tests will find a hit
+      s[rnd() % s.size()] = 'a';  // yes, this could mean 'a' is a duplicate
+    }
+    ffoDelim.push_back(s);
+  }
+}
+
+}  // anonymous namespace
+
+BENCHMARK(FindSingleCharMemchr, n) {
+  StringPiece haystack(str);
+  FOR_EACH_RANGE (i, 0, n) {
+    doNotOptimizeAway(haystack.find('b'));
+    char x = haystack[0];
+    doNotOptimizeAway(&x);
+  }
+}
+
+BENCHMARK_RELATIVE(FindSingleCharRange, n) {
+  const char c = 'b';
+  StringPiece haystack(str);
+  folly::StringPiece needle(&c, &c + 1);
+  FOR_EACH_RANGE (i, 0, n) {
+    doNotOptimizeAway(haystack.find(needle));
+    char x = haystack[0];
+    doNotOptimizeAway(&x);
+  }
+}
+
+BENCHMARK_DRAW_LINE();
+
+// it's useful to compare our custom implementations vs. the standard library
+inline size_t qfind_first_byte_of_std(const StringPiece& haystack,
+                                      const StringPiece& needles) {
+  return qfind_first_of(haystack, needles, asciiCaseSensitive);
+}
+
+template <class Func>
+void findFirstOfRange(StringPiece needles, Func func, size_t n) {
+  FOR_EACH_RANGE (i, 0, n) {
+    const StringPiece& haystack = vstr[i % kVstrSize];
+    doNotOptimizeAway(func(haystack, needles));
+    char x = haystack[0];
+    doNotOptimizeAway(&x);
+  }
+}
+
+const string delims2 = "bc";
+
+BENCHMARK(FindFirstOf2NeedlesBase, n) {
+  findFirstOfRange(delims2, detail::qfind_first_byte_of, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOf2NeedlesNoSSE, n) {
+  findFirstOfRange(delims2, detail::qfind_first_byte_of_nosse, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOf2NeedlesStd, n) {
+  findFirstOfRange(delims2, qfind_first_byte_of_std, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOf2NeedlesMemchr, n) {
+  findFirstOfRange(delims2, detail::qfind_first_byte_of_memchr, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOf2NeedlesByteSet, n) {
+  findFirstOfRange(delims2, detail::qfind_first_byte_of_byteset, n);
+}
+
+BENCHMARK_DRAW_LINE();
+
+const string delims4 = "bcde";
+
+BENCHMARK(FindFirstOf4NeedlesBase, n) {
+  findFirstOfRange(delims4, detail::qfind_first_byte_of, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOf4NeedlesNoSSE, n) {
+  findFirstOfRange(delims4, detail::qfind_first_byte_of_nosse, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOf4NeedlesStd, n) {
+  findFirstOfRange(delims4, qfind_first_byte_of_std, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOf4NeedlesMemchr, n) {
+  findFirstOfRange(delims4, detail::qfind_first_byte_of_memchr, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOf4NeedlesByteSet, n) {
+  findFirstOfRange(delims4, detail::qfind_first_byte_of_byteset, n);
+}
+
+BENCHMARK_DRAW_LINE();
+
+const string delims8 = "0123456b";
+
+BENCHMARK(FindFirstOf8NeedlesBase, n) {
+  findFirstOfRange(delims8, detail::qfind_first_byte_of, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOf8NeedlesNoSSE, n) {
+  findFirstOfRange(delims8, detail::qfind_first_byte_of_nosse, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOf8NeedlesStd, n) {
+  findFirstOfRange(delims8, qfind_first_byte_of_std, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOf8NeedlesMemchr, n) {
+  findFirstOfRange(delims8, detail::qfind_first_byte_of_memchr, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOf8NeedlesByteSet, n) {
+  findFirstOfRange(delims8, detail::qfind_first_byte_of_byteset, n);
+}
+
+BENCHMARK_DRAW_LINE();
+
+const string delims16 = "0123456789bcdefg";
+
+BENCHMARK(FindFirstOf16NeedlesBase, n) {
+  findFirstOfRange(delims16, detail::qfind_first_byte_of, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOf16NeedlesNoSSE, n) {
+  findFirstOfRange(delims16, detail::qfind_first_byte_of_nosse, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOf16NeedlesStd, n) {
+  findFirstOfRange(delims16, qfind_first_byte_of_std, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOf16NeedlesMemchr, n) {
+  findFirstOfRange(delims16, detail::qfind_first_byte_of_memchr, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOf16NeedlesByteSet, n) {
+  findFirstOfRange(delims16, detail::qfind_first_byte_of_byteset, n);
+}
+
+BENCHMARK_DRAW_LINE();
+
+const string delims32 = "!bcdefghijklmnopqrstuvwxyz_012345";
+
+BENCHMARK(FindFirstOf32NeedlesBase, n) {
+  findFirstOfRange(delims32, detail::qfind_first_byte_of, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOf32NeedlesNoSSE, n) {
+  findFirstOfRange(delims32, detail::qfind_first_byte_of_nosse, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOf32NeedlesStd, n) {
+  findFirstOfRange(delims32, qfind_first_byte_of_std, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOf32NeedlesMemchr, n) {
+  findFirstOfRange(delims32, detail::qfind_first_byte_of_memchr, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOf32NeedlesByteSet, n) {
+  findFirstOfRange(delims32, detail::qfind_first_byte_of_byteset, n);
+}
+
+BENCHMARK_DRAW_LINE();
+
+const string delims64 = "!bcdefghijklmnopqrstuvwxyz_"
+                        "ABCDEFGHIJKLMNOPQRSTUVWXYZ-0123456789$";
+
+BENCHMARK(FindFirstOf64NeedlesBase, n) {
+  findFirstOfRange(delims64, detail::qfind_first_byte_of, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOf64NeedlesNoSSE, n) {
+  findFirstOfRange(delims64, detail::qfind_first_byte_of_nosse, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOf64NeedlesStd, n) {
+  findFirstOfRange(delims64, qfind_first_byte_of_std, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOf64NeedlesMemchr, n) {
+  findFirstOfRange(delims64, detail::qfind_first_byte_of_memchr, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOf64NeedlesByteSet, n) {
+  findFirstOfRange(delims64, detail::qfind_first_byte_of_byteset, n);
+}
+
+BENCHMARK_DRAW_LINE();
+
+template <class Func>
+void findFirstOfRandom(Func func, size_t iters) {
+  for (int i = 0; i < iters; ++i) {
+    auto test = i % ffoDelim.size();
+    auto p = func(ffoTestString, ffoDelim[test]);
+    doNotOptimizeAway(p);
+  }
+}
+
+BENCHMARK(FindFirstOfRandomBase, n) {
+  findFirstOfRandom(detail::qfind_first_byte_of, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOfRandomNoSSE, n) {
+  findFirstOfRandom(detail::qfind_first_byte_of_nosse, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOfRandomStd, n) {
+  findFirstOfRandom(qfind_first_byte_of_std, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOfRandomMemchr, n) {
+  findFirstOfRandom(detail::qfind_first_byte_of_memchr, n);
+}
+
+BENCHMARK_RELATIVE(FindFirstOfRandomByteSet, n) {
+  findFirstOfRandom(detail::qfind_first_byte_of_byteset, n);
+}
+
+BENCHMARK_DRAW_LINE();
+
+BENCHMARK(FindFirstOfOffsetRange, n) {
+  StringPiece haystack(str);
+  folly::StringPiece needles("bc");
+  DCHECK_EQ(haystack.size() - 1, haystack.find_first_of(needles, 1)); // works!
+  FOR_EACH_RANGE (i, 0, n) {
+    size_t pos = i % 2; // not a constant to prevent optimization
+    doNotOptimizeAway(haystack.find_first_of(needles, pos));
+    char x = haystack[0];
+    doNotOptimizeAway(&x);
+  }
+}
+
+BENCHMARK_DRAW_LINE();
+
+int main(int argc, char** argv) {
+  google::ParseCommandLineFlags(&argc, &argv, true);
+
+  for (int len : {1, 8, 10, 16, 32, 64, 128, 256, 10*1024, 1024*1024}) {
+    initStr(len);
+    initDelims(len);
+    runBenchmarks();
+  }
+  return 0;
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/RangeTest.cpp
@@ -0,0 +1,960 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// @author Kristina Holst (kholst@fb.com)
+// @author Andrei Alexandrescu (andrei.alexandrescu@fb.com)
+
+#include "folly/Range.h"
+
+#include <array>
+#include <boost/range/concepts.hpp>
+#include <cstdlib>
+#include <gtest/gtest.h>
+#include <iterator>
+#include <limits>
+#include <string>
+#include <sys/mman.h>
+#include <vector>
+
+namespace folly { namespace detail {
+
+// declaration of functions in Range.cpp
+size_t qfind_first_byte_of_memchr(const StringPiece& haystack,
+                                  const StringPiece& needles);
+
+size_t qfind_first_byte_of_byteset(const StringPiece& haystack,
+                                   const StringPiece& needles);
+
+}}  // namespaces
+
+using namespace folly;
+using namespace std;
+
+BOOST_CONCEPT_ASSERT((boost::RandomAccessRangeConcept<StringPiece>));
+
+TEST(StringPiece, All) {
+  const char* foo = "foo";
+  const char* foo2 = "foo";
+  string fooStr(foo);
+  string foo2Str(foo2);
+
+  // we expect the compiler to optimize things so that there's only one copy
+  // of the string literal "foo", even though we've got it in multiple places
+  EXPECT_EQ(foo, foo2);  // remember, this uses ==, not strcmp, so it's a ptr
+                         // comparison rather than lexical
+
+  // the string object creates copies though, so the c_str of these should be
+  // distinct
+  EXPECT_NE(fooStr.c_str(), foo2Str.c_str());
+
+  // test the basic StringPiece functionality
+  StringPiece s(foo);
+  EXPECT_EQ(s.size(), 3);
+
+  EXPECT_EQ(s.start(), foo);              // ptr comparison
+  EXPECT_NE(s.start(), fooStr.c_str());   // ptr comparison
+  EXPECT_NE(s.start(), foo2Str.c_str());  // ptr comparison
+
+  EXPECT_EQ(s.toString(), foo);              // lexical comparison
+  EXPECT_EQ(s.toString(), fooStr.c_str());   // lexical comparison
+  EXPECT_EQ(s.toString(), foo2Str.c_str());  // lexical comparison
+
+  EXPECT_EQ(s, foo);                      // lexical comparison
+  EXPECT_EQ(s, fooStr);                   // lexical comparison
+  EXPECT_EQ(s, foo2Str);                  // lexical comparison
+  EXPECT_EQ(foo, s);
+
+  // check using StringPiece to reference substrings
+  const char* foobarbaz = "foobarbaz";
+
+  // the full "foobarbaz"
+  s.reset(foobarbaz, strlen(foobarbaz));
+  EXPECT_EQ(s.size(), 9);
+  EXPECT_EQ(s.start(), foobarbaz);
+  EXPECT_EQ(s, "foobarbaz");
+
+  // only the 'foo'
+  s.assign(foobarbaz, foobarbaz + 3);
+  EXPECT_EQ(s.size(), 3);
+  EXPECT_EQ(s.start(), foobarbaz);
+  EXPECT_EQ(s, "foo");
+
+  // find
+  s.reset(foobarbaz, strlen(foobarbaz));
+  EXPECT_EQ(s.find("bar"), 3);
+  EXPECT_EQ(s.find("ba", 3), 3);
+  EXPECT_EQ(s.find("ba", 4), 6);
+  EXPECT_EQ(s.find("notfound"), StringPiece::npos);
+  EXPECT_EQ(s.find("notfound", 1), StringPiece::npos);
+  EXPECT_EQ(s.find("bar", 4), StringPiece::npos);  // starting position too far
+  // starting pos that is obviously past the end -- This works for std::string
+  EXPECT_EQ(s.toString().find("notfound", 55), StringPiece::npos);
+  EXPECT_EQ(s.find("z", s.size()), StringPiece::npos);
+  EXPECT_EQ(s.find("z", 55), StringPiece::npos);
+  // empty needle
+  EXPECT_EQ(s.find(""), std::string().find(""));
+  EXPECT_EQ(s.find(""), 0);
+
+  // single char finds
+  EXPECT_EQ(s.find('b'), 3);
+  EXPECT_EQ(s.find('b', 3), 3);
+  EXPECT_EQ(s.find('b', 4), 6);
+  EXPECT_EQ(s.find('o', 2), 2);
+  EXPECT_EQ(s.find('y'), StringPiece::npos);
+  EXPECT_EQ(s.find('y', 1), StringPiece::npos);
+  EXPECT_EQ(s.find('o', 4), StringPiece::npos);  // starting position too far
+  // starting pos that is obviously past the end -- This works for std::string
+  EXPECT_EQ(s.toString().find('y', 55), StringPiece::npos);
+  EXPECT_EQ(s.find('z', s.size()), StringPiece::npos);
+  EXPECT_EQ(s.find('z', 55), StringPiece::npos);
+  // null char
+  EXPECT_EQ(s.find('\0'), std::string().find('\0'));
+  EXPECT_EQ(s.find('\0'), StringPiece::npos);
+
+  // single char rfinds
+  EXPECT_EQ(s.rfind('b'), 6);
+  EXPECT_EQ(s.rfind('y'), StringPiece::npos);
+  EXPECT_EQ(s.str().rfind('y'), StringPiece::npos);
+  EXPECT_EQ(ByteRange(s).rfind('b'), 6);
+  EXPECT_EQ(ByteRange(s).rfind('y'), StringPiece::npos);
+  // null char
+  EXPECT_EQ(s.rfind('\0'), s.str().rfind('\0'));
+  EXPECT_EQ(s.rfind('\0'), StringPiece::npos);
+
+  // find_first_of
+  s.reset(foobarbaz, strlen(foobarbaz));
+  EXPECT_EQ(s.find_first_of("bar"), 3);
+  EXPECT_EQ(s.find_first_of("ba", 3), 3);
+  EXPECT_EQ(s.find_first_of("ba", 4), 4);
+  EXPECT_EQ(s.find_first_of("xyxy"), StringPiece::npos);
+  EXPECT_EQ(s.find_first_of("xyxy", 1), StringPiece::npos);
+  // starting position too far
+  EXPECT_EQ(s.find_first_of("foo", 4), StringPiece::npos);
+  // starting pos that is obviously past the end -- This works for std::string
+  EXPECT_EQ(s.toString().find_first_of("xyxy", 55), StringPiece::npos);
+  EXPECT_EQ(s.find_first_of("z", s.size()), StringPiece::npos);
+  EXPECT_EQ(s.find_first_of("z", 55), StringPiece::npos);
+  // empty needle. Note that this returns npos, while find() returns 0!
+  EXPECT_EQ(s.find_first_of(""), std::string().find_first_of(""));
+  EXPECT_EQ(s.find_first_of(""), StringPiece::npos);
+
+  // single char find_first_ofs
+  EXPECT_EQ(s.find_first_of('b'), 3);
+  EXPECT_EQ(s.find_first_of('b', 3), 3);
+  EXPECT_EQ(s.find_first_of('b', 4), 6);
+  EXPECT_EQ(s.find_first_of('o', 2), 2);
+  EXPECT_EQ(s.find_first_of('y'), StringPiece::npos);
+  EXPECT_EQ(s.find_first_of('y', 1), StringPiece::npos);
+  // starting position too far
+  EXPECT_EQ(s.find_first_of('o', 4), StringPiece::npos);
+  // starting pos that is obviously past the end -- This works for std::string
+  EXPECT_EQ(s.toString().find_first_of('y', 55), StringPiece::npos);
+  EXPECT_EQ(s.find_first_of('z', s.size()), StringPiece::npos);
+  EXPECT_EQ(s.find_first_of('z', 55), StringPiece::npos);
+  // null char
+  EXPECT_EQ(s.find_first_of('\0'), std::string().find_first_of('\0'));
+  EXPECT_EQ(s.find_first_of('\0'), StringPiece::npos);
+
+  // just "barbaz"
+  s.reset(foobarbaz + 3, strlen(foobarbaz + 3));
+  EXPECT_EQ(s.size(), 6);
+  EXPECT_EQ(s.start(), foobarbaz + 3);
+  EXPECT_EQ(s, "barbaz");
+
+  // just "bar"
+  s.reset(foobarbaz + 3, 3);
+  EXPECT_EQ(s.size(), 3);
+  EXPECT_EQ(s, "bar");
+
+  // clear
+  s.clear();
+  EXPECT_EQ(s.toString(), "");
+
+  // test an empty StringPiece
+  StringPiece s2;
+  EXPECT_EQ(s2.size(), 0);
+
+  // Test comparison operators
+  foo = "";
+  EXPECT_LE(s, foo);
+  EXPECT_LE(foo, s);
+  EXPECT_GE(s, foo);
+  EXPECT_GE(foo, s);
+  EXPECT_EQ(s, foo);
+  EXPECT_EQ(foo, s);
+
+  foo = "abc";
+  EXPECT_LE(s, foo);
+  EXPECT_LT(s, foo);
+  EXPECT_GE(foo, s);
+  EXPECT_GT(foo, s);
+  EXPECT_NE(s, foo);
+
+  EXPECT_LE(s, s);
+  EXPECT_LE(s, s);
+  EXPECT_GE(s, s);
+  EXPECT_GE(s, s);
+  EXPECT_EQ(s, s);
+  EXPECT_EQ(s, s);
+
+  s = "abc";
+  s2 = "abc";
+  EXPECT_LE(s, s2);
+  EXPECT_LE(s2, s);
+  EXPECT_GE(s, s2);
+  EXPECT_GE(s2, s);
+  EXPECT_EQ(s, s2);
+  EXPECT_EQ(s2, s);
+}
+
+template <class T>
+void expectLT(const T& a, const T& b) {
+  EXPECT_TRUE(a < b);
+  EXPECT_TRUE(a <= b);
+  EXPECT_FALSE(a == b);
+  EXPECT_FALSE(a >= b);
+  EXPECT_FALSE(a > b);
+
+  EXPECT_FALSE(b < a);
+  EXPECT_FALSE(b <= a);
+  EXPECT_TRUE(b >= a);
+  EXPECT_TRUE(b > a);
+}
+
+template <class T>
+void expectEQ(const T& a, const T& b) {
+  EXPECT_FALSE(a < b);
+  EXPECT_TRUE(a <= b);
+  EXPECT_TRUE(a == b);
+  EXPECT_TRUE(a >= b);
+  EXPECT_FALSE(a > b);
+}
+
+TEST(StringPiece, EightBitComparisons) {
+  char values[] = {'\x00', '\x20', '\x40', '\x7f', '\x80', '\xc0', '\xff'};
+  constexpr size_t count = sizeof(values) / sizeof(values[0]);
+  for (size_t i = 0; i < count; ++i) {
+    std::string a(1, values[i]);
+    // Defeat copy-on-write
+    std::string aCopy(a.data(), a.size());
+    expectEQ(a, aCopy);
+    expectEQ(StringPiece(a), StringPiece(aCopy));
+
+    for (size_t j = i + 1; j < count; ++j) {
+      std::string b(1, values[j]);
+      expectLT(a, b);
+      expectLT(StringPiece(a), StringPiece(b));
+    }
+  }
+}
+
+TEST(StringPiece, ToByteRange) {
+  StringPiece a("hello");
+  ByteRange b(a);
+  EXPECT_EQ(static_cast<const void*>(a.begin()),
+            static_cast<const void*>(b.begin()));
+  EXPECT_EQ(static_cast<const void*>(a.end()),
+            static_cast<const void*>(b.end()));
+
+  // and convert back again
+  StringPiece c(b);
+  EXPECT_EQ(a.begin(), c.begin());
+  EXPECT_EQ(a.end(), c.end());
+}
+
+TEST(StringPiece, InvalidRange) {
+  StringPiece a("hello");
+  EXPECT_EQ(a, a.subpiece(0, 10));
+  EXPECT_EQ(StringPiece("ello"), a.subpiece(1));
+  EXPECT_EQ(StringPiece("ello"), a.subpiece(1, std::string::npos));
+  EXPECT_EQ(StringPiece("ell"), a.subpiece(1, 3));
+  EXPECT_THROW(a.subpiece(6, 7), std::out_of_range);
+  EXPECT_THROW(a.subpiece(6), std::out_of_range);
+
+  std::string b("hello");
+  EXPECT_EQ(a, StringPiece(b, 0, 10));
+  EXPECT_EQ("ello", a.subpiece(1));
+  EXPECT_EQ("ello", a.subpiece(1, std::string::npos));
+  EXPECT_EQ("ell", a.subpiece(1, 3));
+  EXPECT_THROW(a.subpiece(6, 7), std::out_of_range);
+  EXPECT_THROW(a.subpiece(6), std::out_of_range);
+}
+
+#if FOLLY_HAVE_CONSTEXPR_STRLEN
+constexpr char helloArray[] = "hello";
+
+TEST(StringPiece, Constexpr) {
+  constexpr StringPiece hello1("hello");
+  EXPECT_EQ("hello", hello1);
+
+  constexpr StringPiece hello2(helloArray);
+  EXPECT_EQ("hello", hello2);
+}
+#endif
+
+TEST(StringPiece, Prefix) {
+  StringPiece a("hello");
+  EXPECT_TRUE(a.startsWith(""));
+  EXPECT_TRUE(a.startsWith("h"));
+  EXPECT_TRUE(a.startsWith('h'));
+  EXPECT_TRUE(a.startsWith("hello"));
+  EXPECT_FALSE(a.startsWith("hellox"));
+  EXPECT_FALSE(a.startsWith('x'));
+  EXPECT_FALSE(a.startsWith("x"));
+
+  {
+    auto b = a;
+    EXPECT_TRUE(b.removePrefix(""));
+    EXPECT_EQ("hello", b);
+  }
+  {
+    auto b = a;
+    EXPECT_TRUE(b.removePrefix("h"));
+    EXPECT_EQ("ello", b);
+  }
+  {
+    auto b = a;
+    EXPECT_TRUE(b.removePrefix('h'));
+    EXPECT_EQ("ello", b);
+  }
+  {
+    auto b = a;
+    EXPECT_TRUE(b.removePrefix("hello"));
+    EXPECT_EQ("", b);
+  }
+  {
+    auto b = a;
+    EXPECT_FALSE(b.removePrefix("hellox"));
+    EXPECT_EQ("hello", b);
+  }
+  {
+    auto b = a;
+    EXPECT_FALSE(b.removePrefix("x"));
+    EXPECT_EQ("hello", b);
+  }
+  {
+    auto b = a;
+    EXPECT_FALSE(b.removePrefix('x'));
+    EXPECT_EQ("hello", b);
+  }
+}
+
+TEST(StringPiece, Suffix) {
+  StringPiece a("hello");
+  EXPECT_TRUE(a.endsWith(""));
+  EXPECT_TRUE(a.endsWith("o"));
+  EXPECT_TRUE(a.endsWith('o'));
+  EXPECT_TRUE(a.endsWith("hello"));
+  EXPECT_FALSE(a.endsWith("xhello"));
+  EXPECT_FALSE(a.endsWith("x"));
+  EXPECT_FALSE(a.endsWith('x'));
+
+  {
+    auto b = a;
+    EXPECT_TRUE(b.removeSuffix(""));
+    EXPECT_EQ("hello", b);
+  }
+  {
+    auto b = a;
+    EXPECT_TRUE(b.removeSuffix("o"));
+    EXPECT_EQ("hell", b);
+  }
+  {
+    auto b = a;
+    EXPECT_TRUE(b.removeSuffix('o'));
+    EXPECT_EQ("hell", b);
+  }
+  {
+    auto b = a;
+    EXPECT_TRUE(b.removeSuffix("hello"));
+    EXPECT_EQ("", b);
+  }
+  {
+    auto b = a;
+    EXPECT_FALSE(b.removeSuffix("xhello"));
+    EXPECT_EQ("hello", b);
+  }
+  {
+    auto b = a;
+    EXPECT_FALSE(b.removeSuffix("x"));
+    EXPECT_EQ("hello", b);
+  }
+  {
+    auto b = a;
+    EXPECT_FALSE(b.removeSuffix('x'));
+    EXPECT_EQ("hello", b);
+  }
+}
+
+TEST(StringPiece, PrefixEmpty) {
+  StringPiece a;
+  EXPECT_TRUE(a.startsWith(""));
+  EXPECT_FALSE(a.startsWith("a"));
+  EXPECT_FALSE(a.startsWith('a'));
+  EXPECT_TRUE(a.removePrefix(""));
+  EXPECT_EQ("", a);
+  EXPECT_FALSE(a.removePrefix("a"));
+  EXPECT_EQ("", a);
+  EXPECT_FALSE(a.removePrefix('a'));
+  EXPECT_EQ("", a);
+}
+
+TEST(StringPiece, SuffixEmpty) {
+  StringPiece a;
+  EXPECT_TRUE(a.endsWith(""));
+  EXPECT_FALSE(a.endsWith("a"));
+  EXPECT_FALSE(a.endsWith('a'));
+  EXPECT_TRUE(a.removeSuffix(""));
+  EXPECT_EQ("", a);
+  EXPECT_FALSE(a.removeSuffix("a"));
+  EXPECT_EQ("", a);
+  EXPECT_FALSE(a.removeSuffix('a'));
+  EXPECT_EQ("", a);
+}
+
+TEST(StringPiece, split_step_char_delimiter) {
+  //              0         1         2
+  //              012345678901234567890123456
+  auto const s = "this is just  a test string";
+  auto const e = std::next(s, std::strlen(s));
+  EXPECT_EQ('\0', *e);
+
+  folly::StringPiece p(s);
+  EXPECT_EQ(s, p.begin());
+  EXPECT_EQ(e, p.end());
+
+  auto x = p.split_step(' ');
+  EXPECT_EQ(std::next(s, 5), p.begin());
+  EXPECT_EQ(e, p.end());
+  EXPECT_EQ("this", x);
+
+  x = p.split_step(' ');
+  EXPECT_EQ(std::next(s, 8), p.begin());
+  EXPECT_EQ(e, p.end());
+  EXPECT_EQ("is", x);
+
+  x = p.split_step('u');
+  EXPECT_EQ(std::next(s, 10), p.begin());
+  EXPECT_EQ(e, p.end());
+  EXPECT_EQ("j", x);
+
+  x = p.split_step(' ');
+  EXPECT_EQ(std::next(s, 13), p.begin());
+  EXPECT_EQ(e, p.end());
+  EXPECT_EQ("st", x);
+
+  x = p.split_step(' ');
+  EXPECT_EQ(std::next(s, 14), p.begin());
+  EXPECT_EQ(e, p.end());
+  EXPECT_EQ("", x);
+
+  x = p.split_step(' ');
+  EXPECT_EQ(std::next(s, 16), p.begin());
+  EXPECT_EQ(e, p.end());
+  EXPECT_EQ("a", x);
+
+  x = p.split_step(' ');
+  EXPECT_EQ(std::next(s, 21), p.begin());
+  EXPECT_EQ(e, p.end());
+  EXPECT_EQ("test", x);
+
+  x = p.split_step(' ');
+  EXPECT_EQ(e, p.begin());
+  EXPECT_EQ(e, p.end());
+  EXPECT_EQ("string", x);
+
+  x = p.split_step(' ');
+  EXPECT_EQ(e, p.begin());
+  EXPECT_EQ(e, p.end());
+  EXPECT_EQ("", x);
+}
+
+TEST(StringPiece, split_step_range_delimiter) {
+  //              0         1         2         3
+  //              0123456789012345678901234567890123
+  auto const s = "this  is  just    a   test  string";
+  auto const e = std::next(s, std::strlen(s));
+  EXPECT_EQ('\0', *e);
+
+  folly::StringPiece p(s);
+  EXPECT_EQ(s, p.begin());
+  EXPECT_EQ(e, p.end());
+
+  auto x = p.split_step("  ");
+  EXPECT_EQ(std::next(s, 6), p.begin());
+  EXPECT_EQ(e, p.end());
+  EXPECT_EQ("this", x);
+
+  x = p.split_step("  ");
+  EXPECT_EQ(std::next(s, 10), p.begin());
+  EXPECT_EQ(e, p.end());
+  EXPECT_EQ("is", x);
+
+  x = p.split_step("u");
+  EXPECT_EQ(std::next(s, 12), p.begin());
+  EXPECT_EQ(e, p.end());
+  EXPECT_EQ("j", x);
+
+  x = p.split_step("  ");
+  EXPECT_EQ(std::next(s, 16), p.begin());
+  EXPECT_EQ(e, p.end());
+  EXPECT_EQ("st", x);
+
+  x = p.split_step("  ");
+  EXPECT_EQ(std::next(s, 18), p.begin());
+  EXPECT_EQ(e, p.end());
+  EXPECT_EQ("", x);
+
+  x = p.split_step("  ");
+  EXPECT_EQ(std::next(s, 21), p.begin());
+  EXPECT_EQ(e, p.end());
+  EXPECT_EQ("a", x);
+
+  x = p.split_step("  ");
+  EXPECT_EQ(std::next(s, 28), p.begin());
+  EXPECT_EQ(e, p.end());
+  EXPECT_EQ(" test", x);
+
+  x = p.split_step("  ");
+  EXPECT_EQ(e, p.begin());
+  EXPECT_EQ(e, p.end());
+  EXPECT_EQ("string", x);
+
+  x = p.split_step("  ");
+  EXPECT_EQ(e, p.begin());
+  EXPECT_EQ(e, p.end());
+  EXPECT_EQ("", x);
+
+  x = p.split_step(" ");
+  EXPECT_EQ(e, p.begin());
+  EXPECT_EQ(e, p.end());
+  EXPECT_EQ("", x);
+}
+
+void split_step_with_process_noop(folly::StringPiece) {}
+
+TEST(StringPiece, split_step_with_process_char_delimiter) {
+  //              0         1         2
+  //              012345678901234567890123456
+  auto const s = "this is just  a test string";
+  auto const e = std::next(s, std::strlen(s));
+  EXPECT_EQ('\0', *e);
+
+  folly::StringPiece p(s);
+  EXPECT_EQ(s, p.begin());
+  EXPECT_EQ(e, p.end());
+
+  EXPECT_EQ(1, (p.split_step(' ', [&](folly::StringPiece x) {
+    EXPECT_EQ(std::next(s, 5), p.begin());
+    EXPECT_EQ(e, p.end());
+    EXPECT_EQ("this", x);
+    return 1;
+  })));
+
+  EXPECT_EQ(2, (p.split_step(' ', [&](folly::StringPiece x) {
+    EXPECT_EQ(std::next(s, 8), p.begin());
+    EXPECT_EQ(e, p.end());
+    EXPECT_EQ("is", x);
+    return 2;
+  })));
+
+  EXPECT_EQ(3, (p.split_step('u', [&](folly::StringPiece x) {
+    EXPECT_EQ(std::next(s, 10), p.begin());
+    EXPECT_EQ(e, p.end());
+    EXPECT_EQ("j", x);
+    return 3;
+  })));
+
+  EXPECT_EQ(4, (p.split_step(' ', [&](folly::StringPiece x) {
+    EXPECT_EQ(std::next(s, 13), p.begin());
+    EXPECT_EQ(e, p.end());
+    EXPECT_EQ("st", x);
+    return 4;
+  })));
+
+  EXPECT_EQ(5, (p.split_step(' ', [&](folly::StringPiece x) {
+    EXPECT_EQ(std::next(s, 14), p.begin());
+    EXPECT_EQ(e, p.end());
+    EXPECT_EQ("", x);
+    return 5;
+  })));
+
+  EXPECT_EQ(6, (p.split_step(' ', [&](folly::StringPiece x) {
+    EXPECT_EQ(std::next(s, 16), p.begin());
+    EXPECT_EQ(e, p.end());
+    EXPECT_EQ("a", x);
+    return 6;
+  })));
+
+  EXPECT_EQ(7, (p.split_step(' ', [&](folly::StringPiece x) {
+    EXPECT_EQ(std::next(s, 21), p.begin());
+    EXPECT_EQ(e, p.end());
+    EXPECT_EQ("test", x);
+    return 7;
+  })));
+
+  EXPECT_EQ(8, (p.split_step(' ', [&](folly::StringPiece x) {
+    EXPECT_EQ(e, p.begin());
+    EXPECT_EQ(e, p.end());
+    EXPECT_EQ("string", x);
+    return 8;
+  })));
+
+  EXPECT_EQ(9, (p.split_step(' ', [&](folly::StringPiece x) {
+    EXPECT_EQ(e, p.begin());
+    EXPECT_EQ(e, p.end());
+    EXPECT_EQ("", x);
+    return 9;
+  })));
+
+  EXPECT_TRUE((std::is_same<
+    void,
+    decltype(p.split_step(' ', split_step_with_process_noop))
+  >::value));
+
+  EXPECT_NO_THROW(p.split_step(' ', split_step_with_process_noop));
+}
+
+TEST(StringPiece, split_step_with_process_range_delimiter) {
+  //              0         1         2         3
+  //              0123456789012345678901234567890123
+  auto const s = "this  is  just    a   test  string";
+  auto const e = std::next(s, std::strlen(s));
+  EXPECT_EQ('\0', *e);
+
+  folly::StringPiece p(s);
+  EXPECT_EQ(s, p.begin());
+  EXPECT_EQ(e, p.end());
+
+  EXPECT_EQ(1, (p.split_step("  ", [&](folly::StringPiece x) {
+    EXPECT_EQ(std::next(s, 6), p.begin());
+    EXPECT_EQ(e, p.end());
+    EXPECT_EQ("this", x);
+    return 1;
+  })));
+
+  EXPECT_EQ(2, (p.split_step("  ", [&](folly::StringPiece x) {
+    EXPECT_EQ(std::next(s, 10), p.begin());
+    EXPECT_EQ(e, p.end());
+    EXPECT_EQ("is", x);
+    return 2;
+  })));
+
+  EXPECT_EQ(3, (p.split_step("u", [&](folly::StringPiece x) {
+    EXPECT_EQ(std::next(s, 12), p.begin());
+    EXPECT_EQ(e, p.end());
+    EXPECT_EQ("j", x);
+    return 3;
+  })));
+
+  EXPECT_EQ(4, (p.split_step("  ", [&](folly::StringPiece x) {
+    EXPECT_EQ(std::next(s, 16), p.begin());
+    EXPECT_EQ(e, p.end());
+    EXPECT_EQ("st", x);
+    return 4;
+  })));
+
+  EXPECT_EQ(5, (p.split_step("  ", [&](folly::StringPiece x) {
+    EXPECT_EQ(std::next(s, 18), p.begin());
+    EXPECT_EQ(e, p.end());
+    EXPECT_EQ("", x);
+    return 5;
+  })));
+
+  EXPECT_EQ(6, (p.split_step("  ", [&](folly::StringPiece x) {
+    EXPECT_EQ(std::next(s, 21), p.begin());
+    EXPECT_EQ(e, p.end());
+    EXPECT_EQ("a", x);
+    return 6;
+  })));
+
+  EXPECT_EQ(7, (p.split_step("  ", [&](folly::StringPiece x) {
+    EXPECT_EQ(std::next(s, 28), p.begin());
+    EXPECT_EQ(e, p.end());
+    EXPECT_EQ(" test", x);
+    return 7;
+  })));
+
+  EXPECT_EQ(8, (p.split_step("  ", [&](folly::StringPiece x) {
+    EXPECT_EQ(e, p.begin());
+    EXPECT_EQ(e, p.end());
+    EXPECT_EQ("string", x);
+    return 8;
+  })));
+
+  EXPECT_EQ(9, (p.split_step("  ", [&](folly::StringPiece x) {
+    EXPECT_EQ(e, p.begin());
+    EXPECT_EQ(e, p.end());
+    EXPECT_EQ("", x);
+    return 9;
+  })));
+
+  EXPECT_EQ(10, (p.split_step("  ", [&](folly::StringPiece x) {
+    EXPECT_EQ(e, p.begin());
+    EXPECT_EQ(e, p.end());
+    EXPECT_EQ("", x);
+    return 10;
+  })));
+
+  EXPECT_TRUE((std::is_same<
+    void,
+    decltype(p.split_step(' ', split_step_with_process_noop))
+  >::value));
+
+  EXPECT_NO_THROW(p.split_step(' ', split_step_with_process_noop));
+}
+
+TEST(qfind, UInt32_Ranges) {
+  vector<uint32_t> a({1, 2, 3, 260, 5});
+  vector<uint32_t> b({2, 3, 4});
+
+  auto a_range = folly::Range<const uint32_t*>(&a[0], a.size());
+  auto b_range = folly::Range<const uint32_t*>(&b[0], b.size());
+
+  EXPECT_EQ(qfind(a_range, b_range), string::npos);
+
+  a[3] = 4;
+  EXPECT_EQ(qfind(a_range, b_range), 1);
+}
+
+template <typename NeedleFinder>
+class NeedleFinderTest : public ::testing::Test {
+ public:
+  static size_t find_first_byte_of(StringPiece haystack, StringPiece needles) {
+    return NeedleFinder::find_first_byte_of(haystack, needles);
+  }
+};
+
+struct SseNeedleFinder {
+  static size_t find_first_byte_of(StringPiece haystack, StringPiece needles) {
+    // This will only use the SSE version if it is supported on this CPU
+    // (selected using ifunc).
+    return detail::qfind_first_byte_of(haystack, needles);
+  }
+};
+
+struct NoSseNeedleFinder {
+  static size_t find_first_byte_of(StringPiece haystack, StringPiece needles) {
+    return detail::qfind_first_byte_of_nosse(haystack, needles);
+  }
+};
+
+struct MemchrNeedleFinder {
+  static size_t find_first_byte_of(StringPiece haystack, StringPiece needles) {
+    return detail::qfind_first_byte_of_memchr(haystack, needles);
+  }
+};
+
+struct ByteSetNeedleFinder {
+  static size_t find_first_byte_of(StringPiece haystack, StringPiece needles) {
+    return detail::qfind_first_byte_of_byteset(haystack, needles);
+  }
+};
+
+typedef ::testing::Types<SseNeedleFinder, NoSseNeedleFinder, MemchrNeedleFinder,
+                         ByteSetNeedleFinder> NeedleFinders;
+TYPED_TEST_CASE(NeedleFinderTest, NeedleFinders);
+
+TYPED_TEST(NeedleFinderTest, Null) {
+  { // null characters in the string
+    string s(10, char(0));
+    s[5] = 'b';
+    string delims("abc");
+    EXPECT_EQ(5, this->find_first_byte_of(s, delims));
+  }
+  { // null characters in delim
+    string s("abc");
+    string delims(10, char(0));
+    delims[3] = 'c';
+    delims[7] = 'b';
+    EXPECT_EQ(1, this->find_first_byte_of(s, delims));
+  }
+  { // range not terminated by null character
+    string buf = "abcdefghijklmnopqrstuvwxyz";
+    StringPiece s(buf.data() + 5, 3);
+    StringPiece delims("z");
+    EXPECT_EQ(string::npos, this->find_first_byte_of(s, delims));
+  }
+}
+
+TYPED_TEST(NeedleFinderTest, DelimDuplicates) {
+  string delims(1000, 'b');
+  EXPECT_EQ(1, this->find_first_byte_of("abc", delims));
+  EXPECT_EQ(string::npos, this->find_first_byte_of("ac", delims));
+}
+
+TYPED_TEST(NeedleFinderTest, Empty) {
+  string a = "abc";
+  string b = "";
+  EXPECT_EQ(string::npos, this->find_first_byte_of(a, b));
+  EXPECT_EQ(string::npos, this->find_first_byte_of(b, a));
+  EXPECT_EQ(string::npos, this->find_first_byte_of(b, b));
+}
+
+TYPED_TEST(NeedleFinderTest, Unaligned) {
+  // works correctly even if input buffers are not 16-byte aligned
+  string s = "0123456789ABCDEFGH";
+  for (int i = 0; i < s.size(); ++i) {
+    StringPiece a(s.c_str() + i);
+    for (int j = 0; j < s.size(); ++j) {
+      StringPiece b(s.c_str() + j);
+      EXPECT_EQ((i > j) ? 0 : j - i, this->find_first_byte_of(a, b));
+    }
+  }
+}
+
+// for some algorithms (specifically those that create a set of needles),
+// we check for the edge-case of _all_ possible needles being sought.
+TYPED_TEST(NeedleFinderTest, Needles256) {
+  string needles;
+  const auto minValue = std::numeric_limits<StringPiece::value_type>::min();
+  const auto maxValue = std::numeric_limits<StringPiece::value_type>::max();
+  // make the size ~big to avoid any edge-case branches for tiny haystacks
+  const int haystackSize = 50;
+  for (int i = minValue; i <= maxValue; i++) {  // <=
+    needles.push_back(i);
+  }
+  EXPECT_EQ(StringPiece::npos, this->find_first_byte_of("", needles));
+  for (int i = minValue; i <= maxValue; i++) {
+    EXPECT_EQ(0, this->find_first_byte_of(string(haystackSize, i), needles));
+  }
+
+  needles.append("these are redundant characters");
+  EXPECT_EQ(StringPiece::npos, this->find_first_byte_of("", needles));
+  for (int i = minValue; i <= maxValue; i++) {
+    EXPECT_EQ(0, this->find_first_byte_of(string(haystackSize, i), needles));
+  }
+}
+
+TYPED_TEST(NeedleFinderTest, Base) {
+  for (int i = 0; i < 32; ++i) {
+    for (int j = 0; j < 32; ++j) {
+      string s = string(i, 'X') + "abca" + string(i, 'X');
+      string delims = string(j, 'Y') + "a" + string(j, 'Y');
+      EXPECT_EQ(i, this->find_first_byte_of(s, delims));
+    }
+  }
+}
+
+const size_t kPageSize = 4096;
+// Updates contents so that any read accesses past the last byte will
+// cause a SIGSEGV.  It accomplishes this by changing access to the page that
+// begins immediately after the end of the contents (as allocators and mmap()
+// all operate on page boundaries, this is a reasonable assumption).
+// This function will also initialize buf, which caller must free().
+void createProtectedBuf(StringPiece& contents, char** buf) {
+  ASSERT_LE(contents.size(), kPageSize);
+  const size_t kSuccess = 0;
+  if (kSuccess != posix_memalign((void**)buf, kPageSize, 4 * kPageSize)) {
+    ASSERT_FALSE(true);
+  }
+  mprotect(*buf + kPageSize, kPageSize, PROT_NONE);
+  size_t newBegin = kPageSize - contents.size();
+  memcpy(*buf + newBegin, contents.data(), contents.size());
+  contents.reset(*buf + newBegin, contents.size());
+}
+
+void freeProtectedBuf(char* buf) {
+  mprotect(buf + kPageSize, kPageSize, PROT_READ | PROT_WRITE);
+  free(buf);
+}
+
+TYPED_TEST(NeedleFinderTest, NoSegFault) {
+  const string base = string(32, 'a') + string("b");
+  const string delims = string(32, 'c') + string("b");
+  for (int i = 0; i <= 32; i++) {
+    for (int j = 0; j <= 33; j++) {
+      for (int shouldFind = 0; shouldFind <= 1; ++shouldFind) {
+        StringPiece s1(base);
+        s1.advance(i);
+        ASSERT_TRUE(!s1.empty());
+        if (!shouldFind) {
+          s1.pop_back();
+        }
+        StringPiece s2(delims);
+        s2.advance(j);
+        char* buf1;
+        char* buf2;
+        createProtectedBuf(s1, &buf1);
+        createProtectedBuf(s2, &buf2);
+        // printf("s1: '%s' (%ld) \ts2: '%s' (%ld)\n",
+        //        string(s1.data(), s1.size()).c_str(), s1.size(),
+        //        string(s2.data(), s2.size()).c_str(), s2.size());
+        auto r1 = this->find_first_byte_of(s1, s2);
+        auto f1 = std::find_first_of(s1.begin(), s1.end(),
+                                     s2.begin(), s2.end());
+        auto e1 = (f1 == s1.end()) ? StringPiece::npos : f1 - s1.begin();
+        EXPECT_EQ(r1, e1);
+        auto r2 = this->find_first_byte_of(s2, s1);
+        auto f2 = std::find_first_of(s2.begin(), s2.end(),
+                                     s1.begin(), s1.end());
+        auto e2 = (f2 == s2.end()) ? StringPiece::npos : f2 - s2.begin();
+        EXPECT_EQ(r2, e2);
+        freeProtectedBuf(buf1);
+        freeProtectedBuf(buf2);
+      }
+    }
+  }
+}
+
+TEST(NonConstTest, StringPiece) {
+  std::string hello("hello");
+  MutableStringPiece sp(&hello.front(), hello.size());
+  sp[0] = 'x';
+  EXPECT_EQ("xello", hello);
+  {
+    StringPiece s(sp);
+    EXPECT_EQ("xello", s);
+  }
+  {
+    ByteRange r1(sp);
+    MutableByteRange r2(sp);
+  }
+}
+
+template<class C>
+void testRangeFunc(C&& x, size_t n) {
+  const auto& cx = x;
+  // type, conversion checks
+  Range<int*> r1 = range(std::forward<C>(x));
+  Range<const int*> r2 = range(std::forward<C>(x));
+  Range<const int*> r3 = range(cx);
+  Range<const int*> r5 = range(std::move(cx));
+  EXPECT_EQ(r1.begin(), &x[0]);
+  EXPECT_EQ(r1.end(), &x[n]);
+  EXPECT_EQ(n, r1.size());
+  EXPECT_EQ(n, r2.size());
+  EXPECT_EQ(n, r3.size());
+  EXPECT_EQ(n, r5.size());
+}
+
+TEST(RangeFunc, Vector) {
+  std::vector<int> x;
+  testRangeFunc(x, 0);
+  x.push_back(2);
+  testRangeFunc(x, 1);
+  testRangeFunc(std::vector<int>{1, 2}, 2);
+}
+
+TEST(RangeFunc, Array) {
+  std::array<int, 3> x;
+  testRangeFunc(x, 3);
+}
+
+TEST(RangeFunc, CArray) {
+  int x[] {1, 2, 3, 4};
+  testRangeFunc(x, 4);
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/RWSpinLockTest.cpp
@@ -0,0 +1,242 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+//
+// @author xliu (xliux@fb.com)
+//
+
+#include <stdlib.h>
+#include <unistd.h>
+#include <vector>
+#include <thread>
+
+#include <gtest/gtest.h>
+#include <gflags/gflags.h>
+#include <glog/logging.h>
+#include "folly/RWSpinLock.h"
+
+DEFINE_int32(num_threads, 8, "num threads");
+
+namespace {
+
+static const int kMaxReaders = 50;
+static std::atomic<bool> stopThread;
+using namespace folly;
+
+template<typename RWSpinLockT> struct RWSpinLockTest: public testing::Test {
+  typedef RWSpinLockT RWSpinLockType;
+};
+
+typedef testing::Types<RWSpinLock
+#ifdef RW_SPINLOCK_USE_X86_INTRINSIC_
+        , RWTicketSpinLockT<32, true>,
+        RWTicketSpinLockT<32, false>,
+        RWTicketSpinLockT<64, true>,
+        RWTicketSpinLockT<64, false>
+#endif
+> Implementations;
+
+TYPED_TEST_CASE(RWSpinLockTest, Implementations);
+
+template<typename RWSpinLockType>
+static void run(RWSpinLockType* lock) {
+  int64_t reads = 0;
+  int64_t writes = 0;
+  while (!stopThread.load(std::memory_order_acquire)) {
+    if (rand() % 10 == 0) { // write
+      typename RWSpinLockType::WriteHolder guard(lock);
+      ++writes;
+    } else { // read
+      typename RWSpinLockType::ReadHolder guard(lock);
+      ++reads;
+    }
+  }
+  // VLOG(0) << "total reads: " << reads << "; total writes: " << writes;
+}
+
+
+TYPED_TEST(RWSpinLockTest, Writer_Wait_Readers) {
+  typedef typename TestFixture::RWSpinLockType RWSpinLockType;
+  RWSpinLockType l;
+
+  for (int i = 0; i < kMaxReaders; ++i) {
+    EXPECT_TRUE(l.try_lock_shared());
+    EXPECT_FALSE(l.try_lock());
+  }
+
+  for (int i = 0; i < kMaxReaders; ++i) {
+    EXPECT_FALSE(l.try_lock());
+    l.unlock_shared();
+  }
+
+  EXPECT_TRUE(l.try_lock());
+}
+
+TYPED_TEST(RWSpinLockTest, Readers_Wait_Writer) {
+  typedef typename TestFixture::RWSpinLockType RWSpinLockType;
+  RWSpinLockType l;
+
+  EXPECT_TRUE(l.try_lock());
+
+  for (int i = 0; i < kMaxReaders; ++i) {
+    EXPECT_FALSE(l.try_lock_shared());
+  }
+
+  l.unlock_and_lock_shared();
+  for (int i = 0; i < kMaxReaders - 1; ++i) {
+    EXPECT_TRUE(l.try_lock_shared());
+  }
+}
+
+TYPED_TEST(RWSpinLockTest, Writer_Wait_Writer) {
+  typedef typename TestFixture::RWSpinLockType RWSpinLockType;
+  RWSpinLockType l;
+
+  EXPECT_TRUE(l.try_lock());
+  EXPECT_FALSE(l.try_lock());
+  l.unlock();
+
+  EXPECT_TRUE(l.try_lock());
+  EXPECT_FALSE(l.try_lock());
+}
+
+TYPED_TEST(RWSpinLockTest, Read_Holders) {
+  typedef typename TestFixture::RWSpinLockType RWSpinLockType;
+  RWSpinLockType l;
+
+  {
+    typename RWSpinLockType::ReadHolder guard(&l);
+    EXPECT_FALSE(l.try_lock());
+    EXPECT_TRUE(l.try_lock_shared());
+    l.unlock_shared();
+
+    EXPECT_FALSE(l.try_lock());
+  }
+
+  EXPECT_TRUE(l.try_lock());
+  l.unlock();
+}
+
+TYPED_TEST(RWSpinLockTest, Write_Holders) {
+  typedef typename TestFixture::RWSpinLockType RWSpinLockType;
+  RWSpinLockType l;
+  {
+    typename RWSpinLockType::WriteHolder guard(&l);
+    EXPECT_FALSE(l.try_lock());
+    EXPECT_FALSE(l.try_lock_shared());
+  }
+
+  EXPECT_TRUE(l.try_lock_shared());
+  EXPECT_FALSE(l.try_lock());
+  l.unlock_shared();
+  EXPECT_TRUE(l.try_lock());
+}
+
+TYPED_TEST(RWSpinLockTest, ConcurrentTests) {
+  typedef typename TestFixture::RWSpinLockType RWSpinLockType;
+  RWSpinLockType l;
+  srand(time(nullptr));
+
+  std::vector<std::thread> threads;
+  for (int i = 0; i < FLAGS_num_threads; ++i) {
+    threads.push_back(std::thread(&run<RWSpinLockType>, &l));
+  }
+
+  sleep(1);
+  stopThread.store(true, std::memory_order_release);
+
+  for (auto& t : threads) {
+    t.join();
+  }
+}
+
+// RWSpinLock specific tests
+
+TEST(RWSpinLock, lock_unlock_tests) {
+  folly::RWSpinLock lock;
+  EXPECT_TRUE(lock.try_lock_upgrade());
+  EXPECT_FALSE(lock.try_lock_shared());
+  EXPECT_FALSE(lock.try_lock());
+  EXPECT_FALSE(lock.try_lock_upgrade());
+  lock.unlock_upgrade();
+  lock.lock_shared();
+  EXPECT_FALSE(lock.try_lock());
+  EXPECT_TRUE(lock.try_lock_upgrade());
+  lock.unlock_upgrade();
+  lock.unlock_shared();
+  EXPECT_TRUE(lock.try_lock());
+  EXPECT_FALSE(lock.try_lock_upgrade());
+  lock.unlock_and_lock_upgrade();
+  EXPECT_FALSE(lock.try_lock_shared());
+  lock.unlock_upgrade_and_lock_shared();
+  lock.unlock_shared();
+  EXPECT_EQ(0, lock.bits());
+}
+
+TEST(RWSpinLock, concurrent_holder_test) {
+  srand(time(nullptr));
+
+  folly::RWSpinLock lock;
+  std::atomic<int64_t> reads(0);
+  std::atomic<int64_t> writes(0);
+  std::atomic<int64_t> upgrades(0);
+  std::atomic<bool> stop(false);
+
+  auto go = [&] {
+    while (!stop.load(std::memory_order_acquire)) {
+      auto r = (uint32_t)(rand()) % 10;
+      if (r < 3) {          // starts from write lock
+        RWSpinLock::ReadHolder rg{
+          RWSpinLock::UpgradedHolder{
+            RWSpinLock::WriteHolder{&lock}}};
+        writes.fetch_add(1, std::memory_order_acq_rel);;
+      } else if (r < 6) {   // starts from upgrade lock
+        RWSpinLock::UpgradedHolder ug(&lock);
+        if (r < 4) {
+          RWSpinLock::WriteHolder wg(std::move(ug));
+        } else {
+          RWSpinLock::ReadHolder rg(std::move(ug));
+        }
+        upgrades.fetch_add(1, std::memory_order_acq_rel);;
+      } else {
+        RWSpinLock::ReadHolder rg{&lock};
+        reads.fetch_add(1, std::memory_order_acq_rel);
+      }
+    }
+  };
+
+  std::vector<std::thread> threads;
+  for (int i = 0; i < FLAGS_num_threads; ++i) {
+    threads.push_back(std::thread(go));
+  }
+
+  sleep(5);
+  stop.store(true, std::memory_order_release);
+
+  for (auto& t : threads) t.join();
+
+  LOG(INFO) << "reads: " << reads.load(std::memory_order_acquire)
+    << "; writes: " << writes.load(std::memory_order_acquire)
+    << "; upgrades: " << upgrades.load(std::memory_order_acquire);
+}
+
+}
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  return RUN_ALL_TESTS();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/SafeAssertTest.cpp
@@ -0,0 +1,38 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/SafeAssert.h"
+
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+#include "folly/Benchmark.h"
+
+using namespace folly;
+
+void fail() {
+  FOLLY_SAFE_CHECK(0, "hello");
+}
+
+void succeed() {
+  FOLLY_SAFE_CHECK(1, "world");
+}
+
+TEST(SafeAssert, AssertionFailure) {
+  succeed();
+  EXPECT_DEATH(fail(), ".*Assertion failure:.*hello.*");
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/ScopeGuardTest.cpp
@@ -0,0 +1,298 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/ScopeGuard.h"
+#include "folly/Portability.h"
+
+#include <gflags/gflags.h>
+#include <gtest/gtest.h>
+#include <glog/logging.h>
+
+#include <functional>
+#include <stdexcept>
+
+using folly::ScopeGuard;
+using folly::makeGuard;
+using std::vector;
+
+double returnsDouble() {
+  return 0.0;
+}
+
+class MyFunctor {
+ public:
+  explicit MyFunctor(int* ptr) : ptr_(ptr) {}
+
+  void operator()() {
+    ++*ptr_;
+  }
+
+ private:
+  int* ptr_;
+};
+
+TEST(ScopeGuard, DifferentWaysToBind) {
+  {
+    // There is implicit conversion from func pointer
+    // double (*)() to function<void()>.
+    ScopeGuard g = makeGuard(returnsDouble);
+  }
+
+  vector<int> v;
+  void (vector<int>::*push_back)(int const&) = &vector<int>::push_back;
+
+  v.push_back(1);
+  {
+    // binding to member function.
+    ScopeGuard g = makeGuard(std::bind(&vector<int>::pop_back, &v));
+  }
+  EXPECT_EQ(0, v.size());
+
+  {
+    // bind member function with args. v is passed-by-value!
+    ScopeGuard g = makeGuard(std::bind(push_back, v, 2));
+  }
+  EXPECT_EQ(0, v.size()); // push_back happened on a copy of v... fail!
+
+  // pass in an argument by pointer so to avoid copy.
+  {
+    ScopeGuard g = makeGuard(std::bind(push_back, &v, 4));
+  }
+  EXPECT_EQ(1, v.size());
+
+  {
+    // pass in an argument by reference so to avoid copy.
+    ScopeGuard g = makeGuard(std::bind(push_back, std::ref(v), 4));
+  }
+  EXPECT_EQ(2, v.size());
+
+  // lambda with a reference to v
+  {
+    ScopeGuard g = makeGuard([&] { v.push_back(5); });
+  }
+  EXPECT_EQ(3, v.size());
+
+  // lambda with a copy of v
+  {
+    ScopeGuard g = makeGuard([v] () mutable { v.push_back(6); });
+  }
+  EXPECT_EQ(3, v.size());
+
+  // functor object
+  int n = 0;
+  {
+    MyFunctor f(&n);
+    ScopeGuard g = makeGuard(f);
+  }
+  EXPECT_EQ(1, n);
+
+  // temporary functor object
+  n = 0;
+  {
+    ScopeGuard g = makeGuard(MyFunctor(&n));
+  }
+  EXPECT_EQ(1, n);
+
+  // Use auto instead of ScopeGuard
+  n = 2;
+  {
+    auto g = makeGuard(MyFunctor(&n));
+  }
+  EXPECT_EQ(3, n);
+
+  // Use const auto& instead of ScopeGuard
+  n = 10;
+  {
+    const auto& g = makeGuard(MyFunctor(&n));
+  }
+  EXPECT_EQ(11, n);
+}
+
+TEST(ScopeGuard, GuardException) {
+  EXPECT_DEATH({
+    ScopeGuard g = makeGuard([&] {
+      throw std::runtime_error("destructors should never throw!");
+    });
+  },
+  "destructors should never throw!"
+  );
+}
+
+/**
+ * Add an integer to a vector iff it was inserted into the
+ * db successfuly. Here is a schematic of how you would accomplish
+ * this with scope guard.
+ */
+void testUndoAction(bool failure) {
+  vector<int64_t> v;
+  { // defines a "mini" scope
+
+    // be optimistic and insert this into memory
+    v.push_back(1);
+
+    // The guard is triggered to undo the insertion unless dismiss() is called.
+    ScopeGuard guard = makeGuard([&] { v.pop_back(); });
+
+    // Do some action; Use the failure argument to pretend
+    // if it failed or succeeded.
+
+    // if there was no failure, dismiss the undo guard action.
+    if (!failure) {
+      guard.dismiss();
+    }
+  } // all stack allocated in the mini-scope will be destroyed here.
+
+  if (failure) {
+    EXPECT_EQ(0, v.size()); // the action failed => undo insertion
+  } else {
+    EXPECT_EQ(1, v.size()); // the action succeeded => keep insertion
+  }
+}
+
+TEST(ScopeGuard, UndoAction) {
+  testUndoAction(true);
+  testUndoAction(false);
+}
+
+/**
+ * Sometimes in a try catch block we want to execute a piece of code
+ * regardless if an exception happened or not. For example, you want
+ * to close a db connection regardless if an exception was thrown during
+ * insertion. In Java and other languages there is a finally clause that
+ * helps accomplish this:
+ *
+ *   try {
+ *     dbConn.doInsert(sql);
+ *   } catch (const DbException& dbe) {
+ *     dbConn.recordFailure(dbe);
+ *   } catch (const CriticalException& e) {
+ *     throw e; // re-throw the exception
+ *   } finally {
+ *     dbConn.closeConnection(); // executes no matter what!
+ *   }
+ *
+ * We can approximate this behavior in C++ with ScopeGuard.
+ */
+enum class ErrorBehavior {
+  SUCCESS,
+  HANDLED_ERROR,
+  UNHANDLED_ERROR,
+};
+
+void testFinally(ErrorBehavior error) {
+  bool cleanupOccurred = false;
+
+  try {
+    ScopeGuard guard = makeGuard([&] { cleanupOccurred = true; });
+
+    try {
+      if (error == ErrorBehavior::HANDLED_ERROR) {
+        throw std::runtime_error("throwing an expected error");
+      } else if (error == ErrorBehavior::UNHANDLED_ERROR) {
+        throw "never throw raw strings";
+      }
+    } catch (const std::runtime_error&) {
+    }
+  } catch (...) {
+    // Outer catch to swallow the error for the UNHANDLED_ERROR behavior
+  }
+
+  EXPECT_TRUE(cleanupOccurred);
+}
+
+TEST(ScopeGuard, TryCatchFinally) {
+  testFinally(ErrorBehavior::SUCCESS);
+  testFinally(ErrorBehavior::HANDLED_ERROR);
+  testFinally(ErrorBehavior::UNHANDLED_ERROR);
+}
+
+TEST(ScopeGuard, TEST_SCOPE_EXIT) {
+  int x = 0;
+  {
+    SCOPE_EXIT { ++x; };
+    EXPECT_EQ(0, x);
+  }
+  EXPECT_EQ(1, x);
+}
+
+class Foo {
+public:
+  Foo() {}
+  ~Foo() {
+    try {
+      auto e = std::current_exception();
+      int test = 0;
+      {
+        SCOPE_EXIT { ++test; };
+        EXPECT_EQ(0, test);
+      }
+      EXPECT_EQ(1, test);
+    } catch (const std::exception& ex) {
+      LOG(FATAL) << "Unexpected exception: " << ex.what();
+    }
+  }
+};
+
+TEST(ScopeGuard, TEST_SCOPE_FAILURE2) {
+  try {
+    Foo f;
+    throw std::runtime_error("test");
+  } catch (...) {
+  }
+}
+
+void testScopeFailAndScopeSuccess(ErrorBehavior error, bool expectFail) {
+  bool scopeFailExecuted = false;
+  bool scopeSuccessExecuted = false;
+
+  try {
+    SCOPE_FAIL { scopeFailExecuted = true; };
+    SCOPE_SUCCESS { scopeSuccessExecuted = true; };
+
+    try {
+      if (error == ErrorBehavior::HANDLED_ERROR) {
+        throw std::runtime_error("throwing an expected error");
+      } else if (error == ErrorBehavior::UNHANDLED_ERROR) {
+        throw "never throw raw strings";
+      }
+    } catch (const std::runtime_error&) {
+    }
+  } catch (...) {
+    // Outer catch to swallow the error for the UNHANDLED_ERROR behavior
+  }
+
+  EXPECT_EQ(expectFail, scopeFailExecuted);
+  EXPECT_EQ(!expectFail, scopeSuccessExecuted);
+}
+
+TEST(ScopeGuard, TEST_SCOPE_FAIL_AND_SCOPE_SUCCESS) {
+  testScopeFailAndScopeSuccess(ErrorBehavior::SUCCESS, false);
+  testScopeFailAndScopeSuccess(ErrorBehavior::HANDLED_ERROR, false);
+  testScopeFailAndScopeSuccess(ErrorBehavior::UNHANDLED_ERROR, true);
+}
+
+TEST(ScopeGuard, TEST_SCOPE_SUCCESS_THROW) {
+  auto lambda = []() {
+    SCOPE_SUCCESS { throw std::runtime_error("ehm"); };
+  };
+  EXPECT_THROW(lambda(), std::runtime_error);
+}
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  return RUN_ALL_TESTS();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/SmallLocksTest.cpp
@@ -0,0 +1,163 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/SmallLocks.h"
+#include <cassert>
+#include <cstdio>
+#include <mutex>
+#include <string>
+#include <vector>
+#include <pthread.h>
+#include <unistd.h>
+
+#include <thread>
+
+#include <gtest/gtest.h>
+
+using std::string;
+using folly::MicroSpinLock;
+using folly::PicoSpinLock;
+using folly::MSLGuard;
+
+namespace {
+
+struct LockedVal {
+  int ar[1024];
+  MicroSpinLock lock;
+
+  LockedVal() {
+    lock.init();
+    memset(ar, 0, sizeof ar);
+  }
+};
+
+// Compile time test for packed struct support (requires that both of
+// these classes are POD).
+struct ignore1 { MicroSpinLock msl; int16_t foo; } __attribute__((packed));
+struct ignore2 { PicoSpinLock<uint32_t> psl; int16_t foo; }
+  __attribute__((packed));
+static_assert(sizeof(ignore1) == 3, "Size check failed");
+static_assert(sizeof(ignore2) == 6, "Size check failed");
+
+LockedVal v;
+void splock_test() {
+
+  const int max = 1000;
+  unsigned int seed = (uintptr_t)pthread_self();
+  for (int i = 0; i < max; i++) {
+    asm("pause");
+    MSLGuard g(v.lock);
+
+    int first = v.ar[0];
+    for (int i = 1; i < sizeof v.ar / sizeof i; ++i) {
+      EXPECT_EQ(first, v.ar[i]);
+    }
+
+    int byte = rand_r(&seed);
+    memset(v.ar, char(byte), sizeof v.ar);
+  }
+}
+
+template<class T> struct PslTest {
+  PicoSpinLock<T> lock;
+
+  PslTest() { lock.init(); }
+
+  void doTest() {
+    T ourVal = rand() % (T(1) << (sizeof(T) * 8 - 1));
+    for (int i = 0; i < 10000; ++i) {
+      std::lock_guard<PicoSpinLock<T>> guard(lock);
+      lock.setData(ourVal);
+      for (int n = 0; n < 10; ++n) {
+        asm volatile("pause");
+        EXPECT_EQ(lock.getData(), ourVal);
+      }
+    }
+  }
+};
+
+template<class T>
+void doPslTest() {
+  PslTest<T> testObj;
+
+  const int nthrs = 17;
+  std::vector<std::thread> threads;
+  for (int i = 0; i < nthrs; ++i) {
+    threads.push_back(std::thread(&PslTest<T>::doTest, &testObj));
+  }
+  for (auto& t : threads) {
+    t.join();
+  }
+}
+
+struct TestClobber {
+  TestClobber() {
+    lock_.init();
+  }
+
+  void go() {
+    std::lock_guard<MicroSpinLock> g(lock_);
+    // This bug depends on gcc register allocation and is very sensitive. We
+    // have to use DCHECK instead of EXPECT_*.
+    DCHECK(!lock_.try_lock());
+  }
+
+ private:
+  MicroSpinLock lock_;
+};
+
+}
+
+TEST(SmallLocks, SpinLockCorrectness) {
+  EXPECT_EQ(sizeof(MicroSpinLock), 1);
+
+  int nthrs = sysconf(_SC_NPROCESSORS_ONLN) * 2;
+  std::vector<std::thread> threads;
+  for (int i = 0; i < nthrs; ++i) {
+    threads.push_back(std::thread(splock_test));
+  }
+  for (auto& t : threads) {
+    t.join();
+  }
+}
+
+TEST(SmallLocks, PicoSpinCorrectness) {
+  doPslTest<int16_t>();
+  doPslTest<uint16_t>();
+  doPslTest<int32_t>();
+  doPslTest<uint32_t>();
+  doPslTest<int64_t>();
+  doPslTest<uint64_t>();
+}
+
+TEST(SmallLocks, PicoSpinSigned) {
+  typedef PicoSpinLock<int16_t,0> Lock;
+  Lock val;
+  val.init(-4);
+  EXPECT_EQ(val.getData(), -4);
+
+  {
+    std::lock_guard<Lock> guard(val);
+    EXPECT_EQ(val.getData(), -4);
+    val.setData(-8);
+    EXPECT_EQ(val.getData(), -8);
+  }
+  EXPECT_EQ(val.getData(), -8);
+}
+
+TEST(SmallLocks, RegClobber) {
+  TestClobber().go();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/small_vector_test.cpp
@@ -0,0 +1,757 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/small_vector.h"
+
+#include <gtest/gtest.h>
+#include <string>
+#include <memory>
+#include <iostream>
+#include <limits>
+
+#include <boost/algorithm/string.hpp>
+
+#include "folly/Conv.h"
+
+using folly::small_vector;
+using namespace folly::small_vector_policy;
+
+#if defined(__x86_64__)
+
+static_assert(sizeof(small_vector<int>) == 16,
+              "Object size is not what we expect for small_vector<int>");
+static_assert(sizeof(small_vector<int32_t,2>) == 16,
+              "Object size is not what we expect for "
+              "small_vector<int32_t,2>");
+static_assert(sizeof(small_vector<int,10>) ==
+                10 * sizeof(int) + sizeof(std::size_t),
+              "Object size is not what we expect for small_vector<int,10>");
+
+static_assert(sizeof(small_vector<int32_t,1,uint32_t>) ==
+                8 + 4,
+              "small_vector<int32_t,1,uint32_t> is wrong size");
+static_assert(sizeof(small_vector<int32_t,1,uint16_t>) ==
+                8 + 2,
+              "small_vector<int32_t,1,uint32_t> is wrong size");
+static_assert(sizeof(small_vector<int32_t,1,uint8_t>) ==
+                8 + 1,
+              "small_vector<int32_t,1,uint32_t> is wrong size");
+
+static_assert(sizeof(small_vector<int32_t,1,OneBitMutex>) == 16,
+              "OneBitMutex took more space than expected");
+
+static_assert(sizeof(small_vector<int16_t,4,uint16_t>) == 10,
+              "Sizeof unexpectedly large");
+static_assert(sizeof(small_vector<int16_t,4,uint16_t,OneBitMutex>) == 10,
+              "Sizeof unexpectedly large");
+static_assert(sizeof(small_vector<int16_t,4,NoHeap,uint16_t,
+                                  OneBitMutex>) == 10,
+              "Sizeof unexpectedly large");
+
+#endif
+
+static_assert(!FOLLY_IS_TRIVIALLY_COPYABLE(std::unique_ptr<int>),
+              "std::unique_ptr<> is trivially copyable");
+
+namespace {
+
+struct NontrivialType {
+  static int ctored;
+  explicit NontrivialType() : a(0) {}
+
+  /* implicit */ NontrivialType(int a) : a(a) {
+    ++ctored;
+  }
+
+  NontrivialType(NontrivialType const& s) {
+    ++ctored;
+  }
+
+  NontrivialType& operator=(NontrivialType const& o) {
+    a = o.a;
+    return *this;
+  }
+
+  int32_t a;
+};
+static_assert(!FOLLY_IS_TRIVIALLY_COPYABLE(NontrivialType),
+              "NontrivialType is trivially copyable");
+
+int NontrivialType::ctored = 0;
+
+struct TestException {};
+
+int throwCounter = 1;
+void MaybeThrow() {
+  if (!--throwCounter) {
+    throw TestException();
+  }
+}
+
+const int kMagic = 0xdeadbeef;
+struct Thrower {
+  static int alive;
+
+  Thrower() : magic(kMagic) {
+    EXPECT_EQ(magic, kMagic);
+    MaybeThrow();
+    ++alive;
+  }
+  Thrower(Thrower const& other) : magic(other.magic) {
+    EXPECT_EQ(magic, kMagic);
+    MaybeThrow();
+    ++alive;
+  }
+  ~Thrower() noexcept {
+    EXPECT_EQ(magic, kMagic);
+    magic = 0;
+    --alive;
+  }
+
+  Thrower& operator=(Thrower const& other) {
+    EXPECT_EQ(magic, kMagic);
+    MaybeThrow();
+    return *this;
+  }
+
+  // This is just to try to make sure we don't get our member
+  // functions called on uninitialized memory.
+  int magic;
+};
+
+int Thrower::alive = 0;
+
+// Type that counts how many exist and doesn't support copy
+// construction.
+struct NoncopyableCounter {
+  static int alive;
+  NoncopyableCounter() {
+    ++alive;
+  }
+  ~NoncopyableCounter() {
+    --alive;
+  }
+  NoncopyableCounter(NoncopyableCounter&&) { ++alive; }
+  NoncopyableCounter(NoncopyableCounter const&) = delete;
+  NoncopyableCounter& operator=(NoncopyableCounter const&) const = delete;
+  NoncopyableCounter& operator=(NoncopyableCounter&&) { return *this; }
+};
+int NoncopyableCounter::alive = 0;
+
+static_assert(!FOLLY_IS_TRIVIALLY_COPYABLE(NoncopyableCounter),
+              "NoncopyableCounter is trivially copyable");
+
+// Check that throws don't break the basic guarantee for some cases.
+// Uses the method for testing exception safety described at
+// http://www.boost.org/community/exception_safety.html, to force all
+// throwing code paths to occur.
+struct TestBasicGuarantee {
+  folly::small_vector<Thrower,3> vec;
+  int const prepopulate;
+
+  explicit TestBasicGuarantee(int prepopulate)
+    : prepopulate(prepopulate)
+  {
+    throwCounter = 1000;
+    for (int i = 0; i < prepopulate; ++i) {
+      vec.push_back(Thrower());
+    }
+  }
+
+  ~TestBasicGuarantee() {
+    throwCounter = 1000;
+  }
+
+  template<class Operation>
+  void operator()(int insertCount, Operation const& op) {
+    bool done = false;
+
+    std::unique_ptr<folly::small_vector<Thrower,3> > workingVec;
+    for (int counter = 1; !done; ++counter) {
+      throwCounter = 1000;
+      workingVec.reset(new folly::small_vector<Thrower,3>(vec));
+      throwCounter = counter;
+      EXPECT_EQ(Thrower::alive, prepopulate * 2);
+      try {
+        op(*workingVec);
+        done = true;
+      } catch (...) {
+        // Note that the size of the vector can change if we were
+        // inserting somewhere other than the end (it's a basic only
+        // guarantee).  All we're testing here is that we have the
+        // right amount of uninitialized vs initialized memory.
+        EXPECT_EQ(Thrower::alive, workingVec->size() + vec.size());
+        continue;
+      }
+
+      // If things succeeded.
+      EXPECT_EQ(workingVec->size(), prepopulate + insertCount);
+      EXPECT_EQ(Thrower::alive, prepopulate * 2 + insertCount);
+    }
+  }
+};
+
+}
+
+TEST(small_vector, BasicGuarantee) {
+  for (int prepop = 1; prepop < 30; ++prepop) {
+    (TestBasicGuarantee(prepop))( // parens or a mildly vexing parse :(
+      1,
+      [&] (folly::small_vector<Thrower,3>& v) {
+        v.push_back(Thrower());
+      }
+    );
+
+    EXPECT_EQ(Thrower::alive, 0);
+
+    (TestBasicGuarantee(prepop))(
+      1,
+      [&] (folly::small_vector<Thrower,3>& v) {
+        v.insert(v.begin(), Thrower());
+      }
+    );
+
+    EXPECT_EQ(Thrower::alive, 0);
+
+    (TestBasicGuarantee(prepop))(
+      1,
+      [&] (folly::small_vector<Thrower,3>& v) {
+        v.insert(v.begin() + 1, Thrower());
+      }
+    );
+
+    EXPECT_EQ(Thrower::alive, 0);
+  }
+
+  TestBasicGuarantee(4)(
+    3,
+    [&] (folly::small_vector<Thrower,3>& v) {
+      std::vector<Thrower> b;
+      b.push_back(Thrower());
+      b.push_back(Thrower());
+      b.push_back(Thrower());
+
+      /*
+       * Apparently if you do the following initializer_list instead
+       * of the above push_back's, and one of the Throwers throws,
+       * g++4.6 doesn't destruct the previous ones.  Heh.
+       */
+      //b = { Thrower(), Thrower(), Thrower() };
+      v.insert(v.begin() + 1, b.begin(), b.end());
+    }
+  );
+
+  TestBasicGuarantee(2)(
+    6,
+    [&] (folly::small_vector<Thrower,3>& v) {
+      std::vector<Thrower> b;
+      for (int i = 0; i < 6; ++i) {
+        b.push_back(Thrower());
+      }
+
+      v.insert(v.begin() + 1, b.begin(), b.end());
+    }
+  );
+
+  EXPECT_EQ(Thrower::alive, 0);
+  try {
+    throwCounter = 4;
+    folly::small_vector<Thrower,1> p(14, Thrower());
+  } catch (...) {
+  }
+  EXPECT_EQ(Thrower::alive, 0);
+}
+
+// Run this with.
+// MALLOC_CONF=prof_leak:true
+// LD_PRELOAD=${JEMALLOC_PATH}/lib/libjemalloc.so.1
+// LD_PRELOAD="$LD_PRELOAD:"${UNWIND_PATH}/lib/libunwind.so.7
+TEST(small_vector, leak_test) {
+  for (int j = 0; j < 1000; ++j) {
+    folly::small_vector<int, 10> someVec(300);
+    for (int i = 0; i < 10000; ++i) {
+      someVec.push_back(12);
+    }
+  }
+}
+
+TEST(small_vector, Insert) {
+  folly::small_vector<int> someVec(3, 3);
+  someVec.insert(someVec.begin(), 12, 12);
+  EXPECT_EQ(someVec.size(), 15);
+  for (int i = 0; i < someVec.size(); ++i) {
+    if (i < 12) {
+      EXPECT_EQ(someVec[i], 12);
+    } else {
+      EXPECT_EQ(someVec[i], 3);
+    }
+  }
+
+  auto oldSize = someVec.size();
+  someVec.insert(someVec.begin() + 1, 12, 12);
+  EXPECT_EQ(someVec.size(), oldSize + 12);
+
+  folly::small_vector<std::string> v1(6, "asd"), v2(7, "wat");
+  v1.insert(v1.begin() + 1, v2.begin(), v2.end());
+  EXPECT_TRUE(v1.size() == 6 + 7);
+  EXPECT_EQ(v1.front(), "asd");
+  EXPECT_EQ(v1[1], "wat");
+}
+
+TEST(small_vector, Swap) {
+  folly::small_vector<int,10> somethingVec, emptyVec;
+  somethingVec.push_back(1);
+  somethingVec.push_back(2);
+  somethingVec.push_back(3);
+  somethingVec.push_back(4);
+
+  // Swapping intern'd with intern'd.
+  auto vec = somethingVec;
+  EXPECT_TRUE(vec == somethingVec);
+  EXPECT_FALSE(vec == emptyVec);
+  EXPECT_FALSE(somethingVec == emptyVec);
+
+  // Swapping a heap vector with an intern vector.
+  folly::small_vector<int,10> junkVec;
+  junkVec.assign(12, 12);
+  EXPECT_EQ(junkVec.size(), 12);
+  for (auto i : junkVec) {
+    EXPECT_EQ(i, 12);
+  }
+  swap(junkVec, vec);
+  EXPECT_TRUE(junkVec == somethingVec);
+  EXPECT_EQ(vec.size(), 12);
+  for (auto i : vec) {
+    EXPECT_EQ(i, 12);
+  }
+
+  // Swapping two heap vectors.
+  folly::small_vector<int,10> moreJunk(15, 15);
+  EXPECT_EQ(moreJunk.size(), 15);
+  for (auto i : moreJunk) {
+    EXPECT_EQ(i, 15);
+  }
+  swap(vec, moreJunk);
+  EXPECT_EQ(moreJunk.size(), 12);
+  for (auto i : moreJunk) {
+    EXPECT_EQ(i, 12);
+  }
+  EXPECT_EQ(vec.size(), 15);
+  for (auto i : vec) {
+    EXPECT_EQ(i, 15);
+  }
+
+  // Making a vector heap, then smaller than another non-heap vector,
+  // then swapping.
+  folly::small_vector<int,5> shrinker, other(4, 10);
+  shrinker = { 0, 1, 2, 3, 4, 5, 6, 7, 8 };
+  shrinker.erase(shrinker.begin() + 2, shrinker.end());
+  EXPECT_LT(shrinker.size(), other.size());
+  swap(shrinker, other);
+  EXPECT_EQ(shrinker.size(), 4);
+  EXPECT_TRUE(boost::all(shrinker, boost::is_any_of(std::vector<int>{10})));
+  EXPECT_TRUE((other == small_vector<int,5>{ 0, 1 }));
+}
+
+TEST(small_vector, Emplace) {
+  NontrivialType::ctored = 0;
+
+  folly::small_vector<NontrivialType> vec;
+  vec.reserve(1024);
+  vec.emplace_back(12);
+  EXPECT_EQ(NontrivialType::ctored, 1);
+  EXPECT_EQ(vec.front().a, 12);
+  vec.emplace_back(13);
+  EXPECT_EQ(vec.front().a, 12);
+  EXPECT_EQ(vec.back().a, 13);
+  EXPECT_EQ(NontrivialType::ctored, 2);
+
+  NontrivialType::ctored = 0;
+  for (int i = 0; i < 120; ++i) {
+    vec.emplace_back(i);
+  }
+  EXPECT_EQ(NontrivialType::ctored, 120);
+  EXPECT_EQ(vec[0].a, 12);
+  EXPECT_EQ(vec[1].a, 13);
+  EXPECT_EQ(vec.back().a, 119);
+
+  // We implement emplace() with a temporary (see the implementation
+  // for a comment about why), so this should make 2 ctor calls.
+  NontrivialType::ctored = 0;
+  vec.emplace(vec.begin(), 12);
+  EXPECT_EQ(NontrivialType::ctored, 2);
+}
+
+TEST(small_vector, Erase) {
+  folly::small_vector<int,4> notherVec = { 1, 2, 3, 4, 5 };
+  EXPECT_EQ(notherVec.front(), 1);
+  EXPECT_EQ(notherVec.size(), 5);
+  notherVec.erase(notherVec.begin());
+  EXPECT_EQ(notherVec.front(), 2);
+  EXPECT_EQ(notherVec.size(), 4);
+  EXPECT_EQ(notherVec[2], 4);
+  EXPECT_EQ(notherVec[3], 5);
+  notherVec.erase(notherVec.begin() + 2);
+  EXPECT_EQ(notherVec.size(), 3);
+  EXPECT_EQ(notherVec[2], 5);
+
+  folly::small_vector<int,2> vec2 = { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 };
+  vec2.erase(vec2.begin() + 1, vec2.end() - 1);
+  folly::small_vector<int,2> expected = { 1, 10 };
+  EXPECT_TRUE(vec2 == expected);
+
+  folly::small_vector<std::string,3> v(102, "ASD");
+  v.resize(1024, "D");
+  EXPECT_EQ(v.size(), 1024);
+  EXPECT_EQ(v.back(), "D");
+  EXPECT_EQ(v.front(), "ASD");
+  v.resize(1);
+  EXPECT_EQ(v.front(), "ASD");
+  EXPECT_EQ(v.size(), 1);
+  v.resize(0);
+  EXPECT_TRUE(v.empty());
+}
+
+TEST(small_vector, GrowShrinkGrow) {
+  folly::small_vector<NontrivialType,7> vec = { 1, 2, 3, 4, 5 };
+  std::generate_n(std::back_inserter(vec), 102, std::rand);
+
+  auto capacity = vec.capacity();
+
+  auto oldSize = vec.size();
+  for (int i = 0; i < oldSize; ++i) {
+    vec.erase(vec.begin() + (std::rand() % vec.size()));
+    EXPECT_EQ(vec.capacity(), capacity);
+  }
+  EXPECT_TRUE(vec.empty());
+
+  EXPECT_EQ(vec.capacity(), capacity);
+  std::generate_n(std::back_inserter(vec), 102, std::rand);
+  EXPECT_EQ(vec.capacity(), capacity);
+
+  std::generate_n(std::back_inserter(vec), 4096, std::rand);
+  EXPECT_GT(vec.capacity(), capacity);
+
+  vec.resize(10);
+  vec.shrink_to_fit();
+  EXPECT_LT(vec.capacity(), capacity);
+  vec.resize(4);
+  vec.shrink_to_fit();
+  EXPECT_EQ(vec.capacity(), 7); // in situ size
+}
+
+TEST(small_vector, Iteration) {
+  folly::small_vector<std::string,3> vec = { "foo", "bar" };
+  vec.push_back("blah");
+  vec.push_back("blah2");
+  vec.push_back("blah3");
+  vec.erase(vec.begin() + 2);
+
+  std::vector<std::string> otherVec;
+  for (auto& s : vec) {
+    otherVec.push_back(s);
+  }
+  EXPECT_EQ(otherVec.size(), vec.size());
+  if (otherVec.size() == vec.size()) {
+    EXPECT_TRUE(std::equal(otherVec.begin(), otherVec.end(), vec.begin()));
+  }
+
+  std::reverse(otherVec.begin(), otherVec.end());
+  auto oit = otherVec.begin();
+  auto rit = vec.crbegin();
+  for (; rit != vec.crend(); ++oit, ++rit) {
+    EXPECT_EQ(*oit, *rit);
+  }
+}
+
+TEST(small_vector, NonCopyableType) {
+  folly::small_vector<NontrivialType,2> vec;
+
+  for (int i = 0; i < 10; ++i) {
+    vec.emplace(vec.begin(), 13);
+  }
+  EXPECT_EQ(vec.size(), 10);
+  auto vec2 = std::move(vec);
+  EXPECT_EQ(vec.size(), 0);
+  EXPECT_EQ(vec2.size(), 10);
+  vec2.clear();
+
+  folly::small_vector<NoncopyableCounter,3> vec3;
+  for (int i = 0; i < 10; ++i) {
+    EXPECT_EQ(vec3.size(), i);
+    EXPECT_EQ(NoncopyableCounter::alive, i);
+    vec3.insert(vec3.begin(), NoncopyableCounter());
+  }
+  EXPECT_EQ(vec3.size(), 10);
+  EXPECT_EQ(NoncopyableCounter::alive, 10);
+
+  vec3.insert(vec3.begin() + 3, NoncopyableCounter());
+  EXPECT_EQ(NoncopyableCounter::alive, 11);
+  auto vec4 = std::move(vec3);
+  EXPECT_EQ(NoncopyableCounter::alive, 11);
+  vec4.resize(30);
+  EXPECT_EQ(NoncopyableCounter::alive, 30);
+  vec4.erase(vec4.begin(), vec4.end());
+  EXPECT_EQ(vec4.size(), 0);
+  EXPECT_EQ(NoncopyableCounter::alive, 0);
+}
+
+TEST(small_vector, MoveConstructor) {
+  folly::small_vector<std::string,10> v1;
+  v1.push_back("asd");
+  v1.push_back("bsd");
+  auto v2 = std::move(v1);
+  EXPECT_EQ(v2.size(), 2);
+  EXPECT_EQ(v2[0], "asd");
+  EXPECT_EQ(v2[1], "bsd");
+
+  v1 = std::move(v2);
+  EXPECT_EQ(v1.size(), 2);
+  EXPECT_EQ(v1[0], "asd");
+  EXPECT_EQ(v1[1], "bsd");
+}
+
+TEST(small_vector, NoHeap) {
+  typedef folly::small_vector<std::string,10,
+    std::size_t,folly::small_vector_policy::NoHeap> Vector;
+
+  Vector v;
+  static_assert(v.max_size() == 10, "max_size is incorrect");
+
+  for (int i = 0; i < 10; ++i) {
+    v.push_back(folly::to<std::string>(i));
+    EXPECT_EQ(v.size(), i + 1);
+  }
+
+  bool caught = false;
+  try {
+    v.insert(v.begin(), "ha");
+  } catch (const std::length_error&) {
+    caught = true;
+  }
+  EXPECT_TRUE(caught);
+
+  // Check max_size works right with various policy combinations.
+  folly::small_vector<std::string,32,uint32_t,NoHeap,OneBitMutex> v2;
+  static_assert(v2.max_size() == 32, "max_size is incorrect");
+  folly::small_vector<std::string,32,uint32_t,OneBitMutex> v3;
+  EXPECT_EQ(v3.max_size(), (1ul << 30) - 1);
+  folly::small_vector<std::string,32,uint32_t> v4;
+  EXPECT_EQ(v4.max_size(), (1ul << 31) - 1);
+
+  /*
+   * Test that even when we ask for a small number inlined it'll still
+   * inline at least as much as it takes to store the value_type
+   * pointer.
+   */
+  folly::small_vector<char,1,NoHeap> notsosmall;
+  static_assert(notsosmall.max_size() == sizeof(char*),
+                "max_size is incorrect");
+  caught = false;
+  try {
+    notsosmall.push_back(12);
+    notsosmall.push_back(13);
+    notsosmall.push_back(14);
+  } catch (const std::length_error&) {
+    caught = true;
+  }
+  EXPECT_FALSE(caught);
+}
+
+TEST(small_vector, MaxSize) {
+  folly::small_vector<int,2,uint8_t> vec;
+  EXPECT_EQ(vec.max_size(), 127);
+  folly::small_vector<int,2,uint16_t> vec2;
+  EXPECT_EQ(vec2.max_size(), (1 << 15) - 1);
+  folly::small_vector<int,2,uint16_t,OneBitMutex> vec3;
+  EXPECT_EQ(vec3.max_size(), (1 << 14) - 1);
+}
+
+TEST(small_vector, AllHeap) {
+  // Use something bigger than the pointer so it can't get inlined.
+  struct SomeObj {
+    double a, b, c, d, e; int val;
+    SomeObj(int val) : val(val) {}
+    bool operator==(SomeObj const& o) const {
+      return o.val == val;
+    }
+  };
+
+  folly::small_vector<SomeObj,0> vec = { 1 };
+  EXPECT_EQ(vec.size(), 1);
+  if (!vec.empty()) {
+    EXPECT_TRUE(vec[0] == 1);
+  }
+  vec.insert(vec.begin(), { 0, 1, 2, 3 });
+  EXPECT_EQ(vec.size(), 5);
+  EXPECT_TRUE((vec == folly::small_vector<SomeObj,0>{ 0, 1, 2, 3, 1 }));
+}
+
+TEST(small_vector, Basic) {
+  typedef folly::small_vector<int,3,uint32_t
+#ifdef __x86_64__
+    ,OneBitMutex
+#endif
+  > Vector;
+
+  Vector a;
+
+#ifdef __x86_64__
+  a.lock();
+  a.unlock();
+#endif
+
+  a.push_back(12);
+  EXPECT_EQ(a.front(), 12);
+  EXPECT_EQ(a.size(), 1);
+  a.push_back(13);
+  EXPECT_EQ(a.size(), 2);
+  EXPECT_EQ(a.front(), 12);
+  EXPECT_EQ(a.back(), 13);
+
+  a.emplace(a.end(), 32);
+  EXPECT_EQ(a.back(), 32);
+
+  a.emplace(a.begin(), 12);
+  EXPECT_EQ(a.front(), 12);
+  EXPECT_EQ(a.back(), 32);
+  a.erase(a.end() - 1);
+  EXPECT_EQ(a.back(), 13);
+
+  a.push_back(12);
+  EXPECT_EQ(a.back(), 12);
+  a.pop_back();
+  EXPECT_EQ(a.back(), 13);
+
+  const int s = 12;
+  a.push_back(s); // lvalue reference
+
+  Vector b, c;
+  b = a;
+  EXPECT_TRUE(b == a);
+  c = std::move(b);
+  EXPECT_TRUE(c == a);
+  EXPECT_TRUE(c != b && b != a);
+
+  EXPECT_GT(c.size(), 0);
+  c.resize(1);
+  EXPECT_EQ(c.size(), 1);
+
+  Vector intCtor(12);
+}
+
+TEST(small_vector, Capacity) {
+  folly::small_vector<unsigned long, 1> vec;
+  EXPECT_EQ(vec.size(), 0);
+  EXPECT_EQ(vec.capacity(), 1);
+
+  vec.push_back(0);
+  EXPECT_EQ(vec.size(), 1);
+  EXPECT_EQ(vec.capacity(), 1);
+
+  vec.push_back(1);
+  EXPECT_EQ(vec.size(), 2);
+  EXPECT_GT(vec.capacity(), 1);
+
+
+  folly::small_vector<unsigned long, 2> vec2;
+  EXPECT_EQ(vec2.size(), 0);
+  EXPECT_EQ(vec2.capacity(), 2);
+
+  vec2.push_back(0);
+  vec2.push_back(1);
+  EXPECT_EQ(vec2.size(), 2);
+  EXPECT_EQ(vec2.capacity(), 2);
+
+  vec2.push_back(2);
+  EXPECT_EQ(vec2.size(), 3);
+  EXPECT_GT(vec2.capacity(), 2);
+
+  // Test capacity heapifying logic
+  folly::small_vector<unsigned char, 1> vec3;
+  const size_t hc_size = 1000000;
+  for (size_t i = 0; i < hc_size; ++i) {
+    auto v = (unsigned char)i;
+    vec3.push_back(v);
+    EXPECT_EQ(vec3[i], v);
+    EXPECT_EQ(vec3.size(), i + 1);
+    EXPECT_GT(vec3.capacity(), i);
+  }
+  for (auto i = hc_size; i > 0; --i) {
+    auto v = (unsigned char)(i - 1);
+    EXPECT_EQ(vec3.back(), v);
+    vec3.pop_back();
+    EXPECT_EQ(vec3.size(), i - 1);
+  }
+}
+
+TEST(small_vector, SelfPushBack) {
+  for (int i = 1; i < 33; ++i) {
+    folly::small_vector<std::string> vec;
+    for (int j = 0; j < i; ++j) {
+      vec.push_back("abc");
+    }
+    EXPECT_EQ(vec.size(), i);
+    vec.push_back(std::move(vec[0]));
+    EXPECT_EQ(vec.size(), i + 1);
+
+    EXPECT_EQ(vec[i], "abc");
+  }
+}
+
+TEST(small_vector, SelfEmplaceBack) {
+  for (int i = 1; i < 33; ++i) {
+    folly::small_vector<std::string> vec;
+    for (int j = 0; j < i; ++j) {
+      vec.emplace_back("abc");
+    }
+    EXPECT_EQ(vec.size(), i);
+    vec.emplace_back(std::move(vec[0]));
+    EXPECT_EQ(vec.size(), i + 1);
+
+    EXPECT_EQ(vec[i], "abc");
+  }
+}
+
+TEST(small_vector, SelfInsert) {
+  // end insert
+  for (int i = 1; i < 33; ++i) {
+    folly::small_vector<std::string> vec;
+    for (int j = 0; j < i; ++j) {
+      vec.push_back("abc");
+    }
+    EXPECT_EQ(vec.size(), i);
+    vec.insert(vec.end(), std::move(vec[0]));
+    EXPECT_EQ(vec.size(), i + 1);
+
+    EXPECT_EQ(vec[i], "abc");
+    EXPECT_EQ(vec[vec.size() - 1], "abc");
+  }
+
+  // middle insert
+  for (int i = 2; i < 33; ++i) {
+    folly::small_vector<std::string> vec;
+    for (int j = 0; j < i; ++j) {
+      vec.push_back("abc");
+    }
+    EXPECT_EQ(vec.size(), i);
+    vec.insert(vec.end()-1, std::move(vec[0]));
+    EXPECT_EQ(vec.size(), i + 1);
+
+    EXPECT_EQ(vec[i-1], "abc");
+    EXPECT_EQ(vec[i], "abc");
+  }
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/sorted_vector_test.cpp
@@ -0,0 +1,321 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/sorted_vector_types.h"
+#include <gtest/gtest.h>
+#include <list>
+
+using folly::sorted_vector_set;
+using folly::sorted_vector_map;
+
+namespace {
+
+template<class T>
+struct less_invert : std::binary_function<T,T,bool> {
+  bool operator()(const T& a, const T& b) const {
+    return b < a;
+  }
+};
+
+template<class Container>
+void check_invariant(Container& c) {
+  auto it = c.begin();
+  auto end = c.end();
+  if (it == end)
+    return;
+  auto prev = it;
+  ++it;
+  for (; it != end; ++it, ++prev) {
+    EXPECT_TRUE(c.value_comp()(*prev, *it));
+  }
+}
+
+struct OneAtATimePolicy {
+  template<class Container>
+  void increase_capacity(Container& c) {
+    if (c.size() == c.capacity()) {
+      c.reserve(c.size() + 1);
+    }
+  }
+};
+
+struct CountCopyCtor {
+  explicit CountCopyCtor() : val_(0) {}
+
+  explicit CountCopyCtor(int val) : val_(val), count_(0) {}
+
+  CountCopyCtor(const CountCopyCtor& c)
+    : val_(c.val_)
+    , count_(c.count_ + 1)
+  {}
+
+  bool operator<(const CountCopyCtor& o) const {
+    return val_ < o.val_;
+  }
+
+  int val_;
+  int count_;
+};
+
+}
+
+TEST(SortedVectorTypes, SimpleSetTest) {
+  sorted_vector_set<int> s;
+  EXPECT_TRUE(s.empty());
+  for (int i = 0; i < 1000; ++i) {
+    s.insert(rand() % 100000);
+  }
+  EXPECT_FALSE(s.empty());
+  check_invariant(s);
+
+  sorted_vector_set<int> s2;
+  s2.insert(s.begin(), s.end());
+  check_invariant(s2);
+  EXPECT_TRUE(s == s2);
+
+  auto it = s2.lower_bound(32);
+  if (*it == 32) {
+    s2.erase(it);
+    it = s2.lower_bound(32);
+  }
+  check_invariant(s2);
+  auto oldSz = s2.size();
+  s2.insert(it, 32);
+  EXPECT_TRUE(s2.size() == oldSz + 1);
+  check_invariant(s2);
+
+  const sorted_vector_set<int>& cs2 = s2;
+  auto range = cs2.equal_range(32);
+  auto lbound = cs2.lower_bound(32);
+  auto ubound = cs2.upper_bound(32);
+  EXPECT_TRUE(range.first == lbound);
+  EXPECT_TRUE(range.second == ubound);
+  EXPECT_TRUE(range.first != cs2.end());
+  EXPECT_TRUE(range.second != cs2.end());
+  EXPECT_TRUE(cs2.count(32) == 1);
+  EXPECT_FALSE(cs2.find(32) == cs2.end());
+
+  // Bad insert hint.
+  s2.insert(s2.begin() + 3, 33);
+  EXPECT_TRUE(s2.find(33) != s2.begin());
+  EXPECT_TRUE(s2.find(33) != s2.end());
+  check_invariant(s2);
+  s2.erase(33);
+  check_invariant(s2);
+
+  it = s2.find(32);
+  EXPECT_FALSE(it == s2.end());
+  s2.erase(it);
+  EXPECT_TRUE(s2.size() == oldSz);
+  check_invariant(s2);
+
+  sorted_vector_set<int> cpy(s);
+  check_invariant(cpy);
+  EXPECT_TRUE(cpy == s);
+  sorted_vector_set<int> cpy2(s);
+  cpy2.insert(100001);
+  EXPECT_TRUE(cpy2 != cpy);
+  EXPECT_TRUE(cpy2 != s);
+  check_invariant(cpy2);
+  EXPECT_TRUE(cpy2.count(100001) == 1);
+  s.swap(cpy2);
+  check_invariant(cpy2);
+  check_invariant(s);
+  EXPECT_TRUE(s != cpy);
+  EXPECT_TRUE(s != cpy2);
+  EXPECT_TRUE(cpy2 == cpy);
+}
+
+TEST(SortedVectorTypes, SimpleMapTest) {
+  sorted_vector_map<int,float> m;
+  for (int i = 0; i < 1000; ++i) {
+    m[i] = i / 1000.0;
+  }
+  check_invariant(m);
+
+  m[32] = 100.0;
+  check_invariant(m);
+  EXPECT_TRUE(m.count(32) == 1);
+  EXPECT_FALSE(m.find(32) == m.end());
+  m.erase(32);
+  EXPECT_TRUE(m.find(32) == m.end());
+  check_invariant(m);
+
+  sorted_vector_map<int,float> m2 = m;
+  EXPECT_TRUE(m2 == m);
+  EXPECT_FALSE(m2 != m);
+  auto it = m2.lower_bound(1 << 20);
+  EXPECT_TRUE(it == m2.end());
+  m2.insert(it, std::make_pair(1 << 20, 10.0f));
+  check_invariant(m2);
+  EXPECT_TRUE(m2.count(1 << 20) == 1);
+  EXPECT_TRUE(m < m2);
+  EXPECT_TRUE(m <= m2);
+
+  const sorted_vector_map<int,float>& cm = m;
+  auto range = cm.equal_range(42);
+  auto lbound = cm.lower_bound(42);
+  auto ubound = cm.upper_bound(42);
+  EXPECT_TRUE(range.first == lbound);
+  EXPECT_TRUE(range.second == ubound);
+  EXPECT_FALSE(range.first == cm.end());
+  EXPECT_FALSE(range.second == cm.end());
+  m.erase(m.lower_bound(42));
+  check_invariant(m);
+
+  sorted_vector_map<int,float> m3;
+  m3.insert(m2.begin(), m2.end());
+  check_invariant(m3);
+  EXPECT_TRUE(m3 == m2);
+  EXPECT_FALSE(m3 == m);
+
+  EXPECT_TRUE(m != m2);
+  EXPECT_TRUE(m2 == m3);
+  EXPECT_TRUE(m3 != m);
+  m.swap(m3);
+  check_invariant(m);
+  check_invariant(m2);
+  check_invariant(m3);
+  EXPECT_TRUE(m3 != m2);
+  EXPECT_TRUE(m3 != m);
+  EXPECT_TRUE(m == m2);
+
+  // Bad insert hint.
+  m.insert(m.begin() + 3, std::make_pair(1 << 15, 1.0f));
+  check_invariant(m);
+}
+
+TEST(SortedVectorTypes, Sizes) {
+  EXPECT_EQ(sizeof(sorted_vector_set<int>),
+            sizeof(std::vector<int>));
+  EXPECT_EQ(sizeof(sorted_vector_map<int,int>),
+            sizeof(std::vector<std::pair<int,int> >));
+
+  typedef sorted_vector_set<int,std::less<int>,
+    std::allocator<int>,OneAtATimePolicy> SetT;
+  typedef sorted_vector_map<int,int,std::less<int>,
+    std::allocator<int>,OneAtATimePolicy> MapT;
+
+  EXPECT_EQ(sizeof(SetT), sizeof(std::vector<int>));
+  EXPECT_EQ(sizeof(MapT), sizeof(std::vector<std::pair<int,int> >));
+}
+
+TEST(SortedVectorTypes, InitializerLists) {
+  sorted_vector_set<int> empty_initialized_set{};
+  EXPECT_TRUE(empty_initialized_set.empty());
+
+  sorted_vector_set<int> singleton_initialized_set{1};
+  EXPECT_EQ(1, singleton_initialized_set.size());
+  EXPECT_EQ(1, *singleton_initialized_set.begin());
+
+  sorted_vector_set<int> forward_initialized_set{1, 2};
+  sorted_vector_set<int> backward_initialized_set{2, 1};
+  EXPECT_EQ(2, forward_initialized_set.size());
+  EXPECT_EQ(1, *forward_initialized_set.begin());
+  EXPECT_EQ(2, *forward_initialized_set.rbegin());
+  EXPECT_TRUE(forward_initialized_set == backward_initialized_set);
+
+  sorted_vector_map<int,int> empty_initialized_map{};
+  EXPECT_TRUE(empty_initialized_map.empty());
+
+  sorted_vector_map<int,int> singleton_initialized_map{{1,10}};
+  EXPECT_EQ(1, singleton_initialized_map.size());
+  EXPECT_EQ(10, singleton_initialized_map[1]);
+
+  sorted_vector_map<int,int> forward_initialized_map{{1,10}, {2,20}};
+  sorted_vector_map<int,int> backward_initialized_map{{2,20}, {1,10}};
+  EXPECT_EQ(2, forward_initialized_map.size());
+  EXPECT_EQ(10, forward_initialized_map[1]);
+  EXPECT_EQ(20, forward_initialized_map[2]);
+  EXPECT_TRUE(forward_initialized_map == backward_initialized_map);
+}
+
+TEST(SortedVectorTypes, CustomCompare) {
+  sorted_vector_set<int,less_invert<int> > s;
+  for (int i = 0; i < 200; ++i)
+    s.insert(i);
+  check_invariant(s);
+
+  sorted_vector_map<int,float,less_invert<int> > m;
+  for (int i = 0; i < 200; ++i)
+    m[i] = 12.0;
+  check_invariant(m);
+}
+
+TEST(SortedVectorTypes, GrowthPolicy) {
+  typedef sorted_vector_set<CountCopyCtor,
+                            std::less<CountCopyCtor>,
+                            std::allocator<CountCopyCtor>,
+                            OneAtATimePolicy>
+    SetT;
+
+  SetT a;
+  for (int i = 0; i < 20; ++i) {
+    a.insert(CountCopyCtor(i));
+  }
+  check_invariant(a);
+  SetT::iterator it = a.begin();
+  EXPECT_FALSE(it == a.end());
+  if (it != a.end()) {
+    EXPECT_EQ(it->val_, 0);
+    // 1 copy for the initial insertion, 19 more for reallocs on the
+    // additional insertions.
+    EXPECT_EQ(it->count_, 20);
+  }
+
+  std::list<CountCopyCtor> v;
+  for (int i = 0; i < 20; ++i) {
+    v.push_back(CountCopyCtor(20 + i));
+  }
+  a.insert(v.begin(), v.end());
+  check_invariant(a);
+
+  it = a.begin();
+  EXPECT_FALSE(it == a.end());
+  if (it != a.end()) {
+    EXPECT_EQ(it->val_, 0);
+    // Should be only 1 more copy for inserting this above range.
+    EXPECT_EQ(it->count_, 21);
+  }
+}
+
+TEST(SortedVectorTest, EmptyTest) {
+  sorted_vector_set<int> emptySet;
+  EXPECT_TRUE(emptySet.lower_bound(10) == emptySet.end());
+  EXPECT_TRUE(emptySet.find(10) == emptySet.end());
+
+  sorted_vector_map<int,int> emptyMap;
+  EXPECT_TRUE(emptyMap.lower_bound(10) == emptyMap.end());
+  EXPECT_TRUE(emptyMap.find(10) == emptyMap.end());
+}
+
+TEST(SortedVectorTest, MoveTest) {
+  sorted_vector_set<std::unique_ptr<int>> s;
+  s.insert(std::unique_ptr<int>(new int(5)));
+  s.insert(s.end(), std::unique_ptr<int>(new int(10)));
+  EXPECT_EQ(s.size(), 2);
+
+  for (const auto& p : s) {
+    EXPECT_TRUE(*p == 5 || *p == 10);
+  }
+
+  sorted_vector_map<int, std::unique_ptr<int>> m;
+  m.insert(std::make_pair(5, std::unique_ptr<int>(new int(5))));
+  m.insert(m.end(), std::make_pair(10, std::unique_ptr<int>(new int(10))));
+
+  EXPECT_EQ(*m[5], 5);
+  EXPECT_EQ(*m[10], 10);
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/SpookyHashV1Test.cpp
@@ -0,0 +1,544 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+// SpookyHash: a 128-bit noncryptographic hash function
+// By Bob Jenkins, public domain
+
+#ifndef __STDC_FORMAT_MACROS
+#define __STDC_FORMAT_MACROS 1
+#endif
+
+#include "folly/SpookyHashV1.h"
+#include "folly/Benchmark.h"
+
+#include <cinttypes>
+#include <cstdio>
+#include <cstddef>
+#include <cstring>
+#include <cstdlib>
+#include <ctime>
+
+using namespace ::folly::hash;
+
+static bool failed = false;
+
+static uint64_t GetTickCount() {
+  timespec ts;
+  clock_gettime(CLOCK_REALTIME, &ts);
+  return ts.tv_sec * 1000 + ts.tv_nsec / 1000000;  // milliseconds
+}
+
+class Random
+{
+public:
+    inline uint64_t Value()
+    {
+        uint64_t e = m_a - Rot64(m_b, 23);
+        m_a = m_b ^ Rot64(m_c, 16);
+        m_b = m_c + Rot64(m_d, 11);
+        m_c = m_d + e;
+        m_d = e + m_a;
+        return m_d;
+    }
+
+    inline void Init( uint64_t seed)
+    {
+        m_a = 0xdeadbeef;
+        m_b = m_c = m_d = seed;
+        for (int i=0; i<20; ++i)
+            (void)Value();
+    }
+
+private:
+    static inline uint64_t Rot64(uint64_t x, int k)
+    {
+        return (x << k) | (x >> (64-(k)));
+    }
+
+    uint64_t m_a;
+    uint64_t m_b;
+    uint64_t m_c;
+    uint64_t m_d;
+};
+
+// fastest conceivable hash function (for comparison)
+static void Add(const void *data, size_t length, uint64_t *hash1, uint64_t *hash2)
+{
+    uint64_t *p64 = (uint64_t *)data;
+    uint64_t *end = p64 + length/8;
+    uint64_t hash = *hash1 + *hash2;
+    while (p64 < end)
+    {
+      hash += *p64;
+      ++p64;
+    }
+    *hash1 = hash;
+    *hash2 = hash;
+}
+
+#define BUFSIZE (512)
+void TestResults()
+{
+    printf("\ntesting results ...\n");
+    static const uint32_t expected[BUFSIZE] = {
+        0xa24295ec, 0xfe3a05ce, 0x257fd8ef, 0x3acd5217,
+        0xfdccf85c, 0xc7b5f143, 0x3b0c3ff0, 0x5220f13c,
+        0xa6426724, 0x4d5426b4, 0x43e76b26, 0x051bc437,
+        0xd8f28a02, 0x23ccc30e, 0x811d1a2d, 0x039128d4,
+        0x9cd96a73, 0x216e6a8d, 0x97293fe8, 0xe4fc6d09,
+        0x1ad34423, 0x9722d7e4, 0x5a6fdeca, 0x3c94a7e1,
+        0x81a9a876, 0xae3f7c0e, 0x624b50ee, 0x875e5771,
+        0x0095ab74, 0x1a7333fb, 0x056a4221, 0xa38351fa,
+
+        0x73f575f1, 0x8fded05b, 0x9097138f, 0xbd74620c,
+        0x62d3f5f2, 0x07b78bd0, 0xbafdd81e, 0x0638f2ff,
+        0x1f6e3aeb, 0xa7786473, 0x71700e1d, 0x6b4625ab,
+        0xf02867e1, 0xb2b2408f, 0x9ce21ce5, 0xa62baaaf,
+        0x26720461, 0x434813ee, 0x33bc0f14, 0xaaab098a,
+        0x750af488, 0xc31bf476, 0x9cecbf26, 0x94793cf3,
+        0xe1a27584, 0xe80c4880, 0x1299f748, 0x25e55ed2,
+        0x405e3feb, 0x109e2412, 0x3e55f94f, 0x59575864,
+
+        0x365c869d, 0xc9852e6a, 0x12c30c62, 0x47f5b286,
+        0xb47e488d, 0xa6667571, 0x78220d67, 0xa49e30b9,
+        0x2005ef88, 0xf6d3816d, 0x6926834b, 0xe6116805,
+        0x694777aa, 0x464af25b, 0x0e0e2d27, 0x0ea92eae,
+        0x602c2ca9, 0x1d1d79c5, 0x6364f280, 0x939ee1a4,
+        0x3b851bd8, 0x5bb6f19f, 0x80b9ed54, 0x3496a9f1,
+        0xdf815033, 0x91612339, 0x14c516d6, 0xa3f0a804,
+        0x5e78e975, 0xf408bcd9, 0x63d525ed, 0xa1e459c3,
+
+        0xfde303af, 0x049fc17f, 0xe7ed4489, 0xfaeefdb6,
+        0x2b1b2fa8, 0xc67579a6, 0x5505882e, 0xe3e1c7cb,
+        0xed53bf30, 0x9e628351, 0x8fa12113, 0x7500c30f,
+        0xde1bee00, 0xf1fefe06, 0xdc759c00, 0x4c75e5ab,
+        0xf889b069, 0x695bf8ae, 0x47d6600f, 0xd2a84f87,
+        0xa0ca82a9, 0x8d2b750c, 0xe03d8cd7, 0x581fea33,
+        0x969b0460, 0x36c7b7de, 0x74b3fd20, 0x2bb8bde6,
+        0x13b20dec, 0xa2dcee89, 0xca36229d, 0x06fdb74e,
+
+
+        0x6d9a982d, 0x02503496, 0xbdb4e0d9, 0xbd1f94cf,
+        0x6d26f82d, 0xcf5e41cd, 0x88b67b65, 0x3e1b3ee4,
+        0xb20e5e53, 0x1d9be438, 0xcef9c692, 0x299bd1b2,
+        0xb1279627, 0x210b5f3d, 0x5569bd88, 0x9652ed43,
+        0x7e8e0f8c, 0xdfa01085, 0xcd6d6343, 0xb8739826,
+        0xa52ce9a0, 0xd33ef231, 0x1b4d92c2, 0xabfa116d,
+        0xcdf47800, 0x3a4eefdc, 0xd01f3bcf, 0x30a32f46,
+        0xfb54d851, 0x06a98f67, 0xbdcd0a71, 0x21a00949,
+
+        0xfe7049c9, 0x67ef46d2, 0xa1fabcbc, 0xa4c72db4,
+        0x4a8a910d, 0x85a890ad, 0xc37e9454, 0xfc3d034a,
+        0x6f46cc52, 0x742be7a8, 0xe94ecbc5, 0x5f993659,
+        0x98270309, 0x8d1adae9, 0xea6e035e, 0x293d5fae,
+        0x669955b3, 0x5afe23b5, 0x4c74efbf, 0x98106505,
+        0xfbe09627, 0x3c00e8df, 0x5b03975d, 0x78edc83c,
+        0x117c49c6, 0x66cdfc73, 0xfa55c94f, 0x5bf285fe,
+        0x2db49b7d, 0xfbfeb8f0, 0xb7631bab, 0x837849f3,
+
+        0xf77f3ae5, 0x6e5db9bc, 0xfdd76f15, 0x545abf92,
+        0x8b538102, 0xdd5c9b65, 0xa5adfd55, 0xecbd7bc5,
+        0x9f99ebdd, 0x67500dcb, 0xf5246d1f, 0x2b0c061c,
+        0x927a3747, 0xc77ba267, 0x6da9f855, 0x6240d41a,
+        0xe9d1701d, 0xc69f0c55, 0x2c2c37cf, 0x12d82191,
+        0x47be40d3, 0x165b35cd, 0xb7db42e1, 0x358786e4,
+        0x84b8fc4e, 0x92f57c28, 0xf9c8bbd7, 0xab95a33d,
+        0x11009238, 0xe9770420, 0xd6967e2a, 0x97c1589f,
+
+        0x2ee7e7d3, 0x32cc86da, 0xe47767d1, 0x73e9b61e,
+        0xd35bac45, 0x835a62bb, 0x5d9217b0, 0x43f3f0ed,
+        0x8a97911e, 0x4ec7eb55, 0x4b5a988c, 0xb9056683,
+        0x45456f97, 0x1669fe44, 0xafb861b8, 0x8e83a19c,
+        0x0bab08d6, 0xe6a145a9, 0xc31e5fc2, 0x27621f4c,
+        0x795692fa, 0xb5e33ab9, 0x1bc786b6, 0x45d1c106,
+        0x986531c9, 0x40c9a0ec, 0xff0fdf84, 0xa7359a42,
+        0xfd1c2091, 0xf73463d4, 0x51b0d635, 0x1d602fb4,
+
+
+        0xc56b69b7, 0x6909d3f7, 0xa04d68f4, 0x8d1001a7,
+        0x8ecace50, 0x21ec4765, 0x3530f6b0, 0x645f3644,
+        0x9963ef1e, 0x2b3c70d5, 0xa20c823b, 0x8d26dcae,
+        0x05214e0c, 0x1993896d, 0x62085a35, 0x7b620b67,
+        0x1dd85da2, 0x09ce9b1d, 0xd7873326, 0x063ff730,
+        0xf4ff3c14, 0x09a49d69, 0x532062ba, 0x03ba7729,
+        0xbd9a86cc, 0xe26d02a7, 0x7ccbe5d3, 0x4f662214,
+        0x8b999a66, 0x3d0b92b4, 0x70b210f0, 0xf5b8f16f,
+
+        0x32146d34, 0x430b92bf, 0x8ab6204c, 0x35e6e1ff,
+        0xc2f6c2fa, 0xa2df8a1a, 0x887413ec, 0x7cb7a69f,
+        0x7ac6dbe6, 0x9102d1cb, 0x8892a590, 0xc804fe3a,
+        0xdfc4920a, 0xfc829840, 0x8910d2eb, 0x38a210fd,
+        0x9d840cc9, 0x7b9c827f, 0x3444ca0c, 0x071735ab,
+        0x5e9088e4, 0xc995d60e, 0xbe0bb942, 0x17b089ae,
+        0x050e1054, 0xcf4324f7, 0x1e3e64dd, 0x436414bb,
+        0xc48fc2e3, 0x6b6b83d4, 0x9f6558ac, 0x781b22c5,
+
+        0x7147cfe2, 0x3c221b4d, 0xa5602765, 0x8f01a4f0,
+        0x2a9f14ae, 0x12158cb8, 0x28177c50, 0x1091a165,
+        0x39e4e4be, 0x3e451b7a, 0xd965419c, 0x52053005,
+        0x0798aa53, 0xe6773e13, 0x1207f671, 0xd2ef998b,
+        0xab88a38f, 0xc77a8482, 0xa88fb031, 0x5199e0cd,
+        0x01b30536, 0x46eeb0ef, 0x814259ff, 0x9789a8cf,
+        0x376ec5ac, 0x7087034a, 0x948b6bdd, 0x4281e628,
+        0x2c848370, 0xd76ce66a, 0xe9b6959e, 0x24321a8e,
+
+        0xdeddd622, 0xb890f960, 0xea26c00a, 0x55e7d8b2,
+        0xeab67f09, 0x9227fb08, 0xeebbed06, 0xcac1b0d1,
+        0xb6412083, 0x05d2b0e7, 0x9037624a, 0xc9702198,
+        0x2c8d1a86, 0x3e7d416e, 0xc3f1a39f, 0xf04bdce4,
+        0xc88cdb61, 0xbdc89587, 0x4d29b63b, 0x6f24c267,
+        0x4b529c87, 0x573f5a53, 0xdb3316e9, 0x288eb53b,
+        0xd2c074bd, 0xef44a99a, 0x2b404d2d, 0xf6706464,
+        0xfe824f4c, 0xc3debaf8, 0x12f44f98, 0x03135e76,
+
+
+        0xb4888e7f, 0xb6b2325d, 0x3a138259, 0x513c83ec,
+        0x2386d214, 0x94555500, 0xfbd1522d, 0xda2af018,
+        0x15b054c0, 0x5ad654e6, 0xb6ed00aa, 0xa2f2180e,
+        0x5f662825, 0xecd11366, 0x1de5e99d, 0x07afd2ad,
+        0xcf457b04, 0xe631e10b, 0x83ae8a21, 0x709f0d59,
+        0x3e278bf9, 0x246816db, 0x9f5e8fd3, 0xc5b5b5a2,
+        0xd54a9d5c, 0x4b6f2856, 0x2eb5a666, 0xfc68bdd4,
+        0x1ed1a7f8, 0x98a34b75, 0xc895ada9, 0x2907cc69,
+
+        0x87b0b455, 0xddaf96d9, 0xe7da15a6, 0x9298c82a,
+        0x72bd5cab, 0x2e2a6ad4, 0x7f4b6bb8, 0x525225fe,
+        0x985abe90, 0xac1fd6e1, 0xb8340f23, 0x92985159,
+        0x7d29501d, 0xe75dc744, 0x687501b4, 0x92077dc3,
+        0x58281a67, 0xe7e8e9be, 0xd0e64fd1, 0xb2eb0a30,
+        0x0e1feccd, 0xc0dc4a9e, 0x5c4aeace, 0x2ca5b93c,
+        0xee0ec34f, 0xad78467b, 0x0830e76e, 0x0df63f8b,
+        0x2c2dfd95, 0x9b41ed31, 0x9ff4cddc, 0x1590c412,
+
+        0x2366fc82, 0x7a83294f, 0x9336c4de, 0x2343823c,
+        0x5b681096, 0xf320e4c2, 0xc22b70e2, 0xb5fbfb2a,
+        0x3ebc2fed, 0x11af07bd, 0x429a08c5, 0x42bee387,
+        0x58629e33, 0xfb63b486, 0x52135fbe, 0xf1380e60,
+        0x6355de87, 0x2f0bb19a, 0x167f63ac, 0x507224cf,
+        0xf7c99d00, 0x71646f50, 0x74feb1ca, 0x5f9abfdd,
+        0x278f7d68, 0x70120cd7, 0x4281b0f2, 0xdc8ebe5c,
+        0x36c32163, 0x2da1e884, 0x61877598, 0xbef04402,
+
+        0x304db695, 0xfa8e9add, 0x503bac31, 0x0fe04722,
+        0xf0d59f47, 0xcdc5c595, 0x918c39dd, 0x0cad8d05,
+        0x6b3ed1eb, 0x4d43e089, 0x7ab051f8, 0xdeec371f,
+        0x0f4816ae, 0xf8a1a240, 0xd15317f6, 0xb8efbf0b,
+        0xcdd05df8, 0x4fd5633e, 0x7cf19668, 0x25d8f422,
+        0x72d156f2, 0x2a778502, 0xda7aefb9, 0x4f4f66e8,
+        0x19db6bff, 0x74e468da, 0xa754f358, 0x7339ec50,
+        0x139006f6, 0xefbd0b91, 0x217e9a73, 0x939bd79c
+    };
+
+    uint8_t buf[BUFSIZE];
+    uint32_t saw[BUFSIZE];
+    for (int i=0; i<BUFSIZE; ++i)
+    {
+        buf[i] = i+128;
+        saw[i] = SpookyHashV1::Hash32(buf, i, 0);
+        if (saw[i] != expected[i])
+        {
+            printf("%d: saw 0x%.8x, expected 0x%.8x\n", i, saw[i], expected[i]);
+            failed = true;
+        }
+    }
+}
+#undef BUFSIZE
+
+
+#define NUMBUF (1<<10)
+#define BUFSIZE (1<<20)
+void DoTimingBig(int seed)
+{
+    printf("\ntesting time to hash 2^^30 bytes ...\n");
+
+    char *buf[NUMBUF];
+    for (int i=0; i<NUMBUF; ++i)
+    {
+        buf[i] = (char *)malloc(BUFSIZE);
+        memset(buf[i], (char)seed, BUFSIZE);
+    }
+
+    uint64_t a = GetTickCount();
+    uint64_t hash1 = seed;
+    uint64_t hash2 = seed;
+    for (uint64_t i=0; i<NUMBUF; ++i)
+    {
+        SpookyHashV1::Hash128(buf[i], BUFSIZE, &hash1, &hash2);
+    }
+    uint64_t z = GetTickCount();
+    printf("SpookyHashV1::Hash128, uncached: time is "
+           "%4" PRIu64 " milliseconds\n", z-a);
+
+    a = GetTickCount();
+    for (uint64_t i=0; i<NUMBUF; ++i)
+    {
+        Add(buf[i], BUFSIZE, &hash1, &hash2);
+    }
+    z = GetTickCount();
+    printf("Addition           , uncached: time is "
+           "%4" PRIu64 " milliseconds\n", z-a);
+
+    a = GetTickCount();
+    for (uint64_t i=0; i<NUMBUF*BUFSIZE/1024; ++i)
+    {
+        SpookyHashV1::Hash128(buf[0], 1024, &hash1, &hash2);
+    }
+    z = GetTickCount();
+    printf("SpookyHashV1::Hash128,   cached: time is "
+           "%4" PRIu64 " milliseconds\n", z-a);
+
+    a = GetTickCount();
+    for (uint64_t i=0; i<NUMBUF*BUFSIZE/1024; ++i)
+    {
+        Add(buf[0], 1024, &hash1, &hash2);
+    }
+    z = GetTickCount();
+    printf("Addition           ,   cached: time is "
+           "%4" PRIu64 " milliseconds\n", z-a);
+
+    for (int i=0; i<NUMBUF; ++i)
+    {
+        free(buf[i]);
+        buf[i] = 0;
+    }
+}
+#undef NUMBUF
+#undef BUFSIZE
+
+
+#define BUFSIZE (1<<14)
+#define NUMITER 10000000
+void DoTimingSmall(int seed)
+{
+    printf("\ntesting timing of hashing up to %d cached aligned bytes %d "
+           "times ...\n", BUFSIZE, NUMITER);
+
+    uint64_t buf[BUFSIZE/8];
+    for (int i=0; i<BUFSIZE/8; ++i)
+    {
+        buf[i] = i+seed;
+    }
+
+    for (int i=1; i <= BUFSIZE; i <<= 1)
+    {
+        uint64_t a = GetTickCount();
+        uint64_t hash1 = seed;
+        uint64_t hash2 = seed+i;
+        for (int j=0; j<NUMITER; ++j)
+        {
+            SpookyHashV1::Hash128((char *)buf, i, &hash1, &hash2);
+        }
+        uint64_t z = GetTickCount();
+        printf("%d bytes: hash is %.16" PRIx64 " %.16" PRIx64 ", "
+               "time is %" PRIu64 "\n", i, hash1, hash2, z-a);
+    }
+}
+#undef BUFSIZE
+
+#define BUFSIZE 1024
+void TestAlignment()
+{
+    printf("\ntesting alignment ...\n");
+
+    char buf[BUFSIZE];
+    uint64_t hash[8];
+    for (int i=0; i<BUFSIZE-16; ++i)
+    {
+        for (int j=0; j<8; ++j)
+        {
+            buf[j] = (char)i+j;
+            for (int k=1; k<=i; ++k)
+            {
+                buf[j+k] = k;
+            }
+            buf[j+i+1] = (char)i+j;
+            hash[j] = SpookyHashV1::Hash64((const void *)(buf+j+1), i, 0);
+        }
+        for (int j=1; j<8; ++j)
+        {
+            if (hash[0] != hash[j])
+            {
+                printf("alignment problems: %d %d\n", i, j);
+                failed = true;
+            }
+        }
+    }
+}
+#undef BUFSIZE
+
+// test that all deltas of one or two input bits affect all output bits
+#define BUFSIZE 256
+#define TRIES 50
+#define MEASURES 6
+void TestDeltas(int seed)
+{
+    printf("\nall 1 or 2 bit input deltas get %d tries to flip every output "
+           "bit ...\n", TRIES);
+
+    Random random;
+    random.Init((uint64_t)seed);
+
+    // for messages 0..BUFSIZE-1 bytes
+    for (int h=0; h<BUFSIZE; ++h)
+    {
+        int maxk = 0;
+        // first bit to set
+        for (int i=0; i<h*8; ++i)
+        {
+            // second bit to set, or don't have a second bit
+            for (int j=0; j<=i; ++j)
+            {
+                uint64_t measure[MEASURES][2];
+                uint64_t counter[MEASURES][2];
+                for (int l=0; l<2; ++l)
+                {
+                    for (int m=0; m<MEASURES; ++m)
+                    {
+                        counter[m][l] = 0;
+                    }
+                }
+
+                // try to hit every output bit TRIES times
+                int k;
+                for (k=0; k<TRIES; ++k)
+                {
+                    uint8_t buf1[BUFSIZE];
+                    uint8_t buf2[BUFSIZE];
+                    int done = 1;
+                    for (int l=0; l<h; ++l)
+                    {
+                        buf1[l] = buf2[l] = random.Value();
+                    }
+                    buf1[i/8] ^= (1 << (i%8));
+                    if (j != i)
+                    {
+                        buf1[j/8] ^= (1 << (j%8));
+                    }
+                    SpookyHashV1::Hash128(buf1, h, &measure[0][0], &measure[0][1]);
+                    SpookyHashV1::Hash128(buf2, h, &measure[1][0], &measure[1][1]);
+                    for (int l=0; l<2; ++l) {
+                        measure[2][l] = measure[0][l] ^ measure[1][l];
+                        measure[3][l] = ~(measure[0][l] ^ measure[1][l]);
+                        measure[4][l] = measure[0][l] - measure[1][l];
+                        measure[4][l] ^= (measure[4][l]>>1);
+                        measure[5][l] = measure[0][l] + measure[1][l];
+                        measure[5][l] ^= (measure[4][l]>>1);
+                    }
+                    for (int l=0; l<2; ++l)
+                    {
+                        for (int m=0; m<MEASURES; ++m)
+                        {
+                            counter[m][l] |= measure[m][l];
+                            if (~counter[m][l]) done = 0;
+                        }
+                    }
+                    if (done) break;
+                }
+                if (k == TRIES)
+                {
+                    printf("failed %d %d %d\n", h, i, j);
+                    failed = true;
+                }
+                else if (k > maxk)
+                {
+                    maxk = k;
+                }
+            }
+        }
+        printf("passed for buffer size %d  max %d\n", h, maxk);
+    }
+}
+#undef BUFSIZE
+#undef TRIES
+#undef MEASURES
+
+
+// test that hashing pieces has the same behavior as hashing the whole
+#define BUFSIZE 1024
+void TestPieces()
+{
+    printf("\ntesting pieces ...\n");
+    char buf[BUFSIZE];
+    for (int i=0; i<BUFSIZE; ++i)
+    {
+        buf[i] = i;
+    }
+    for (int i=0; i<BUFSIZE; ++i)
+    {
+        uint64_t a,b,c,d,seed1=1,seed2=2;
+        SpookyHashV1 state;
+
+        // all as one call
+        a = seed1;
+        b = seed2;
+        SpookyHashV1::Hash128(buf, i, &a, &b);
+
+        // all as one piece
+        c = 0xdeadbeefdeadbeef;
+        d = 0xbaceba11baceba11;
+        state.Init(seed1, seed2);
+        state.Update(buf, i);
+        state.Final(&c, &d);
+
+        if (a != c)
+        {
+            printf("wrong a %d: %.16" PRIx64 " %.16" PRIx64 "\n", i, a,c);
+            failed = true;
+        }
+        if (b != d)
+        {
+            printf("wrong b %d: %.16" PRIx64 " %.16" PRIx64 "\n", i, b,d);
+            failed = true;
+        }
+
+        // all possible two consecutive pieces
+        for (int j=0; j<i; ++j)
+        {
+            c = seed1;
+            d = seed2;
+            state.Init(c, d);
+            state.Update(&buf[0], j);
+            state.Update(&buf[j], i-j);
+            state.Final(&c, &d);
+            if (a != c)
+            {
+                printf("wrong a %d %d: %.16" PRIx64 " %.16" PRIx64 "\n",
+                       j, i, a,c);
+                failed = true;
+            }
+            if (b != d)
+            {
+                printf("wrong b %d %d: %.16" PRIx64 " %.16" PRIx64 "\n",
+                       j, i, b,d);
+                failed = true;
+            }
+        }
+    }
+}
+#undef BUFSIZE
+
+int main(int argc, const char **argv)
+{
+    TestResults();
+    TestAlignment();
+    TestPieces();
+    DoTimingBig(argc);
+    // tudorb@fb.com: Commented out slow tests
+#if 0
+    DoTimingSmall(argc);
+    TestDeltas(argc);
+#endif
+    return failed;
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/SpookyHashV2Test.cpp
@@ -0,0 +1,470 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+// SpookyHash: a 128-bit noncryptographic hash function
+// By Bob Jenkins, public domain
+
+#ifndef __STDC_FORMAT_MACROS
+#define __STDC_FORMAT_MACROS 1
+#endif
+
+#include "folly/SpookyHashV2.h"
+#include "folly/Benchmark.h"
+
+#include <cinttypes>
+#include <cstdio>
+#include <cstddef>
+#include <cstring>
+#include <cstdlib>
+#include <ctime>
+
+using namespace ::folly::hash;
+
+static bool failed = false;
+
+static uint64_t GetTickCount() {
+  timespec ts;
+  clock_gettime(CLOCK_REALTIME, &ts);
+  return ts.tv_sec * 1000 + ts.tv_nsec / 1000000;  // milliseconds
+}
+
+class Random
+{
+public:
+    inline uint64_t Value()
+    {
+        uint64_t e = m_a - Rot64(m_b, 23);
+        m_a = m_b ^ Rot64(m_c, 16);
+        m_b = m_c + Rot64(m_d, 11);
+        m_c = m_d + e;
+        m_d = e + m_a;
+        return m_d;
+    }
+
+    inline void Init( uint64_t seed)
+    {
+        m_a = 0xdeadbeef;
+        m_b = m_c = m_d = seed;
+        for (int i=0; i<20; ++i)
+            (void)Value();
+    }
+
+private:
+    static inline uint64_t Rot64(uint64_t x, int k)
+    {
+        return (x << k) | (x >> (64-(k)));
+    }
+
+    uint64_t m_a;
+    uint64_t m_b;
+    uint64_t m_c;
+    uint64_t m_d;
+};
+
+// fastest conceivable hash function (for comparison)
+static void Add(const void *data, size_t length, uint64_t *hash1, uint64_t *hash2)
+{
+    uint64_t *p64 = (uint64_t *)data;
+    uint64_t *end = p64 + length/8;
+    uint64_t hash = *hash1 + *hash2;
+    while (p64 < end)
+    {
+      hash += *p64;
+      ++p64;
+    }
+    *hash1 = hash;
+    *hash2 = hash;
+}
+
+#define BUFSIZE (512)
+void TestResults()
+{
+    printf("\ntesting results ...\n");
+    static const uint64_t expected[BUFSIZE] = {
+      0x6bf50919,0x70de1d26,0xa2b37298,0x35bc5fbf,0x8223b279,0x5bcb315e,0x53fe88a1,0xf9f1a233,
+      0xee193982,0x54f86f29,0xc8772d36,0x9ed60886,0x5f23d1da,0x1ed9f474,0xf2ef0c89,0x83ec01f9,
+      0xf274736c,0x7e9ac0df,0xc7aed250,0xb1015811,0xe23470f5,0x48ac20c4,0xe2ab3cd5,0x608f8363,
+      0xd0639e68,0xc4e8e7ab,0x863c7c5b,0x4ea63579,0x99ae8622,0x170c658b,0x149ba493,0x027bca7c,
+      0xe5cfc8b6,0xce01d9d7,0x11103330,0x5d1f5ed4,0xca720ecb,0xef408aec,0x733b90ec,0x855737a6,
+      0x9856c65f,0x647411f7,0x50777c74,0xf0f1a8b7,0x9d7e55a5,0xc68dd371,0xfc1af2cc,0x75728d0a,
+      0x390e5fdc,0xf389b84c,0xfb0ccf23,0xc95bad0e,0x5b1cb85a,0x6bdae14f,0x6deb4626,0x93047034,
+      0x6f3266c6,0xf529c3bd,0x396322e7,0x3777d042,0x1cd6a5a2,0x197b402e,0xc28d0d2b,0x09c1afb4,
+
+      0x069c8bb7,0x6f9d4e1e,0xd2621b5c,0xea68108d,0x8660cb8f,0xd61e6de6,0x7fba15c7,0xaacfaa97,
+      0xdb381902,0x4ea22649,0x5d414a1e,0xc3fc5984,0xa0fc9e10,0x347dc51c,0x37545fb6,0x8c84b26b,
+      0xf57efa5d,0x56afaf16,0xb6e1eb94,0x9218536a,0xe3cc4967,0xd3275ef4,0xea63536e,0x6086e499,
+      0xaccadce7,0xb0290d82,0x4ebfd0d6,0x46ccc185,0x2eeb10d3,0x474e3c8c,0x23c84aee,0x3abae1cb,
+      0x1499b81a,0xa2993951,0xeed176ad,0xdfcfe84c,0xde4a961f,0x4af13fe6,0xe0069c42,0xc14de8f5,
+      0x6e02ce8f,0x90d19f7f,0xbca4a484,0xd4efdd63,0x780fd504,0xe80310e3,0x03abbc12,0x90023849,
+      0xd6f6fb84,0xd6b354c5,0x5b8575f0,0x758f14e4,0x450de862,0x90704afb,0x47209a33,0xf226b726,
+      0xf858dab8,0x7c0d6de9,0xb05ce777,0xee5ff2d4,0x7acb6d5c,0x2d663f85,0x41c72a91,0x82356bf2,
+
+      0x94e948ec,0xd358d448,0xeca7814d,0x78cd7950,0xd6097277,0x97782a5d,0xf43fc6f4,0x105f0a38,
+      0x9e170082,0x4bfe566b,0x4371d25f,0xef25a364,0x698eb672,0x74f850e4,0x4678ff99,0x4a290dc6,
+      0x3918f07c,0x32c7d9cd,0x9f28e0af,0x0d3c5a86,0x7bfc8a45,0xddf0c7e1,0xdeacb86b,0x970b3c5c,
+      0x5e29e199,0xea28346d,0x6b59e71b,0xf8a8a46a,0x862f6ce4,0x3ccb740b,0x08761e9e,0xbfa01e5f,
+      0xf17cfa14,0x2dbf99fb,0x7a0be420,0x06137517,0xe020b266,0xd25bfc61,0xff10ed00,0x42e6be8b,
+      0x029ef587,0x683b26e0,0xb08afc70,0x7c1fd59e,0xbaae9a70,0x98c8c801,0xb6e35a26,0x57083971,
+      0x90a6a680,0x1b44169e,0x1dce237c,0x518e0a59,0xccb11358,0x7b8175fb,0xb8fe701a,0x10d259bb,
+      0xe806ce10,0x9212be79,0x4604ae7b,0x7fa22a84,0xe715b13a,0x0394c3b2,0x11efbbae,0xe13d9e19,
+
+      0x77e012bd,0x2d05114c,0xaecf2ddd,0xb2a2b4aa,0xb9429546,0x55dce815,0xc89138f8,0x46dcae20,
+      0x1f6f7162,0x0c557ebc,0x5b996932,0xafbbe7e2,0xd2bd5f62,0xff475b9f,0x9cec7108,0xeaddcffb,
+      0x5d751aef,0xf68f7bdf,0xf3f4e246,0x00983fcd,0x00bc82bb,0xbf5fd3e7,0xe80c7e2c,0x187d8b1f,
+      0xefafb9a7,0x8f27a148,0x5c9606a9,0xf2d2be3e,0xe992d13a,0xe4bcd152,0xce40b436,0x63d6a1fc,
+      0xdc1455c4,0x64641e39,0xd83010c9,0x2d535ae0,0x5b748f3e,0xf9a9146b,0x80f10294,0x2859acd4,
+      0x5fc846da,0x56d190e9,0x82167225,0x98e4daba,0xbf7865f3,0x00da7ae4,0x9b7cd126,0x644172f8,
+      0xde40c78f,0xe8803efc,0xdd331a2b,0x48485c3c,0x4ed01ddc,0x9c0b2d9e,0xb1c6e9d7,0xd797d43c,
+      0x274101ff,0x3bf7e127,0x91ebbc56,0x7ffeb321,0x4d42096f,0xd6e9456a,0x0bade318,0x2f40ee0b,
+
+      0x38cebf03,0x0cbc2e72,0xbf03e704,0x7b3e7a9a,0x8e985acd,0x90917617,0x413895f8,0xf11dde04,
+      0xc66f8244,0xe5648174,0x6c420271,0x2469d463,0x2540b033,0xdc788e7b,0xe4140ded,0x0990630a,
+      0xa54abed4,0x6e124829,0xd940155a,0x1c8836f6,0x38fda06c,0x5207ab69,0xf8be9342,0x774882a8,
+      0x56fc0d7e,0x53a99d6e,0x8241f634,0x9490954d,0x447130aa,0x8cc4a81f,0x0868ec83,0xc22c642d,
+      0x47880140,0xfbff3bec,0x0f531f41,0xf845a667,0x08c15fb7,0x1996cd81,0x86579103,0xe21dd863,
+      0x513d7f97,0x3984a1f1,0xdfcdc5f4,0x97766a5e,0x37e2b1da,0x41441f3f,0xabd9ddba,0x23b755a9,
+      0xda937945,0x103e650e,0x3eef7c8f,0x2760ff8d,0x2493a4cd,0x1d671225,0x3bf4bd4c,0xed6e1728,
+      0xc70e9e30,0x4e05e529,0x928d5aa6,0x164d0220,0xb5184306,0x4bd7efb3,0x63830f11,0xf3a1526c,
+
+      0xf1545450,0xd41d5df5,0x25a5060d,0x77b368da,0x4fe33c7e,0xeae09021,0xfdb053c4,0x2930f18d,
+      0xd37109ff,0x8511a781,0xc7e7cdd7,0x6aeabc45,0xebbeaeaa,0x9a0c4f11,0xda252cbb,0x5b248f41,
+      0x5223b5eb,0xe32ab782,0x8e6a1c97,0x11d3f454,0x3e05bd16,0x0059001d,0xce13ac97,0xf83b2b4c,
+      0x71db5c9a,0xdc8655a6,0x9e98597b,0x3fcae0a2,0x75e63ccd,0x076c72df,0x4754c6ad,0x26b5627b,
+      0xd818c697,0x998d5f3d,0xe94fc7b2,0x1f49ad1a,0xca7ff4ea,0x9fe72c05,0xfbd0cbbf,0xb0388ceb,
+      0xb76031e3,0xd0f53973,0xfb17907c,0xa4c4c10f,0x9f2d8af9,0xca0e56b0,0xb0d9b689,0xfcbf37a3,
+      0xfede8f7d,0xf836511c,0x744003fc,0x89eba576,0xcfdcf6a6,0xc2007f52,0xaaaf683f,0x62d2f9ca,
+      0xc996f77f,0x77a7b5b3,0x8ba7d0a4,0xef6a0819,0xa0d903c0,0x01b27431,0x58fffd4c,0x4827f45c,
+
+      0x44eb5634,0xae70edfc,0x591c740b,0x478bf338,0x2f3b513b,0x67bf518e,0x6fef4a0c,0x1e0b6917,
+      0x5ac0edc5,0x2e328498,0x077de7d5,0x5726020b,0x2aeda888,0x45b637ca,0xcf60858d,0x3dc91ae2,
+      0x3e6d5294,0xe6900d39,0x0f634c71,0x827a5fa4,0xc713994b,0x1c363494,0x3d43b615,0xe5fe7d15,
+      0xf6ada4f2,0x472099d5,0x04360d39,0x7f2a71d0,0x88a4f5ff,0x2c28fac5,0x4cd64801,0xfd78dd33,
+      0xc9bdd233,0x21e266cc,0x9bbf419d,0xcbf7d81d,0x80f15f96,0x04242657,0x53fb0f66,0xded11e46,
+      0xf2fdba97,0x8d45c9f1,0x4eeae802,0x17003659,0xb9db81a7,0xe734b1b2,0x9503c54e,0xb7c77c3e,
+      0x271dd0ab,0xd8b906b5,0x0d540ec6,0xf03b86e0,0x0fdb7d18,0x95e261af,0xad9ec04e,0x381f4a64,
+      0xfec798d7,0x09ea20be,0x0ef4ca57,0x1e6195bb,0xfd0da78b,0xcea1653b,0x157d9777,0xf04af50f,
+
+      0xad7baa23,0xd181714a,0x9bbdab78,0x6c7d1577,0x645eb1e7,0xa0648264,0x35839ca6,0x2287ef45,
+      0x32a64ca3,0x26111f6f,0x64814946,0xb0cddaf1,0x4351c59e,0x1b30471c,0xb970788a,0x30e9f597,
+      0xd7e58df1,0xc6d2b953,0xf5f37cf4,0x3d7c419e,0xf91ecb2d,0x9c87fd5d,0xb22384ce,0x8c7ac51c,
+      0x62c96801,0x57e54091,0x964536fe,0x13d3b189,0x4afd1580,0xeba62239,0xb82ea667,0xae18d43a,
+      0xbef04402,0x1942534f,0xc54bf260,0x3c8267f5,0xa1020ddd,0x112fcc8a,0xde596266,0xe91d0856,
+      0xf300c914,0xed84478e,0x5b65009e,0x4764da16,0xaf8e07a2,0x4088dc2c,0x9a0cad41,0x2c3f179b,
+      0xa67b83f7,0xf27eab09,0xdbe10e28,0xf04c911f,0xd1169f87,0x8e1e4976,0x17f57744,0xe4f5a33f,
+      0x27c2e04b,0x0b7523bd,0x07305776,0xc6be7503,0x918fa7c9,0xaf2e2cd9,0x82046f8e,0xcc1c8250
+    };
+
+    uint8_t buf[BUFSIZE];
+    uint32_t saw[BUFSIZE];
+    for (int i=0; i<BUFSIZE; ++i)
+    {
+        buf[i] = i+128;
+        saw[i] = SpookyHashV2::Hash32(buf, i, 0);
+        if (saw[i] != expected[i])
+        {
+            printf("%3d: saw 0x%.8x, expected 0x%.8" PRIx64 "\n", i, saw[i],
+                   expected[i]);
+            failed = true;
+        }
+    }
+}
+#undef BUFSIZE
+
+
+#define NUMBUF (1<<10)
+#define BUFSIZE (1<<20)
+void DoTimingBig(int seed)
+{
+    printf("\ntesting time to hash 2^^30 bytes ...\n");
+
+    char *buf[NUMBUF];
+    for (int i=0; i<NUMBUF; ++i)
+    {
+        buf[i] = (char *)malloc(BUFSIZE);
+        memset(buf[i], (char)seed, BUFSIZE);
+    }
+
+    uint64_t a = GetTickCount();
+    uint64_t hash1 = seed;
+    uint64_t hash2 = seed;
+    for (uint64_t i=0; i<NUMBUF; ++i)
+    {
+        SpookyHashV2::Hash128(buf[i], BUFSIZE, &hash1, &hash2);
+    }
+    uint64_t z = GetTickCount();
+    printf("SpookyHashV2::Hash128, uncached: time is "
+           "%4" PRId64 " milliseconds\n", z-a);
+
+    a = GetTickCount();
+    for (uint64_t i=0; i<NUMBUF; ++i)
+    {
+        Add(buf[i], BUFSIZE, &hash1, &hash2);
+    }
+    z = GetTickCount();
+    printf("Addition           , uncached: time is %4" PRId64 " milliseconds\n",
+           z-a);
+
+    a = GetTickCount();
+    for (uint64_t i=0; i<NUMBUF*BUFSIZE/1024; ++i)
+    {
+        SpookyHashV2::Hash128(buf[0], 1024, &hash1, &hash2);
+    }
+    z = GetTickCount();
+    printf("SpookyHashV2::Hash128,   cached: time is "
+           "%4" PRId64 " milliseconds\n", z-a);
+
+    a = GetTickCount();
+    for (uint64_t i=0; i<NUMBUF*BUFSIZE/1024; ++i)
+    {
+        Add(buf[0], 1024, &hash1, &hash2);
+    }
+    z = GetTickCount();
+    printf("Addition           ,   cached: time is %4" PRId64 " milliseconds\n",
+           z-a);
+
+    for (int i=0; i<NUMBUF; ++i)
+    {
+        free(buf[i]);
+        buf[i] = 0;
+    }
+}
+#undef NUMBUF
+#undef BUFSIZE
+
+
+#define BUFSIZE (1<<14)
+#define NUMITER 10000000
+void DoTimingSmall(int seed)
+{
+    printf("\ntesting timing of hashing up to %d cached aligned bytes %d "
+           "times ...\n", BUFSIZE, NUMITER);
+
+    uint64_t buf[BUFSIZE/8];
+    for (int i=0; i<BUFSIZE/8; ++i)
+    {
+        buf[i] = i+seed;
+    }
+
+    for (int i=1; i <= BUFSIZE; i <<= 1)
+    {
+        uint64_t a = GetTickCount();
+        uint64_t hash1 = seed;
+        uint64_t hash2 = seed+i;
+        for (int j=0; j<NUMITER; ++j)
+        {
+            SpookyHashV2::Hash128((char *)buf, i, &hash1, &hash2);
+        }
+        uint64_t z = GetTickCount();
+        printf("%d bytes: hash is %.16" PRIx64 " %.16" PRIx64 ", "
+               "time is %" PRId64 "\n", i, hash1, hash2, z-a);
+    }
+}
+#undef BUFSIZE
+
+#define BUFSIZE 1024
+void TestAlignment()
+{
+    printf("\ntesting alignment ...\n");
+
+    char buf[BUFSIZE];
+    uint64_t hash[8];
+    for (int i=0; i<BUFSIZE-16; ++i)
+    {
+        for (int j=0; j<8; ++j)
+        {
+            buf[j] = (char)i+j;
+            for (int k=1; k<=i; ++k)
+            {
+                buf[j+k] = k;
+            }
+            buf[j+i+1] = (char)i+j;
+            hash[j] = SpookyHashV2::Hash64((const void *)(buf+j+1), i, 0);
+        }
+        for (int j=1; j<8; ++j)
+        {
+            if (hash[0] != hash[j])
+            {
+                printf("alignment problems: %d %d\n", i, j);
+                failed = true;
+            }
+        }
+    }
+}
+#undef BUFSIZE
+
+// test that all deltas of one or two input bits affect all output bits
+#define BUFSIZE 256
+#define TRIES 50
+#define MEASURES 6
+void TestDeltas(int seed)
+{
+    printf("\nall 1 or 2 bit input deltas get %d tries to flip every output "
+           "bit ...\n", TRIES);
+
+    Random random;
+    random.Init((uint64_t)seed);
+
+    // for messages 0..BUFSIZE-1 bytes
+    for (int h=0; h<BUFSIZE; ++h)
+    {
+        int maxk = 0;
+        // first bit to set
+        for (int i=0; i<h*8; ++i)
+        {
+            // second bit to set, or don't have a second bit
+            for (int j=0; j<=i; ++j)
+            {
+                uint64_t measure[MEASURES][2];
+                uint64_t counter[MEASURES][2];
+                for (int l=0; l<2; ++l)
+                {
+                    for (int m=0; m<MEASURES; ++m)
+                    {
+                        counter[m][l] = 0;
+                    }
+                }
+
+                // try to hit every output bit TRIES times
+                int k;
+                for (k=0; k<TRIES; ++k)
+                {
+                    uint8_t buf1[BUFSIZE];
+                    uint8_t buf2[BUFSIZE];
+                    int done = 1;
+                    for (int l=0; l<h; ++l)
+                    {
+                        buf1[l] = buf2[l] = random.Value();
+                    }
+                    buf1[i/8] ^= (1 << (i%8));
+                    if (j != i)
+                    {
+                        buf1[j/8] ^= (1 << (j%8));
+                    }
+                    SpookyHashV2::Hash128(buf1, h, &measure[0][0], &measure[0][1]);
+                    SpookyHashV2::Hash128(buf2, h, &measure[1][0], &measure[1][1]);
+                    for (int l=0; l<2; ++l) {
+                        measure[2][l] = measure[0][l] ^ measure[1][l];
+                        measure[3][l] = ~(measure[0][l] ^ measure[1][l]);
+                        measure[4][l] = measure[0][l] - measure[1][l];
+                        measure[4][l] ^= (measure[4][l]>>1);
+                        measure[5][l] = measure[0][l] + measure[1][l];
+                        measure[5][l] ^= (measure[4][l]>>1);
+                    }
+                    for (int l=0; l<2; ++l)
+                    {
+                        for (int m=0; m<MEASURES; ++m)
+                        {
+                            counter[m][l] |= measure[m][l];
+                            if (~counter[m][l]) done = 0;
+                        }
+                    }
+                    if (done) break;
+                }
+                if (k == TRIES)
+                {
+                    printf("failed %d %d %d\n", h, i, j);
+                    failed = true;
+                }
+                else if (k > maxk)
+                {
+                    maxk = k;
+                }
+            }
+        }
+        printf("passed for buffer size %d  max %d\n", h, maxk);
+    }
+}
+#undef BUFSIZE
+#undef TRIES
+#undef MEASURES
+
+
+// test that hashing pieces has the same behavior as hashing the whole
+#define BUFSIZE 1024
+void TestPieces()
+{
+    printf("\ntesting pieces ...\n");
+    char buf[BUFSIZE];
+    for (int i=0; i<BUFSIZE; ++i)
+    {
+        buf[i] = i;
+    }
+    for (int i=0; i<BUFSIZE; ++i)
+    {
+        uint64_t a,b,c,d,seed1=1,seed2=2;
+        SpookyHashV2 state;
+
+        // all as one call
+        a = seed1;
+        b = seed2;
+        SpookyHashV2::Hash128(buf, i, &a, &b);
+
+        // all as one piece
+        c = 0xdeadbeefdeadbeef;
+        d = 0xbaceba11baceba11;
+        state.Init(seed1, seed2);
+        state.Update(buf, i);
+        state.Final(&c, &d);
+
+        if (a != c)
+        {
+            printf("wrong a %d: %.16" PRIx64 " %.16" PRIx64 "\n", i, a,c);
+            failed = true;
+        }
+        if (b != d)
+        {
+            printf("wrong b %d: %.16" PRIx64 " %.16" PRIx64 "\n", i, b,d);
+            failed = true;
+        }
+
+        // all possible two consecutive pieces
+        for (int j=0; j<i; ++j)
+        {
+            c = seed1;
+            d = seed2;
+            state.Init(c, d);
+            state.Update(&buf[0], j);
+            state.Update(&buf[j], i-j);
+            state.Final(&c, &d);
+            if (a != c)
+            {
+                printf("wrong a %d %d: %.16" PRIx64 " %.16" PRIx64 "\n",
+                       j, i, a,c);
+                failed = true;
+            }
+            if (b != d)
+            {
+                printf("wrong b %d %d: %.16" PRIx64 " %.16" PRIx64 "\n",
+                       j, i, b,d);
+                failed = true;
+            }
+        }
+    }
+}
+#undef BUFSIZE
+
+int main(int argc, const char **argv)
+{
+    TestResults();
+    TestAlignment();
+    TestPieces();
+    DoTimingBig(argc);
+    // tudorb@fb.com: Commented out slow tests
+#if 0
+    DoTimingSmall(argc);
+    TestDeltas(argc);
+#endif
+    return failed;
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/stl_tests/Benchmark.cpp
@@ -0,0 +1,1032 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// @author Nicholas Ormrod <njormrod@fb.com>
+
+/******************************************************************************
+ *
+ * This file is not perfect - benchmarking is a finicky process.
+ *
+ * TAKE THE NUMBERS YOU SEE TO MIND, NOT TO HEART
+ *
+ *****************************************************************************/
+
+#include <vector>
+#include "OFBVector.h"
+#define FOLLY_BENCHMARK_USE_NS_IFOLLY
+#include "folly/FBVector.h"
+
+#include <chrono>
+#include <deque>
+#include <iomanip>
+#include <iostream>
+#include <locale>
+#include <memory>
+#include <string>
+
+#include <boost/preprocessor.hpp>
+
+using namespace std;
+
+static const bool enableColors = true;
+
+//=============================================================================
+// use the timestamp counter for time measurements
+
+static inline
+void clear_icache() {} // placeholder
+
+// return the CPU timestamp counter
+static uint64_t readTSC() {
+   unsigned reslo, reshi;
+
+   __asm__ __volatile__  (
+   "xorl %%eax,%%eax \n cpuid \n"
+    ::: "%eax", "%ebx", "%ecx", "%edx");
+   __asm__ __volatile__  (
+   "rdtsc\n"
+    : "=a" (reslo), "=d" (reshi) );
+   __asm__ __volatile__  (
+   "xorl %%eax,%%eax \n cpuid \n"
+    ::: "%eax", "%ebx", "%ecx", "%edx");
+
+   return ((uint64_t)reshi << 32) | reslo;
+}
+
+//=============================================================================
+// Timer
+
+// The TIME* macros expand to a sequence of functions and classes whose aim
+//  is to run a benchmark function several times with different types and
+//  sizes.
+//
+// The first and last thing that TIME* expands to is a function declaration,
+//  through the DECL macro. The declared function is templated on the full
+//  vector type, its value_type, its allocator_type, and a number N.
+// The first DECL is a forward declaration, and is followed by a
+//  semicolon. The second DECL ends the macro - a function body is to be
+//  supplied after the macro invocation.
+// The declared function returns a uint64_t, which is assumed to be a time.
+//
+// The GETTER macro calls the DECL function repeatedly. It returns the
+//  minimum time taken to execute DECL. GETTER runs DECL between 2 and 100
+//  times (it will not run the full 100 if the tests take a long time).
+//
+// The EVALUATOR macro calls the GETTER macro with each of std::vector,
+//  the original fbvector (folly::fbvector), and the new fbvector
+//  (Ifolly::fbvector). It runs all three twice, and then calls the
+//  pretty printer to display the results. Before calling the pretty
+//  printer, the EVALUATOR outputs the three message strings.
+//
+// The EXECUTOR macro calls the EVALUATOR with different values of N.
+//  It also defines the string message for N.
+//
+// The RUNNER macro defines a struct. That struct defines the testname
+//  string. The constructor calls the EXECUTOR with each test type, and
+//  also defines the test type string.
+// The RUNNER class is also instantiated, so the constructor will be run
+//  before entering main().
+//
+
+#define TIME(str, types)    TIME_I(str, types, (0))
+#define TIME_N(str, types)  TIME_I(str, types, (0)(16)(64)(1024)(16384)(262144))
+
+#define TIME_I(str, types, values) \
+  TIME_II(str, BOOST_PP_CAT(t_, __LINE__), types, values)
+
+#define TIME_II(str, name, types, values) \
+  DECL(name);                             \
+  GETTER(name)                            \
+  EVALUATOR(name)                         \
+  EXECUTOR(name, values)                  \
+  RUNNER(str, name, types)                \
+  DECL(name)
+
+#define DECL(name)                                                \
+  template <class Vector, typename T, typename Allocator, int N>  \
+  static inline uint64_t BOOST_PP_CAT(run_, name) ()
+
+#define GETTER(name)                                                          \
+  template <class Vector, int N>                                              \
+  static uint64_t BOOST_PP_CAT(get_, name) () {                               \
+    auto s = chrono::high_resolution_clock::now();                            \
+    uint64_t minticks = ~uint64_t(0);                                         \
+    int burst = 0;                                                            \
+    for (; burst < 100; ++burst) {                                            \
+      auto t = BOOST_PP_CAT(run_, name) <Vector,                              \
+        typename Vector::value_type, typename Vector::allocator_type, N> ();  \
+      minticks = min(minticks, t);                                            \
+      if (minticks * burst > 10000000) break;                                 \
+    }                                                                         \
+    auto e = chrono::high_resolution_clock::now();                            \
+    chrono::nanoseconds d(e - s);                                             \
+    return minticks;                                                          \
+    return d.count() / burst;                                                 \
+  }
+
+#define EVALUATOR(name)                                                       \
+  template <typename T, typename Allocator, int N>                            \
+  void BOOST_PP_CAT(evaluate_, name)                                          \
+  ( string& part1, string& part2, string& part3 ) {                           \
+    cout << setw(25) << left << part1                                         \
+         << setw(4) << left << part2                                          \
+         << setw(6) << right << part3;                                        \
+    part1.clear(); part2.clear(); part3.clear();                              \
+    auto v1 = BOOST_PP_CAT(get_, name)                                        \
+      <Ifolly::fbvector<T, Allocator>, N> ();                                 \
+    auto v2 = BOOST_PP_CAT(get_, name)                                        \
+      < folly::fbvector<T, Allocator>, N> ();                                 \
+    auto v3 = BOOST_PP_CAT(get_, name)                                        \
+      <   std::  vector<T, Allocator>, N> ();                                 \
+    auto v1b = BOOST_PP_CAT(get_, name)                                       \
+      <Ifolly::fbvector<T, Allocator>, N> ();                                 \
+    auto v2b = BOOST_PP_CAT(get_, name)                                       \
+      < folly::fbvector<T, Allocator>, N> ();                                 \
+    auto v3b = BOOST_PP_CAT(get_, name)                                       \
+      <   std::  vector<T, Allocator>, N> ();                                 \
+    prettyPrint(min(v1, v1b), min(v2, v2b), min(v3, v3b));                    \
+    cout << endl;                                                             \
+  }
+
+#define EXECUTOR(name, values)                                                \
+  template <typename T, typename Allocator>                                   \
+  void BOOST_PP_CAT(execute_, name) ( string& part1, string& part2 ) {        \
+    BOOST_PP_SEQ_FOR_EACH(EVALUATE, name, values)                             \
+  }
+
+#define EVALUATE(r, name, value)                                              \
+  { string part3(BOOST_PP_STRINGIZE(value));                                  \
+    BOOST_PP_CAT(evaluate_, name) <T, Allocator, value>                       \
+    ( part1, part2, part3 ); }
+
+#define RUNNER(str, name, types)                                              \
+  struct BOOST_PP_CAT(Runner_, name) {                                        \
+    BOOST_PP_CAT(Runner_, name) () {                                          \
+      string part1(str);                                                      \
+      BOOST_PP_SEQ_FOR_EACH(EXECUTE, (part1, name), types)                    \
+    }                                                                         \
+  } BOOST_PP_CAT(runner_, name);
+
+#define EXECUTE(r, pn, type)                                                  \
+  { string part2(BOOST_PP_STRINGIZE(type));                                   \
+    BOOST_PP_CAT(execute_, BOOST_PP_TUPLE_ELEM(2, 1, pn))                     \
+    <typename type::first_type, typename type::second_type>                   \
+    ( BOOST_PP_TUPLE_ELEM(2, 0, pn), part2 ); }
+
+//=============================================================================
+// pretty printer
+
+// The pretty printer displays the times for each of the three vectors.
+// The fastest time (or times, if there is a tie) is highlighted in green.
+// Additionally, if the new fbvector (time v1) is not the fastest, then
+//  it is highlighted with red or blue. It is highlighted with blue only
+//  if it lost by a small margin (5 clock cycles or 2%, whichever is
+//  greater).
+
+void prettyPrint(uint64_t v1, uint64_t v2, uint64_t v3) {
+  // rdtsc takes some time to run; about 157 clock cycles
+  // if we see a smaller positive number, adjust readtsc
+  uint64_t readtsc_time = 157;
+  if (v1 != 0 && v1 < readtsc_time) readtsc_time = v1;
+  if (v2 != 0 && v2 < readtsc_time) readtsc_time = v2;
+  if (v3 != 0 && v3 < readtsc_time) readtsc_time = v3;
+
+  if (v1 == 0) v1 = ~uint64_t(0); else v1 -= readtsc_time;
+  if (v2 == 0) v2 = ~uint64_t(0); else v2 -= readtsc_time;
+  if (v3 == 0) v3 = ~uint64_t(0); else v3 -= readtsc_time;
+
+  auto least = min({ v1, v2, v3 });
+  // a good time is less than 2% or 5 clock cycles slower
+  auto good = max(least + 5, (uint64_t)(least * 1.02));
+
+  string w("\x1b[1;;42m"); // green
+  string g("\x1b[1;;44m"); // blue
+  string b("\x1b[1;;41m"); // red
+  string e("\x1b[0m");     // reset
+
+  if (!enableColors) {
+    w = b = e = "";
+  }
+
+  cout << " ";
+
+  if (v1 == least) cout << w;
+  else if (v1 <= good) cout << g;
+  else cout << b;
+  cout << setw(13) << right;
+  if (v1 == ~uint64_t(0)) cout << "-"; else cout << v1;
+  cout << " "  << e << " ";
+
+  if (v2 == least) cout << w;
+  cout << setw(13) << right;
+  if (v2 == ~uint64_t(0)) cout << "-"; else cout << v2;
+  cout << " " << e << " ";
+
+  if (v3 == least) cout << w;
+  cout << setw(13) << right;
+  if (v3 == ~uint64_t(0)) cout << "-"; else cout << v3;
+  cout << " " << e << " ";
+}
+
+//=============================================================================
+// table formatting
+
+// Much like the TIME macros, the Leader and Line struct/macros
+//  instantiate a class before main, and work is done inside the
+//  constructors. The Leader and Line struct print out pretty
+//  table boundaries and titles.
+
+uint64_t leader_elapsed() {
+  static auto t = chrono::high_resolution_clock::now();
+  chrono::nanoseconds d(chrono::high_resolution_clock::now() - t);
+  return d.count() / 1000000000;
+}
+
+struct Leader {
+  Leader() {
+    leader_elapsed();
+    std::cout.imbue(std::locale(""));
+
+    cout << endl;
+    cout << "========================================"
+         << "========================================" << endl;
+    cout << setw(35) << left << "Test";
+    cout << setw(15) << right << "new fbvector ";
+    cout << setw(15) << right << "old fbvector ";
+    cout << setw(15) << right << "std::vector ";
+    cout << endl;
+    cout << "========================================"
+         << "========================================" << endl;
+  }
+  ~Leader() {
+    cout << endl;
+    cout << "========================================"
+         << "========================================" << endl;
+    cout << setw(78) << right << leader_elapsed() << " s" << endl;
+  }
+} leader;
+
+struct Line {
+  explicit Line(string text) {
+    cout << "\n--- " << text  << " ---" << endl;
+  }
+};
+#define SECTION(str) Line BOOST_PP_CAT(l_, __LINE__) ( str )
+
+//=============================================================================
+// Test types
+
+typedef pair<int, std::allocator<int>> T1;
+typedef pair<vector<int>, std::allocator<vector<int>>> T2;
+
+uint64_t v1_T1 = 0, v2_T1 = 0, v3_T1 = 0;
+uint64_t v1_T2 = 0, v2_T2 = 0, v3_T2 = 0;
+
+#define BASICS (T1)(T2)
+
+//=============================================================================
+// prevent optimizing
+
+std::vector<int> O_vi(10000000);
+
+void O(int i) {
+  O_vi.push_back(i);
+}
+template <class V>
+void O(const V& v) {
+  int s = v.size();
+  for (int i = 0; i < s; ++i) O(v[i]);
+}
+
+//=============================================================================
+// Benchmarks
+
+// #if 0
+//-----------------------------------------------------------------------------
+//SECTION("Dev");
+//#undef BASICS
+//#define BASICS (T1)
+
+
+// #else
+
+//-----------------------------------------------------------------------------
+SECTION("Essentials");
+
+TIME_N("~Vector()", BASICS) {
+  Vector a(N);
+  O(a);
+  clear_icache(); auto b = readTSC();
+  a.~Vector();
+  auto e = readTSC();
+  new (&a) Vector();
+  O(a);
+  return e - b;
+}
+
+TIME_N("a.clear()", BASICS) {
+  Vector a(N);
+  O(a);
+  clear_icache(); auto b = readTSC();
+  a.clear();
+  auto e = readTSC();
+  O(a);
+  return e - b;
+}
+
+TIME("Vector u", BASICS) {
+  clear_icache(); auto b = readTSC();
+  Vector u;
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME_N("Vector u(a)", BASICS) {
+  static const Vector a(N);
+  clear_icache(); auto b = readTSC();
+  Vector u(a);
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME_N("Vector u(move(a))", BASICS) {
+  Vector a(N);
+  clear_icache(); auto b = readTSC();
+  Vector u(move(a));
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+//-----------------------------------------------------------------------------
+SECTION("Constructors");
+
+TIME_N("Vector u(n)", BASICS) {
+  clear_icache(); auto b = readTSC();
+  Vector u(N);
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME_N("Vector u(n, t)", BASICS) {
+  static const T t(1);
+  clear_icache(); auto b = readTSC();
+  Vector u(N, t);
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME_N("Vector u(first, last)", BASICS) {
+  static const deque<T> d(N);
+  clear_icache(); auto b = readTSC();
+  Vector u(d.begin(), d.end());
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+//-----------------------------------------------------------------------------
+SECTION("Assignment");
+
+TIME_N("a = b", BASICS) {
+  Vector a(N);
+  static const Vector c(N/2 + 10);
+  O(a);
+  clear_icache(); auto b = readTSC();
+  a = c;
+  auto e = readTSC();
+  O(a);
+  return e - b;
+}
+
+TIME_N("a = move(b)", BASICS) {
+  Vector a(N);
+  Vector c(N/2 + 10);
+  O(a);
+  O(c);
+  clear_icache(); auto b = readTSC();
+  a = move(c);
+  auto e = readTSC();
+  O(a);
+  O(c);
+  return e - b;
+}
+
+TIME_N("a = destructive_move(b)", BASICS) {
+  Vector a(N);
+  Vector c(N/2 + 10);
+  O(a);
+  O(c);
+  clear_icache(); auto b = readTSC();
+  a = move(c);
+  c.clear();
+  auto e = readTSC();
+  O(a);
+  O(c);
+  return e - b;
+}
+
+TIME_N("a.assign(N, t)", BASICS) {
+  Vector a(N/2 + 10);
+  const T t(1);
+  O(a);
+  clear_icache(); auto b = readTSC();
+  a.assign(N, t);
+  auto e = readTSC();
+  O(a);
+  return e - b;
+}
+
+TIME_N("a.assign(first, last)", BASICS) {
+  static const deque<T> d(N);
+  Vector a(N/2 + 10);
+  clear_icache(); auto b = readTSC();
+  a.assign(d.begin(), d.end());
+  auto e = readTSC();
+  O(a);
+  return e - b;
+}
+
+TIME_N("a.swap(b)", BASICS) {
+  Vector a(N/2 + 10);
+  Vector c(N);
+  O(a);
+  O(c);
+  clear_icache(); auto b = readTSC();
+  a.swap(c);
+  auto e = readTSC();
+  O(a);
+  O(c);
+  return e - b;
+}
+
+//-----------------------------------------------------------------------------
+SECTION("Iterators");
+
+TIME("a.begin()", BASICS) {
+  static Vector a(1);
+  clear_icache(); auto b = readTSC();
+  auto r = a.begin();
+  auto e = readTSC();
+  O(*r);
+  return e - b;
+}
+
+TIME("a.cbegin()", BASICS) {
+  static Vector a(1);
+  clear_icache(); auto b = readTSC();
+  auto r = a.cbegin();
+  auto e = readTSC();
+  O(*r);
+  return e - b;
+}
+
+TIME("a.rbegin()", BASICS) {
+  static Vector a(1);
+  clear_icache(); auto b = readTSC();
+  auto r = a.rbegin();
+  auto e = readTSC();
+  O(*r);
+  return e - b;
+}
+
+//-----------------------------------------------------------------------------
+SECTION("Capacity");
+
+TIME_N("a.size()", BASICS) {
+  static const Vector a(N);
+  clear_icache(); auto b = readTSC();
+  int n = a.size();
+  auto e = readTSC();
+  O(n);
+  return e - b;
+}
+
+TIME("a.max_size()", BASICS) {
+  static Vector a;
+  clear_icache(); auto b = readTSC();
+  int n = a.max_size();
+  auto e = readTSC();
+  O(n);
+  return e - b;
+}
+
+TIME_N("a.capacity()", BASICS) {
+  static const Vector a(N);
+  clear_icache(); auto b = readTSC();
+  int n = a.capacity();
+  auto e = readTSC();
+  O(n);
+  return e - b;
+}
+
+TIME_N("a.empty()", BASICS) {
+  static const Vector a(N);
+  clear_icache(); auto b = readTSC();
+  int n = a.empty();
+  auto e = readTSC();
+  O(n);
+  return e - b;
+}
+
+TIME_N("reserve(n)", BASICS) {
+  Vector u;
+  O(u);
+  clear_icache(); auto b = readTSC();
+  u.reserve(N);
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME_N("resize(n)", BASICS) {
+  Vector u;
+  O(u);
+  clear_icache(); auto b = readTSC();
+  u.resize(N);
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME_N("resize(n, t)", BASICS) {
+  static const T t(1);
+  Vector u;
+  O(u);
+  clear_icache(); auto b = readTSC();
+  u.resize(N, t);
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME("staged reserve", BASICS) {
+  Vector u;
+  O(u);
+  clear_icache(); auto b = readTSC();
+  u.reserve(500);
+  u.reserve(1000);
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME("staged resize", BASICS) {
+  Vector u;
+  O(u);
+  clear_icache(); auto b = readTSC();
+  u.resize(500);
+  u.resize(1000);
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME("resize then reserve", BASICS) {
+  Vector u;
+  O(u);
+  clear_icache(); auto b = readTSC();
+  u.resize(500);
+  u.reserve(1000);
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME("shrink", BASICS) {
+  Vector u;
+  O(u);
+  u.resize(500);
+  u.reserve(1000);
+  clear_icache(); auto b = readTSC();
+  u.shrink_to_fit();
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+//-----------------------------------------------------------------------------
+SECTION("Access");
+
+TIME("operator[]", BASICS) {
+  static const Vector a(10);
+  clear_icache(); auto b = readTSC();
+  const auto& v = a[8];
+  auto e = readTSC();
+  O(v);
+  return e - b;
+}
+
+TIME("at()", BASICS) {
+  static const Vector a(10);
+  clear_icache(); auto b = readTSC();
+  const auto& v = a.at(8);
+  auto e = readTSC();
+  O(v);
+  return e - b;
+}
+
+TIME("front()", BASICS) {
+  static const Vector a(10);
+  clear_icache(); auto b = readTSC();
+  const auto& v = a.front();
+  auto e = readTSC();
+  O(v);
+  return e - b;
+}
+
+TIME("back()", BASICS) {
+  static const Vector a(10);
+  clear_icache(); auto b = readTSC();
+  const auto& v = a.back();
+  auto e = readTSC();
+  O(v);
+  return e - b;
+}
+
+TIME("data()", BASICS) {
+  static const Vector a(10);
+  clear_icache(); auto b = readTSC();
+  const auto& v = a.data();
+  auto e = readTSC();
+  O(*v);
+  return e - b;
+}
+
+//-----------------------------------------------------------------------------
+SECTION("Append");
+
+TIME("reserved emplace", BASICS) {
+  Vector u;
+  u.reserve(1);
+  O(u);
+  clear_icache(); auto b = readTSC();
+  u.emplace_back(0);
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME("full emplace", BASICS) {
+  Vector u;
+  O(u);
+  clear_icache(); auto b = readTSC();
+  u.emplace_back(0);
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME("reserved push_back", BASICS) {
+  static T t(0);
+  Vector u;
+  u.reserve(1);
+  O(u);
+  clear_icache(); auto b = readTSC();
+  u.push_back(t);
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME("full push_back", BASICS) {
+  static T t(0);
+  Vector u;
+  O(u);
+  clear_icache(); auto b = readTSC();
+  u.push_back(t);
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME("reserved push_back&&", BASICS) {
+  T t(0);
+  Vector u;
+  u.reserve(1);
+  O(u);
+  clear_icache(); auto b = readTSC();
+  u.push_back(std::move(t));
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME("full push_back&&", BASICS) {
+  T t(0);
+  Vector u;
+  O(u);
+  clear_icache(); auto b = readTSC();
+  u.push_back(std::move(t));
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME("reserved push emplace", BASICS) {
+  Vector u;
+  u.reserve(1);
+  O(u);
+  clear_icache(); auto b = readTSC();
+  u.push_back(T(0));
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME("full push emplace", BASICS) {
+  Vector u;
+  O(u);
+  clear_icache(); auto b = readTSC();
+  u.push_back(T(0));
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME_N("bulk append", BASICS) {
+  static deque<T> d(N);
+  Vector u(N/2 + 10);
+  O(u);
+  clear_icache(); auto b = readTSC();
+  u.insert(u.end(), d.begin(), d.end());
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME_N("erase end", BASICS) {
+  Vector u(N);
+  O(u);
+  auto it = u.begin();
+  it += u.size() / 2;
+  if (it != u.end()) O(*it);
+  clear_icache(); auto b = readTSC();
+  u.erase(it, u.end());
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME("pop_back", BASICS) {
+  Vector u(1);
+  O(u);
+  clear_icache(); auto b = readTSC();
+  u.pop_back();
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+//-----------------------------------------------------------------------------
+SECTION("Insert/Erase - Bad Ops");
+
+TIME("insert", BASICS) {
+  Vector u(100);
+  T t(1);
+  auto it = u.begin();
+  it += 50;
+  O(u);
+  O(*it);
+  O(t);
+  clear_icache(); auto b = readTSC();
+  u.insert(it, t);
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME("insert&&", BASICS) {
+  Vector u(100);
+  T t(1);
+  auto it = u.begin();
+  it += 50;
+  O(u);
+  O(*it);
+  O(t);
+  clear_icache(); auto b = readTSC();
+  u.insert(it, std::move(t));
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME("insert n few", BASICS) {
+  Vector u(100);
+  T t(1);
+  auto it = u.begin();
+  it += 50;
+  O(u);
+  O(*it);
+  O(t);
+  clear_icache(); auto b = readTSC();
+  u.insert(it, 10, t);
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME("insert n many", BASICS) {
+  Vector u(100);
+  T t(1);
+  auto it = u.begin();
+  it += 50;
+  O(u);
+  O(*it);
+  O(t);
+  clear_icache(); auto b = readTSC();
+  u.insert(it, 200, t);
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME("iterator insert few", BASICS) {
+  static deque<T> d(10);
+  Vector u(100);
+  T t(1);
+  auto it = u.begin();
+  it += 50;
+  O(u);
+  O(*it);
+  O(t);
+  clear_icache(); auto b = readTSC();
+  u.insert(it, d.begin(), d.end());
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME("iterator insert many", BASICS) {
+  static deque<T> d(200);
+  Vector u(100);
+  T t(1);
+  auto it = u.begin();
+  it += 50;
+  O(u);
+  O(*it);
+  O(t);
+  clear_icache(); auto b = readTSC();
+  u.insert(it, d.begin(), d.end());
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME("erase", BASICS) {
+  Vector u(100);
+  O(u);
+  auto it = u.begin();
+  it += 50;
+  O(*it);
+  clear_icache(); auto b = readTSC();
+  u.erase(it);
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME("erase many", BASICS) {
+  Vector u(100);
+  O(u);
+  auto it1 = u.begin();
+  it1 += 33;
+  O(*it1);
+  auto it2 = u.begin();
+  it2 += 66;
+  O(*it2);
+  clear_icache(); auto b = readTSC();
+  u.erase(it1, it2);
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+//-----------------------------------------------------------------------------
+SECTION("Large Tests");
+
+TIME_N("reserved bulk push_back", BASICS) {
+  Vector u;
+  u.reserve(N);
+  T t(0);
+  O(u);
+  clear_icache(); auto b = readTSC();
+  for (int i = 0; i < N; ++i) u.emplace_back(t);
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME_N("reserved bulk emplace", BASICS) {
+  Vector u;
+  u.reserve(N);
+  O(u);
+  clear_icache(); auto b = readTSC();
+  for (int i = 0; i < N; ++i) u.emplace_back(0);
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME_N("populate", BASICS) {
+  static T t(0);
+  Vector u;
+  O(u);
+  clear_icache(); auto b = readTSC();
+  for (int i = 0; i < N; ++i) u.push_back(t);
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME("jigsaw growth", BASICS) {
+  int sizes[] =
+    { 1, 5, 2, 80, 17, 8, 9, 8, 140, 130, 1000, 130, 10000, 0, 8000, 2000 };
+  clear_icache(); auto b = readTSC();
+  Vector u;
+  for (auto s : sizes) {
+    if (s < u.size()) {
+      int toAdd = u.size() - s;
+      for (int i = 0; i < toAdd / 2; ++i) u.emplace_back(0);
+      u.insert(u.end(), (toAdd + 1) / 2, T(1));
+    } else {
+      int toRm = u.size() - s;
+      for (int i = 0; i < toRm / 2; ++i) u.pop_back();
+      auto it = u.begin();
+      std::advance(it, s);
+      if (it < u.end()) u.erase(it, u.end());
+    }
+  }
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME("random access and modify", (T1)) {
+  static const int n = 1024 * 1024 * 16;
+  Vector u(n);
+  O(u);
+  clear_icache(); auto b = readTSC();
+  int j = 7;
+  for (int i = 0; i < 100000; ++i) {
+    j = (j * 2 + j) ^ 0xdeadbeef;
+    j = j & (n - 1);
+    u[j] = i;
+    u.at(n - j) = -i;
+  }
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+TIME_N("iterate", (T1)) {
+  static Vector u(N);
+  O(u);
+  clear_icache(); auto b = readTSC();
+  int acc = 0;
+  for (auto& e : u) {
+    acc += e;
+    e++;
+  }
+  auto e = readTSC();
+  O(acc);
+  O(u);
+  return e - b;
+}
+
+TIME("emplace massive", BASICS) {
+  clear_icache(); auto b = readTSC();
+  Vector u;
+  for (int i = 0; i < 10000000; ++i) {
+    u.emplace_back(0);
+  }
+  auto e = readTSC();
+  O(u);
+  return e - b;
+}
+
+// #endif
+
+//=============================================================================
+
+int main() {
+  return 0;
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/stl_tests/OFBVector.h
@@ -0,0 +1,924 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// Andrei Alexandrescu (aalexandre)
+
+/**
+ * Vector type. Drop-in replacement for std::vector featuring
+ * significantly faster primitives, see e.g. benchmark results at
+ * https:*phabricator.fb.com/D235852.
+ *
+ * In order for a type to be used with fbvector, it must be
+ * relocatable, see Traits.h.
+ *
+ * For user-defined types you must specialize templates
+ * appropriately. Consult Traits.h for ways to do so and for a handy
+ * family of macros FOLLY_ASSUME_FBVECTOR_COMPATIBLE*.
+ *
+ * For more information and documentation see folly/docs/FBVector.md
+ */
+
+#ifndef FOLLY_FBVECTOR_H_
+#define FOLLY_FBVECTOR_H_
+
+#include "folly/Foreach.h"
+#include "folly/Malloc.h"
+#include "folly/Traits.h"
+#include <iterator>
+#include <algorithm>
+#include <stdexcept>
+#include <limits>
+#include <cassert>
+#include <boost/type_traits.hpp>
+#include <boost/operators.hpp>
+#include <boost/utility/enable_if.hpp>
+#include <type_traits>
+
+namespace folly {
+/**
+ * Forward declaration for use by FOLLY_ASSUME_FBVECTOR_COMPATIBLE_2,
+ * see folly/Traits.h.
+ */
+template <typename T, class Allocator = std::allocator<T> >
+class fbvector;
+}
+
+// You can define an fbvector of fbvectors.
+FOLLY_ASSUME_FBVECTOR_COMPATIBLE_2(folly::fbvector);
+
+namespace folly {
+namespace fbvector_detail {
+
+/**
+ * isForwardIterator<T>::value yields true if T is a forward iterator
+ * or better, and false otherwise.
+ */
+template <class It> struct isForwardIterator {
+  enum { value = boost::is_convertible<
+         typename std::iterator_traits<It>::iterator_category,
+         std::forward_iterator_tag>::value
+  };
+};
+
+/**
+ * Destroys all elements in the range [b, e). If the type referred to
+ * by the iterators has a trivial destructor, does nothing.
+ */
+template <class It>
+void destroyRange(It b, It e) {
+  typedef typename boost::remove_reference<decltype(*b)>::type T;
+  if (boost::has_trivial_destructor<T>::value) return;
+  for (; b != e; ++b) {
+    (*b).~T();
+  }
+}
+
+/**
+ * Moves the "interesting" part of value to the uninitialized memory
+ * at address addr, and leaves value in a destroyable state.
+ */
+
+template <class T>
+typename boost::enable_if_c<
+  boost::has_trivial_assign<T>::value
+>::type
+uninitialized_destructive_move(T& value, T* addr) {
+  // Just assign the thing; this is most efficient
+  *addr = value;
+}
+
+template <class T>
+typename boost::enable_if_c<
+  !boost::has_trivial_assign<T>::value &&
+  boost::has_nothrow_constructor<T>::value
+>::type
+uninitialized_destructive_move(T& value, T* addr) {
+  // Cheap default constructor - move and reinitialize
+  memcpy(addr, &value, sizeof(T));
+  new(&value) T;
+}
+
+template <class T>
+typename std::enable_if<
+  !boost::has_trivial_assign<T>::value &&
+  !boost::has_nothrow_constructor<T>::value
+>::type
+uninitialized_destructive_move(T& value, T* addr) {
+  // User defined move construction.
+
+  // TODO: we should probably prefer this over the above memcpy()
+  // version when the type has a user-defined move constructor.  We
+  // don't right now because 4.6 doesn't implement
+  // std::is_move_constructible<> yet.
+  new (addr) T(std::move(value));
+}
+
+/**
+ * Fills n objects of type T starting at address b with T's default
+ * value. If the operation throws, destroys all objects constructed so
+ * far and calls free(b).
+ */
+template <class T>
+void uninitializedFillDefaultOrFree(T * b, size_t n) {
+  if (boost::is_arithmetic<T>::value || boost::is_pointer<T>::value) {
+    if (n <= 16384 / sizeof(T)) {
+      memset(b, 0, n * sizeof(T));
+    } else {
+      goto duff_fill;
+    }
+  } else if (boost::has_nothrow_constructor<T>::value) {
+    duff_fill:
+    auto i = b;
+    auto const e1 = b + (n & ~size_t(7));
+    for (; i != e1; i += 8) {
+      new(i) T();
+      new(i + 1) T();
+      new(i + 2) T();
+      new(i + 3) T();
+      new(i + 4) T();
+      new(i + 5) T();
+      new(i + 6) T();
+      new(i + 7) T();
+    }
+    for (auto const e = b + n; i != e; ++i) {
+      new(i) T();
+    }
+  } else {
+    // Conservative approach
+    auto i = b;
+    try {
+      for (auto const e = b + n; i != e; ++i) {
+        new(i) T;
+      }
+    } catch (...) {
+      destroyRange(b, i);
+      free(b);
+      throw;
+    }
+  }
+}
+
+/**
+ * Fills n objects of type T starting at address b with value. If the
+ * operation throws, destroys all objects constructed so far and calls
+ * free(b).
+ */
+template <class T>
+void uninitializedFillOrFree(T * b, size_t n, const T& value) {
+  auto const e = b + n;
+  if (FOLLY_IS_TRIVIALLY_COPYABLE(T)) {
+    auto i = b;
+    auto const e1 = b + (n & ~size_t(7));
+    for (; i != e1; i += 8) {
+      new(i) T(value);
+      new(i + 1) T(value);
+      new(i + 2) T(value);
+      new(i + 3) T(value);
+      new(i + 4) T(value);
+      new(i + 5) T(value);
+      new(i + 6) T(value);
+      new(i + 7) T(value);
+    }
+    for (; i != e; ++i) {
+      new(i) T(value);
+    }
+  } else {
+    // Conservative approach
+    auto i = b;
+    try {
+      for (; i != e; ++i) {
+        new(i) T(value);
+      }
+    } catch (...) {
+      destroyRange(b, i);
+      free(b);
+      throw;
+    }
+  }
+}
+} // namespace fbvector_detail
+
+/**
+ * This is the std::vector replacement. For conformity, fbvector takes
+ * the same template parameters, but it doesn't use the
+ * allocator. Instead, it uses malloc, and when present, jemalloc's
+ * extensions.
+ */
+template <class T, class Allocator>
+class fbvector : private boost::totally_ordered<fbvector<T,Allocator> > {
+  bool isSane() const {
+    return
+      begin() <= end() &&
+      empty() == (size() == 0) &&
+      empty() == (begin() == end()) &&
+      size() <= max_size() &&
+      capacity() <= max_size() &&
+      size() <= capacity() &&
+
+      // Either we have no capacity or our pointers should make sense:
+      ((!b_ && !e_ && !z_) || (b_ != z_ && e_ <= z_));
+  }
+
+  struct Invariant {
+#ifndef NDEBUG
+    explicit Invariant(const fbvector& s) : s_(s) {
+      assert(s_.isSane());
+    }
+    ~Invariant() {
+      assert(s_.isSane());
+    }
+  private:
+    const fbvector& s_;
+#else
+    explicit Invariant(const fbvector&) {}
+#endif
+    Invariant& operator=(const Invariant&);
+  };
+
+public:
+
+// types:
+  typedef T value_type;
+  typedef value_type& reference;
+  typedef const value_type& const_reference;
+  typedef T* iterator;
+  typedef const T* const_iterator;
+  typedef size_t size_type;
+  typedef ssize_t difference_type;
+  // typedef typename allocator_traits<Allocator>::pointer pointer;
+  // typedef typename allocator_traits<Allocator>::const_pointer const_pointer;
+  typedef Allocator allocator_type;
+  typedef typename Allocator::pointer pointer;
+  typedef typename Allocator::const_pointer const_pointer;
+  typedef std::reverse_iterator<iterator> reverse_iterator;
+  typedef std::reverse_iterator<const_iterator> const_reverse_iterator;
+
+// 23.3.6.1 construct/copy/destroy:
+  fbvector() : b_(NULL), e_(NULL), z_(NULL) {}
+
+  explicit fbvector(const Allocator&) {
+    new(this) fbvector;
+  }
+
+  explicit fbvector(const size_type n) {
+    if (n == 0) {
+      b_ = e_ = z_ = 0;
+      return;
+    }
+
+    auto const nBytes = goodMallocSize(n * sizeof(T));
+    b_ = static_cast<T*>(checkedMalloc(nBytes));
+    fbvector_detail::uninitializedFillDefaultOrFree(b_, n);
+    e_ = b_ + n;
+    z_ = b_ + nBytes / sizeof(T);
+  }
+
+  fbvector(const size_type n, const T& value) {
+    if (!n) {
+      b_ = e_ = z_ = 0;
+      return;
+    }
+
+    auto const nBytes = goodMallocSize(n * sizeof(T));
+    b_ = static_cast<T*>(checkedMalloc(nBytes));
+    fbvector_detail::uninitializedFillOrFree(b_, n, value);
+    e_ = b_ + n;
+    z_ = b_ + nBytes / sizeof(T);
+  }
+
+  fbvector(const size_type n, const T& value, const Allocator&) {
+    new(this) fbvector(n, value);
+  }
+
+  template <class InputIteratorOrNum>
+  fbvector(InputIteratorOrNum first, InputIteratorOrNum last) {
+    new(this) fbvector;
+    assign(first, last);
+  }
+
+  template <class InputIterator>
+  fbvector(InputIterator first, InputIterator last,
+           const Allocator&) {
+    new(this) fbvector(first, last);
+  }
+
+  fbvector(const fbvector& rhs) {
+    new(this) fbvector(rhs.begin(), rhs.end());
+  }
+  fbvector(const fbvector& rhs, const Allocator&) {
+    new(this) fbvector(rhs);
+  }
+
+  fbvector(fbvector&& o, const Allocator& = Allocator())
+    : b_(o.b_)
+    , e_(o.e_)
+    , z_(o.z_)
+  {
+    o.b_ = o.e_ = o.z_ = 0;
+  }
+
+  fbvector(std::initializer_list<T> il, const Allocator& = Allocator()) {
+    new(this) fbvector(il.begin(), il.end());
+  }
+
+  ~fbvector() {
+    // fbvector only works with relocatable objects. We insert this
+    // static check inside the destructor because pretty much any
+    // instantiation of fbvector<T> will generate the destructor (and
+    // therefore refuse compilation if the assertion fails). To see
+    // how you can enable IsRelocatable for your type, refer to the
+    // definition of IsRelocatable in Traits.h.
+    BOOST_STATIC_ASSERT(IsRelocatable<T>::value);
+    if (!b_) return;
+    fbvector_detail::destroyRange(b_, e_);
+    free(b_);
+  }
+  fbvector& operator=(const fbvector& rhs) {
+    assign(rhs.begin(), rhs.end());
+    return *this;
+  }
+
+  fbvector& operator=(fbvector&& v) {
+    clear();
+    swap(v);
+    return *this;
+  }
+
+  fbvector& operator=(std::initializer_list<T> il) {
+    assign(il.begin(), il.end());
+    return *this;
+  }
+
+  bool operator==(const fbvector& rhs) const {
+    return size() == rhs.size() && std::equal(begin(), end(), rhs.begin());
+  }
+
+  bool operator<(const fbvector& rhs) const {
+    return std::lexicographical_compare(begin(), end(),
+                                        rhs.begin(), rhs.end());
+  }
+
+private:
+  template <class InputIterator>
+  void assignImpl(InputIterator first, InputIterator last, boost::false_type) {
+    // Pair of iterators
+    if (fbvector_detail::isForwardIterator<InputIterator>::value) {
+      auto const oldSize = size();
+      auto const newSize = std::distance(first, last);
+
+      if (static_cast<difference_type>(oldSize) >= newSize) {
+        // No reallocation, nice
+        auto const newEnd = std::copy(first, last, b_);
+        fbvector_detail::destroyRange(newEnd, e_);
+        e_ = newEnd;
+        return;
+      }
+
+      // Must reallocate - just do it on the side
+      auto const nBytes = goodMallocSize(newSize * sizeof(T));
+      auto const b = static_cast<T*>(checkedMalloc(nBytes));
+      std::uninitialized_copy(first, last, b);
+      this->fbvector::~fbvector();
+      b_ = b;
+      e_ = b + newSize;
+      z_ = b_ + nBytes / sizeof(T);
+    } else {
+      // Input iterator sucks
+      FOR_EACH (i, *this) {
+        if (first == last) {
+          fbvector_detail::destroyRange(i, e_);
+          e_ = i;
+          return;
+        }
+        *i = *first;
+        ++first;
+      }
+      FOR_EACH_RANGE (i, first, last) {
+        push_back(*i);
+      }
+    }
+  }
+
+  void assignImpl(const size_type newSize, const T value, boost::true_type) {
+    // Arithmetic type, forward back to unambiguous definition
+    assign(newSize, value);
+  }
+
+public:
+  // Classic ambiguity (and a lot of unnecessary complexity) in
+  // std::vector: assign(10, 20) for vector<int> means "assign 10
+  // elements all having the value 20" but is intercepted by the
+  // two-iterators overload assign(first, last). So we need to
+  // disambiguate here. There is no pretty solution. We use here
+  // overloading based on is_arithmetic. Method insert has the same
+  // issue (and the same solution in this implementation).
+  template <class InputIteratorOrNum>
+  void assign(InputIteratorOrNum first, InputIteratorOrNum last) {
+    assignImpl(first, last, boost::is_arithmetic<InputIteratorOrNum>());
+  }
+
+  void assign(const size_type newSize, const T& value) {
+    if (b_ <= &value && &value < e_) {
+      // Need to check for aliased assign, sigh
+      return assign(newSize, T(value));
+    }
+
+    auto const oldSize = size();
+    if (oldSize >= newSize) {
+      // No reallocation, nice
+      auto const newEnd = b_ + newSize;
+      fbvector_detail::destroyRange(newEnd, e_);
+      e_ = newEnd;
+      return;
+    }
+
+    // Need to reallocate
+    if (reserve_in_place(newSize)) {
+      // Careful here, fill and uninitialized_fill may throw. The
+      // latter is transactional, so no need to worry about a
+      // buffer partially filled in case of exception.
+      std::fill(b_, e_, value);
+      auto const newEnd = b_ + newSize;
+      std::uninitialized_fill(e_, newEnd, value);
+      e_ = newEnd;
+      return;
+    }
+
+    // Cannot expand or jemalloc not present at all; must just
+    // allocate a new chunk and discard the old one. This is
+    // tantamount with creating a new fbvector altogether. This won't
+    // recurse infinitely; the constructor implements its own.
+    fbvector temp(newSize, value);
+    temp.swap(*this);
+  }
+
+  void assign(std::initializer_list<T> il) {
+    assign(il.begin(), il.end());
+  }
+
+  allocator_type get_allocator() const {
+    // whatevs
+    return allocator_type();
+  }
+
+// iterators:
+  iterator begin() {
+    return b_;
+  }
+  const_iterator begin() const {
+    return b_;
+  }
+  iterator end() {
+    return e_;
+  }
+  const_iterator end() const {
+    return e_;
+  }
+  reverse_iterator rbegin() {
+    return reverse_iterator(end());
+  }
+  const_reverse_iterator rbegin() const {
+    return const_reverse_iterator(end());
+  }
+  reverse_iterator rend() {
+    return reverse_iterator(begin());
+  }
+  const_reverse_iterator rend() const {
+    return const_reverse_iterator(begin());
+  }
+  const_iterator cbegin() const {
+    return b_;
+  }
+  const_iterator cend() const {
+    return e_;
+  }
+
+// 23.3.6.2 capacity:
+  size_type size() const {
+    return e_ - b_;
+  }
+
+  size_type max_size() {
+    // good luck gettin' there
+    return ~size_type(0);
+  }
+
+  void resize(const size_type sz) {
+    auto const oldSize = size();
+    if (sz <= oldSize) {
+      auto const newEnd = b_ + sz;
+      fbvector_detail::destroyRange(newEnd, e_);
+      e_ = newEnd;
+    } else {
+      // Must expand
+      reserve(sz);
+      auto newEnd = b_ + sz;
+      std::uninitialized_fill(e_, newEnd, T());
+      e_ = newEnd;
+    }
+  }
+
+  void resize(const size_type sz, const T& c) {
+    auto const oldSize = size();
+    if (sz <= oldSize) {
+      auto const newEnd = b_ + sz;
+      fbvector_detail::destroyRange(newEnd, e_);
+      e_ = newEnd;
+    } else {
+      // Must expand
+      reserve(sz);
+      auto newEnd = b_ + sz;
+      std::uninitialized_fill(e_, newEnd, c);
+      e_ = newEnd;
+    }
+  }
+
+  size_type capacity() const {
+    return z_ - b_;
+  }
+  bool empty() const {
+    return b_ == e_;
+  }
+
+private:
+  bool reserve_in_place(const size_type n) {
+    auto const crtCapacity = capacity();
+    if (n <= crtCapacity) return true;
+    if (!rallocm) return false;
+
+    // using jemalloc's API. Don't forget that jemalloc can never grow
+    // in place blocks smaller than 4096 bytes.
+    auto const crtCapacityBytes = crtCapacity * sizeof(T);
+    if (crtCapacityBytes < jemallocMinInPlaceExpandable) return false;
+
+    auto const newCapacityBytes = goodMallocSize(n * sizeof(T));
+    void* p = b_;
+    if (rallocm(&p, NULL, newCapacityBytes, 0, ALLOCM_NO_MOVE)
+        != ALLOCM_SUCCESS) {
+      return false;
+    }
+
+    // Managed to expand in place, reflect that in z_
+    assert(b_ == p);
+    z_ = b_ + newCapacityBytes / sizeof(T);
+    return true;
+  }
+
+  void reserve_with_move(const size_type n) {
+    // Here we can be sure we'll need to do a full reallocation
+    auto const crtCapacity = capacity();
+    assert(crtCapacity < n); // reserve_in_place should have taken
+                             // care of this
+    auto const newCapacityBytes = goodMallocSize(n * sizeof(T));
+    auto b = static_cast<T*>(checkedMalloc(newCapacityBytes));
+    auto const oldSize = size();
+    memcpy(b, b_, oldSize * sizeof(T));
+    // Done with the old chunk. Free but don't call destructors!
+    free(b_);
+    b_ = b;
+    e_ = b_ + oldSize;
+    z_ = b_ + newCapacityBytes / sizeof(T);
+    // done with the old chunk
+  }
+
+public:
+  void reserve(const size_type n) {
+    if (reserve_in_place(n)) return;
+    reserve_with_move(n);
+  }
+
+  void shrink_to_fit() {
+    if (!rallocm) return;
+
+    // using jemalloc's API. Don't forget that jemalloc can never
+    // shrink in place blocks smaller than 4096 bytes.
+    void* p = b_;
+    auto const crtCapacityBytes = capacity() * sizeof(T);
+    auto const newCapacityBytes = goodMallocSize(size() * sizeof(T));
+    if (crtCapacityBytes >= jemallocMinInPlaceExpandable &&
+        rallocm(&p, NULL, newCapacityBytes, 0, ALLOCM_NO_MOVE)
+        == ALLOCM_SUCCESS) {
+      // Celebrate
+      z_ = b_ + newCapacityBytes / sizeof(T);
+    }
+  }
+
+// element access
+  reference operator[](size_type n) {
+    assert(n < size());
+    return b_[n];
+  }
+  const_reference operator[](size_type n) const {
+    assert(n < size());
+    return b_[n];
+  }
+  const_reference at(size_type n) const {
+    if (n > size()) {
+      throw std::out_of_range("fbvector: index is greater than size.");
+    }
+    return (*this)[n];
+  }
+  reference at(size_type n) {
+    auto const& cThis = *this;
+    return const_cast<reference>(cThis.at(n));
+  }
+  reference front() {
+    assert(!empty());
+    return *b_;
+  }
+  const_reference front() const {
+    assert(!empty());
+    return *b_;
+  }
+  reference back()  {
+    assert(!empty());
+    return e_[-1];
+  }
+  const_reference back() const {
+    assert(!empty());
+    return e_[-1];
+  }
+
+// 23.3.6.3 data access
+  T* data() {
+    return b_;
+  }
+  const T* data() const {
+    return b_;
+  }
+
+private:
+  size_t computePushBackCapacity() const {
+    return empty() ? std::max(64 / sizeof(T), size_t(1))
+      : capacity() < jemallocMinInPlaceExpandable ? capacity() * 2
+      : (capacity() * 3) / 2;
+  }
+
+public:
+// 23.3.6.4 modifiers:
+  template <class... Args>
+  void emplace_back(Args&&... args) {
+    if (e_ == z_) {
+      if (!reserve_in_place(size() + 1)) {
+        reserve_with_move(computePushBackCapacity());
+      }
+    }
+    new (e_) T(std::forward<Args>(args)...);
+    ++e_;
+  }
+
+  void push_back(T x) {
+    if (e_ == z_) {
+      if (!reserve_in_place(size() + 1)) {
+        reserve_with_move(computePushBackCapacity());
+      }
+    }
+    fbvector_detail::uninitialized_destructive_move(x, e_);
+    ++e_;
+  }
+
+private:
+  bool expand() {
+    if (!rallocm) return false;
+    auto const capBytes = capacity() * sizeof(T);
+    if (capBytes < jemallocMinInPlaceExpandable) return false;
+    auto const newCapBytes = goodMallocSize(capBytes + sizeof(T));
+    void * bv = b_;
+    if (rallocm(&bv, NULL, newCapBytes, 0, ALLOCM_NO_MOVE) != ALLOCM_SUCCESS) {
+      return false;
+    }
+    // Managed to expand in place
+    assert(bv == b_); // nothing moved
+    z_ = b_ + newCapBytes / sizeof(T);
+    assert(capacity() > capBytes / sizeof(T));
+    return true;
+  }
+
+public:
+  void pop_back() {
+    assert(!empty());
+    --e_;
+    if (!boost::has_trivial_destructor<T>::value) {
+      e_->T::~T();
+    }
+  }
+  // template <class... Args>
+  // iterator emplace(const_iterator position, Args&&... args);
+
+  iterator insert(const_iterator position, T x) {
+    size_t newSize; // intentionally uninitialized
+    if (e_ == z_ && !reserve_in_place(newSize = size() + 1)) {
+      // Can't reserve in place, make a copy
+      auto const offset = position - cbegin();
+      fbvector tmp;
+      tmp.reserve(newSize);
+      memcpy(tmp.b_, b_, offset * sizeof(T));
+      fbvector_detail::uninitialized_destructive_move(
+        x,
+        tmp.b_ + offset);
+      memcpy(tmp.b_ + offset + 1, b_ + offset, (size() - offset) * sizeof(T));
+      // Brutally reassign this to refer to tmp's guts
+      free(b_);
+      b_ = tmp.b_;
+      e_ = b_ + newSize;
+      z_ = tmp.z_;
+      // get rid of tmp's guts
+      new(&tmp) fbvector;
+      return begin() + offset;
+    }
+    // Here we have enough room
+    memmove(const_cast<T*>(&*position) + 1,
+            const_cast<T*>(&*position),
+            sizeof(T) * (e_ - position));
+    fbvector_detail::uninitialized_destructive_move(
+      x,
+      const_cast<T*>(&*position));
+    ++e_;
+    return const_cast<iterator>(position);
+  }
+
+  iterator insert(const_iterator position, const size_type n, const T& x) {
+    if (e_ + n >= z_) {
+      if (b_ <= &x && &x < e_) {
+        // Ew, aliased insert
+        auto copy = x;
+        return insert(position, n, copy);
+      }
+      auto const m = position - b_;
+      reserve(size() + n);
+      position = b_ + m;
+    }
+    memmove(const_cast<T*>(position) + n,
+            position,
+            sizeof(T) * (e_ - position));
+    if (FOLLY_IS_TRIVIALLY_COPYABLE(T)) {
+      std::uninitialized_fill(const_cast<T*>(position),
+                              const_cast<T*>(position) + n,
+                              x);
+    } else {
+      try {
+        std::uninitialized_fill(const_cast<T*>(position),
+                                const_cast<T*>(position) + n,
+                                x);
+      } catch (...) {
+        // Oops, put things back where they were
+        memmove(const_cast<T*>(position),
+                position + n,
+                sizeof(T) * (e_ - position));
+        throw;
+      }
+    }
+    e_ += n;
+    return const_cast<iterator>(position);
+  }
+
+private:
+  template <class InputIterator>
+  iterator insertImpl(const_iterator position,
+                      InputIterator first, InputIterator last,
+                      boost::false_type) {
+    // Pair of iterators
+    if (fbvector_detail::isForwardIterator<InputIterator>::value) {
+      // Can compute distance
+      auto const n = std::distance(first, last);
+      if (e_ + n >= z_) {
+        auto const m = position - b_;
+        reserve(size() + n);
+        position = b_ + m;
+      }
+      memmove(const_cast<T*>(position) + n,
+              position,
+              sizeof(T) * (e_ - position));
+      try {
+        std::uninitialized_copy(first, last,
+                           const_cast<T*>(position));
+      } catch (...) {
+        // Oops, put things back where they were
+        memmove(const_cast<T*>(position),
+                position + n,
+                sizeof(T) * (e_ - position));
+        throw;
+      }
+      e_ += n;
+      return const_cast<iterator>(position);
+    } else {
+      // Cannot compute distance, crappy approach
+      // TODO: OPTIMIZE
+      fbvector result(cbegin(), position);
+      auto const offset = result.size();
+      FOR_EACH_RANGE (i, first, last) {
+        result.push_back(*i);
+      }
+      result.insert(result.end(), position, cend());
+      result.swap(*this);
+      return begin() + offset;
+    }
+  }
+
+  iterator insertImpl(const_iterator position,
+                      const size_type count, const T value, boost::true_type) {
+    // Forward back to unambiguous function
+    return insert(position, count, value);
+  }
+
+public:
+  template <class InputIteratorOrNum>
+  iterator insert(const_iterator position, InputIteratorOrNum first,
+                  InputIteratorOrNum last) {
+    return insertImpl(position, first, last,
+                      boost::is_arithmetic<InputIteratorOrNum>());
+  }
+
+  iterator insert(const_iterator position, std::initializer_list<T> il) {
+    return insert(position, il.begin(), il.end());
+  }
+
+  iterator erase(const_iterator position) {
+    if (position == e_) return e_;
+    auto p = const_cast<T*>(position);
+    (*p).T::~T();
+    memmove(p, p + 1, sizeof(T) * (e_ - p - 1));
+    --e_;
+    return p;
+  }
+
+  iterator erase(const_iterator first, const_iterator last) {
+    assert(first <= last);
+    auto p1 = const_cast<T*>(first);
+    auto p2 = const_cast<T*>(last);
+    fbvector_detail::destroyRange(p1, p2);
+    memmove(p1, last, sizeof(T) * (e_ - last));
+    e_ -= last - first;
+    return p1;
+  }
+
+  void swap(fbvector& rhs) {
+    std::swap(b_, rhs.b_);
+    std::swap(e_, rhs.e_);
+    std::swap(z_, rhs.z_);
+  }
+
+  void clear() {
+    fbvector_detail::destroyRange(b_, e_);
+    e_ = b_;
+  }
+
+private:
+  // Data
+  T *b_, *e_, *z_;
+};
+
+template <class T, class A>
+bool operator!=(const fbvector<T, A>& lhs,
+                const fbvector<T, A>& rhs) {
+  return !(lhs == rhs);
+}
+
+template <class T, class A>
+void swap(fbvector<T, A>& lhs, fbvector<T, A>& rhs) {
+  lhs.swap(rhs);
+}
+
+/**
+ * Resizes *v to exactly n elements.  May reallocate the vector to a
+ * smaller buffer if too much space will be left unused.
+ */
+template <class T>
+static void compactResize(folly::fbvector<T> * v, size_t size) {
+  auto const oldCap = v->capacity();
+  if (oldCap > size + 1024 && size < oldCap * 0.3) {
+    // Too much slack memory, reallocate a smaller buffer
+    auto const oldSize = v->size();
+    if (size <= oldSize) {
+      // Shrink
+      folly::fbvector<T>(v->begin(), v->begin() + size).swap(*v);
+    } else {
+      // Expand
+      folly::fbvector<T> temp;
+      temp.reserve(size);
+      copy(v->begin(), v->end(), back_inserter(temp));
+      temp.resize(size);
+      temp.swap(*v);
+    }
+  } else {
+    // Nolo contendere
+    v->resize(size);
+  }
+}
+
+} // namespace folly
+
+#endif // FOLLY_FBVECTOR_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/stl_tests/StlVectorTest.cpp
@@ -0,0 +1,2749 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// @author Nicholas Ormrod <njormrod@fb.com>
+
+/*
+
+This file contains an extensive STL compliance test suite for an STL vector
+implementation (such as FBVector).
+
+GCC 4.7 is required.
+
+*/
+
+// only compile if GCC is at least 4.7
+#if __GNUC__ > 4 || __GNUC__ == 4 && __GNUC_MINOR__ >= 7
+
+#if 0
+#define USING_STD_VECTOR
+#endif
+
+/*
+
+The insanity of this file deserves a superficial explanation.
+
+This file tests an implementation of STL vector. It is extremely comprehensive.
+If it compiles (more on that later) it generates a binary which, when run,
+exhaustively tests its vector for standard compliance.
+
+Limitations:
+-If it doesn't compile, the compiler errors are mind-boggling.
+-Not everything is testable. There are a few comments in the code where
+ the implementation must be inspected, as opposed to tested. These are very
+ simple inspections. Search for 'whitebox'.
+-It does not test boolean specialization.
+
+==========================
+How this file is organized
+
+--------------
+Data and Alloc
+
+Data is a class designed to provide diagnostics when stored in a vector. It
+counts the number of operations performed on it, can have any function
+disabled or labeled as noexcept, throws errors from anywhere that is not
+noexcept, tracks its supposed location in memory (optional), tracks
+aggregate information, and can print a trace of its action.
+
+Alloc, like Data, is a full-blown diagnostic allocator. It keeps track of
+all space it has allocated, keeps counters, throws exceptions, and can easily
+compare equal or not equal with other Allocs.
+
+These two classes have a few useful helper functions:
+isSane - checks that all the tracked variables make sense
+softReset - simplifies the variables before a test
+hardReset - brutally resets all variables to the default state
+
+--------
+STL_TEST
+
+Google test is not quite good enough for this test file, because we need to
+run tests across different input values and different types.
+
+The STL_TEST macro takes a few arguments:
+string - what is being tested
+id - unique id, passed to TEST
+restriction - requirements for test types
+parameters - which variables to range over
+
+Eg: STL_TEST("23.2.3", isCopyable, is_copy_constructible, a) { ... }
+
+The restriction is used to select which types get tested. Copy construction,
+for example, requires a data type which is copy constructible, whereas to test
+the clear operation, the data only needs to be destructible. If the type does
+not pass the restriction, then the test is not instantiated with that type (if
+it were, then there would be a compiler error).
+
+The variable names in the standard have very specific meaning. For example,
+a and b are always vectors, i and j are always external iterators, etc. These
+bindings are used in the STL_TEST - if you need a vector and an int, have
+parameters a and n.
+
+There is a list (BOOST_PP_SEQ) of test types and interface types. If the
+type passes the restriction, then the test body is instantiated with that
+type as its template parameter. Instantiation ensures that the contractual
+elements of the standard are satisfied.  Only the test types, however, and
+not the interfact types, are actually tested.
+
+If a test type passes the restriction, then it is run with a variety of
+arguments. Each variable (e.g. a and b) have a generator, which generates
+a range of values for that variable before each test. Generated values are not
+reused - they are remade for every run. This causes long runtimes, but ensures
+that corner cases are not missed.
+
+There are two implicit extra parameters, z and ticks. Ignore z. Ticks, on the
+other hand, is very important. Each is test is run multiple times with the
+same arguments; the first time with no ticks (and hence no Data or Alloc
+exceptions), and then once again for each and every location that an
+exception can be thrown. This ensures that exception corner cases are alse
+thoroughly tested.
+
+At the end of each test, a set of verification functions is run to ensure
+that nothing was corrupted.
+
+---------
+The tests
+
+All specifications from N3337 Chapter 23 (Containers) that pertains to
+vector is tested (if possible). Each aspect has a dedicated STL_TEST, so that
+there are no compounding errors. The tests are organized as they appear in
+N3337.
+
+The backbone of the testing framework is based on a small set of vector
+operations:
+-empty construction
+-copy construction (a little bit)
+-size
+-capacity
+-data
+-emplace_back
+
+These functions are used to generate and verify the tests. If they fail, then
+the cascade of errors will be enormous. They are, therefore, tested first.
+
+*/
+/*
+
+THOUGHTS:
+
+-Not all complexity checks are verified. These will be relentlessly hunted down
+ in the benchmarking phase.
+
+-It seems that initializer lists with implicit arguments are constructed before
+ being passed into the vector. When one of the constructors fails, it fails in
+ the initializer list, before it even gets to the vector. The IL, however,
+ doesn't clean up properly, and already-constructed elements are not
+ destroyed. This causes a memory leak, and the tests break, but it is not the
+ fault of the vector itself. Further, since the implementation for the
+ initializer lists is specified in the standard as calling an associated
+ function with (il.begin(), il.end()), we really just have to check the throws
+ cases for the associated functions (which all work fine). Initializer lists
+ also do not work with explicit constructors.
+
+-The implementation of std::copy from iterators prevents Data(int) from being
+ explicit. Explicitness is, perhaps, a desirable quality, but with fundamental
+ std library code like copy not supporting it, it seems impractical.
+
+*/
+
+// include the vector first, to ensure its header is self-sufficient
+#ifdef USING_STD_VECTOR
+#include <vector>
+#define VECTOR_ std::vector
+#else
+#define FOLLY_BENCHMARK_USE_NS_IFOLLY
+#include "folly/FBVector.h"
+#define VECTOR_ Ifolly::fbvector
+#endif
+
+//#define USING_STD_VECTOR
+
+#include <iostream>
+#include <sstream>
+#include <typeinfo>
+#include <type_traits>
+#include <map>
+#include <set>
+#include <string>
+#include <stdexcept>
+#include <exception>
+#include <climits>
+#include <cstddef>
+#include <iomanip>
+
+#include "folly/ScopeGuard.h"
+#include "folly/Conv.h"
+#include <boost/preprocessor.hpp>
+#include <boost/iterator/iterator_adaptor.hpp>
+#include <gflags/gflags.h>
+#include <gtest/gtest.h>
+
+using namespace std;
+using namespace folly;
+namespace Ifolly {}
+using namespace Ifolly;
+
+//=============================================================================
+//=============================================================================
+// Data type
+
+//-----------------------------------------------------------------------------
+// Flags
+
+typedef uint32_t Flags;
+
+// each method has 3 options: normal, noexcept, throw, and deleted
+// normal is the default
+// throw is mutually exclusive with noexcept
+//
+// DC - default constructor
+// CC - copy constructor
+// MC - move constructor
+// OC - other constructor
+// CA - copy assignment
+// MA - move assignment
+enum FlagVals : Flags {
+  DC_NOEXCEPT = 0x1,
+  DC_THROW    = 0x2,
+  DC_DELETE   = 0x8000,
+  CC_NOEXCEPT = 0x4,
+  CC_THROW    = 0x8,
+  CC_DELETE   = 0x10000,
+  MC_NOEXCEPT = 0x10,
+  MC_THROW    = 0x20,
+  MC_DELETE   = 0x20000,
+  OC_NOEXCEPT = 0x40,
+  OC_THROW    = 0x80,
+  // OC_DELETE - DNE
+
+  CA_NOEXCEPT = 0x100,
+  CA_THROW    = 0x200,
+  CA_DELETE   = 0x40000,
+  MA_NOEXCEPT = 0x400,
+  MA_THROW    = 0x800,
+  MA_DELETE   = 0x80000,
+
+  ALL_DELETE  = DC_DELETE | CC_DELETE | MC_DELETE
+              | CA_DELETE | MA_DELETE,
+
+  IS_RELOCATABLE
+              = 0x2000,
+
+  // for the allocator
+  PROP_COPY = 0x100000,
+  PROP_MOVE = 0x200000,
+  PROP_SWAP = 0x400000,
+};
+
+//-----------------------------------------------------------------------------
+// Deletors
+
+template <bool b> struct D0 {
+  D0() = default;
+  D0(const D0&) = default;
+  D0(D0&&) = default;
+  explicit D0(std::nullptr_t) {}
+  D0& operator=(const D0&) = default;
+  D0& operator=(D0&&) = default;
+};
+template <> struct D0<true> {
+  D0() = delete;
+  D0(const D0&) = default;
+  D0(D0&&) = default;
+  explicit D0(std::nullptr_t) {}
+  D0& operator=(const D0&) = default;
+  D0& operator=(D0&&) = default;
+};
+
+template <bool b> struct D1 {
+  D1() = default;
+  D1(const D1&) = default;
+  D1(D1&&) = default;
+  explicit D1(std::nullptr_t) {}
+  D1& operator=(const D1&) = default;
+  D1& operator=(D1&&) = default;
+};
+template <> struct D1<true> {
+  D1() = default;
+  D1(const D1&) = delete;
+  D1(D1&&) = default;
+  explicit D1(std::nullptr_t) {}
+  D1& operator=(const D1&) = default;
+  D1& operator=(D1&&) = default;
+};
+
+template <bool b> struct D2 {
+  D2() = default;
+  D2(const D2&) = default;
+  D2(D2&&) = default;
+  explicit D2(std::nullptr_t) {}
+  D2& operator=(const D2&) = default;
+  D2& operator=(D2&&) = default;
+};
+template <> struct D2<true> {
+  D2() = default;
+  D2(const D2&) = default;
+  D2(D2&&) = delete;
+  explicit D2(std::nullptr_t) {}
+  D2& operator=(const D2&) = default;
+  D2& operator=(D2&&) = default;
+};
+
+template <bool b> struct D3 {
+  D3() = default;
+  D3(const D3&) = default;
+  D3(D3&&) = default;
+  explicit D3(std::nullptr_t) {}
+  D3& operator=(const D3&) = default;
+  D3& operator=(D3&&) = default;
+};
+template <> struct D3<true> {
+  D3() = default;
+  D3(const D3&) = default;
+  D3(D3&&) = default;
+  explicit D3(std::nullptr_t) {}
+  D3& operator=(const D3&) = delete;
+  D3& operator=(D3&&) = default;
+};
+
+template <bool b> struct D4 {
+  D4() = default;
+  D4(const D4&) = default;
+  D4(D4&&) = default;
+  explicit D4(std::nullptr_t) {}
+  D4& operator=(const D4&) = default;
+  D4& operator=(D4&&) = default;
+};
+template <> struct D4<true> {
+  D4() = default;
+  D4(const D4&) = default;
+  D4(D4&&) = default;
+  explicit D4(std::nullptr_t) {}
+  D4& operator=(const D4&) = default;
+  D4& operator=(D4&&) = delete;
+};
+
+template <Flags f>
+struct Delete : D0<f & DC_DELETE>
+              , D1<f & CC_DELETE>
+              , D2<f & MC_DELETE>
+              , D3<f & CA_DELETE>
+              , D4<f & MA_DELETE> {
+  Delete() = default;
+  Delete(const Delete&) = default;
+  Delete(Delete&&) = default;
+  Delete& operator=(const Delete&) = default;
+  Delete& operator=(Delete&&) = default;
+
+  explicit Delete(std::nullptr_t)
+      : D0<f & DC_DELETE>(nullptr)
+      , D1<f & CC_DELETE>(nullptr)
+      , D2<f & MC_DELETE>(nullptr)
+      , D3<f & CA_DELETE>(nullptr)
+      , D4<f & MA_DELETE>(nullptr)
+      {}
+};
+
+//-----------------------------------------------------------------------------
+// Ticker
+
+struct TickException : std::runtime_error {
+  explicit TickException(const std::string& s)
+    : std::runtime_error("tick: " + s) {}
+};
+
+struct Ticker {
+  static int CountTicks;
+  static int TicksLeft;
+  static void Tick(const std::string& s) {
+    if (TicksLeft == 0) throw TickException(s);
+    CountTicks++;
+    TicksLeft--;
+  }
+};
+
+int Ticker::CountTicks = 0;
+int Ticker::TicksLeft = -1;
+
+template <Flags f>
+struct DataTicker : Ticker {
+  DataTicker() noexcept(f & DC_NOEXCEPT) {
+    if (!(f & DC_NOEXCEPT)) Tick("Data()");
+  }
+  DataTicker(const DataTicker&) noexcept(f & CC_NOEXCEPT) {
+    if (!(f & CC_NOEXCEPT)) Tick("Data(const Data&)");
+  }
+  DataTicker(DataTicker&&) noexcept(f & MC_NOEXCEPT) {
+    if (!(f & MC_NOEXCEPT)) Tick("Data(Data&&)");
+  }
+  explicit DataTicker(std::nullptr_t) noexcept(f & OC_NOEXCEPT) {
+    if (!(f & OC_NOEXCEPT)) Tick("Data(int)");
+  }
+  ~DataTicker() noexcept {}
+  void operator=(const DataTicker&) noexcept(f & CA_NOEXCEPT) {
+    if (!(f & CA_NOEXCEPT)) Tick("op=(const Data&)");
+  }
+  void operator=(DataTicker&&) noexcept(f & MA_NOEXCEPT) {
+    if (!(f & MA_NOEXCEPT)) Tick("op=(Data&&)");
+  }
+};
+
+//-----------------------------------------------------------------------------
+// Operation counter
+
+struct Counter {
+  static int CountDC, CountCC, CountMC, CountOC, CountCA, CountMA;
+  static int CountDestroy, CountTotalOps, CountLoggedConstruction;
+
+  Counter()                         noexcept { CountTotalOps++; CountDC++; }
+  Counter(const Counter&)           noexcept { CountTotalOps++; CountCC++; }
+  Counter(Counter&&)                noexcept { CountTotalOps++; CountMC++; }
+  explicit Counter(std::nullptr_t)  noexcept { CountTotalOps++; CountOC++; }
+  void operator=(const Counter&)    noexcept { CountTotalOps++; CountCA++; }
+  void operator=(Counter&&)         noexcept { CountTotalOps++; CountMA++; }
+  ~Counter()                      noexcept { CountTotalOps++; CountDestroy++; }
+};
+
+int Counter::CountDC = 0;
+int Counter::CountCC = 0;
+int Counter::CountMC = 0;
+int Counter::CountOC = 0;
+int Counter::CountCA = 0;
+int Counter::CountMA = 0;
+int Counter::CountDestroy = 0;
+int Counter::CountTotalOps = 0;
+int Counter::CountLoggedConstruction = 0;
+
+//-----------------------------------------------------------------------------
+// Tracker
+
+struct Tracker {
+  static int UID;
+  static std::map<int, int> UIDCount;
+  static int UIDTotal;
+  static std::map<const Tracker*, int> Locations;
+  static bool Print;
+
+  Tracker* self;
+  int uid;
+
+  Tracker(Tracker* self, int uid) : self(self), uid(uid) {}
+};
+
+template <bool isRelocatable>
+struct DataTracker : Tracker {
+  DataTracker() noexcept : Tracker(this, UID++) {
+    UIDCount[uid]++;
+    UIDTotal++;
+    if (!isRelocatable) Locations[self] = uid;
+    print("Data()");
+  }
+  DataTracker(const DataTracker& o) noexcept : Tracker(this, o.uid) {
+    UIDCount[uid]++;
+    UIDTotal++;
+    if (!isRelocatable) Locations[self] = uid;
+    print("Data(const Data&)");
+  }
+  DataTracker(DataTracker&& o) noexcept : Tracker(this, o.uid) {
+    UIDCount[uid]++;
+    UIDTotal++;
+    if (!isRelocatable) Locations[self] = uid;
+    print("Data(Data&&)");
+  }
+
+  explicit DataTracker(int uid) noexcept : Tracker(this, uid) {
+    UIDCount[uid]++;
+    UIDTotal++;
+    if (!isRelocatable) Locations[self] = uid;
+    print("Data(int)");
+  }
+
+  ~DataTracker() noexcept {
+    UIDCount[uid]--;
+    UIDTotal--;
+    if (!isRelocatable) Locations.erase(self);
+    print("~Data()");
+    uid = 0xdeadbeef;
+    self = (DataTracker*)0xfeebdaed;
+  }
+
+  DataTracker& operator=(const DataTracker& o) noexcept {
+    UIDCount[uid]--;
+    uid = o.uid;
+    UIDCount[uid]++;
+    if (!isRelocatable) Locations[self] = uid;
+    print("op=(const Data&)");
+    return *this;
+  }
+  DataTracker& operator=(DataTracker&& o) noexcept {
+    UIDCount[uid]--;
+    uid = o.uid;
+    UIDCount[uid]++;
+    if (!isRelocatable) Locations[self] = uid;
+    print("op=(Data&&)");
+    return *this;
+  }
+
+  void print(const std::string& fun) {
+    if (Print) {
+      std::cerr << std::setw(20) << fun << ": uid = " << std::setw(3) << uid;
+      if (!isRelocatable) std::cerr << ", self = " << self;
+      std::cerr << std::endl;
+    }
+  }
+};
+
+int Tracker::UID = 1234;
+std::map<int, int> Tracker::UIDCount;
+int Tracker::UIDTotal = 0;
+std::map<const Tracker*, int> Tracker::Locations;
+bool Tracker::Print = false;
+
+//-----------------------------------------------------------------------------
+//-----------------------------------------------------------------------------
+// Data
+
+template <Flags f = 0, size_t pad = 0>
+struct Data : DataTracker<f & IS_RELOCATABLE>,
+              Counter, DataTicker<f>, Delete<f> {
+  static const Flags flags = f;
+  char spacehog[pad ? pad : 1];
+
+  Data() = default;
+  Data(const Data&) = default;
+  Data(Data&&) = default;
+  /* implicit */ Data(int i)
+    : DataTracker<f & IS_RELOCATABLE>(i), Counter()
+    , DataTicker<f>(nullptr)
+    , Delete<f>(nullptr)
+  {}
+  ~Data() = default;
+  Data& operator=(const Data&) = default;
+  Data& operator=(Data&&) = default;
+
+private:
+  int operator&() const;
+};
+
+namespace folly {
+template <Flags f, size_t pad>
+struct IsRelocatable<Data<f, pad>>
+  : std::integral_constant<bool,
+      f & IS_RELOCATABLE
+    > {};
+};
+
+//-----------------------------------------------------------------------------
+//-----------------------------------------------------------------------------
+// Allocator
+
+template <typename T>
+struct isPropCopy : true_type {};
+template <Flags f, size_t pad>
+struct isPropCopy<Data<f, pad>> :
+  std::integral_constant<bool, f & PROP_COPY> {};
+
+template <typename T>
+struct isPropMove : true_type {};
+template <Flags f, size_t pad>
+struct isPropMove<Data<f, pad>> :
+  std::integral_constant<bool, f & PROP_MOVE> {};
+
+template <typename T>
+struct isPropSwap : true_type {};
+template <Flags f, size_t pad>
+struct isPropSwap<Data<f, pad>> :
+  std::integral_constant<bool, f & PROP_SWAP> {};
+
+
+struct AllocTracker {
+  static int Constructed;
+  static int Destroyed;
+  static map<void*, size_t> Allocated;
+  static map<void*, int> Owner;
+};
+int AllocTracker::Constructed = 0;
+int AllocTracker::Destroyed = 0;
+map<void*, size_t> AllocTracker::Allocated;
+map<void*, int> AllocTracker::Owner;
+
+template <class T>
+struct Alloc : AllocTracker, Ticker {
+  typedef typename std::allocator<T>::pointer pointer;
+  typedef typename std::allocator<T>::const_pointer const_pointer;
+  typedef typename std::allocator<T>::size_type size_type;
+  typedef typename std::allocator<T>::value_type value_type;
+
+  //-----
+  // impl
+
+  std::allocator<T> a;
+  int id;
+  explicit Alloc(int i = 8) : a(a), id(i) {}
+  Alloc(const Alloc& o) : a(o.a), id(o.id) {}
+  Alloc(Alloc&& o) : a(move(o.a)), id(o.id) {}
+  Alloc& operator=(const Alloc&) = default;
+  Alloc& operator=(Alloc&&) = default;
+  bool operator==(const Alloc& o) const { return a == o.a && id == o.id; }
+  bool operator!=(const Alloc& o) const { return !(*this == o); }
+
+  //---------
+  // tracking
+
+  pointer allocate(size_type n) {
+    if (n == 0) {
+      cerr << "called allocate(0)" << endl;
+      throw runtime_error("allocate fail");
+    }
+    Tick("allocate");
+    auto p = a.allocate(n);
+    Allocated[p] = n;
+    Owner[p] = id;
+    return p;
+  }
+
+  void deallocate(pointer p, size_type n) {
+    if (p == nullptr) {
+      cerr << "deallocate(nullptr, " << n << ")" << endl;
+      FAIL() << "deallocate failed";
+    }
+    if (Allocated[p] != n) {
+      cerr << "deallocate(" << p << ", " << n << ") invalid: ";
+      if (Allocated[p] == 0) cerr << "never allocated";
+      else if (Allocated[p] == -1) cerr << "already deallocated";
+      else cerr << "wrong number (want " << Allocated[p] << ")";
+      cerr << endl;
+      FAIL() << "deallocate failed";
+    }
+    if (Owner[p] != id) {
+      cerr << "deallocate(" << p << "), where pointer is owned by "
+           << Owner[p] << ", instead of self - " << id << endl;
+      FAIL() << "deallocate failed";
+    }
+    Allocated[p] = -1;
+    a.deallocate(p, n);
+  }
+
+  template <class U, class... Args>
+  void construct(U* p, Args&&... args) {
+    Tick("construct");
+    a.construct(p, std::forward<Args>(args)...);
+    Constructed++;
+  }
+
+  template <class U>
+  void destroy(U* p) {
+    Destroyed++;
+    a.destroy(p);
+  }
+
+  //--------------
+  // container ops
+
+  Alloc select_on_container_copy_construction() const {
+    Tick("select allocator for copy");
+    return Alloc(id + 1);
+  }
+
+  typedef isPropCopy<T> propagate_on_container_copy_assignment;
+  typedef isPropMove<T> propagate_on_container_move_assignment;
+  typedef isPropSwap<T> propagate_on_container_swap;
+};
+
+//=============================================================================
+//=============================================================================
+// Verification and resetting
+
+void softReset(int ticks = -1) {
+  Counter::CountLoggedConstruction +=
+    Counter::CountDC + Counter::CountCC + Counter::CountMC
+    + Counter::CountOC - Counter::CountDestroy;
+  Counter::CountDC = Counter::CountCC = Counter::CountMC
+    = Counter::CountOC = Counter::CountCA = Counter::CountMA = 0;
+  Counter::CountDestroy = Counter::CountTotalOps = 0;
+  Ticker::CountTicks = 0;
+  Ticker::TicksLeft = ticks;
+}
+
+void hardReset() {
+  Tracker::UIDCount.clear();
+  Tracker::UIDTotal = 0;
+  Tracker::Locations.clear();
+  softReset();
+  Counter::CountLoggedConstruction = 0;
+
+  AllocTracker::Constructed = 0;
+  AllocTracker::Destroyed = 0;
+  AllocTracker::Allocated.clear();
+  AllocTracker::Owner.clear();
+}
+
+int getTotal() {
+  int con = Counter::CountDC + Counter::CountCC
+          + Counter::CountMC + Counter::CountOC
+          + Counter::CountLoggedConstruction;
+  int del = Counter::CountDestroy;
+  return con - del;
+}
+
+void isSane() {
+  int tot = getTotal();
+  ASSERT_GE(tot, 0) << "more objects deleted than constructed";
+
+  ASSERT_EQ(tot, Tracker::UIDTotal)
+    << "UIDTotal has incorrect number of objects";
+
+  int altTot = 0;
+  for (const auto& kv : Tracker::UIDCount) {
+    ASSERT_TRUE(kv.second >= 0) << "there exists " << kv.second << " Data "
+      "with uid " << kv.first;
+    altTot += kv.second;
+  }
+  ASSERT_EQ(tot, altTot) << "UIDCount corrupted";
+
+  if (!Tracker::Locations.empty()) { // implied by IsRelocatable
+    ASSERT_EQ(tot, Tracker::Locations.size())
+      << "Locations has incorrect number of objects";
+    for (const auto& du : Tracker::Locations) {
+      ASSERT_EQ(du.second, du.first->uid) << "Locations contains wrong uid";
+      ASSERT_EQ(du.first, du.first->self) << "Data.self is corrupted";
+    }
+  }
+}
+
+//-----------------------------------------------------------------------------
+// Traits
+
+template <typename T>
+struct is_copy_constructibleAndAssignable
+  : std::integral_constant<bool,
+      std::is_copy_constructible<T>::value &&
+      std::is_copy_assignable<T>::value
+    > {};
+
+template <typename T>
+struct is_move_constructibleAndAssignable
+  : std::integral_constant<bool,
+      std::is_move_constructible<T>::value &&
+      std::is_move_assignable<T>::value
+    > {};
+
+template <class Vector>
+struct customAllocator
+  : std::integral_constant<bool,
+      !is_same<
+        typename Vector::allocator_type,
+        std::allocator<typename Vector::value_type>
+      >::value
+    > {};
+
+template <typename T>
+struct special_move_assignable
+  : is_move_constructibleAndAssignable<T> {};
+template <Flags f, size_t pad>
+struct special_move_assignable<Data<f, pad>>
+  : std::integral_constant<bool,
+      is_move_constructibleAndAssignable<Data<f, pad>>::value ||
+      f & PROP_MOVE
+    > {};
+
+//=============================================================================
+//=============================================================================
+// Framework
+
+//-----------------------------------------------------------------------------
+// Timing
+
+uint64_t ReadTSC() {
+   unsigned reslo, reshi;
+
+    __asm__ __volatile__  (
+    "xorl %%eax,%%eax \n cpuid \n"
+     ::: "%eax", "%ebx", "%ecx", "%edx");
+    __asm__ __volatile__  (
+    "rdtsc\n"
+     : "=a" (reslo), "=d" (reshi) );
+    __asm__ __volatile__  (
+    "xorl %%eax,%%eax \n cpuid \n"
+     ::: "%eax", "%ebx", "%ecx", "%edx");
+
+   return ((uint64_t)reshi << 32) | reslo;
+}
+
+//-----------------------------------------------------------------------------
+// New Boost
+
+#define IBOOST_PP_VARIADIC_SIZE(...) IBOOST_PP_VARIADIC_SIZE_I(__VA_ARGS__,   \
+  64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, \
+  45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, \
+  26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8,   \
+  7, 6, 5, 4, 3, 2, 1,)
+#define IBOOST_PP_VARIADIC_SIZE_I(e0, e1, e2, e3, e4, e5, e6, e7, e8, e9,     \
+  e10, e11, e12, e13, e14, e15, e16, e17, e18, e19, e20, e21, e22, e23, e24,  \
+  e25, e26, e27, e28, e29, e30, e31, e32, e33, e34, e35, e36, e37, e38, e39,  \
+  e40, e41, e42, e43, e44, e45, e46, e47, e48, e49, e50, e51, e52, e53, e54,  \
+  e55, e56, e57, e58, e59, e60, e61, e62, e63, size, ...) size
+#define IBOOST_PP_VARIADIC_TO_SEQ(args...) \
+  BOOST_PP_TUPLE_TO_SEQ(IBOOST_PP_VARIADIC_SIZE(args), (args))
+
+//-----------------------------------------------------------------------------
+// STL_TEST
+
+#define GEN_TEST(r, name, type)                                   \
+  {                                                               \
+    string atype = PrettyType<typename type::allocator_type>()(); \
+    string ptype = PrettyType<typename type::value_type>()();     \
+    SCOPED_TRACE("allocator: " + atype); {                        \
+    SCOPED_TRACE("datatype: " + ptype); {                         \
+    test_ ## name ## 3 <type> ();                                 \
+    if (::testing::Test::HasFatalFailure()) return;               \
+  }}}
+#define GEN_TYPE_TEST(r, name, type) \
+  if (0) test_I_ ## name ## 3 <type> ();
+#define GEN_RUNNABLE_TEST(r, name, type) \
+  one = test_I_ ## name ## 3 <type> () || one;
+
+#define GEN_LOOPER(r, d, arg) BOOST_PP_CAT(LOOPER_, arg)
+#define GEN_VMAKER(r, d, arg) { BOOST_PP_CAT(VMAKER_, arg) {
+#define GEN_UMAKER(r, d, arg) } BOOST_PP_CAT(UMAKER_, arg) }
+#define GEN_CLOSER(r, d, arg) BOOST_PP_CAT(CLOSER_, arg)
+
+#define TYPIFY(r, d, name) BOOST_PP_CAT(TYPIFY_, name)
+#define ARGIFY(r, d, name) TYPIFY(r, d, name) name
+
+#define MAKE_TEST(ref, name, types, restriction, argseq, rawargs...)     \
+  template <class Vector> void test_ ## name ## 2 (std::false_type) {}   \
+  template <class Vector> void test_ ## name ## 2 (std::true_type) {     \
+    BOOST_PP_SEQ_FOR_EACH(GEN_LOOPER, _, argseq)                         \
+    { SETUP {                                                            \
+    BOOST_PP_SEQ_FOR_EACH(GEN_VMAKER, _, argseq)                         \
+    {                                                                    \
+    test_ ## name <Vector, typename Vector::value_type,                  \
+      typename Vector::allocator_type> ( rawargs );                      \
+    if (::testing::Test::HasFatalFailure()) return;                      \
+    }                                                                    \
+    BOOST_PP_SEQ_FOR_EACH(GEN_UMAKER, _, BOOST_PP_SEQ_REVERSE(argseq))   \
+    } TEARDOWN }                                                         \
+    BOOST_PP_SEQ_FOR_EACH(GEN_CLOSER, _, BOOST_PP_SEQ_REVERSE(argseq))   \
+  }                                                                      \
+  template <class Vector> void test_ ## name ## 3 () {                   \
+    test_ ## name ## 2 <Vector> (std::integral_constant<bool,            \
+        restriction<typename Vector::value_type>::value &&               \
+        is_copy_constructible<typename Vector::value_type>::value        \
+      >());                                                              \
+  }                                                                      \
+                                                                         \
+  template <class Vector> bool test_I_ ## name ## 2 (std::false_type)    \
+    { return false; }                                                    \
+  template <class Vector> bool test_I_ ## name ## 2 (std::true_type) {   \
+    return true;                                                         \
+    auto f = test_ ## name <Vector,                                      \
+      typename Vector::value_type, typename Vector::allocator_type>;     \
+    return true;                                                         \
+  }                                                                      \
+  template <class Vector> bool test_I_ ## name ## 3 () {                 \
+    return test_I_ ## name ## 2 <Vector> (std::integral_constant<bool,   \
+      restriction<typename Vector::value_type>::value>());               \
+    return false;                                                        \
+  }                                                                      \
+                                                                         \
+  TEST(FBVector, name) {                                                 \
+    SCOPED_TRACE("N3337 reference: " ref);                               \
+    BOOST_PP_SEQ_FOR_EACH(GEN_TEST, name, types)                         \
+    BOOST_PP_SEQ_FOR_EACH(GEN_TYPE_TEST, name, INTERFACE_TYPES)          \
+    bool one = false;                                                    \
+    BOOST_PP_SEQ_FOR_EACH(GEN_RUNNABLE_TEST, name, types)                \
+    if (!one) FAIL() << "No tests qualified to run";                     \
+  }
+
+#define DECL(name, args...)                                                   \
+  template <class Vector, typename T, typename Allocator>                     \
+  void test_ ## name (BOOST_PP_SEQ_ENUM(BOOST_PP_SEQ_TRANSFORM(               \
+    ARGIFY, _, IBOOST_PP_VARIADIC_TO_SEQ(args))))
+
+#define STL_TEST_I(ref, name, restriction, args...)                           \
+  DECL(name, args);                                                           \
+  MAKE_TEST(ref, name, TEST_TYPES, restriction,                               \
+    IBOOST_PP_VARIADIC_TO_SEQ(args), args)                                    \
+  DECL(name, args)
+
+#define STL_TEST(ref, name, restriction, args...) \
+  STL_TEST_I(ref, name, restriction, z, ## args, ticks)
+
+//-----------------------------------------------------------------------------
+// Test Types
+
+typedef Data<> ED1;
+typedef Data<0, 4080> ED2;
+typedef Data<MC_NOEXCEPT> ED3;
+typedef Data<MC_NOEXCEPT | CC_DELETE> ED4;
+typedef Data<IS_RELOCATABLE> ED5;
+
+typedef VECTOR_<int, std::allocator<int>> _TVIS;
+typedef VECTOR_<int, Alloc<int>> _TVI;
+typedef VECTOR_<ED1, std::allocator<ED1>> _TV1;
+typedef VECTOR_<ED2, std::allocator<ED2>> _TV2;
+typedef VECTOR_<ED3, std::allocator<ED3>> _TV3;
+typedef VECTOR_<ED4, std::allocator<ED4>> _TV4;
+typedef VECTOR_<ED5, std::allocator<ED5>> _TV5v1;
+typedef VECTOR_<ED5, Alloc<ED5>> _TV5;
+
+typedef Data<PROP_COPY> EP1;
+typedef Data<PROP_MOVE> EP2;
+typedef Data<PROP_SWAP> EP3;
+
+typedef VECTOR_<EP1, Alloc<EP1>> _TP1;
+typedef VECTOR_<EP2, Alloc<EP2>> _TP2;
+typedef VECTOR_<EP3, Alloc<EP3>> _TP3;
+
+#define TEST_TYPES (_TVIS)(_TVI)(_TV1)(_TV2)(_TV3)(_TV4)(_TV5v1)(_TV5) \
+  (_TP1)(_TP2)(_TP3)
+
+typedef Data<ALL_DELETE> DD1; // unoperable
+typedef Data<DC_DELETE | CC_DELETE | MC_DELETE> DD2; // unconstructible
+typedef Data<CA_DELETE | MA_DELETE> DD3; // unassignable
+typedef Data<CC_DELETE | MC_DELETE> DD4; // uncopyable
+typedef Data<ALL_DELETE & ~DC_DELETE> DD5; // only default constructible
+typedef Data<CC_DELETE> DD6; // move-only copy construction
+typedef Data<CA_DELETE> DD7; // move-only assignment
+
+typedef Data<ALL_DELETE | PROP_MOVE> DDSMA;
+typedef VECTOR_<DDSMA, Alloc<DDSMA>> _TSpecialMA;
+
+#define INTERFACE_TYPES \
+  (_TVI)(VECTOR_<DD1>)(VECTOR_<DD2>)(VECTOR_<DD3>) \
+  (VECTOR_<DD4>)(VECTOR_<DD5>)(VECTOR_<DD6>) \
+  (VECTOR_<DD7>)(_TSpecialMA)
+
+//-----------------------------------------------------------------------------
+// Pretty printers
+
+template <typename T>
+struct PrettyType {
+  string operator()() {
+    if (is_same<T, int>::value) return "int";
+    if (is_same<T, char>::value) return "char";
+    if (is_same<T, uint64_t>::value) return "uint64_t";
+    return typeid(T).name();
+  }
+};
+
+template <Flags f, size_t pad>
+struct PrettyType<Data<f, pad>> {
+  string operator()() {
+    stringstream tpe;
+    tpe << "Data";
+
+    if ((f & DC_DELETE) ||
+        (f & CC_DELETE) ||
+        (f & MC_DELETE) ||
+        (f & CA_DELETE) ||
+        (f & MA_DELETE)) {
+      tpe << "[^";
+      if (f & DC_DELETE) tpe << " DC,";
+      if (f & CC_DELETE) tpe << " CC,";
+      if (f & MC_DELETE) tpe << " MC,";
+      if (f & CA_DELETE) tpe << " CA,";
+      if (f & MA_DELETE) tpe << " MA,";
+      tpe << "]";
+    }
+
+    if ((f & DC_NOEXCEPT) ||
+        (f & CC_NOEXCEPT) ||
+        (f & MC_NOEXCEPT) ||
+        (f & CA_NOEXCEPT) ||
+        (f & MA_NOEXCEPT)) {
+      tpe << "[safe";
+      if (f & DC_NOEXCEPT) tpe << " DC,";
+      if (f & CC_NOEXCEPT) tpe << " CC,";
+      if (f & MC_NOEXCEPT) tpe << " MC,";
+      if (f & CA_NOEXCEPT) tpe << " CA,";
+      if (f & MA_NOEXCEPT) tpe << " MA,";
+      tpe << "]";
+    }
+
+    if (f & IS_RELOCATABLE) {
+      tpe << "(relocatable)";
+    }
+
+    if (pad != 0) {
+      tpe << "{pad " << pad << "}";
+    }
+
+    return tpe.str();
+  }
+};
+
+template <typename T>
+struct PrettyType<std::allocator<T>> {
+  string operator()() {
+    return "std::allocator<" + PrettyType<T>()() + ">";
+  }
+};
+
+template <typename T>
+struct PrettyType<Alloc<T>> {
+  string operator()() {
+    return "Alloc<" + PrettyType<T>()() + ">";
+  }
+};
+
+//-----------------------------------------------------------------------------
+// Setup, teardown, runup, rundown
+
+// These four macros are run once per test. Setup and runup occur before the
+// test, teardown and rundown after. Setup and runup straddle the
+// initialization sequence, whereas rundown and teardown straddle the
+// cleanup.
+
+#define SETUP hardReset();
+#define TEARDOWN
+
+//-----------------------------------------------------------------------------
+// Types and typegens
+
+//------
+// dummy
+
+#define TYPIFY_z std::nullptr_t
+#define LOOPER_z                                 \
+  Vector* a_p = nullptr; Vector* b_p = nullptr;  \
+  typename Vector::value_type* t_p = nullptr;
+#define VMAKER_z std::nullptr_t z = nullptr;
+#define UMAKER_z                                                      \
+  verify<Vector>(0);                                                  \
+  if (::testing::Test::HasFatalFailure()) return;
+#define CLOSER_z
+
+//------
+// ticks
+
+#define VERIFICATION                                        \
+  if (b_p != nullptr) verify(t_p != nullptr ,*a_p, *b_p);   \
+  else if (a_p != nullptr) verify(t_p != nullptr, *a_p);    \
+  else verify<Vector>(t_p != nullptr);                      \
+  if (::testing::Test::HasFatalFailure()) return;
+
+#define TYPIFY_ticks int
+#define LOOPER_ticks          \
+  int _maxTicks_ = 0;         \
+  bool ticks_thrown = false;  \
+  for (int ticks = -1; ticks < _maxTicks_; ++ticks) {
+#define VMAKER_ticks                                        \
+  string ticks_st = folly::to<string>("ticks = ", ticks);   \
+  SCOPED_TRACE(ticks_st);                                   \
+  { SCOPED_TRACE("pre-run verification");                   \
+    VERIFICATION }                                          \
+  try {                                                     \
+    softReset(ticks);
+#define UMAKER_ticks _maxTicks_ = Ticker::CountTicks; }           \
+  catch (const TickException&) { ticks_thrown = true; }           \
+  catch (const std::exception& e)                                 \
+    { FAIL() << "EXCEPTION: " << e.what(); }                      \
+  catch (...)                                                     \
+    { FAIL() << "UNKNOWN EXCEPTION"; }                            \
+  if (ticks >= 0 && Ticker::CountTicks > ticks && !ticks_thrown)  \
+    FAIL() << "CountTicks = " << Ticker::CountTicks << " > "      \
+           << ticks << " = ticks"                                 \
+           << ", but no tick error was observed";                 \
+  VERIFICATION
+#define CLOSER_ticks }
+
+
+//--------------------------------------------------
+// vectors (second could be .equal, ==, or distinct)
+
+static const vector<pair<int, int>> VectorSizes = {
+  {  0, -1},
+  {  1, -1},
+  {  2, -1},
+  { 10, -1}, { 10, 1}, { 10, 0},
+  {100, -1}, {100, 1},
+
+  //{   10, -1}, {   10, 0}, {   10, 1}, {   10, 2}, {   10, 10},
+  //{  100, -1}, {  100, 0}, {  100, 1}, {  100, 2}, {  100, 10}, {  100, 100},
+  //{ 1000, -1}, { 1000, 0}, { 1000, 1}, { 1000, 2}, { 1000, 10}, { 1000, 100},
+  //  { 1000, 1000},
+};
+
+int populateIndex = 1426;
+template <class Vector>
+void populate(Vector& v, const pair<int, int>& ss) {
+  int i = 0;
+  for (; i < ss.first; ++i) {
+    v.emplace_back(populateIndex++);
+  }
+  if (ss.second >= 0) {
+    while (v.capacity() - v.size() != ss.second) {
+      v.emplace_back(populateIndex++);
+    }
+  }
+}
+
+template <typename A>
+struct allocGen {
+  static A get() { return A(); }
+};
+template <typename T>
+struct allocGen<Alloc<T>> {
+  static Alloc<T> get() {
+    static int c = 0;
+    c += 854;
+    return Alloc<T>(c);
+  }
+};
+
+#define TYPIFY_a Vector&
+#define LOOPER_a for (const auto& a_ss : VectorSizes) {
+#define VMAKER_a                                                            \
+  Vector a(allocGen<typename Vector::allocator_type>::get());               \
+  a_p = &a;                                                                 \
+  populate(*a_p, a_ss);                                                     \
+  string a_st = folly::to<string>("a (", a.size(), "/", a.capacity(), ")"); \
+  SCOPED_TRACE(a_st);
+#define UMAKER_a verify(0, a); if (::testing::Test::HasFatalFailure()) return;
+#define CLOSER_a }
+
+#define TYPIFY_b Vector&
+#define LOOPER_b for (int b_i = -2; b_i < (int)VectorSizes.size(); ++b_i) {
+#define VMAKER_b                                                            \
+  Vector b_s(allocGen<typename Vector::allocator_type>::get());             \
+  b_p = &b_s; string b_st;                                                  \
+  if (b_i == -2) {                                                          \
+    b_p = &a;                                                               \
+    b_st = "b is an alias of a";                                            \
+  }                                                                         \
+  else if (b_i == -1) {                                                     \
+    b_s.~Vector();                                                          \
+    new (&b_s) Vector(a);                                                   \
+    b_st = "b is a deep copy of a";                                         \
+  }                                                                         \
+  else {                                                                    \
+    populate(b_s, VectorSizes[b_i]);                                        \
+    b_st = folly::to<string>("b (", b_s.size(), "/", b_s.capacity(), ")");  \
+  }                                                                         \
+  Vector& b = *b_p;                                                         \
+  SCOPED_TRACE(b_st);
+#define UMAKER_b \
+  verify(0, a, b); if (::testing::Test::HasFatalFailure()) return;
+#define CLOSER_b }
+
+//----
+// int
+
+static const vector<int> nSizes = { 0, 1, 2, 9, 10, 11 };
+
+#define TYPIFY_n int
+#define LOOPER_n for (int n : nSizes) {
+#define VMAKER_n \
+  string n_st = folly::to<string>("n = ", n); SCOPED_TRACE(n_st);
+#define UMAKER_n
+#define CLOSER_n }
+
+//-----------------------
+// non-internal iterators
+
+static int ijarr[12] = { 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89 };
+static int ijarC[12] = { 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89 };
+
+#define TYPIFY_i int*
+#define LOOPER_i
+#define VMAKER_i int* i = ijarr; SCOPED_TRACE("i = fib[0]");
+#define UMAKER_i
+#define CLOSER_i
+
+#define TYPIFY_j int*
+#define LOOPER_j for (int j_i = 0; j_i < 12; ++j_i) {
+#define VMAKER_j                                          \
+  int* j = ijarr + j_i;                                   \
+  string j_st = folly::to<string>("j = fib[", j_i, "]");  \
+  SCOPED_TRACE(j_st);
+#define UMAKER_j \
+  for (int j_c = 0; j_c < 12; ++j_c) ASSERT_EQ(ijarC[j_c], ijarr[j_c]);
+#define CLOSER_j }
+
+//-------------------
+// internal iterators
+
+template <class Vector>
+std::pair<typename Vector::iterator, string>
+iterSpotter(Vector& v, int i) {
+  typename Vector::iterator it;
+  string msg;
+
+  switch(i) {
+  case 1:
+    if (v.empty()) ; // fall through
+    else {
+      it = v.begin();
+      ++it;
+      msg = "a[1]";
+      break;
+    }
+  case 0:
+    it = v.begin();
+    msg = "a.begin";
+    break;
+
+  case 2:
+    if (v.empty()) ; // fall through
+    else {
+      it = v.end();
+      --it;
+      msg = "a[-1]";
+      break;
+    }
+  case 3:
+    it = v.end();
+    msg = "a.end";
+    break;
+
+  default:
+    cerr << "internal error" << endl;
+    exit(1);
+  }
+
+  return make_pair(it, msg);
+}
+
+#define TYPIFY_p typename Vector::iterator
+#define LOOPER_p for (int p_i = 0; p_i < 4; ++p_i) {
+#define VMAKER_p                    \
+  auto p_im = iterSpotter(a, p_i);  \
+  auto& p = p_im.first;             \
+  auto& p_m = p_im.second;          \
+  SCOPED_TRACE("p = " + p_m);
+#define UMAKER_p
+#define CLOSER_p }
+
+#define TYPIFY_q typename Vector::iterator
+#define LOOPER_q for (int q_i = p_i; q_i < 4; ++q_i) {
+#define VMAKER_q                    \
+  auto q_im = iterSpotter(a, q_i);  \
+  auto& q = q_im.first;             \
+  auto& q_m = q_im.second;          \
+  SCOPED_TRACE("q = " + q_m);
+#define UMAKER_q
+#define CLOSER_q }
+
+//---------
+// datatype
+
+static const vector<int> tVals = { 0, 1, 2, 3, 17, 66, 521 };
+
+#define TYPIFY_t typename Vector::value_type&
+#define LOOPER_t for (int t_v : tVals) {
+#define VMAKER_t                                                            \
+  typename Vector::value_type t_s(t_v);                                     \
+  t_p = addressof(t_s);                                                     \
+  string t_st = folly::to<string>("t(", t_v, ")");                          \
+  if (t_v < 4 && a_p != nullptr) {                                          \
+    auto t_im = iterSpotter(*a_p, t_v);                                     \
+    if (t_im.first != a_p->end()) {                                         \
+      t_p = addressof(*t_im.first);                                         \
+      t_st = "t is " + t_im.second;                                         \
+    }                                                                       \
+  }                                                                         \
+  typename Vector::value_type& t = *t_p;                                    \
+  SCOPED_TRACE(t_st);
+#define UMAKER_t
+#define CLOSER_t }
+
+//----------
+// allocator
+
+#define TYPIFY_m typename Vector::allocator_type
+#define LOOPER_m                          \
+  int m_max = 1 + (a_p != nullptr);       \
+  for (int m_i = 0; m_i < m_max; ++m_i) {
+#define VMAKER_m                                \
+  typename Vector::allocator_type m = m_i == 0  \
+    ? typename Vector::allocator_type()         \
+    : a_p->get_allocator();
+#define UMAKER_m
+#define CLOSER_m }
+
+//-----------------------------------------------------------------------------
+// Verifiers
+
+// verify a vector
+template <class Vector>
+void verifyVector(const Vector& v) {
+  ASSERT_TRUE(v.begin() <= v.end()) << "end is before begin";
+  ASSERT_TRUE(v.empty() == (v.begin() == v.end())) << "empty != (begin == end)";
+  ASSERT_TRUE(v.size() == distance(v.begin(), v.end()))
+    << "size != end - begin";
+  ASSERT_TRUE(v.size() <= v.capacity()) << "size > capacity";
+  ASSERT_TRUE(v.capacity() <= v.max_size()) << "capacity > max_size";
+  ASSERT_TRUE(v.data() || true); // message won't print - it will just crash
+  ASSERT_TRUE(v.size() == 0 || v.data() != nullptr)
+    << "nullptr data points to at least one element";
+}
+
+void verifyAllocator(int ele, int cap) {
+  ASSERT_EQ(ele, AllocTracker::Constructed - AllocTracker::Destroyed);
+
+  int tot = 0;
+  for (auto kv : AllocTracker::Allocated)
+    if (kv.second != -1) tot += kv.second;
+  ASSERT_EQ(cap, tot) << "the allocator counts " << tot << " space, "
+    "but the vector(s) have (combined) capacity " << cap;
+}
+
+// Master verifier
+template <class Vector>
+void verify(int extras) {
+  if (!is_arithmetic<typename Vector::value_type>::value)
+    ASSERT_EQ(0 + extras, getTotal()) << "there exist Data but no vectors";
+  isSane();
+  if (::testing::Test::HasFatalFailure()) return;
+  if (customAllocator<Vector>::value) verifyAllocator(0, 0);
+}
+template <class Vector>
+void verify(int extras, const Vector& v) {
+  verifyVector(v);
+  if (!is_arithmetic<typename Vector::value_type>::value)
+    ASSERT_EQ(v.size() + extras, getTotal())
+      << "not all Data are in the vector";
+  isSane();
+  if (::testing::Test::HasFatalFailure()) return;
+  if (customAllocator<Vector>::value) verifyAllocator(v.size(), v.capacity());
+}
+template <class Vector>
+void verify(int extras, const Vector& v1, const Vector& v2) {
+  verifyVector(v1);
+  verifyVector(v2);
+  auto size = v1.size();
+  auto cap = v1.capacity();
+  if (&v1 != &v2) {
+    size += v2.size();
+    cap += v2.capacity();
+  }
+  if (!is_arithmetic<typename Vector::value_type>::value)
+    ASSERT_EQ(size + extras, getTotal()) << "not all Data are in the vector(s)";
+  isSane();
+  if (::testing::Test::HasFatalFailure()) return;
+  if (customAllocator<Vector>::value) verifyAllocator(size, cap);
+}
+
+//=============================================================================
+// Helpers
+
+// save the state of a vector
+int convertToInt(int t) {
+  return t;
+}
+template <Flags f, size_t pad>
+int convertToInt(const Data<f, pad>& t) {
+  return t.uid;
+}
+template <typename T>
+int convertToInt(const std::allocator<T>&) {
+  return -1;
+}
+template <typename T>
+int convertToInt(const Alloc<T>& a) {
+  return a.id;
+}
+
+template <class Vector>
+class DataState {
+  typedef typename Vector::size_type size_type;
+  size_type size_;
+  int* data_;
+public:
+  /* implicit */ DataState(const Vector& v) {
+    size_ = v.size();
+    if (size_ != 0) {
+      data_ = new int[size_];
+      for (size_type i = 0; i < size_; ++i) {
+        data_[i] = convertToInt(v.data()[i]);
+      }
+    } else {
+      data_ = nullptr;
+    }
+  }
+  ~DataState() {
+    delete[] data_;
+  }
+
+  bool operator==(const DataState& o) const {
+    if (size_ != o.size_) return false;
+    for (size_type i = 0; i < size_; ++i) {
+      if (data_[i] != o.data_[i]) return false;
+    }
+    return true;
+  }
+
+  int operator[](size_type i) {
+    if (i >= size_) {
+      cerr << "trying to access DataState out of bounds" << endl;
+      exit(1);
+    }
+    return data_[i];
+  }
+
+  size_type size() { return size_; }
+};
+
+// downgrade iterators
+template <typename It, class tag>
+class Transformer : public boost::iterator_adaptor<
+                            Transformer<It, tag>,
+                            It,
+                            typename iterator_traits<It>::value_type,
+                            tag
+                           > {
+  friend class boost::iterator_core_access;
+  shared_ptr<set<It>> dereferenced;
+
+public:
+  explicit Transformer(const It& it)
+    : Transformer::iterator_adaptor_(it)
+    , dereferenced(new set<It>()) {}
+
+  typename iterator_traits<It>::value_type& dereference() const {
+    if (dereferenced->find(this->base_reference()) != dereferenced->end()) {
+      cerr << "iterator dereferenced more than once" << endl;
+      exit(1);
+    }
+    dereferenced->insert(this->base_reference());
+    return *this->base_reference();
+  }
+};
+
+template <typename It>
+Transformer<It, forward_iterator_tag> makeForwardIterator(const It& it) {
+  return Transformer<It, forward_iterator_tag>(it);
+}
+template <typename It>
+Transformer<It, input_iterator_tag> makeInputIterator(const It& it) {
+  return Transformer<It, input_iterator_tag>(it);
+}
+
+// mutate a value (in contract only)
+void mutate(int& i) {
+  if (false) i = 0;
+}
+void mutate(uint64_t& i) {
+  if (false) i = 0;
+}
+template <Flags f, size_t pad>
+void mutate(Data<f, pad>& ds) {
+  if (false) ds.uid = 0;
+}
+
+//=============================================================================
+// Tests
+
+// #if 0
+
+
+
+// #else
+
+//-----------------------------------------------------------------------------
+// Container
+
+STL_TEST("23.2.1 Table 96.1-7", containerTypedefs, is_destructible) {
+  static_assert(is_same<T, typename Vector::value_type>::value,
+    "T != Vector::value_type");
+  static_assert(is_same<T&, typename Vector::reference>::value,
+    "T& != Vector::reference");
+  static_assert(is_same<const T&, typename Vector::const_reference>::value,
+    "const T& != Vector::const_reference");
+  static_assert(is_convertible<
+      typename iterator_traits<typename Vector::iterator>::iterator_category,
+      forward_iterator_tag>::value,
+    "Vector::iterator is not a forward iterator");
+  static_assert(is_same<T,
+      typename iterator_traits<typename Vector::iterator>::value_type>::value,
+    "Vector::iterator does not iterate over type T");
+  static_assert(is_convertible<
+      typename iterator_traits<typename Vector::const_iterator>
+        ::iterator_category,
+      forward_iterator_tag>::value,
+    "Vector::const_iterator is not a forward iterator");
+  static_assert(is_same<T,
+      typename iterator_traits<typename Vector::const_iterator>
+        ::value_type>::value,
+    "Vector::const_iterator does not iterate over type T");
+  static_assert(is_convertible<
+      typename Vector::iterator, typename Vector::const_iterator>::value,
+    "Vector::iterator is not convertible to Vector::const_iterator");
+  static_assert(is_signed<typename Vector::difference_type>::value,
+    "Vector::difference_type is not signed");
+  static_assert(is_same<typename Vector::difference_type,
+        typename iterator_traits<typename Vector::iterator>
+      ::difference_type>::value,
+    "Vector::difference_type != Vector::iterator::difference_type");
+  static_assert(is_same<typename Vector::difference_type,
+        typename iterator_traits<typename Vector::const_iterator>
+      ::difference_type>::value,
+    "Vector::difference_type != Vector::const_iterator::difference_type");
+  static_assert(is_unsigned<typename Vector::size_type>::value,
+    "Vector::size_type is not unsigned");
+  static_assert(sizeof(typename Vector::size_type) >=
+      sizeof(typename Vector::difference_type),
+    "Vector::size_type is smaller than Vector::difference_type");
+}
+
+STL_TEST("23.2.1 Table 96.8-9", emptyConstruction, is_destructible) {
+  Vector u;
+
+  ASSERT_TRUE(u.get_allocator() == Allocator());
+  ASSERT_EQ(0, Counter::CountTotalOps);
+
+  ASSERT_TRUE(u.empty()) << u.size();
+  ASSERT_EQ(0, u.capacity());
+
+  if (false) {
+    Vector();
+  }
+}
+
+STL_TEST("framework", populate, is_copy_constructible) {
+  // We use emplace_back to construct vectors for testing, as well as size,
+  // data, and capacity. We make sure these work before proceeding with tests.
+
+  Vector u;
+  ASSERT_EQ(0, u.size());
+  ASSERT_EQ(nullptr, u.data());
+
+  u.emplace_back(17);
+  ASSERT_EQ(1, u.size());
+  ASSERT_LT(u.capacity(), 100)
+    << "single push_back increased capacity to " << u.capacity();
+  ASSERT_NE(nullptr, u.data());
+  ASSERT_EQ(17, convertToInt(u.data()[0]))
+    << "first object did not get emplaced correctly";
+
+  for (int i = 0; i < 3; ++i) {
+    auto cap = u.capacity();
+    while (u.size() < cap) {
+      u.emplace_back(22);
+      ASSERT_EQ(cap, u.capacity()) << "Vector grew when it did not need to";
+      ASSERT_EQ(22, convertToInt(u.data()[u.size() - 1]))
+        << "push_back with excess capacity failed";
+    }
+
+    ASSERT_EQ(cap, u.size());
+
+    u.emplace_back(4);
+    ASSERT_GT(u.capacity(), cap) << "capacity did not grow on overflow";
+    ASSERT_EQ(cap + 1, u.size());
+    ASSERT_EQ(4, convertToInt(u.data()[u.size() - 1]))
+      << "grow object did not get emplaced correctly";
+  }
+}
+
+STL_TEST("23.2.1 Table 96.10-11", copyConstruction,
+          is_copy_constructible, a) {
+  const auto& ca = a;
+  DataState<Vector> dsa(ca);
+  auto am = a.get_allocator();
+
+  Vector u(ca);
+
+  ASSERT_TRUE(std::allocator_traits<Allocator>::
+    select_on_container_copy_construction(am) == u.get_allocator());
+  ASSERT_TRUE(dsa == u);
+  ASSERT_TRUE(
+    (ca.data() == nullptr && u.data() == nullptr) ||
+    (ca.data() != u.data())
+  ) << "only a shallow copy was made";
+
+  if (false) {
+    Vector(ca);
+    Vector u = ca;
+  }
+}
+
+STL_TEST("23.2.1 Table 96.12", moveConstruction, is_destructible, a) {
+  DataState<Vector> dsa(a);
+  auto m = a.get_allocator();
+
+  Vector u(move(a));
+
+  ASSERT_TRUE(m == u.get_allocator());
+  ASSERT_EQ(0, Counter::CountTotalOps);
+
+  ASSERT_TRUE(dsa == u);
+
+  if (false) {
+    Vector u = move(a);
+  }
+}
+
+STL_TEST("23.2.1 Table 96.13", moveAssignment, special_move_assignable, a, b) {
+  DataState<Vector> dsb(b);
+  auto am = a.get_allocator();
+  auto bm = b.get_allocator();
+
+  Vector& ret = a = std::move(b);
+
+  if (std::allocator_traits<Allocator>::
+      propagate_on_container_move_assignment::value) {
+    ASSERT_TRUE(bm == a.get_allocator());
+  } else {
+    ASSERT_TRUE(am == a.get_allocator());
+  }
+  ASSERT_TRUE(&ret == &a);
+  ASSERT_TRUE(&a == &b || dsb == a) << "move assignment did not create a copy";
+  // The source of the move may be left in any (albeit valid) state.
+}
+
+STL_TEST("23.2.1 Table 96.14", destructible, is_destructible) {
+  // The test generators check this clause already.
+}
+
+STL_TEST("23.2.1 Table 96.15-18", iterators, is_destructible, a) {
+  DataState<Vector> dsa(a);
+  const auto& ca = a;
+
+  auto  itb =  a.begin();
+  auto citb = ca.begin();
+  auto Citb =  a.cbegin();
+  auto  ite =  a.end();
+  auto cite = ca.end();
+  auto Cite =  a.cend();
+
+  ASSERT_EQ(0, Counter::CountTotalOps);
+
+  ASSERT_TRUE(dsa == a) << "call to begin or end modified internal data";
+
+  ASSERT_TRUE(citb == Citb) << "cv.begin != v.cbegin";
+  ASSERT_TRUE(cite == Cite) << "cv.end != v.cend";
+
+  if (ca.size() == 0) {
+    ASSERT_TRUE( itb ==  ite) << "begin != end when empty";
+    ASSERT_TRUE(Citb == Cite) << "cbegin != cend when empty";
+  } else {
+    ASSERT_TRUE( itb !=  ite) << "begin == end when non-empty";
+    ASSERT_TRUE(Citb != Cite) << "cbegin == cend when non-empty";
+  }
+
+  auto  dist = std::distance( itb,  ite);
+  auto Cdist = std::distance(Citb, Cite);
+  ASSERT_TRUE( dist == ca.size()) << "distance(begin, end) != size";
+  ASSERT_TRUE(Cdist == ca.size()) << "distance(cbegin, cend) != size";
+}
+
+STL_TEST("23.2.1 Table 96.19-20", equitable, is_arithmetic, a, b) {
+  const auto& ca = a;
+  const auto& cb = b;
+  DataState<Vector> dsa(a);
+  DataState<Vector> dsb(b);
+
+  ASSERT_TRUE((bool)(ca == cb) == (bool)(dsa == dsb))
+    << "== does not return equality";
+  ASSERT_TRUE((bool)(ca == cb) != (bool)(ca != cb))
+    << "!= is not the opposite of ==";
+
+  // Data is uncomparable, by design; therefore this test's restriction
+  // is 'is_arithmetic'
+}
+
+STL_TEST("23.2.1 Table 96.21", memberSwappable, is_destructible, a, b) {
+  if (!std::allocator_traits<Allocator>::
+        propagate_on_container_swap::value &&
+      convertToInt(a.get_allocator()) != convertToInt(b.get_allocator())) {
+    // undefined behaviour
+    return;
+  }
+
+  DataState<Vector> dsa(a);
+  DataState<Vector> dsb(b);
+  auto adata = a.data();
+  auto bdata = b.data();
+  auto am = a.get_allocator();
+  auto bm = b.get_allocator();
+
+  try {
+    a.swap(b);
+  } catch (...) {
+    FAIL() << "swap is noexcept";
+  }
+
+  if (std::allocator_traits<Allocator>::
+      propagate_on_container_swap::value) {
+    ASSERT_TRUE(bm == a.get_allocator());
+    ASSERT_TRUE(am == b.get_allocator());
+  } else {
+    ASSERT_TRUE(am == a.get_allocator());
+    ASSERT_TRUE(bm == b.get_allocator());
+  }
+  ASSERT_EQ(0, Counter::CountTotalOps);
+
+  ASSERT_TRUE(adata == b.data() && bdata == a.data());
+  ASSERT_TRUE(dsa == b && dsb == a) << "swap did not swap";
+}
+
+STL_TEST("23.2.1 Table 96.22", nonmemberSwappable,
+         is_destructible, a, b) {
+  if (!std::allocator_traits<Allocator>::
+        propagate_on_container_swap::value &&
+      convertToInt(a.get_allocator()) != convertToInt(b.get_allocator())) {
+    // undefined behaviour
+    return;
+  }
+
+  DataState<Vector> dsa(a);
+  DataState<Vector> dsb(b);
+  auto adata = a.data();
+  auto bdata = b.data();
+  auto am = a.get_allocator();
+  auto bm = b.get_allocator();
+
+  try {
+    swap(a, b);
+  } catch (...) {
+    FAIL() << "swap is noexcept";
+  }
+
+  if (std::allocator_traits<Allocator>::
+      propagate_on_container_swap::value) {
+    ASSERT_TRUE(bm == a.get_allocator());
+    ASSERT_TRUE(am == b.get_allocator());
+  } else {
+    ASSERT_TRUE(am == a.get_allocator());
+    ASSERT_TRUE(bm == b.get_allocator());
+  }
+  ASSERT_EQ(0, Counter::CountTotalOps);
+
+  ASSERT_TRUE(adata == b.data() && bdata == a.data());
+  ASSERT_TRUE(dsa == b && dsb == a) << "swap did not swap";
+}
+
+STL_TEST("23.2.1 Table 96.23", copyAssign,
+          is_copy_constructibleAndAssignable, a, b) {
+  // it is possible to make use of just the copy constructor.
+
+  #ifdef USING_STD_VECTOR
+  if (std::allocator_traits<Allocator>::
+        propagate_on_container_copy_assignment::value &&
+      convertToInt(a.get_allocator()) != convertToInt(b.get_allocator())) {
+    // Bug. By the looks of things, in the above case, their bez is being
+    // cleared and deallocated, but then the garbage pointers are being used.
+    return;
+  }
+  #endif
+
+  const auto& cb = b;
+  DataState<Vector> dsb(cb);
+  auto am = a.get_allocator();
+  auto bm = b.get_allocator();
+
+  Vector& ret = a = cb;
+
+  if (std::allocator_traits<Allocator>::
+      propagate_on_container_copy_assignment::value) {
+    ASSERT_TRUE(bm == a.get_allocator());
+  } else {
+    ASSERT_TRUE(am == a.get_allocator());
+  }
+  ASSERT_TRUE(&ret == &a);
+  ASSERT_TRUE(dsb == a) << "copy-assign not equal to original";
+}
+
+STL_TEST("23.2.1 Table 96.24-26", sizeops, is_destructible) {
+  // This check generators check this clause already.
+}
+
+//-----------------------------------------------------------------------------
+// Reversible container
+
+STL_TEST("23.2.1 Table 97.1-2", reversibleContainerTypedefs,
+          is_destructible) {
+  static_assert(is_same<typename Vector::reverse_iterator,
+      std::reverse_iterator<typename Vector::iterator>>::value,
+    "Vector::reverse_iterator != reverse_iterator<Vector:iterator");
+  static_assert(is_same<typename Vector::const_reverse_iterator,
+      std::reverse_iterator<typename Vector::const_iterator>>::value,
+    "Vector::const_reverse_iterator != "
+    "const_reverse_iterator<Vector::iterator");
+}
+
+STL_TEST("23.2.1 Table 97.3-5", reversibleIterators, is_destructible, a) {
+  const auto& ca = a;
+  DataState<Vector> ds(a);
+
+  auto  ritb =  a.rbegin();
+  auto critb = ca.rbegin();
+  auto Critb =  a.crbegin();
+  auto  rite =  a.rend();
+  auto crite = ca.rend();
+  auto Crite =  a.crend();
+
+  ASSERT_EQ(0, Counter::CountTotalOps);
+
+  ASSERT_TRUE(ds == a) << "call to rbegin or rend modified internal data";
+
+  ASSERT_TRUE(critb == Critb) << "cv.rbegin != v.crbegin";
+  ASSERT_TRUE(crite == Crite) << "cv.rend != v.crend";
+
+  if (ca.size() == 0) {
+    ASSERT_TRUE( ritb ==  rite) << "rbegin != rend when empty";
+    ASSERT_TRUE(Critb == Crite) << "crbegin != crend when empty";
+  } else {
+    ASSERT_TRUE( ritb !=  rite) << "rbegin == rend when non-empty";
+    ASSERT_TRUE(Critb != Crite) << "crbegin == crend when non-empty";
+  }
+
+  auto  dist = std::distance( ritb,  rite);
+  auto Cdist = std::distance(Critb, Crite);
+  ASSERT_TRUE( dist == ca.size()) << "distance(rbegin, rend) != size";
+  ASSERT_TRUE(Cdist == ca.size()) << "distance(crbegin, crend) != size";
+}
+
+//-----------------------------------------------------------------------------
+// Lexicographical functions
+
+STL_TEST("23.2.1 Table 98", comparable, is_arithmetic) {
+  const Vector v1 = { 1, 2, 3, 4 };
+  const Vector v2 = { 1, 2, 3, 4, 5 };
+  const Vector v3 = { 1, 2, 2 };
+  const Vector v4 = { 1, 2, 2, 4, 5 };
+  const Vector v5 = { };
+  const Vector v6 = { 1, 2, 3, 4 };
+
+  ASSERT_TRUE(v1 < v2);
+  ASSERT_TRUE(v1 > v3);
+  ASSERT_TRUE(v1 > v4);
+  ASSERT_TRUE(v1 > v5);
+  ASSERT_TRUE(v1 <= v6);
+  ASSERT_TRUE(v1 >= v6);
+}
+
+//-----------------------------------------------------------------------------
+// Allocator-aware requirements (AA)
+
+STL_TEST("23.2.1 Table 99.1", allocatorTypedefs, is_destructible) {
+  static_assert(is_same<T, typename Vector::allocator_type::value_type>::value,
+    "Vector and vector's allocator value_type mismatch");
+}
+
+STL_TEST("23.2.1 Table 99.2", getAllocator, is_destructible) {
+  // whitebox: ensure that a.get_allocator() returns a copy of its allocator
+}
+
+STL_TEST("23.2.1 Table 99.3", defaultAllocator, is_destructible) {
+  // there is nothing new to test here
+}
+
+STL_TEST("23.2.1 Table 99.4", customAllocator, is_destructible, m) {
+  const auto& cm = m;
+
+  Vector u(cm);
+
+  ASSERT_TRUE(u.get_allocator() == m);
+
+  if (false) {
+    Vector(m);
+  }
+}
+
+STL_TEST("23.2.1 Table 99.5", copyWithAllocator, is_copy_constructible, a, m) {
+  DataState<Vector> dsa(a);
+  const auto& ca = a;
+  const auto& cm = m;
+
+  Vector u(ca, cm);
+
+  ASSERT_TRUE(u.get_allocator() == m);
+  ASSERT_TRUE(dsa == u);
+  ASSERT_TRUE(
+    (ca.data() == nullptr && u.data() == nullptr) ||
+    (ca.data() != u.data())
+  ) << "only a shallow copy was made";
+}
+
+STL_TEST("23.2.1 Table 99.6", moveConstructionWithAllocator,
+         is_destructible, a) {
+  // there is nothing new to test here
+}
+
+STL_TEST("23.2.1 Table 99.6", moveConstructionWithAllocatorSupplied,
+         is_move_constructible, a, m) {
+  bool deep = m != a.get_allocator();
+  auto osize = a.size();
+  auto oalloc = AllocTracker::Constructed;
+  const auto& cm = m;
+
+  Vector u(std::move(a), cm);
+
+  ASSERT_TRUE(u.get_allocator() == m);
+
+  if (deep) {
+    if (!AllocTracker::Allocated.empty()) {
+      ASSERT_EQ(osize, AllocTracker::Constructed - oalloc);
+    }
+  } else {
+    ASSERT_EQ(0, Counter::CountTotalOps);
+  }
+}
+
+STL_TEST("23.2.1 Table 99.7-9", allocAssign, is_destructible) {
+  // there is nothing new to test here
+}
+
+STL_TEST("23.2.1-7", nAllocConstruction, is_copy_constructible, n, m) {
+  #ifndef USING_STD_VECTOR
+  const auto& cm = m;
+
+  Vector u(n, cm);
+
+  ASSERT_TRUE(m == u.get_allocator());
+  #endif
+}
+
+STL_TEST("23.2.1-7", nCopyAllocConstruction, is_copy_constructible, n, t, m) {
+  const auto& cm = m;
+  const auto& ct = t;
+
+  Vector u(n, ct, cm);
+
+  ASSERT_TRUE(m == u.get_allocator());
+}
+
+STL_TEST("23.2.1-7", forwardIteratorAllocConstruction,
+         is_destructible, i, j, m) {
+  auto fi = makeForwardIterator(i);
+  auto fj = makeForwardIterator(j);
+  const auto& cfi = fi;
+  const auto& cfj = fj;
+  const auto& cm = m;
+
+  Vector u(cfi, cfj, cm);
+
+  ASSERT_TRUE(m == u.get_allocator());
+}
+
+STL_TEST("23.2.1-7", inputIteratorAllocConstruction,
+         is_move_constructible, i, j, m) {
+  #ifdef USING_STD_VECTOR
+  if (Ticker::TicksLeft >= 0) return;
+  #endif
+
+  auto ii = makeInputIterator(i);
+  auto ij = makeInputIterator(j);
+  const auto& cii = ii;
+  const auto& cij = ij;
+  const auto& cm = m;
+
+  Vector u(cii, cij, cm);
+
+  ASSERT_TRUE(m == u.get_allocator());
+}
+
+STL_TEST("23.2.1-7", ilAllocConstruction, is_arithmetic, m) {
+  // gcc fail
+  if (Ticker::TicksLeft >= 0) return;
+
+  const auto& cm = m;
+
+  Vector u({ 1, 4, 7 }, cm);
+
+  ASSERT_TRUE(m == u.get_allocator());
+}
+
+//-----------------------------------------------------------------------------
+// Data races
+
+STL_TEST("23.2.2", dataRaces, is_destructible) {
+  if (false) {
+    const Vector* cv = nullptr;
+    typename Vector::size_type* s = nullptr;
+
+    cv->begin();
+    cv->end();
+    cv->rbegin();
+    cv->rend();
+    cv->front();
+    cv->back();
+    cv->data();
+
+    (*cv).at(*s);
+    (*cv)[*s];
+  }
+
+  // White-box: check that the non-const versions of each of the above
+  // functions is implemented in terms of (or the same as) the const version
+}
+
+//-----------------------------------------------------------------------------
+// Sequence container
+
+STL_TEST("23.2.3 Table 100.1, alt", nConstruction, is_constructible, n) {
+  Vector u(n);
+
+  ASSERT_TRUE(Allocator() == u.get_allocator());
+  ASSERT_EQ(n, u.size());
+  ASSERT_EQ(Counter::CountTotalOps, Counter::CountDC);
+}
+
+STL_TEST("23.2.3 Table 100.1", nCopyConstruction,
+         is_copy_constructible, n, t) {
+  const auto& ct = t;
+
+  Vector u(n, ct);
+
+  ASSERT_TRUE(Allocator() == u.get_allocator());
+  ASSERT_EQ(n, u.size()) << "Vector(n, t).size() != n" << endl;
+  for (const auto& val : u) ASSERT_EQ(convertToInt(t), convertToInt(val))
+    << "not all elements of Vector(n, t) are equal to t";
+}
+
+STL_TEST("23.2.3 Table 100.2", forwardIteratorConstruction,
+         is_destructible, i, j) {
+  // All data is emplace-constructible from int, so we restrict to
+  // is_destructible
+
+  auto fi = makeForwardIterator(i);
+  auto fj = makeForwardIterator(j);
+  const auto& cfi = fi;
+  const auto& cfj = fj;
+
+  Vector u(cfi, cfj);
+
+  ASSERT_TRUE(Allocator() == u.get_allocator());
+  ASSERT_LE(Counter::CountTotalOps, j-i);
+
+  ASSERT_EQ(j - i, u.size()) << "u(i,j).size() != j-i";
+  for (auto it = u.begin(); it != u.end(); ++it, ++i)
+    ASSERT_EQ(*i, convertToInt(*it)) << "u(i,j) constructed incorrectly";
+}
+
+STL_TEST("23.2.3 Table 100.2", inputIteratorConstruction,
+         is_move_constructible, i, j) {
+  #ifdef USING_STD_VECTOR
+  if (Ticker::TicksLeft >= 0) return;
+  #endif
+
+  auto ii = makeInputIterator(i);
+  auto ij = makeInputIterator(j);
+  const auto& cii = ii;
+  const auto& cij = ij;
+
+  Vector u(cii, cij);
+
+  ASSERT_TRUE(Allocator() == u.get_allocator());
+  ASSERT_EQ(j - i, u.size()) << "u(i,j).size() != j-i";
+  for (auto it = u.begin(); it != u.end(); ++it, ++i)
+    ASSERT_EQ(*i, convertToInt(*it)) << "u(i,j) constructed incorrectly";
+}
+
+STL_TEST("23.2.3 Table 100.3", ilConstruction, is_arithmetic) {
+  // whitebox: ensure that Vector(il) is implemented in terms of
+  // Vector(il.begin(), il.end())
+
+  // gcc fail
+  if (Ticker::TicksLeft >= 0) return;
+
+  Vector u = { 1, 4, 7 };
+
+  ASSERT_TRUE(Allocator() == u.get_allocator());
+  ASSERT_EQ(3, u.size()) << "u(il).size() fail";
+  int i = 1;
+  auto it = u.begin();
+  for (; it != u.end(); ++it, i += 3)
+    ASSERT_EQ(i, convertToInt(*it)) << "u(il) constructed incorrectly";
+}
+
+STL_TEST("23.2.3 Table 100.4", ilAssignment,
+         is_arithmetic, a) {
+  // whitebox: ensure that assign(il) is implemented in terms of
+  // assign(il.begin(), il.end())
+
+  // gcc fail
+  if (Ticker::TicksLeft >= 0) return;
+
+  auto am = a.get_allocator();
+
+  Vector& b = a = { 1, 4, 7 };
+
+  ASSERT_TRUE(am == a.get_allocator());
+  ASSERT_TRUE(&b == &a) << "'a = ...' did not return *this";
+
+  ASSERT_EQ(3, a.size()) << "u(il).size() fail";
+  int i = 1;
+  auto it = a.begin();
+  for (; it != a.end(); ++it, i += 3)
+    ASSERT_EQ(i, convertToInt(*it)) << "u(il) constructed incorrectly";
+}
+
+//----------------------------
+// insert-and-erase subsection
+
+template <class Vector>
+void insertNTCheck(const Vector& a, DataState<Vector>& dsa,
+                   int idx, int n, int val) {
+  ASSERT_EQ(dsa.size() + n, a.size());
+  int i = 0;
+  for (; i < idx; ++i) {
+    ASSERT_EQ(dsa[i], convertToInt(a.data()[i])) << i;
+  }
+  for (; i < idx + n; ++i) {
+    ASSERT_EQ(val, convertToInt(a.data()[i])) << i;
+  }
+  for (; i < a.size(); ++i) {
+    ASSERT_EQ(dsa[i-n], convertToInt(a.data()[i])) << i;
+  }
+}
+
+STL_TEST("23.2.3 Table 100.5", iteratorEmplacement,
+         is_move_constructibleAndAssignable, a, p) {
+  DataState<Vector> dsa(a);
+  int idx = distance(a.begin(), p);
+  auto am = a.get_allocator();
+
+  auto q = a.emplace(p, 44);
+
+  ASSERT_TRUE(am == a.get_allocator());
+  ASSERT_EQ(idx, distance(a.begin(), q)) << "incorrect iterator returned";
+  insertNTCheck(a, dsa, idx, 1, 44);
+}
+
+STL_TEST("23.2.3 Table 100.6", iteratorInsertion,
+         is_copy_constructibleAndAssignable, a, p, t) {
+  DataState<Vector> dsa(a);
+  int idx = distance(a.begin(), p);
+  int tval = convertToInt(t);
+  auto am = a.get_allocator();
+  const auto& ct = t;
+
+  auto q = a.insert(p, ct);
+
+  ASSERT_TRUE(am == a.get_allocator());
+  ASSERT_EQ(idx, distance(a.begin(), q)) << "incorrect iterator returned";
+  insertNTCheck(a, dsa, idx, 1, tval);
+}
+
+STL_TEST("23.2.3 Table 100.7", iteratorInsertionRV,
+         is_move_constructibleAndAssignable, a, p, t) {
+  // rvalue-references cannot have their address checked for aliased inserts
+  if (a.data() <= addressof(t) && addressof(t) < a.data() + a.size()) return;
+
+  DataState<Vector> dsa(a);
+  int idx = distance(a.begin(), p);
+  int tval = convertToInt(t);
+  auto am = a.get_allocator();
+
+  auto q = a.insert(p, std::move(t));
+
+  ASSERT_TRUE(am == a.get_allocator());
+  ASSERT_EQ(idx, distance(a.begin(), q)) << "incorrect iterator returned";
+  insertNTCheck(a, dsa, idx, 1, tval);
+}
+
+STL_TEST("23.2.3 Table 100.8", iteratorInsertionN,
+         is_copy_constructibleAndAssignable, a, p, n, t) {
+  DataState<Vector> dsa(a);
+  int idx = distance(a.begin(), p);
+  int tval = convertToInt(t);
+  auto am = a.get_allocator();
+  const auto& ct = t;
+
+  #ifndef USING_STD_VECTOR
+  auto q =
+  #endif
+
+  a.insert(p, n, ct);
+
+  ASSERT_TRUE(am == a.get_allocator());
+  #ifndef USING_STD_VECTOR
+  ASSERT_EQ(idx, distance(a.begin(), q)) << "incorrect iterator returned";
+  #endif
+
+  insertNTCheck(a, dsa, idx, n, tval);
+}
+
+template <class Vector>
+void insertItCheck(const Vector& a, DataState<Vector>& dsa,
+                   int idx, int* b, int* e) {
+  ASSERT_EQ(dsa.size() + (e - b), a.size());
+  int i = 0;
+  for (; i < idx; ++i) {
+    ASSERT_EQ(dsa[i], convertToInt(a.data()[i]));
+  }
+  for (; i < idx + (e - b); ++i) {
+    ASSERT_EQ(*(b + i - idx), convertToInt(a.data()[i]));
+  }
+  for (; i < a.size(); ++i) {
+    ASSERT_EQ(dsa[i - (e - b)], convertToInt(a.data()[i]));
+  }
+}
+
+STL_TEST("23.2.3 Table 100.9", iteratorInsertionIterator,
+         is_move_constructibleAndAssignable, a, p, i, j) {
+  DataState<Vector> dsa(a);
+  int idx = distance(a.begin(), p);
+
+  auto fi = makeForwardIterator(i);
+  auto fj = makeForwardIterator(j);
+  auto am = a.get_allocator();
+  const auto& cfi = fi;
+  const auto& cfj = fj;
+
+  #ifndef USING_STD_VECTOR
+  auto q =
+  #endif
+
+  a.insert(p, cfi, cfj);
+
+  ASSERT_TRUE(am == a.get_allocator());
+  #ifndef USING_STD_VECTOR
+  ASSERT_EQ(idx, distance(a.begin(), q)) << "incorrect iterator returned";
+  #endif
+
+  insertItCheck(a, dsa, idx, i, j);
+}
+
+STL_TEST("23.2.3 Table 100.9", iteratorInsertionInputIterator,
+         is_move_constructibleAndAssignable, a, p, i, j) {
+  DataState<Vector> dsa(a);
+  int idx = distance(a.begin(), p);
+
+  auto ii = makeInputIterator(i);
+  auto ij = makeInputIterator(j);
+  auto am = a.get_allocator();
+  const auto& cii = ii;
+  const auto& cij = ij;
+
+  #ifndef USING_STD_VECTOR
+  auto q =
+  #endif
+
+  a.insert(p, cii, cij);
+
+  ASSERT_TRUE(am == a.get_allocator());
+  #ifndef USING_STD_VECTOR
+  ASSERT_EQ(idx, distance(a.begin(), q)) << "incorrect iterator returned";
+  #endif
+
+  insertItCheck(a, dsa, idx, i, j);
+}
+
+STL_TEST("23.2.3 Table 100.10", iteratorInsertIL,
+         is_arithmetic, a, p) {
+  // gcc fail
+  if (Ticker::TicksLeft >= 0) return;
+
+  // whitebox: ensure that insert(p, il) is implemented in terms of
+  // insert(p, il.begin(), il.end())
+
+  DataState<Vector> dsa(a);
+  int idx = distance(a.begin(), p);
+  auto am = a.get_allocator();
+
+  #ifndef USING_STD_VECTOR
+  auto q =
+  #endif
+
+  a.insert(p, {1, 4, 7});
+
+  ASSERT_TRUE(am == a.get_allocator());
+  #ifndef USING_STD_VECTOR
+  ASSERT_EQ(idx, distance(a.begin(), q)) << "incorrect iterator returned";
+  #endif
+
+  int ila[] = { 1, 4, 7 };
+  int* i = ila;
+  int* j = ila + 3;
+  insertItCheck(a, dsa, idx, i, j);
+}
+
+template <class Vector>
+void eraseCheck(Vector& a, DataState<Vector>& dsa, int idx, int n) {
+  ASSERT_EQ(dsa.size() - n, a.size());
+  size_t i = 0;
+  auto it = a.begin();
+  for (; it != a.end(); ++it, ++i) {
+    if (i == idx) i += n;
+    ASSERT_EQ(dsa[i], convertToInt(*it));
+  }
+}
+
+STL_TEST("23.2.3 Table 100.11", iteratorErase, is_move_assignable, a, p) {
+  if (p == a.end()) return;
+
+  DataState<Vector> dsa(a);
+  int idx = distance(a.begin(), p);
+  auto am = a.get_allocator();
+
+  auto rit = a.erase(p);
+
+  ASSERT_TRUE(am == a.get_allocator());
+  ASSERT_EQ(idx, distance(a.begin(), rit)) << "wrong iterator returned";
+  eraseCheck(a, dsa, idx, 1);
+}
+
+STL_TEST("23.2.3 Table 100.12", iteratorEraseRange,
+         is_move_assignable, a, p, q) {
+  if (p == a.end()) return;
+
+  DataState<Vector> dsa(a);
+  int idx = distance(a.begin(), p);
+  auto am = a.get_allocator();
+
+  auto rit = a.erase(p, q);
+
+  ASSERT_TRUE(am == a.get_allocator());
+  ASSERT_EQ(idx, distance(a.begin(), rit)) << "wrong iterator returned";
+  eraseCheck(a, dsa, idx, distance(p,q));
+}
+
+//--------------------------------
+// end insert-and-erase subsection
+
+STL_TEST("23.2.3 Table 100.13", clear, is_destructible, a) {
+
+  auto am = a.get_allocator();
+
+  try {
+    a.clear();
+  } catch (...) {
+    FAIL() << "clear must be noexcept";
+  }
+
+  ASSERT_TRUE(am == a.get_allocator());
+  ASSERT_TRUE(a.empty());
+}
+
+STL_TEST("23.2.3 Table 100.14", assignRange, is_move_assignable, a, i, j) {
+  auto fi = makeForwardIterator(i);
+  auto fj = makeForwardIterator(j);
+  const auto& cfi = fi;
+  const auto& cfj = fj;
+  auto am = a.get_allocator();
+
+  a.assign(cfi, cfj);
+
+  ASSERT_TRUE(am == a.get_allocator());
+  ASSERT_EQ(distance(i, j), a.size());
+  for (auto it = a.begin(); it != a.end(); ++it, ++i)
+    ASSERT_EQ(*i, convertToInt(*it));
+}
+
+STL_TEST("23.2.3 Table 100.14", assignInputRange,
+         is_move_constructibleAndAssignable, a, i, j) {
+  auto ii = makeInputIterator(i);
+  auto ij = makeInputIterator(j);
+  const auto& cii = ii;
+  const auto& cij = ij;
+  auto am = a.get_allocator();
+
+  a.assign(cii, cij);
+
+  ASSERT_TRUE(am == a.get_allocator());
+  ASSERT_EQ(distance(i, j), a.size());
+  for (auto it = a.begin(); it != a.end(); ++it, ++i)
+    ASSERT_EQ(*i, convertToInt(*it));
+}
+
+STL_TEST("23.2.3 Table 100.15", assignIL,
+         is_arithmetic, a) {
+
+  // whitebox: ensure that assign(il) is implemented in terms of
+  // assign(il.begin(), il.end())
+
+  // gcc fail
+  if (Ticker::TicksLeft >= 0) return;
+
+  auto am = a.get_allocator();
+
+  a.assign({1, 4, 7});
+
+  ASSERT_TRUE(am == a.get_allocator());
+  int ila[] = { 1, 4, 7 };
+  int* i = ila;
+
+  ASSERT_EQ(3, a.size());
+  for (auto it = a.begin(); it != a.end(); ++it, ++i)
+    ASSERT_EQ(*i, convertToInt(*it));
+}
+
+STL_TEST("23.2.3 Table 100.16", assignN,
+         is_copy_constructibleAndAssignable, a, n, t) {
+  auto am = a.get_allocator();
+  auto const& ct = t;
+  auto tval = convertToInt(t);
+
+  a.assign(n, ct);
+
+  ASSERT_TRUE(am == a.get_allocator());
+  ASSERT_EQ(n, a.size());
+  for (auto it = a.begin(); it != a.end(); ++it)
+    ASSERT_EQ(tval, convertToInt(*it));
+}
+
+STL_TEST("23.2.3 Table 101.1", front, is_destructible, a) {
+  if (a.empty()) return;
+
+  ASSERT_TRUE(addressof(a.front()) == a.data());
+
+  ASSERT_EQ(0, Counter::CountTotalOps);
+
+  if (false) {
+    mutate(a.front());
+    const Vector& ca = a;
+    ca.front();
+  }
+}
+
+STL_TEST("23.2.3 Table 101.2", back, is_destructible, a) {
+  if (a.empty()) return;
+
+  ASSERT_TRUE(addressof(a.back()) == a.data() + a.size() - 1);
+
+  ASSERT_EQ(0, Counter::CountTotalOps);
+
+  if (false) {
+    mutate(a.back());
+    const Vector& ca = a;
+    ca.back();
+  }
+}
+
+STL_TEST("23.2.3 Table 101.4", emplaceBack,
+         is_move_constructible, a) {
+  DataState<Vector> dsa(a);
+  auto adata = a.data();
+  int excess = a.capacity() - a.size();
+  auto am = a.get_allocator();
+
+  try {
+    a.emplace_back(44);
+  } catch (...) {
+    ASSERT_TRUE(dsa == a) << "failed strong exception guarantee";
+    throw;
+  }
+
+  ASSERT_TRUE(am == a.get_allocator());
+  if (excess > 0) ASSERT_TRUE(a.data() == adata) << "unnecessary relocation";
+  ASSERT_EQ(dsa.size() + 1, a.size());
+  size_t i = 0;
+  auto it = a.begin();
+  for (; i < dsa.size(); ++i, ++it)
+    ASSERT_EQ(dsa[i], convertToInt(*it));
+  ASSERT_EQ(44, convertToInt(a.back()));
+}
+
+STL_TEST("23.2.3 Table 101.7", pushBack, is_copy_constructible, a, t) {
+  DataState<Vector> dsa(a);
+  int tval = convertToInt(t);
+  auto adata = a.data();
+  int excess = a.capacity() - a.size();
+  auto am = a.get_allocator();
+  const auto& ct = t;
+
+  try {
+    a.push_back(ct);
+  } catch (...) {
+    ASSERT_TRUE(dsa == a) << "failed strong exception guarantee";
+    throw;
+  }
+
+  ASSERT_TRUE(am == a.get_allocator());
+  if (excess > 0) ASSERT_TRUE(a.data() == adata) << "unnecessary relocation";
+  ASSERT_EQ(dsa.size() + 1, a.size());
+  size_t i = 0;
+  auto it = a.begin();
+  for (; i < dsa.size(); ++i, ++it)
+    ASSERT_EQ(dsa[i], convertToInt(*it));
+  ASSERT_EQ(tval, convertToInt(a.back()));
+}
+
+STL_TEST("23.2.3 Table 101.8", pushBackRV,
+         is_move_constructible, a, t) {
+  DataState<Vector> dsa(a);
+  int tval = convertToInt(t);
+  auto adata = a.data();
+  int excess = a.capacity() - a.size();
+  auto am = a.get_allocator();
+
+  try {
+    a.push_back(move(t));
+  } catch (...) {
+    ASSERT_TRUE(dsa == a) << "failed strong exception guarantee";
+    throw;
+  }
+
+  ASSERT_TRUE(am == a.get_allocator());
+  if (excess > 0) ASSERT_TRUE(a.data() == adata) << "unnecessary relocation";
+  ASSERT_EQ(dsa.size() + 1, a.size());
+  size_t i = 0;
+  auto it = a.begin();
+  for (; i < dsa.size(); ++i, ++it)
+    ASSERT_EQ(dsa[i], convertToInt(*it));
+  ASSERT_EQ(tval, convertToInt(a.back()));
+}
+
+STL_TEST("23.2.3 Table 100.10", popBack, is_destructible, a) {
+  if (a.empty()) return;
+
+  DataState<Vector> dsa(a);
+  auto am = a.get_allocator();
+
+  a.pop_back();
+
+  ASSERT_TRUE(am == a.get_allocator());
+  ASSERT_EQ(dsa.size() - 1, a.size());
+  size_t i = 0;
+  auto it = a.begin();
+  for (; it != a.end(); ++it, ++i)
+    ASSERT_EQ(dsa[i], convertToInt(*it));
+}
+
+STL_TEST("23.2.3 Table 100.11", operatorBrace, is_destructible, a) {
+  const auto& ca = a;
+  for (int i = 0; i < ca.size(); ++i)
+    ASSERT_TRUE(addressof(ca[i]) == ca.data()+i);
+
+  ASSERT_EQ(0, Counter::CountTotalOps);
+
+  if (false) {
+    mutate(a[0]);
+  }
+}
+
+STL_TEST("23.2.3 Table 100.12", at, is_destructible, a) {
+  const auto& ca = a;
+  for (int i = 0; i < ca.size(); ++i)
+    ASSERT_TRUE(addressof(ca.at(i)) == ca.data()+i);
+
+  ASSERT_EQ(0, Counter::CountTotalOps);
+
+  try {
+    ca.at(ca.size());
+    FAIL() << "at(size) should have thrown an error";
+  } catch (const std::out_of_range& e) {
+  } catch (...) {
+    FAIL() << "at(size) threw error other than out_of_range";
+  }
+
+  if (false) {
+    mutate(a.at(0));
+  }
+}
+
+STL_TEST("move iterators", moveIterators, is_copy_constructibleAndAssignable) {
+  if (false) {
+    int* i = nullptr;
+    int* j = nullptr;
+
+    auto mfi = make_move_iterator(makeForwardIterator(i));
+    auto mfj = make_move_iterator(makeForwardIterator(j));
+    auto mii = make_move_iterator(makeInputIterator(i));
+    auto mij = make_move_iterator(makeInputIterator(j));
+
+    Vector u1(mfi, mfj);
+    Vector u2(mii, mij);
+
+    u1.insert(u1.begin(), mfi, mfj);
+    u1.insert(u1.begin(), mii, mij);
+
+    u1.assign(mfi, mfj);
+    u1.assign(mii, mij);
+  }
+}
+
+//-----------------------------------------------------------------------------
+// Vector-specifics
+
+STL_TEST("23.3.6.4", dataAndCapacity, is_destructible) {
+  // there isn't anything new to test here - data and capacity are used as the
+  // backbone of DataState. The minimal testing we might want to do is already
+  // done in the populate test
+}
+
+STL_TEST("23.3.6.3", reserve, is_move_constructible, a, n) {
+  auto adata = a.data();
+  auto ocap = a.capacity();
+  auto am = a.get_allocator();
+
+  a.reserve(n);
+
+  ASSERT_TRUE(am == a.get_allocator());
+  if (n <= ocap) {
+    ASSERT_EQ(0, Counter::CountTotalOps);
+    ASSERT_TRUE(adata == a.data());
+  } else {
+    ASSERT_TRUE(a.capacity() >= n);
+    ASSERT_LE(Counter::CountTotalOps, 2*a.size()); // move and delete
+  }
+}
+
+STL_TEST("23.3.6.3", lengthError, is_move_constructible) {
+  auto mx = Vector().max_size();
+  auto big = mx+1;
+  if (mx >= big) return; // max_size is the biggest size_type; overflowed
+
+  Vector u;
+  try {
+    u.reserve(big);
+    FAIL() << "reserve(big) should have thrown an error";
+  } catch (const std::length_error& e) {
+  } catch (...) {
+    FAIL() << "reserve(big) threw error other than length_error";
+  }
+}
+
+STL_TEST("23.3.6.3", resize, is_copy_constructible, a, n) {
+  DataState<Vector> dsa(a);
+  int sz = a.size();
+  auto am = a.get_allocator();
+
+  a.resize(n);
+
+  ASSERT_TRUE(am == a.get_allocator());
+  ASSERT_EQ(n, a.size());
+
+  if (n <= sz) {
+    for (int i = 0; i < n; ++i) {
+      ASSERT_EQ(dsa[i], convertToInt(a[i]));
+    }
+  } else {
+    for (int i = 0; i < sz; ++i) {
+      ASSERT_EQ(dsa[i], convertToInt(a[i]));
+    }
+  }
+}
+
+STL_TEST("23.3.6.3", resizeT, is_copy_constructibleAndAssignable, a, n, t) {
+  #ifdef USING_STD_VECTOR
+  if (a.data() <= addressof(t) && addressof(t) < a.data() + a.size()) return;
+  #endif
+
+  DataState<Vector> dsa(a);
+  int sz = a.size();
+  auto am = a.get_allocator();
+  const auto& ct = t;
+  int val = convertToInt(t);
+
+  a.resize(n, ct);
+
+  ASSERT_TRUE(am == a.get_allocator());
+  ASSERT_EQ(n, a.size());
+
+  if (n <= sz) {
+    for (int i = 0; i < n; ++i) {
+      ASSERT_EQ(dsa[i], convertToInt(a[i]));
+    }
+  } else {
+    int i = 0;
+    for ( ; i < sz; ++i) {
+      ASSERT_EQ(dsa[i], convertToInt(a[i]));
+    }
+    for ( ; i < n; ++i) {
+      ASSERT_EQ(val, convertToInt(a[i]));
+    }
+  }
+}
+
+STL_TEST("23.3.6.3", shrinkToFit, is_move_constructible, a) {
+  bool willThrow = Ticker::TicksLeft >= 0;
+
+  a.reserve(a.capacity() * 11);
+
+  auto ocap = a.capacity();
+  DataState<Vector> dsa(a);
+
+  auto am = a.get_allocator();
+
+  try {
+    a.shrink_to_fit();
+  } catch (...) {
+    FAIL() << "shrink_to_fit should swallow errors";
+  }
+
+  ASSERT_TRUE(am == a.get_allocator());
+  ASSERT_TRUE(dsa == a);
+  if (willThrow) {
+    //ASSERT_EQ(ocap, a.capacity()); might shrink in place
+    throw TickException("I swallowed the error");
+  } else {
+    ASSERT_TRUE(a.capacity() == 0 || a.capacity() < ocap) << "Look into this";
+  }
+}
+
+#ifndef USING_STD_VECTOR
+STL_TEST("EBO", ebo, is_destructible) {
+  static_assert(!is_same<Allocator, std::allocator<T>>::value ||
+                sizeof(Vector) == 3 * sizeof(void*),
+    "fbvector has default allocator, but has size != 3*sizeof(void*)");
+}
+
+STL_TEST("relinquish", relinquish, is_destructible, a) {
+  auto sz = a.size();
+  auto cap = a.capacity();
+  auto data = a.data();
+
+  auto guts = relinquish(a);
+
+  ASSERT_EQ(data, guts);
+  ASSERT_TRUE(a.empty());
+  ASSERT_EQ(0, a.capacity());
+
+  auto alloc = a.get_allocator();
+  for (size_t i = 0; i < sz; ++i)
+    std::allocator_traits<decltype(alloc)>::destroy(alloc, guts + i);
+  if (guts != nullptr)
+    std::allocator_traits<decltype(alloc)>::deallocate(alloc, guts, cap);
+}
+
+STL_TEST("attach", attach, is_destructible, a) {
+  DataState<Vector> dsa(a);
+
+  auto sz = a.size();
+  auto cap = a.capacity();
+  auto guts = relinquish(a);
+
+  ASSERT_EQ(a.data(), nullptr);
+  attach(a, guts, sz, cap);
+
+  ASSERT_TRUE(dsa == a);
+}
+
+#endif
+
+// #endif
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+
+  return RUN_ALL_TESTS();
+}
+
+#else // GCC 4.7 guard
+
+#include <gflags/gflags.h>
+#include <gtest/gtest.h>
+
+// do nothing
+TEST(placeholder, gccversion) {}
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+
+  return RUN_ALL_TESTS();
+}
+
+#endif // GCC 4.7 guard
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/StringTest.cpp
@@ -0,0 +1,1058 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/String.h"
+
+#include <random>
+#include <boost/algorithm/string.hpp>
+#include <gtest/gtest.h>
+
+#include "folly/Benchmark.h"
+
+using namespace folly;
+using namespace std;
+
+TEST(StringPrintf, BasicTest) {
+  EXPECT_EQ("abc", stringPrintf("%s", "abc"));
+  EXPECT_EQ("abc", stringPrintf("%sbc", "a"));
+  EXPECT_EQ("abc", stringPrintf("a%sc", "b"));
+  EXPECT_EQ("abc", stringPrintf("ab%s", "c"));
+
+  EXPECT_EQ("abc", stringPrintf("abc"));
+}
+
+TEST(StringPrintf, NumericFormats) {
+  EXPECT_EQ("12", stringPrintf("%d", 12));
+  EXPECT_EQ("5000000000", stringPrintf("%ld", 5000000000UL));
+  EXPECT_EQ("5000000000", stringPrintf("%ld", 5000000000L));
+  EXPECT_EQ("-5000000000", stringPrintf("%ld", -5000000000L));
+  EXPECT_EQ("-1", stringPrintf("%d", 0xffffffff));
+  EXPECT_EQ("-1", stringPrintf("%ld", 0xffffffffffffffff));
+  EXPECT_EQ("-1", stringPrintf("%ld", 0xffffffffffffffffUL));
+
+  EXPECT_EQ("7.7", stringPrintf("%1.1f", 7.7));
+  EXPECT_EQ("7.7", stringPrintf("%1.1lf", 7.7));
+  EXPECT_EQ("7.70000000000000018",
+            stringPrintf("%.17f", 7.7));
+  EXPECT_EQ("7.70000000000000018",
+            stringPrintf("%.17lf", 7.7));
+}
+
+TEST(StringPrintf, Appending) {
+  string s;
+  stringAppendf(&s, "a%s", "b");
+  stringAppendf(&s, "%c", 'c');
+  EXPECT_EQ(s, "abc");
+  stringAppendf(&s, " %d", 123);
+  EXPECT_EQ(s, "abc 123");
+}
+
+TEST(StringPrintf, VariousSizes) {
+  // Test a wide variety of output sizes
+  for (int i = 0; i < 100; ++i) {
+    string expected(i + 1, 'a');
+    EXPECT_EQ("X" + expected + "X", stringPrintf("X%sX", expected.c_str()));
+  }
+
+  EXPECT_EQ("abc12345678910111213141516171819202122232425xyz",
+            stringPrintf("abc%d%d%d%d%d%d%d%d%d%d%d%d%d%d"
+                         "%d%d%d%d%d%d%d%d%d%d%dxyz",
+                         1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,
+                         17, 18, 19, 20, 21, 22, 23, 24, 25));
+}
+
+TEST(StringPrintf, oldStringPrintfTests) {
+  EXPECT_EQ(string("a/b/c/d"),
+            stringPrintf("%s/%s/%s/%s", "a", "b", "c", "d"));
+
+  EXPECT_EQ(string("    5    10"),
+            stringPrintf("%5d %5d", 5, 10));
+
+  // check printing w/ a big buffer
+  for (int size = (1 << 8); size <= (1 << 15); size <<= 1) {
+    string a(size, 'z');
+    string b = stringPrintf("%s", a.c_str());
+    EXPECT_EQ(a.size(), b.size());
+  }
+}
+
+TEST(StringPrintf, oldStringAppendf) {
+  string s = "hello";
+  stringAppendf(&s, "%s/%s/%s/%s", "a", "b", "c", "d");
+  EXPECT_EQ(string("helloa/b/c/d"), s);
+}
+
+BENCHMARK(new_stringPrintfSmall, iters) {
+  for (int64_t i = 0; i < iters; ++i) {
+    int32_t x = int32_t(i);
+    int32_t y = int32_t(i + 1);
+    string s =
+      stringPrintf("msg msg msg msg msg msg msg msg:  %d, %d, %s",
+                   x, y, "hello");
+  }
+}
+
+TEST(Escape, cEscape) {
+  EXPECT_EQ("hello world", cEscape<std::string>("hello world"));
+  EXPECT_EQ("hello \\\\world\\\" goodbye",
+            cEscape<std::string>("hello \\world\" goodbye"));
+  EXPECT_EQ("hello\\nworld", cEscape<std::string>("hello\nworld"));
+  EXPECT_EQ("hello\\377\\376", cEscape<std::string>("hello\xff\xfe"));
+}
+
+TEST(Escape, cUnescape) {
+  EXPECT_EQ("hello world", cUnescape<std::string>("hello world"));
+  EXPECT_EQ("hello \\world\" goodbye",
+            cUnescape<std::string>("hello \\\\world\\\" goodbye"));
+  EXPECT_EQ("hello\nworld", cUnescape<std::string>("hello\\nworld"));
+  EXPECT_EQ("hello\nworld", cUnescape<std::string>("hello\\012world"));
+  EXPECT_EQ("hello\nworld", cUnescape<std::string>("hello\\x0aworld"));
+  EXPECT_EQ("hello\xff\xfe", cUnescape<std::string>("hello\\377\\376"));
+  EXPECT_EQ("hello\xff\xfe", cUnescape<std::string>("hello\\xff\\xfe"));
+
+  EXPECT_THROW({cUnescape<std::string>("hello\\");},
+               std::invalid_argument);
+  EXPECT_THROW({cUnescape<std::string>("hello\\x");},
+               std::invalid_argument);
+  EXPECT_THROW({cUnescape<std::string>("hello\\q");},
+               std::invalid_argument);
+}
+
+TEST(Escape, uriEscape) {
+  EXPECT_EQ("hello%2c%20%2fworld", uriEscape<std::string>("hello, /world"));
+  EXPECT_EQ("hello%2c%20/world", uriEscape<std::string>("hello, /world",
+                                                        UriEscapeMode::PATH));
+  EXPECT_EQ("hello%2c+%2fworld", uriEscape<std::string>("hello, /world",
+                                                        UriEscapeMode::QUERY));
+}
+
+TEST(Escape, uriUnescape) {
+  EXPECT_EQ("hello, /world", uriUnescape<std::string>("hello, /world"));
+  EXPECT_EQ("hello, /world", uriUnescape<std::string>("hello%2c%20%2fworld"));
+  EXPECT_EQ("hello,+/world", uriUnescape<std::string>("hello%2c+%2fworld"));
+  EXPECT_EQ("hello, /world", uriUnescape<std::string>("hello%2c+%2fworld",
+                                                      UriEscapeMode::QUERY));
+  EXPECT_EQ("hello/", uriUnescape<std::string>("hello%2f"));
+  EXPECT_EQ("hello/", uriUnescape<std::string>("hello%2F"));
+  EXPECT_THROW({uriUnescape<std::string>("hello%");},
+               std::invalid_argument);
+  EXPECT_THROW({uriUnescape<std::string>("hello%2");},
+               std::invalid_argument);
+  EXPECT_THROW({uriUnescape<std::string>("hello%2g");},
+               std::invalid_argument);
+}
+
+namespace {
+void expectPrintable(StringPiece s) {
+  for (char c : s) {
+    EXPECT_LE(32, c);
+    EXPECT_GE(127, c);
+  }
+}
+}  // namespace
+
+TEST(Escape, uriEscapeAllCombinations) {
+  char c[3];
+  c[2] = '\0';
+  StringPiece in(c, 2);
+  fbstring tmp;
+  fbstring out;
+  for (int i = 0; i < 256; ++i) {
+    c[0] = i;
+    for (int j = 0; j < 256; ++j) {
+      c[1] = j;
+      tmp.clear();
+      out.clear();
+      uriEscape(in, tmp);
+      expectPrintable(tmp);
+      uriUnescape(tmp, out);
+      EXPECT_EQ(in, out);
+    }
+  }
+}
+
+namespace {
+bool isHex(int v) {
+  return ((v >= '0' && v <= '9') ||
+          (v >= 'A' && v <= 'F') ||
+          (v >= 'a' && v <= 'f'));
+}
+}  // namespace
+
+TEST(Escape, uriUnescapePercentDecoding) {
+  char c[4] = {'%', '\0', '\0', '\0'};
+  StringPiece in(c, 3);
+  fbstring out;
+  unsigned int expected = 0;
+  for (int i = 0; i < 256; ++i) {
+    c[1] = i;
+    for (int j = 0; j < 256; ++j) {
+      c[2] = j;
+      if (isHex(i) && isHex(j)) {
+        out.clear();
+        uriUnescape(in, out);
+        EXPECT_EQ(1, out.size());
+        EXPECT_EQ(1, sscanf(c + 1, "%x", &expected));
+        unsigned char v = out[0];
+        EXPECT_EQ(expected, v);
+      } else {
+        EXPECT_THROW({uriUnescape(in, out);}, std::invalid_argument);
+      }
+    }
+  }
+}
+
+namespace {
+fbstring cbmString;
+fbstring cbmEscapedString;
+fbstring cEscapedString;
+fbstring cUnescapedString;
+const size_t kCBmStringLength = 64 << 10;
+const uint32_t kCPrintablePercentage = 90;
+
+fbstring uribmString;
+fbstring uribmEscapedString;
+fbstring uriEscapedString;
+fbstring uriUnescapedString;
+const size_t kURIBmStringLength = 256;
+const uint32_t kURIPassThroughPercentage = 50;
+
+void initBenchmark() {
+  std::mt19937 rnd;
+
+  // C escape
+  std::uniform_int_distribution<uint32_t> printable(32, 126);
+  std::uniform_int_distribution<uint32_t> nonPrintable(0, 160);
+  std::uniform_int_distribution<uint32_t> percentage(0, 99);
+
+  cbmString.reserve(kCBmStringLength);
+  for (size_t i = 0; i < kCBmStringLength; ++i) {
+    unsigned char c;
+    if (percentage(rnd) < kCPrintablePercentage) {
+      c = printable(rnd);
+    } else {
+      c = nonPrintable(rnd);
+      // Generate characters in both non-printable ranges:
+      // 0..31 and 127..255
+      if (c >= 32) {
+        c += (126 - 32) + 1;
+      }
+    }
+    cbmString.push_back(c);
+  }
+
+  cbmEscapedString = cEscape<fbstring>(cbmString);
+
+  // URI escape
+  std::uniform_int_distribution<uint32_t> passthrough('a', 'z');
+  std::string encodeChars = " ?!\"',+[]";
+  std::uniform_int_distribution<uint32_t> encode(0, encodeChars.size() - 1);
+
+  uribmString.reserve(kURIBmStringLength);
+  for (size_t i = 0; i < kURIBmStringLength; ++i) {
+    unsigned char c;
+    if (percentage(rnd) < kURIPassThroughPercentage) {
+      c = passthrough(rnd);
+    } else {
+      c = encodeChars[encode(rnd)];
+    }
+    uribmString.push_back(c);
+  }
+
+  uribmEscapedString = uriEscape<fbstring>(uribmString);
+}
+
+BENCHMARK(BM_cEscape, iters) {
+  while (iters--) {
+    cEscapedString = cEscape<fbstring>(cbmString);
+    doNotOptimizeAway(cEscapedString.size());
+  }
+}
+
+BENCHMARK(BM_cUnescape, iters) {
+  while (iters--) {
+    cUnescapedString = cUnescape<fbstring>(cbmEscapedString);
+    doNotOptimizeAway(cUnescapedString.size());
+  }
+}
+
+BENCHMARK(BM_uriEscape, iters) {
+  while (iters--) {
+    uriEscapedString = uriEscape<fbstring>(uribmString);
+    doNotOptimizeAway(uriEscapedString.size());
+  }
+}
+
+BENCHMARK(BM_uriUnescape, iters) {
+  while (iters--) {
+    uriUnescapedString = uriUnescape<fbstring>(uribmEscapedString);
+    doNotOptimizeAway(uriUnescapedString.size());
+  }
+}
+
+}  // namespace
+
+namespace {
+
+double pow2(int exponent) {
+  return double(int64_t(1) << exponent);
+}
+
+}  // namespace
+
+TEST(PrettyPrint, Basic) {
+  // check time printing
+  EXPECT_EQ(string("8.53e+07 s "), prettyPrint(85.3e6, PRETTY_TIME));
+  EXPECT_EQ(string("85.3 s "), prettyPrint(85.3, PRETTY_TIME));
+  EXPECT_EQ(string("85.3 ms"), prettyPrint(85.3e-3, PRETTY_TIME));
+  EXPECT_EQ(string("85.3 us"), prettyPrint(85.3e-6, PRETTY_TIME));
+  EXPECT_EQ(string("85.3 ns"), prettyPrint(85.3e-9, PRETTY_TIME));
+  EXPECT_EQ(string("85.3 ps"), prettyPrint(85.3e-12, PRETTY_TIME));
+  EXPECT_EQ(string("8.53e-14 s "), prettyPrint(85.3e-15, PRETTY_TIME));
+
+  EXPECT_EQ(string("0 s "), prettyPrint(0, PRETTY_TIME));
+  EXPECT_EQ(string("1 s "), prettyPrint(1.0, PRETTY_TIME));
+  EXPECT_EQ(string("1 ms"), prettyPrint(1.0e-3, PRETTY_TIME));
+  EXPECT_EQ(string("1 us"), prettyPrint(1.0e-6, PRETTY_TIME));
+  EXPECT_EQ(string("1 ns"), prettyPrint(1.0e-9, PRETTY_TIME));
+  EXPECT_EQ(string("1 ps"), prettyPrint(1.0e-12, PRETTY_TIME));
+
+  // check bytes printing
+  EXPECT_EQ(string("853 B "), prettyPrint(853., PRETTY_BYTES));
+  EXPECT_EQ(string("833 kB"), prettyPrint(853.e3, PRETTY_BYTES));
+  EXPECT_EQ(string("813.5 MB"), prettyPrint(853.e6, PRETTY_BYTES));
+  EXPECT_EQ(string("7.944 GB"), prettyPrint(8.53e9, PRETTY_BYTES));
+  EXPECT_EQ(string("794.4 GB"), prettyPrint(853.e9, PRETTY_BYTES));
+  EXPECT_EQ(string("775.8 TB"), prettyPrint(853.e12, PRETTY_BYTES));
+
+  EXPECT_EQ(string("0 B "), prettyPrint(0, PRETTY_BYTES));
+  EXPECT_EQ(string("1 B "), prettyPrint(pow2(0), PRETTY_BYTES));
+  EXPECT_EQ(string("1 kB"), prettyPrint(pow2(10), PRETTY_BYTES));
+  EXPECT_EQ(string("1 MB"), prettyPrint(pow2(20), PRETTY_BYTES));
+  EXPECT_EQ(string("1 GB"), prettyPrint(pow2(30), PRETTY_BYTES));
+  EXPECT_EQ(string("1 TB"), prettyPrint(pow2(40), PRETTY_BYTES));
+
+  EXPECT_EQ(string("853 B  "), prettyPrint(853., PRETTY_BYTES_IEC));
+  EXPECT_EQ(string("833 KiB"), prettyPrint(853.e3, PRETTY_BYTES_IEC));
+  EXPECT_EQ(string("813.5 MiB"), prettyPrint(853.e6, PRETTY_BYTES_IEC));
+  EXPECT_EQ(string("7.944 GiB"), prettyPrint(8.53e9, PRETTY_BYTES_IEC));
+  EXPECT_EQ(string("794.4 GiB"), prettyPrint(853.e9, PRETTY_BYTES_IEC));
+  EXPECT_EQ(string("775.8 TiB"), prettyPrint(853.e12, PRETTY_BYTES_IEC));
+
+  EXPECT_EQ(string("0 B  "), prettyPrint(0, PRETTY_BYTES_IEC));
+  EXPECT_EQ(string("1 B  "), prettyPrint(pow2(0), PRETTY_BYTES_IEC));
+  EXPECT_EQ(string("1 KiB"), prettyPrint(pow2(10), PRETTY_BYTES_IEC));
+  EXPECT_EQ(string("1 MiB"), prettyPrint(pow2(20), PRETTY_BYTES_IEC));
+  EXPECT_EQ(string("1 GiB"), prettyPrint(pow2(30), PRETTY_BYTES_IEC));
+  EXPECT_EQ(string("1 TiB"), prettyPrint(pow2(40), PRETTY_BYTES_IEC));
+
+  // check bytes metric printing
+  EXPECT_EQ(string("853 B "), prettyPrint(853., PRETTY_BYTES_METRIC));
+  EXPECT_EQ(string("853 kB"), prettyPrint(853.e3, PRETTY_BYTES_METRIC));
+  EXPECT_EQ(string("853 MB"), prettyPrint(853.e6, PRETTY_BYTES_METRIC));
+  EXPECT_EQ(string("8.53 GB"), prettyPrint(8.53e9, PRETTY_BYTES_METRIC));
+  EXPECT_EQ(string("853 GB"), prettyPrint(853.e9, PRETTY_BYTES_METRIC));
+  EXPECT_EQ(string("853 TB"), prettyPrint(853.e12, PRETTY_BYTES_METRIC));
+
+  EXPECT_EQ(string("0 B "), prettyPrint(0, PRETTY_BYTES_METRIC));
+  EXPECT_EQ(string("1 B "), prettyPrint(1.0, PRETTY_BYTES_METRIC));
+  EXPECT_EQ(string("1 kB"), prettyPrint(1.0e+3, PRETTY_BYTES_METRIC));
+  EXPECT_EQ(string("1 MB"), prettyPrint(1.0e+6, PRETTY_BYTES_METRIC));
+
+  EXPECT_EQ(string("1 GB"), prettyPrint(1.0e+9, PRETTY_BYTES_METRIC));
+  EXPECT_EQ(string("1 TB"), prettyPrint(1.0e+12, PRETTY_BYTES_METRIC));
+
+  // check metric-units (powers of 1000) printing
+  EXPECT_EQ(string("853  "), prettyPrint(853., PRETTY_UNITS_METRIC));
+  EXPECT_EQ(string("853 k"), prettyPrint(853.e3, PRETTY_UNITS_METRIC));
+  EXPECT_EQ(string("853 M"), prettyPrint(853.e6, PRETTY_UNITS_METRIC));
+  EXPECT_EQ(string("8.53 bil"), prettyPrint(8.53e9, PRETTY_UNITS_METRIC));
+  EXPECT_EQ(string("853 bil"), prettyPrint(853.e9, PRETTY_UNITS_METRIC));
+  EXPECT_EQ(string("853 tril"), prettyPrint(853.e12, PRETTY_UNITS_METRIC));
+
+  // check binary-units (powers of 1024) printing
+  EXPECT_EQ(string("0  "), prettyPrint(0, PRETTY_UNITS_BINARY));
+  EXPECT_EQ(string("1  "), prettyPrint(pow2(0), PRETTY_UNITS_BINARY));
+  EXPECT_EQ(string("1 k"), prettyPrint(pow2(10), PRETTY_UNITS_BINARY));
+  EXPECT_EQ(string("1 M"), prettyPrint(pow2(20), PRETTY_UNITS_BINARY));
+  EXPECT_EQ(string("1 G"), prettyPrint(pow2(30), PRETTY_UNITS_BINARY));
+  EXPECT_EQ(string("1 T"), prettyPrint(pow2(40), PRETTY_UNITS_BINARY));
+
+  EXPECT_EQ(string("1023  "),
+      prettyPrint(pow2(10) - 1, PRETTY_UNITS_BINARY));
+  EXPECT_EQ(string("1024 k"),
+      prettyPrint(pow2(20) - 1, PRETTY_UNITS_BINARY));
+  EXPECT_EQ(string("1024 M"),
+      prettyPrint(pow2(30) - 1, PRETTY_UNITS_BINARY));
+  EXPECT_EQ(string("1024 G"),
+      prettyPrint(pow2(40) - 1, PRETTY_UNITS_BINARY));
+
+  EXPECT_EQ(string("0   "), prettyPrint(0, PRETTY_UNITS_BINARY_IEC));
+  EXPECT_EQ(string("1   "), prettyPrint(pow2(0), PRETTY_UNITS_BINARY_IEC));
+  EXPECT_EQ(string("1 Ki"), prettyPrint(pow2(10), PRETTY_UNITS_BINARY_IEC));
+  EXPECT_EQ(string("1 Mi"), prettyPrint(pow2(20), PRETTY_UNITS_BINARY_IEC));
+  EXPECT_EQ(string("1 Gi"), prettyPrint(pow2(30), PRETTY_UNITS_BINARY_IEC));
+  EXPECT_EQ(string("1 Ti"), prettyPrint(pow2(40), PRETTY_UNITS_BINARY_IEC));
+
+  EXPECT_EQ(string("1023   "),
+      prettyPrint(pow2(10) - 1, PRETTY_UNITS_BINARY_IEC));
+  EXPECT_EQ(string("1024 Ki"),
+      prettyPrint(pow2(20) - 1, PRETTY_UNITS_BINARY_IEC));
+  EXPECT_EQ(string("1024 Mi"),
+      prettyPrint(pow2(30) - 1, PRETTY_UNITS_BINARY_IEC));
+  EXPECT_EQ(string("1024 Gi"),
+      prettyPrint(pow2(40) - 1, PRETTY_UNITS_BINARY_IEC));
+
+  // check that negative values work
+  EXPECT_EQ(string("-85.3 s "), prettyPrint(-85.3, PRETTY_TIME));
+  EXPECT_EQ(string("-85.3 ms"), prettyPrint(-85.3e-3, PRETTY_TIME));
+  EXPECT_EQ(string("-85.3 us"), prettyPrint(-85.3e-6, PRETTY_TIME));
+  EXPECT_EQ(string("-85.3 ns"), prettyPrint(-85.3e-9, PRETTY_TIME));
+}
+
+TEST(PrettyPrint, HexDump) {
+  std::string a("abc\x00\x02\xa0", 6);  // embedded NUL
+  EXPECT_EQ(
+    "00000000  61 62 63 00 02 a0                                 "
+    "|abc...          |\n",
+    hexDump(a.data(), a.size()));
+
+  a = "abcdefghijklmnopqrstuvwxyz";
+  EXPECT_EQ(
+    "00000000  61 62 63 64 65 66 67 68  69 6a 6b 6c 6d 6e 6f 70  "
+    "|abcdefghijklmnop|\n"
+    "00000010  71 72 73 74 75 76 77 78  79 7a                    "
+    "|qrstuvwxyz      |\n",
+    hexDump(a.data(), a.size()));
+}
+
+TEST(System, errnoStr) {
+  errno = EACCES;
+  EXPECT_EQ(EACCES, errno);
+  EXPECT_EQ(EACCES, errno);  // twice to make sure EXPECT_EQ doesn't change it
+
+  fbstring expected = strerror(ENOENT);
+
+  errno = EACCES;
+  EXPECT_EQ(expected, errnoStr(ENOENT));
+  // Ensure that errno isn't changed
+  EXPECT_EQ(EACCES, errno);
+
+  // Per POSIX, all errno values are positive, so -1 is invalid
+  errnoStr(-1);
+
+  // Ensure that errno isn't changed
+  EXPECT_EQ(EACCES, errno);
+}
+
+namespace folly_test {
+struct ThisIsAVeryLongStructureName {
+};
+}  // namespace folly_test
+
+#if FOLLY_HAVE_CPLUS_DEMANGLE_V3_CALLBACK
+TEST(System, demangle) {
+  char expected[] = "folly_test::ThisIsAVeryLongStructureName";
+  EXPECT_STREQ(
+      expected,
+      demangle(typeid(folly_test::ThisIsAVeryLongStructureName)).c_str());
+
+  {
+    char buf[sizeof(expected)];
+    EXPECT_EQ(sizeof(expected) - 1,
+              demangle(typeid(folly_test::ThisIsAVeryLongStructureName),
+                       buf, sizeof(buf)));
+    EXPECT_STREQ(expected, buf);
+
+    EXPECT_EQ(sizeof(expected) - 1,
+              demangle(typeid(folly_test::ThisIsAVeryLongStructureName),
+                       buf, 11));
+    EXPECT_STREQ("folly_test", buf);
+  }
+}
+#endif
+
+namespace {
+
+template<template<class,class> class VectorType>
+void splitTest() {
+  VectorType<string,std::allocator<string> > parts;
+
+  folly::split(',', "a,b,c", parts);
+  EXPECT_EQ(parts.size(), 3);
+  EXPECT_EQ(parts[0], "a");
+  EXPECT_EQ(parts[1], "b");
+  EXPECT_EQ(parts[2], "c");
+  parts.clear();
+
+  folly::split(',', StringPiece("a,b,c"), parts);
+  EXPECT_EQ(parts.size(), 3);
+  EXPECT_EQ(parts[0], "a");
+  EXPECT_EQ(parts[1], "b");
+  EXPECT_EQ(parts[2], "c");
+  parts.clear();
+
+  folly::split(',', string("a,b,c"), parts);
+  EXPECT_EQ(parts.size(), 3);
+  EXPECT_EQ(parts[0], "a");
+  EXPECT_EQ(parts[1], "b");
+  EXPECT_EQ(parts[2], "c");
+  parts.clear();
+
+  folly::split(',', "a,,c", parts);
+  EXPECT_EQ(parts.size(), 3);
+  EXPECT_EQ(parts[0], "a");
+  EXPECT_EQ(parts[1], "");
+  EXPECT_EQ(parts[2], "c");
+  parts.clear();
+
+  folly::split(',', string("a,,c"), parts);
+  EXPECT_EQ(parts.size(), 3);
+  EXPECT_EQ(parts[0], "a");
+  EXPECT_EQ(parts[1], "");
+  EXPECT_EQ(parts[2], "c");
+  parts.clear();
+
+  folly::split(',', "a,,c", parts, true);
+  EXPECT_EQ(parts.size(), 2);
+  EXPECT_EQ(parts[0], "a");
+  EXPECT_EQ(parts[1], "c");
+  parts.clear();
+
+  folly::split(',', string("a,,c"), parts, true);
+  EXPECT_EQ(parts.size(), 2);
+  EXPECT_EQ(parts[0], "a");
+  EXPECT_EQ(parts[1], "c");
+  parts.clear();
+
+  folly::split(',', string(",,a,,c,,,"), parts, true);
+  EXPECT_EQ(parts.size(), 2);
+  EXPECT_EQ(parts[0], "a");
+  EXPECT_EQ(parts[1], "c");
+  parts.clear();
+
+  // test multiple split w/o clear
+  folly::split(',', ",,a,,c,,,", parts, true);
+  EXPECT_EQ(parts.size(), 2);
+  EXPECT_EQ(parts[0], "a");
+  EXPECT_EQ(parts[1], "c");
+  folly::split(',', ",,a,,c,,,", parts, true);
+  EXPECT_EQ(parts.size(), 4);
+  EXPECT_EQ(parts[2], "a");
+  EXPECT_EQ(parts[3], "c");
+  parts.clear();
+
+  // test splits that with multi-line delimiter
+  folly::split("ab", "dabcabkdbkab", parts, true);
+  EXPECT_EQ(parts.size(), 3);
+  EXPECT_EQ(parts[0], "d");
+  EXPECT_EQ(parts[1], "c");
+  EXPECT_EQ(parts[2], "kdbk");
+  parts.clear();
+
+  string orig = "ab2342asdfv~~!";
+  folly::split("", orig, parts, true);
+  EXPECT_EQ(parts.size(), 1);
+  EXPECT_EQ(parts[0], orig);
+  parts.clear();
+
+  folly::split("452x;o38asfsajsdlfdf.j", "asfds", parts, true);
+  EXPECT_EQ(parts.size(), 1);
+  EXPECT_EQ(parts[0], "asfds");
+  parts.clear();
+
+  folly::split("a", "", parts, true);
+  EXPECT_EQ(parts.size(), 0);
+  parts.clear();
+
+  folly::split("a", "", parts);
+  EXPECT_EQ(parts.size(), 1);
+  EXPECT_EQ(parts[0], "");
+  parts.clear();
+
+  folly::split("a", StringPiece(), parts, true);
+  EXPECT_EQ(parts.size(), 0);
+  parts.clear();
+
+  folly::split("a", StringPiece(), parts);
+  EXPECT_EQ(parts.size(), 1);
+  EXPECT_EQ(parts[0], "");
+  parts.clear();
+
+  folly::split("a", "abcdefg", parts, true);
+  EXPECT_EQ(parts.size(), 1);
+  EXPECT_EQ(parts[0], "bcdefg");
+  parts.clear();
+
+  orig = "All, , your base, are , , belong to us";
+  folly::split(", ", orig, parts, true);
+  EXPECT_EQ(parts.size(), 4);
+  EXPECT_EQ(parts[0], "All");
+  EXPECT_EQ(parts[1], "your base");
+  EXPECT_EQ(parts[2], "are ");
+  EXPECT_EQ(parts[3], "belong to us");
+  parts.clear();
+  folly::split(", ", orig, parts);
+  EXPECT_EQ(parts.size(), 6);
+  EXPECT_EQ(parts[0], "All");
+  EXPECT_EQ(parts[1], "");
+  EXPECT_EQ(parts[2], "your base");
+  EXPECT_EQ(parts[3], "are ");
+  EXPECT_EQ(parts[4], "");
+  EXPECT_EQ(parts[5], "belong to us");
+  parts.clear();
+
+  orig = ", Facebook, rul,es!, ";
+  folly::split(", ", orig, parts, true);
+  EXPECT_EQ(parts.size(), 2);
+  EXPECT_EQ(parts[0], "Facebook");
+  EXPECT_EQ(parts[1], "rul,es!");
+  parts.clear();
+  folly::split(", ", orig, parts);
+  EXPECT_EQ(parts.size(), 4);
+  EXPECT_EQ(parts[0], "");
+  EXPECT_EQ(parts[1], "Facebook");
+  EXPECT_EQ(parts[2], "rul,es!");
+  EXPECT_EQ(parts[3], "");
+}
+
+template<template<class,class> class VectorType>
+void piecesTest() {
+  VectorType<StringPiece,std::allocator<StringPiece> > pieces;
+  VectorType<StringPiece,std::allocator<StringPiece> > pieces2;
+
+  folly::split(',', "a,b,c", pieces);
+  EXPECT_EQ(pieces.size(), 3);
+  EXPECT_EQ(pieces[0], "a");
+  EXPECT_EQ(pieces[1], "b");
+  EXPECT_EQ(pieces[2], "c");
+
+  pieces.clear();
+
+  folly::split(',', "a,,c", pieces);
+  EXPECT_EQ(pieces.size(), 3);
+  EXPECT_EQ(pieces[0], "a");
+  EXPECT_EQ(pieces[1], "");
+  EXPECT_EQ(pieces[2], "c");
+  pieces.clear();
+
+  folly::split(',', "a,,c", pieces, true);
+  EXPECT_EQ(pieces.size(), 2);
+  EXPECT_EQ(pieces[0], "a");
+  EXPECT_EQ(pieces[1], "c");
+  pieces.clear();
+
+  folly::split(',', ",,a,,c,,,", pieces, true);
+  EXPECT_EQ(pieces.size(), 2);
+  EXPECT_EQ(pieces[0], "a");
+  EXPECT_EQ(pieces[1], "c");
+  pieces.clear();
+
+  // test multiple split w/o clear
+  folly::split(',', ",,a,,c,,,", pieces, true);
+  EXPECT_EQ(pieces.size(), 2);
+  EXPECT_EQ(pieces[0], "a");
+  EXPECT_EQ(pieces[1], "c");
+  folly::split(',', ",,a,,c,,,", pieces, true);
+  EXPECT_EQ(pieces.size(), 4);
+  EXPECT_EQ(pieces[2], "a");
+  EXPECT_EQ(pieces[3], "c");
+  pieces.clear();
+
+  // test multiple split rounds
+  folly::split(",", "a_b,c_d", pieces);
+  EXPECT_EQ(pieces.size(), 2);
+  EXPECT_EQ(pieces[0], "a_b");
+  EXPECT_EQ(pieces[1], "c_d");
+  folly::split("_", pieces[0], pieces2);
+  EXPECT_EQ(pieces2.size(), 2);
+  EXPECT_EQ(pieces2[0], "a");
+  EXPECT_EQ(pieces2[1], "b");
+  pieces2.clear();
+  folly::split("_", pieces[1], pieces2);
+  EXPECT_EQ(pieces2.size(), 2);
+  EXPECT_EQ(pieces2[0], "c");
+  EXPECT_EQ(pieces2[1], "d");
+  pieces.clear();
+  pieces2.clear();
+
+  // test splits that with multi-line delimiter
+  folly::split("ab", "dabcabkdbkab", pieces, true);
+  EXPECT_EQ(pieces.size(), 3);
+  EXPECT_EQ(pieces[0], "d");
+  EXPECT_EQ(pieces[1], "c");
+  EXPECT_EQ(pieces[2], "kdbk");
+  pieces.clear();
+
+  string orig = "ab2342asdfv~~!";
+  folly::split("", orig.c_str(), pieces, true);
+  EXPECT_EQ(pieces.size(), 1);
+  EXPECT_EQ(pieces[0], orig);
+  pieces.clear();
+
+  folly::split("452x;o38asfsajsdlfdf.j", "asfds", pieces, true);
+  EXPECT_EQ(pieces.size(), 1);
+  EXPECT_EQ(pieces[0], "asfds");
+  pieces.clear();
+
+  folly::split("a", "", pieces, true);
+  EXPECT_EQ(pieces.size(), 0);
+  pieces.clear();
+
+  folly::split("a", "", pieces);
+  EXPECT_EQ(pieces.size(), 1);
+  EXPECT_EQ(pieces[0], "");
+  pieces.clear();
+
+  folly::split("a", "abcdefg", pieces, true);
+  EXPECT_EQ(pieces.size(), 1);
+  EXPECT_EQ(pieces[0], "bcdefg");
+  pieces.clear();
+
+  orig = "All, , your base, are , , belong to us";
+  folly::split(", ", orig, pieces, true);
+  EXPECT_EQ(pieces.size(), 4);
+  EXPECT_EQ(pieces[0], "All");
+  EXPECT_EQ(pieces[1], "your base");
+  EXPECT_EQ(pieces[2], "are ");
+  EXPECT_EQ(pieces[3], "belong to us");
+  pieces.clear();
+  folly::split(", ", orig, pieces);
+  EXPECT_EQ(pieces.size(), 6);
+  EXPECT_EQ(pieces[0], "All");
+  EXPECT_EQ(pieces[1], "");
+  EXPECT_EQ(pieces[2], "your base");
+  EXPECT_EQ(pieces[3], "are ");
+  EXPECT_EQ(pieces[4], "");
+  EXPECT_EQ(pieces[5], "belong to us");
+  pieces.clear();
+
+  orig = ", Facebook, rul,es!, ";
+  folly::split(", ", orig, pieces, true);
+  EXPECT_EQ(pieces.size(), 2);
+  EXPECT_EQ(pieces[0], "Facebook");
+  EXPECT_EQ(pieces[1], "rul,es!");
+  pieces.clear();
+  folly::split(", ", orig, pieces);
+  EXPECT_EQ(pieces.size(), 4);
+  EXPECT_EQ(pieces[0], "");
+  EXPECT_EQ(pieces[1], "Facebook");
+  EXPECT_EQ(pieces[2], "rul,es!");
+  EXPECT_EQ(pieces[3], "");
+  pieces.clear();
+
+  const char* str = "a,b";
+  folly::split(',', StringPiece(str), pieces);
+  EXPECT_EQ(pieces.size(), 2);
+  EXPECT_EQ(pieces[0], "a");
+  EXPECT_EQ(pieces[1], "b");
+  EXPECT_EQ(pieces[0].start(), str);
+  EXPECT_EQ(pieces[1].start(), str + 2);
+
+  std::set<StringPiece> unique;
+  folly::splitTo<StringPiece>(":", "asd:bsd:asd:asd:bsd:csd::asd",
+    std::inserter(unique, unique.begin()), true);
+  EXPECT_EQ(unique.size(), 3);
+  if (unique.size() == 3) {
+    EXPECT_EQ(*unique.begin(), "asd");
+    EXPECT_EQ(*--unique.end(), "csd");
+  }
+
+  VectorType<fbstring,std::allocator<fbstring> > blah;
+  folly::split('-', "a-b-c-d-f-e", blah);
+  EXPECT_EQ(blah.size(), 6);
+}
+
+}
+
+TEST(Split, split_vector) {
+  splitTest<std::vector>();
+}
+TEST(Split, split_fbvector) {
+  splitTest<folly::fbvector>();
+}
+TEST(Split, pieces_vector) {
+  piecesTest<std::vector>();
+}
+TEST(Split, pieces_fbvector) {
+  piecesTest<folly::fbvector>();
+}
+
+TEST(Split, fixed) {
+  StringPiece a, b, c, d;
+
+  EXPECT_TRUE(folly::split<false>('.', "a.b.c.d", a, b, c, d));
+  EXPECT_TRUE(folly::split<false>('.', "a.b.c", a, b, c));
+  EXPECT_TRUE(folly::split<false>('.', "a.b", a, b));
+  EXPECT_TRUE(folly::split<false>('.', "a", a));
+
+  EXPECT_TRUE(folly::split('.', "a.b.c.d", a, b, c, d));
+  EXPECT_TRUE(folly::split('.', "a.b.c", a, b, c));
+  EXPECT_TRUE(folly::split('.', "a.b", a, b));
+  EXPECT_TRUE(folly::split('.', "a", a));
+
+  EXPECT_TRUE(folly::split<false>('.', "a.b.c", a, b, c));
+  EXPECT_EQ("a", a);
+  EXPECT_EQ("b", b);
+  EXPECT_EQ("c", c);
+  EXPECT_FALSE(folly::split<false>('.', "a.b", a, b, c));
+  EXPECT_TRUE(folly::split<false>('.', "a.b.c", a, b));
+  EXPECT_EQ("a", a);
+  EXPECT_EQ("b.c", b);
+
+  EXPECT_TRUE(folly::split('.', "a.b.c", a, b, c));
+  EXPECT_EQ("a", a);
+  EXPECT_EQ("b", b);
+  EXPECT_EQ("c", c);
+  EXPECT_FALSE(folly::split('.', "a.b.c", a, b));
+  EXPECT_FALSE(folly::split('.', "a.b", a, b, c));
+
+  EXPECT_TRUE(folly::split<false>('.', "a.b", a, b));
+  EXPECT_EQ("a", a);
+  EXPECT_EQ("b", b);
+  EXPECT_FALSE(folly::split<false>('.', "a", a, b));
+  EXPECT_TRUE(folly::split<false>('.', "a.b", a));
+  EXPECT_EQ("a.b", a);
+
+  EXPECT_TRUE(folly::split('.', "a.b", a, b));
+  EXPECT_EQ("a", a);
+  EXPECT_EQ("b", b);
+  EXPECT_FALSE(folly::split('.', "a", a, b));
+  EXPECT_FALSE(folly::split('.', "a.b", a));
+}
+
+TEST(Split, fixed_convert) {
+  StringPiece a, d;
+  int b;
+  double c;
+
+  EXPECT_TRUE(folly::split(':', "a:13:14.7:b", a, b, c, d));
+  EXPECT_EQ("a", a);
+  EXPECT_EQ(13, b);
+  EXPECT_NEAR(14.7, c, 1e-10);
+  EXPECT_EQ("b", d);
+
+  EXPECT_TRUE(folly::split<false>(':', "b:14:15.3:c", a, b, c, d));
+  EXPECT_EQ("b", a);
+  EXPECT_EQ(14, b);
+  EXPECT_NEAR(15.3, c, 1e-10);
+  EXPECT_EQ("c", d);
+
+  EXPECT_FALSE(folly::split(':', "a:13:14.7:b", a, b, d));
+
+  EXPECT_TRUE(folly::split<false>(':', "a:13:14.7:b", a, b, d));
+  EXPECT_EQ("a", a);
+  EXPECT_EQ(13, b);
+  EXPECT_EQ("14.7:b", d);
+
+  EXPECT_THROW(folly::split<false>(':', "a:13:14.7:b", a, b, c),
+               std::range_error);
+}
+
+TEST(String, join) {
+  string output;
+
+  std::vector<int> empty = { };
+  join(":", empty, output);
+  EXPECT_TRUE(output.empty());
+
+  std::vector<std::string> input1 = { "1", "23", "456", "" };
+  join(':', input1, output);
+  EXPECT_EQ(output, "1:23:456:");
+  output = join(':', input1);
+  EXPECT_EQ(output, "1:23:456:");
+
+  auto input2 = { 1, 23, 456 };
+  join("-*-", input2, output);
+  EXPECT_EQ(output, "1-*-23-*-456");
+  output = join("-*-", input2);
+  EXPECT_EQ(output, "1-*-23-*-456");
+
+  auto input3 = { 'f', 'a', 'c', 'e', 'b', 'o', 'o', 'k' };
+  join("", input3, output);
+  EXPECT_EQ(output, "facebook");
+
+  join("_", { "", "f", "a", "c", "e", "b", "o", "o", "k", "" }, output);
+  EXPECT_EQ(output, "_f_a_c_e_b_o_o_k_");
+}
+
+TEST(String, hexlify) {
+  string input1 = "0123";
+  string output1;
+  EXPECT_TRUE(hexlify(input1, output1));
+  EXPECT_EQ(output1, "30313233");
+
+  fbstring input2 = "abcdefg";
+  input2[1] = 0;
+  input2[3] = 0xff;
+  input2[5] = 0xb6;
+  fbstring output2;
+  EXPECT_TRUE(hexlify(input2, output2));
+  EXPECT_EQ(output2, "610063ff65b667");
+}
+
+TEST(String, unhexlify) {
+  string input1 = "30313233";
+  string output1;
+  EXPECT_TRUE(unhexlify(input1, output1));
+  EXPECT_EQ(output1, "0123");
+
+  fbstring input2 = "610063ff65b667";
+  fbstring output2;
+  EXPECT_TRUE(unhexlify(input2, output2));
+  EXPECT_EQ(output2.size(), 7);
+  EXPECT_EQ(output2[0], 'a');
+  EXPECT_EQ(output2[1], 0);
+  EXPECT_EQ(output2[2], 'c');
+  EXPECT_EQ(output2[3] & 0xff, 0xff);
+  EXPECT_EQ(output2[4], 'e');
+  EXPECT_EQ(output2[5] & 0xff, 0xb6);
+  EXPECT_EQ(output2[6], 'g');
+
+  string input3 = "x";
+  string output3;
+  EXPECT_FALSE(unhexlify(input3, output3));
+
+  string input4 = "xy";
+  string output4;
+  EXPECT_FALSE(unhexlify(input4, output4));
+}
+
+TEST(String, backslashify) {
+  EXPECT_EQ("abc", string("abc"));
+  EXPECT_EQ("abc", backslashify(string("abc")));
+  EXPECT_EQ("abc\\r", backslashify(string("abc\r")));
+  EXPECT_EQ("abc\\x0d", backslashify(string("abc\r"), true));
+  EXPECT_EQ("\\0\\0", backslashify(string(2, '\0')));
+}
+
+TEST(String, humanify) {
+  // Simple cases; output is obvious.
+  EXPECT_EQ("abc", humanify(string("abc")));
+  EXPECT_EQ("abc\\\\r", humanify(string("abc\\r")));
+  EXPECT_EQ("0xff", humanify(string("\xff")));
+  EXPECT_EQ("abc\\xff", humanify(string("abc\xff")));
+  EXPECT_EQ("abc\\b", humanify(string("abc\b")));
+  EXPECT_EQ("0x00", humanify(string(1, '\0')));
+  EXPECT_EQ("0x0000", humanify(string(2, '\0')));
+
+
+  // Mostly printable, so backslash!  80, 60, and 40% printable, respectively
+  EXPECT_EQ("aaaa\\xff", humanify(string("aaaa\xff")));
+  EXPECT_EQ("aaa\\xff\\xff", humanify(string("aaa\xff\xff")));
+  EXPECT_EQ("aa\\xff\\xff\\xff", humanify(string("aa\xff\xff\xff")));
+
+  // 20% printable, and the printable portion isn't the prefix; hexify!
+  EXPECT_EQ("0xff61ffffff", humanify(string("\xff" "a\xff\xff\xff")));
+
+  // Same as previous, except swap first two chars; prefix is
+  // printable and within the threshold, so backslashify.
+  EXPECT_EQ("a\\xff\\xff\\xff\\xff", humanify(string("a\xff\xff\xff\xff")));
+
+  // Just too much unprintable; hex, despite prefix.
+  EXPECT_EQ("0x61ffffffffff", humanify(string("a\xff\xff\xff\xff\xff")));
+}
+
+//////////////////////////////////////////////////////////////////////
+
+BENCHMARK(splitOnSingleChar, iters) {
+  static const std::string line = "one:two:three:four";
+  for (int i = 0; i < iters << 4; ++i) {
+    std::vector<StringPiece> pieces;
+    folly::split(':', line, pieces);
+  }
+}
+
+BENCHMARK(splitOnSingleCharFixed, iters) {
+  static const std::string line = "one:two:three:four";
+  for (int i = 0; i < iters << 4; ++i) {
+    StringPiece a, b, c, d;
+    folly::split(':', line, a, b, c, d);
+  }
+}
+
+BENCHMARK(splitOnSingleCharFixedAllowExtra, iters) {
+  static const std::string line = "one:two:three:four";
+  for (int i = 0; i < iters << 4; ++i) {
+    StringPiece a, b, c, d;
+    folly::split<false>(':', line, a, b, c, d);
+  }
+}
+
+BENCHMARK(splitStr, iters) {
+  static const std::string line = "one-*-two-*-three-*-four";
+  for (int i = 0; i < iters << 4; ++i) {
+    std::vector<StringPiece> pieces;
+    folly::split("-*-", line, pieces);
+  }
+}
+
+BENCHMARK(splitStrFixed, iters) {
+  static const std::string line = "one-*-two-*-three-*-four";
+  for (int i = 0; i < iters << 4; ++i) {
+    StringPiece a, b, c, d;
+    folly::split("-*-", line, a, b, c, d);
+  }
+}
+
+BENCHMARK(boost_splitOnSingleChar, iters) {
+  static const std::string line = "one:two:three:four";
+  bool(*pred)(char) = [] (char c) -> bool { return c == ':'; };
+  for (int i = 0; i < iters << 4; ++i) {
+    std::vector<boost::iterator_range<std::string::const_iterator> > pieces;
+    boost::split(pieces, line, pred);
+  }
+}
+
+BENCHMARK(joinCharStr, iters) {
+  static const std::vector<std::string> input = {
+    "one", "two", "three", "four", "five", "six", "seven" };
+  for (int i = 0; i < iters << 4; ++i) {
+    std::string output;
+    folly::join(':', input, output);
+  }
+}
+
+BENCHMARK(joinStrStr, iters) {
+  static const std::vector<std::string> input = {
+    "one", "two", "three", "four", "five", "six", "seven" };
+  for (int i = 0; i < iters << 4; ++i) {
+    std::string output;
+    folly::join(":", input, output);
+  }
+}
+
+BENCHMARK(joinInt, iters) {
+  static const auto input = {
+    123, 456, 78910, 1112, 1314, 151, 61718 };
+  for (int i = 0; i < iters << 4; ++i) {
+    std::string output;
+    folly::join(":", input, output);
+  }
+}
+
+int main(int argc, char *argv[]) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  auto ret = RUN_ALL_TESTS();
+  if (!ret) {
+    initBenchmark();
+    if (FLAGS_benchmark) {
+      folly::runBenchmarks();
+    }
+  }
+  return ret;
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/SubprocessTest.cpp
@@ -0,0 +1,294 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Subprocess.h"
+
+#include <unistd.h>
+#include <sys/types.h>
+#include <dirent.h>
+
+#include <boost/container/flat_set.hpp>
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+#include "folly/Exception.h"
+#include "folly/Format.h"
+#include "folly/String.h"
+#include "folly/gen/Base.h"
+#include "folly/gen/File.h"
+#include "folly/gen/String.h"
+#include "folly/experimental/io/FsUtil.h"
+
+using namespace folly;
+
+TEST(SimpleSubprocessTest, ExitsSuccessfully) {
+  Subprocess proc(std::vector<std::string>{ "/bin/true" });
+  EXPECT_EQ(0, proc.wait().exitStatus());
+}
+
+TEST(SimpleSubprocessTest, ExitsSuccessfullyChecked) {
+  Subprocess proc(std::vector<std::string>{ "/bin/true" });
+  proc.waitChecked();
+}
+
+TEST(SimpleSubprocessTest, ExitsWithError) {
+  Subprocess proc(std::vector<std::string>{ "/bin/false" });
+  EXPECT_EQ(1, proc.wait().exitStatus());
+}
+
+TEST(SimpleSubprocessTest, ExitsWithErrorChecked) {
+  Subprocess proc(std::vector<std::string>{ "/bin/false" });
+  EXPECT_THROW(proc.waitChecked(), CalledProcessError);
+}
+
+#define EXPECT_SPAWN_ERROR(err, errMsg, cmd, ...) \
+  do { \
+    try { \
+      Subprocess proc(std::vector<std::string>{ (cmd), ## __VA_ARGS__ }); \
+      ADD_FAILURE() << "expected an error when running " << (cmd); \
+    } catch (const SubprocessSpawnError& ex) { \
+      EXPECT_EQ((err), ex.errnoValue()); \
+      if (StringPiece(ex.what()).find(errMsg) == StringPiece::npos) { \
+        ADD_FAILURE() << "failed to find \"" << (errMsg) << \
+          "\" in exception: \"" << ex.what() << "\""; \
+      } \
+    } \
+  } while (0)
+
+TEST(SimpleSubprocessTest, ExecFails) {
+  EXPECT_SPAWN_ERROR(ENOENT, "failed to execute /no/such/file:",
+                     "/no/such/file");
+  EXPECT_SPAWN_ERROR(EACCES, "failed to execute /etc/passwd:",
+                     "/etc/passwd");
+  EXPECT_SPAWN_ERROR(ENOTDIR, "failed to execute /etc/passwd/not/a/file:",
+                     "/etc/passwd/not/a/file");
+}
+
+TEST(SimpleSubprocessTest, ShellExitsSuccesssfully) {
+  Subprocess proc("true");
+  EXPECT_EQ(0, proc.wait().exitStatus());
+}
+
+TEST(SimpleSubprocessTest, ShellExitsWithError) {
+  Subprocess proc("false");
+  EXPECT_EQ(1, proc.wait().exitStatus());
+}
+
+namespace {
+boost::container::flat_set<int> getOpenFds() {
+  auto pid = getpid();
+  auto dirname = to<std::string>("/proc/", pid, "/fd");
+
+  boost::container::flat_set<int> fds;
+  for (fs::directory_iterator it(dirname);
+       it != fs::directory_iterator();
+       ++it) {
+    int fd = to<int>(it->path().filename().native());
+    fds.insert(fd);
+  }
+  return fds;
+}
+
+template<class Runnable>
+void checkFdLeak(const Runnable& r) {
+  // Get the currently open fds.  Check that they are the same both before and
+  // after calling the specified function.  We read the open fds from /proc.
+  // (If we wanted to work even on systems that don't have /proc, we could
+  // perhaps create and immediately close a socket both before and after
+  // running the function, and make sure we got the same fd number both times.)
+  auto fdsBefore = getOpenFds();
+  r();
+  auto fdsAfter = getOpenFds();
+  EXPECT_EQ(fdsAfter.size(), fdsBefore.size());
+}
+}
+
+// Make sure Subprocess doesn't leak any file descriptors
+TEST(SimpleSubprocessTest, FdLeakTest) {
+  // Normal execution
+  checkFdLeak([] {
+    Subprocess proc("true");
+    EXPECT_EQ(0, proc.wait().exitStatus());
+  });
+  // Normal execution with pipes
+  checkFdLeak([] {
+    Subprocess proc("echo foo; echo bar >&2",
+                    Subprocess::pipeStdout() | Subprocess::pipeStderr());
+    auto p = proc.communicate();
+    EXPECT_EQ("foo\n", p.first);
+    EXPECT_EQ("bar\n", p.second);
+    proc.waitChecked();
+  });
+
+  // Test where the exec call fails()
+  checkFdLeak([] {
+    EXPECT_SPAWN_ERROR(ENOENT, "failed to execute", "/no/such/file");
+  });
+  // Test where the exec call fails() with pipes
+  checkFdLeak([] {
+    try {
+      Subprocess proc(std::vector<std::string>({"/no/such/file"}),
+                      Subprocess::pipeStdout().stderr(Subprocess::PIPE));
+      ADD_FAILURE() << "expected an error when running /no/such/file";
+    } catch (const SubprocessSpawnError& ex) {
+      EXPECT_EQ(ENOENT, ex.errnoValue());
+    }
+  });
+}
+
+TEST(ParentDeathSubprocessTest, ParentDeathSignal) {
+  // Find out where we are.
+  static constexpr size_t pathLength = 2048;
+  char buf[pathLength + 1];
+  int r = readlink("/proc/self/exe", buf, pathLength);
+  CHECK_ERR(r);
+  buf[r] = '\0';
+
+  fs::path helper(buf);
+  helper.remove_filename();
+  helper /= "subprocess_test_parent_death_helper";
+
+  fs::path tempFile(fs::temp_directory_path() / fs::unique_path());
+
+  std::vector<std::string> args {helper.string(), tempFile.string()};
+  Subprocess proc(args);
+  // The helper gets killed by its child, see details in
+  // SubprocessTestParentDeathHelper.cpp
+  ASSERT_EQ(SIGKILL, proc.wait().killSignal());
+
+  // Now wait for the file to be created, see details in
+  // SubprocessTestParentDeathHelper.cpp
+  while (!fs::exists(tempFile)) {
+    usleep(20000);  // 20ms
+  }
+
+  fs::remove(tempFile);
+}
+
+TEST(PopenSubprocessTest, PopenRead) {
+  Subprocess proc("ls /", Subprocess::pipeStdout());
+  int found = 0;
+  gen::byLine(File(proc.stdout())) |
+    [&] (StringPiece line) {
+      if (line == "etc" || line == "bin" || line == "usr") {
+        ++found;
+      }
+    };
+  EXPECT_EQ(3, found);
+  proc.waitChecked();
+}
+
+TEST(CommunicateSubprocessTest, SimpleRead) {
+  Subprocess proc(std::vector<std::string>{ "/bin/echo", "-n", "foo", "bar"},
+                  Subprocess::pipeStdout());
+  auto p = proc.communicate();
+  EXPECT_EQ("foo bar", p.first);
+  proc.waitChecked();
+}
+
+TEST(CommunicateSubprocessTest, BigWrite) {
+  const int numLines = 1 << 20;
+  std::string line("hello\n");
+  std::string data;
+  data.reserve(numLines * line.size());
+  for (int i = 0; i < numLines; ++i) {
+    data.append(line);
+  }
+
+  Subprocess proc("wc -l", Subprocess::pipeStdin() | Subprocess::pipeStdout());
+  auto p = proc.communicate(data);
+  EXPECT_EQ(folly::format("{}\n", numLines).str(), p.first);
+  proc.waitChecked();
+}
+
+TEST(CommunicateSubprocessTest, Duplex) {
+  // Take 10MB of data and pass them through a filter.
+  // One line, as tr is line-buffered
+  const int bytes = 10 << 20;
+  std::string line(bytes, 'x');
+
+  Subprocess proc("tr a-z A-Z",
+                  Subprocess::pipeStdin() | Subprocess::pipeStdout());
+  auto p = proc.communicate(line);
+  EXPECT_EQ(bytes, p.first.size());
+  EXPECT_EQ(std::string::npos, p.first.find_first_not_of('X'));
+  proc.waitChecked();
+}
+
+TEST(CommunicateSubprocessTest, Duplex2) {
+  checkFdLeak([] {
+    // Pipe 200,000 lines through sed
+    const size_t numCopies = 100000;
+    auto iobuf = IOBuf::copyBuffer("this is a test\nanother line\n");
+    IOBufQueue input;
+    for (int n = 0; n < numCopies; ++n) {
+      input.append(iobuf->clone());
+    }
+
+    std::vector<std::string> cmd({
+      "sed", "-u",
+      "-e", "s/a test/a successful test/",
+      "-e", "/^another line/w/dev/stderr",
+    });
+    auto options = Subprocess::pipeStdin().pipeStdout().pipeStderr().usePath();
+    Subprocess proc(cmd, options);
+    auto out = proc.communicateIOBuf(std::move(input));
+    proc.waitChecked();
+
+    // Convert stdout and stderr to strings so we can call split() on them.
+    fbstring stdoutStr;
+    if (out.first.front()) {
+      stdoutStr = out.first.move()->moveToFbString();
+    }
+    fbstring stderrStr;
+    if (out.second.front()) {
+      stderrStr = out.second.move()->moveToFbString();
+    }
+
+    // stdout should be a copy of stdin, with "a test" replaced by
+    // "a successful test"
+    std::vector<StringPiece> stdoutLines;
+    split('\n', stdoutStr, stdoutLines);
+    EXPECT_EQ(numCopies * 2 + 1, stdoutLines.size());
+    // Strip off the trailing empty line
+    if (!stdoutLines.empty()) {
+      EXPECT_EQ("", stdoutLines.back());
+      stdoutLines.pop_back();
+    }
+    size_t linenum = 0;
+    for (const auto& line : stdoutLines) {
+      if ((linenum & 1) == 0) {
+        EXPECT_EQ("this is a successful test", line);
+      } else {
+        EXPECT_EQ("another line", line);
+      }
+      ++linenum;
+    }
+
+    // stderr should only contain the lines containing "another line"
+    std::vector<StringPiece> stderrLines;
+    split('\n', stderrStr, stderrLines);
+    EXPECT_EQ(numCopies + 1, stderrLines.size());
+    // Strip off the trailing empty line
+    if (!stderrLines.empty()) {
+      EXPECT_EQ("", stderrLines.back());
+      stderrLines.pop_back();
+    }
+    for (const auto& line : stderrLines) {
+      EXPECT_EQ("another line", line);
+    }
+  });
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/SubprocessTestParentDeathHelper.cpp
@@ -0,0 +1,86 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// This is a helper for the parentDeathSignal test in SubprocessTest.cpp.
+//
+// Basically, we create two processes, a parent and a child, and set the
+// child to receive SIGUSR1 when the parent exits.  We set the child to
+// create a file when that happens.  The child then kills the parent; the test
+// will verify that the file actually gets created, which means that everything
+// worked as intended.
+
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <signal.h>
+#include <unistd.h>
+
+#include <gflags/gflags.h>
+#include <glog/logging.h>
+
+#include "folly/Conv.h"
+#include "folly/Subprocess.h"
+
+using folly::Subprocess;
+
+DEFINE_bool(child, false, "");
+
+namespace {
+constexpr int kSignal = SIGUSR1;
+}  // namespace
+
+void runChild(const char* file) {
+  // Block SIGUSR1 so it's queued
+  sigset_t sigs;
+  CHECK_ERR(sigemptyset(&sigs));
+  CHECK_ERR(sigaddset(&sigs, kSignal));
+  CHECK_ERR(sigprocmask(SIG_BLOCK, &sigs, nullptr));
+
+  // Kill the parent, wait for our signal.
+  CHECK_ERR(kill(getppid(), SIGKILL));
+
+  int sig = 0;
+  CHECK_ERR(sigwait(&sigs, &sig));
+  CHECK_EQ(sig, kSignal);
+
+  // Signal completion by creating the file
+  CHECK_ERR(creat(file, 0600));
+}
+
+void runParent(const char* file) {
+  std::vector<std::string> args {"/proc/self/exe", "--child", file};
+  Subprocess proc(
+      args,
+      Subprocess::Options().parentDeathSignal(kSignal));
+  CHECK(proc.poll().running());
+
+  // The child will kill us.
+  for (;;) {
+    pause();
+  }
+}
+
+int main(int argc, char *argv[]) {
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  CHECK_EQ(argc, 2);
+  if (FLAGS_child) {
+    runChild(argv[1]);
+  } else {
+    runParent(argv[1]);
+  }
+  return 0;
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/SynchronizedTest.cpp
@@ -0,0 +1,134 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// @author: Andrei Alexandrescu (aalexandre)
+
+// Test bed for folly/Synchronized.h
+
+#include "folly/Synchronized.h"
+#include "folly/RWSpinLock.h"
+#include "folly/test/SynchronizedTestLib.h"
+#include <gtest/gtest.h>
+
+
+TEST(Synchronized, Basic) {
+  testBasic<std::mutex>();
+  testBasic<std::recursive_mutex>();
+#ifndef __APPLE__
+  testBasic<std::timed_mutex>();
+  testBasic<std::recursive_timed_mutex>();
+#endif
+
+#ifdef RW_SPINLOCK_USE_X86_INTRINSIC_
+  testBasic<folly::RWTicketSpinLock32>();
+#endif
+
+  testBasic<boost::mutex>();
+  testBasic<boost::recursive_mutex>();
+  testBasic<boost::shared_mutex>();
+#ifndef __APPLE__
+  testBasic<boost::timed_mutex>();
+  testBasic<boost::recursive_timed_mutex>();
+#endif
+}
+
+TEST(Synchronized, Concurrency) {
+  testConcurrency<std::mutex>();
+  testConcurrency<std::recursive_mutex>();
+#ifndef __APPLE__
+  testConcurrency<std::timed_mutex>();
+  testConcurrency<std::recursive_timed_mutex>();
+#endif
+
+#ifdef RW_SPINLOCK_USE_X86_INTRINSIC_
+  testConcurrency<folly::RWTicketSpinLock32>();
+#endif
+
+  testConcurrency<boost::mutex>();
+  testConcurrency<boost::recursive_mutex>();
+  testConcurrency<boost::shared_mutex>();
+#ifndef __APPLE__
+  testConcurrency<boost::timed_mutex>();
+  testConcurrency<boost::recursive_timed_mutex>();
+#endif
+}
+
+
+TEST(Synchronized, DualLocking) {
+  testDualLocking<std::mutex>();
+  testDualLocking<std::recursive_mutex>();
+#ifndef __APPLE__
+  testDualLocking<std::timed_mutex>();
+  testDualLocking<std::recursive_timed_mutex>();
+#endif
+
+#ifdef RW_SPINLOCK_USE_X86_INTRINSIC_
+  testDualLocking<folly::RWTicketSpinLock32>();
+#endif
+
+  testDualLocking<boost::mutex>();
+  testDualLocking<boost::recursive_mutex>();
+  testDualLocking<boost::shared_mutex>();
+#ifndef __APPLE__
+  testDualLocking<boost::timed_mutex>();
+  testDualLocking<boost::recursive_timed_mutex>();
+#endif
+}
+
+
+TEST(Synchronized, DualLockingWithConst) {
+  testDualLockingWithConst<std::mutex>();
+  testDualLockingWithConst<std::recursive_mutex>();
+#ifndef __APPLE__
+  testDualLockingWithConst<std::timed_mutex>();
+  testDualLockingWithConst<std::recursive_timed_mutex>();
+#endif
+
+#ifdef RW_SPINLOCK_USE_X86_INTRINSIC_
+  testDualLockingWithConst<folly::RWTicketSpinLock32>();
+#endif
+
+  testDualLockingWithConst<boost::mutex>();
+  testDualLockingWithConst<boost::recursive_mutex>();
+  testDualLockingWithConst<boost::shared_mutex>();
+#ifndef __APPLE__
+  testDualLockingWithConst<boost::timed_mutex>();
+  testDualLockingWithConst<boost::recursive_timed_mutex>();
+#endif
+}
+
+
+#ifndef __APPLE__
+TEST(Synchronized, TimedSynchronized) {
+  testTimedSynchronized<std::timed_mutex>();
+  testTimedSynchronized<std::recursive_timed_mutex>();
+
+  testTimedSynchronized<boost::timed_mutex>();
+  testTimedSynchronized<boost::recursive_timed_mutex>();
+  testTimedSynchronized<boost::shared_mutex>();
+}
+#endif
+
+TEST(Synchronized, ConstCopy) {
+#ifndef __APPLE__
+  testConstCopy<std::timed_mutex>();
+  testConstCopy<std::recursive_timed_mutex>();
+
+  testConstCopy<boost::timed_mutex>();
+  testConstCopy<boost::recursive_timed_mutex>();
+#endif
+  testConstCopy<boost::shared_mutex>();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/SynchronizedTestLib.h
@@ -0,0 +1,47 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_TEST_SYNCHRONIZEDTESTLIB_H
+#define FOLLY_TEST_SYNCHRONIZEDTESTLIB_H
+
+// We have mutex types outside of folly that we want to test with Synchronized.
+// Make it easy for mutex implementators to test their classes with
+// Synchronized by just having a test like:
+//
+// class MyMutex { ... };
+//
+// TEST(Synchronized, Basic) {
+//   testBasic<MyMutex>();
+// }
+//
+// ... similar for testConcurrency, testDualLocking, etc.
+
+
+template <class Mutex> void testBasic();
+
+template <class Mutex> void testConcurrency();
+
+template <class Mutex> void testDualLocking();
+
+template <class Mutex> void testDualLockingWithConst();
+
+template <class Mutex> void testTimedSynchronized();
+
+template <class Mutex> void testConstCopy();
+
+#include "folly/test/SynchronizedTestLib-inl.h"
+
+#endif /* FOLLY_TEST_SYNCHRONIZEDTESTLIB_H */
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/SynchronizedTestLib-inl.h
@@ -0,0 +1,291 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_TEST_SYNCHRONIZEDTESTLIB_INL_H
+#define FOLLY_TEST_SYNCHRONIZEDTESTLIB_INL_H
+
+#include <gtest/gtest.h>
+
+#include <algorithm>
+#include <random>
+#include <functional>
+#include <thread>
+#include <vector>
+#include <glog/logging.h>
+#include "folly/Foreach.h"
+#include "folly/Random.h"
+#include "folly/Synchronized.h"
+
+
+static const auto seed = folly::randomNumberSeed();
+typedef std::mt19937 RandomT;
+static RandomT rng(seed);
+
+template <class Integral1, class Integral2>
+Integral2 random(Integral1 low, Integral2 up) {
+  std::uniform_int_distribution<> range(low, up);
+  return range(rng);
+}
+
+template <class Mutex>
+void testBasic() {
+  folly::Synchronized<std::vector<int>, Mutex> obj;
+
+  obj->resize(1000);
+
+  auto obj2 = obj;
+  EXPECT_EQ(obj2->size(), 1000);
+
+  SYNCHRONIZED (obj) {
+    obj.push_back(10);
+    EXPECT_EQ(obj.size(), 1001);
+    EXPECT_EQ(obj.back(), 10);
+    EXPECT_EQ(obj2->size(), 1000);
+
+    UNSYNCHRONIZED(obj) {
+      EXPECT_EQ(obj->size(), 1001);
+    }
+  }
+
+  SYNCHRONIZED_CONST (obj) {
+    EXPECT_EQ(obj.size(), 1001);
+    UNSYNCHRONIZED(obj) {
+      EXPECT_EQ(obj->size(), 1001);
+    }
+  }
+
+  SYNCHRONIZED (lockedObj, *&obj) {
+    lockedObj.front() = 2;
+  }
+
+  EXPECT_EQ(obj->size(), 1001);
+  EXPECT_EQ(obj->back(), 10);
+  EXPECT_EQ(obj2->size(), 1000);
+
+  EXPECT_EQ(FB_ARG_2_OR_1(1, 2), 2);
+  EXPECT_EQ(FB_ARG_2_OR_1(1), 1);
+}
+
+template <class Mutex> void testConcurrency() {
+  folly::Synchronized<std::vector<int>, Mutex> v;
+
+  struct Local {
+    static bool threadMain(int i,
+                           folly::Synchronized<std::vector<int>, Mutex>& pv) {
+      usleep(::random(100 * 1000, 1000 * 1000));
+
+      // Test operator->
+      pv->push_back(2 * i);
+
+      // Aaand test the SYNCHRONIZED macro
+      SYNCHRONIZED (v, pv) {
+        v.push_back(2 * i + 1);
+      }
+
+      return true;
+    }
+  };
+
+  std::vector<std::thread> results;
+
+  static const size_t threads = 100;
+  FOR_EACH_RANGE (i, 0, threads) {
+    results.push_back(std::thread([&, i]() { Local::threadMain(i, v); }));
+  }
+
+  FOR_EACH (i, results) {
+    i->join();
+  }
+
+  std::vector<int> result;
+  v.swap(result);
+
+  EXPECT_EQ(result.size(), 2 * threads);
+  sort(result.begin(), result.end());
+
+  FOR_EACH_RANGE (i, 0, 2 * threads) {
+    EXPECT_EQ(result[i], i);
+  }
+}
+
+template <class Mutex> void testDualLocking() {
+  folly::Synchronized<std::vector<int>, Mutex> v;
+  folly::Synchronized<std::map<int, int>, Mutex> m;
+
+  struct Local {
+    static bool threadMain(
+      int i,
+      folly::Synchronized<std::vector<int>, Mutex>& pv,
+      folly::Synchronized<std::map<int, int>, Mutex>& pm) {
+
+      usleep(::random(100 * 1000, 1000 * 1000));
+
+      if (i & 1) {
+        SYNCHRONIZED_DUAL (v, pv, m, pm) {
+          v.push_back(i);
+          m[i] = i + 1;
+        }
+      } else {
+        SYNCHRONIZED_DUAL (m, pm, v, pv) {
+          v.push_back(i);
+          m[i] = i + 1;
+        }
+      }
+
+      return true;
+    }
+  };
+
+  std::vector<std::thread> results;
+
+  static const size_t threads = 100;
+  FOR_EACH_RANGE (i, 0, threads) {
+    results.push_back(
+      std::thread([&, i]() { Local::threadMain(i, v, m); }));
+  }
+
+  FOR_EACH (i, results) {
+    i->join();
+  }
+
+  std::vector<int> result;
+  v.swap(result);
+
+  EXPECT_EQ(result.size(), threads);
+  sort(result.begin(), result.end());
+
+  FOR_EACH_RANGE (i, 0, threads) {
+    EXPECT_EQ(result[i], i);
+  }
+}
+
+template <class Mutex> void testDualLockingWithConst() {
+  folly::Synchronized<std::vector<int>, Mutex> v;
+  folly::Synchronized<std::map<int, int>, Mutex> m;
+
+  struct Local {
+    static bool threadMain(
+      int i,
+      folly::Synchronized<std::vector<int>, Mutex>& pv,
+      const folly::Synchronized<std::map<int, int>, Mutex>& pm) {
+
+      usleep(::random(100 * 1000, 1000 * 1000));
+
+      if (i & 1) {
+        SYNCHRONIZED_DUAL (v, pv, m, pm) {
+          size_t s = m.size();
+          v.push_back(i);
+        }
+      } else {
+        SYNCHRONIZED_DUAL (m, pm, v, pv) {
+          size_t s = m.size();
+          v.push_back(i);
+        }
+      }
+
+      return true;
+    }
+  };
+
+  std::vector<std::thread> results;
+
+  static const size_t threads = 100;
+  FOR_EACH_RANGE (i, 0, threads) {
+    results.push_back(
+      std::thread([&, i]() { Local::threadMain(i, v, m); }));
+  }
+
+  FOR_EACH (i, results) {
+    i->join();
+  }
+
+  std::vector<int> result;
+  v.swap(result);
+
+  EXPECT_EQ(result.size(), threads);
+  sort(result.begin(), result.end());
+
+  FOR_EACH_RANGE (i, 0, threads) {
+    EXPECT_EQ(result[i], i);
+  }
+}
+
+template <class Mutex> void testTimedSynchronized() {
+  folly::Synchronized<std::vector<int>, Mutex> v;
+
+  struct Local {
+    static bool threadMain(int i,
+                           folly::Synchronized<std::vector<int>, Mutex>& pv) {
+      usleep(::random(100 * 1000, 1000 * 1000));
+
+      // Test operator->
+      pv->push_back(2 * i);
+
+      // Aaand test the TIMED_SYNCHRONIZED macro
+      for (;;)
+        TIMED_SYNCHRONIZED (10, v, pv) {
+          if (v) {
+            usleep(::random(15 * 1000, 150 * 1000));
+            v->push_back(2 * i + 1);
+            return true;
+          }
+          else {
+            // do nothing
+            usleep(::random(10 * 1000, 100 * 1000));
+          }
+        }
+
+      return true;
+    }
+  };
+
+  std::vector<std::thread> results;
+
+  static const size_t threads = 100;
+  FOR_EACH_RANGE (i, 0, threads) {
+    results.push_back(std::thread([&, i]() { Local::threadMain(i, v); }));
+  }
+
+  FOR_EACH (i, results) {
+    i->join();
+  }
+
+  std::vector<int> result;
+  v.swap(result);
+
+  EXPECT_EQ(result.size(), 2 * threads);
+  sort(result.begin(), result.end());
+
+  FOR_EACH_RANGE (i, 0, 2 * threads) {
+    EXPECT_EQ(result[i], i);
+  }
+}
+
+template <class Mutex> void testConstCopy() {
+  std::vector<int> input = {1, 2, 3};
+  const folly::Synchronized<std::vector<int>, Mutex> v(input);
+
+  std::vector<int> result;
+
+  v.copy(&result);
+  EXPECT_EQ(result, input);
+
+  result = v.copy();
+  EXPECT_EQ(result, input);
+}
+
+
+#endif  /* FOLLY_TEST_SYNCHRONIZEDTESTLIB_INL_H */
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/ThreadCachedArenaTest.cpp
@@ -0,0 +1,266 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/ThreadCachedArena.h"
+#include "folly/Memory.h"
+
+#include <map>
+#include <mutex>
+#include <thread>
+#include <iterator>
+#include <algorithm>
+#include <random>
+#include <unordered_map>
+
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+#include "folly/Range.h"
+#include "folly/Benchmark.h"
+
+using namespace folly;
+
+namespace {
+
+class ArenaTester {
+ public:
+  explicit ArenaTester(ThreadCachedArena& arena) : arena_(&arena) { }
+
+  void allocate(size_t count, size_t maxSize);
+  void verify();
+  void merge(ArenaTester&& other);
+
+ private:
+  std::mutex mergeMutex_;
+  std::vector<std::pair<uint8_t, Range<uint8_t*>>> areas_;
+  ThreadCachedArena* arena_;
+};
+
+void ArenaTester::allocate(size_t count, size_t maxSize) {
+  // Allocate chunks of memory of random sizes
+  std::mt19937 rnd;
+  std::uniform_int_distribution<uint32_t> sizeDist(1, maxSize - 1);
+  areas_.clear();
+  areas_.reserve(count);
+  for (size_t i = 0; i < count; i++) {
+    size_t size = sizeDist(rnd);
+    uint8_t* p = static_cast<uint8_t*>(arena_->allocate(size));
+    areas_.emplace_back(rnd() & 0xff, Range<uint8_t*>(p, size));
+  }
+
+  // Fill each area with a different value, to prove that they don't overlap
+  // Fill in random order.
+  std::random_shuffle(
+      areas_.begin(), areas_.end(),
+      [&rnd] (int n) -> int {
+        return std::uniform_int_distribution<uint32_t>(0, n-1)(rnd);
+      });
+
+  for (auto& p : areas_) {
+    std::fill(p.second.begin(), p.second.end(), p.first);
+  }
+}
+
+void ArenaTester::verify() {
+  for (auto& p : areas_) {
+    for (auto v : p.second) {
+      EXPECT_EQ(p.first, v);
+    }
+  }
+}
+
+void ArenaTester::merge(ArenaTester&& other) {
+  {
+    std::lock_guard<std::mutex> lock(mergeMutex_);
+    std::move(other.areas_.begin(), other.areas_.end(),
+              std::back_inserter(areas_));
+  }
+  other.areas_.clear();
+}
+
+}  // namespace
+
+TEST(ThreadCachedArena, BlockSize) {
+  struct Align { char c; } __attribute__((aligned));
+  static const size_t alignment = alignof(Align);
+  static const size_t requestedBlockSize = 64;
+
+  ThreadCachedArena arena(requestedBlockSize);
+  size_t blockSize = alignment;
+  uint8_t* prev = static_cast<uint8_t*>(arena.allocate(1));
+
+  // Keep allocating until we're no longer one single alignment away from the
+  // previous allocation -- that's when we've gotten to the next block.
+  uint8_t* p;
+  while ((p = static_cast<uint8_t*>(arena.allocate(1))) ==
+         prev + alignment) {
+    prev = p;
+    blockSize += alignment;
+  }
+
+  VLOG(1) << "Requested block size: " << requestedBlockSize << ", actual: "
+          << blockSize;
+  EXPECT_LE(requestedBlockSize, blockSize);
+}
+
+TEST(ThreadCachedArena, SingleThreaded) {
+  static const size_t requestedBlockSize = 64;
+  ThreadCachedArena arena(requestedBlockSize);
+  ArenaTester tester(arena);
+  tester.allocate(100, 100 << 10);
+  tester.verify();
+}
+
+TEST(ThreadCachedArena, MultiThreaded) {
+  static const size_t requestedBlockSize = 64;
+  ThreadCachedArena arena(requestedBlockSize);
+  ArenaTester mainTester(arena);
+
+  // Do this twice, to catch the possibility that memory from the first
+  // round gets freed
+  static const size_t numThreads = 20;
+  for (size_t i = 0; i < 2; i++) {
+    std::vector<std::thread> threads;
+    threads.reserve(numThreads);
+    for (size_t j = 0; j < numThreads; j++) {
+      threads.emplace_back(
+          [&arena, &mainTester] () {
+            ArenaTester tester(arena);
+            tester.allocate(500, 1 << 10);
+            tester.verify();
+            mainTester.merge(std::move(tester));
+          });
+    }
+    for (auto& t : threads) {
+      t.join();
+    }
+  }
+
+  mainTester.verify();
+}
+
+TEST(ThreadCachedArena, StlAllocator) {
+  typedef std::unordered_map<
+    int, int, std::hash<int>, std::equal_to<int>,
+    StlAllocator<ThreadCachedArena, std::pair<const int, int>>> Map;
+
+  static const size_t requestedBlockSize = 64;
+  ThreadCachedArena arena(requestedBlockSize);
+
+  Map map {0, std::hash<int>(), std::equal_to<int>(),
+           StlAllocator<ThreadCachedArena, std::pair<const int, int>>(&arena)};
+
+  for (int i = 0; i < 1000; i++) {
+    map[i] = i;
+  }
+
+  for (int i = 0; i < 1000; i++) {
+    EXPECT_EQ(i, map[i]);
+  }
+}
+
+namespace {
+
+static const int kNumValues = 10000;
+
+BENCHMARK(bmUMStandard, iters) {
+  typedef std::unordered_map<int, int> Map;
+
+  while (iters--) {
+    Map map {0};
+    for (int i = 0; i < kNumValues; i++) {
+      map[i] = i;
+    }
+  }
+}
+
+BENCHMARK(bmUMArena, iters) {
+  typedef std::unordered_map<
+    int, int, std::hash<int>, std::equal_to<int>,
+    StlAllocator<ThreadCachedArena, std::pair<const int, int>>> Map;
+
+  while (iters--) {
+    ThreadCachedArena arena;
+
+    Map map {0, std::hash<int>(), std::equal_to<int>(),
+             StlAllocator<ThreadCachedArena, std::pair<const int, int>>(
+                 &arena)};
+
+    for (int i = 0; i < kNumValues; i++) {
+      map[i] = i;
+    }
+  }
+}
+
+BENCHMARK_DRAW_LINE()
+
+BENCHMARK(bmMStandard, iters) {
+  typedef std::map<int, int> Map;
+
+  while (iters--) {
+    Map map;
+    for (int i = 0; i < kNumValues; i++) {
+      map[i] = i;
+    }
+  }
+}
+
+BENCHMARK_DRAW_LINE()
+
+BENCHMARK(bmMArena, iters) {
+  typedef std::map<
+    int, int, std::less<int>,
+    StlAllocator<ThreadCachedArena, std::pair<const int, int>>> Map;
+
+  while (iters--) {
+    ThreadCachedArena arena;
+
+    Map map {std::less<int>(),
+             StlAllocator<ThreadCachedArena, std::pair<const int, int>>(
+                 &arena)};
+
+    for (int i = 0; i < kNumValues; i++) {
+      map[i] = i;
+    }
+  }
+}
+
+BENCHMARK_DRAW_LINE()
+
+}  // namespace
+
+
+// Benchmark                               Iters   Total t    t/iter iter/sec
+// ----------------------------------------------------------------------------
+// Comparing benchmarks: bmUMStandard,bmUMArena
+//  + 143% bmUMStandard                     1570  2.005 s   1.277 ms  782.9
+// *       bmUMArena                        3817  2.003 s   524.7 us  1.861 k
+// ----------------------------------------------------------------------------
+// Comparing benchmarks: bmMStandard,bmMArena
+//  +79.0% bmMStandard                      1197  2.009 s   1.678 ms  595.8
+// *       bmMArena                         2135  2.002 s   937.6 us  1.042 k
+// ----------------------------------------------------------------------------
+
+int main(int argc, char *argv[]) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  auto ret = RUN_ALL_TESTS();
+  if (!ret && FLAGS_benchmark) {
+    folly::runBenchmarks();
+  }
+  return ret;
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/ThreadCachedIntTest.cpp
@@ -0,0 +1,285 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/ThreadCachedInt.h"
+#include "folly/Hash.h"
+
+#include <atomic>
+#include <thread>
+#include <gtest/gtest.h>
+#include <gflags/gflags.h>
+#include <glog/logging.h>
+#include "folly/Benchmark.h"
+
+using namespace folly;
+
+TEST(ThreadCachedInt, SingleThreadedNotCached) {
+  ThreadCachedInt<int64_t> val(0, 0);
+  EXPECT_EQ(0, val.readFast());
+  ++val;
+  EXPECT_EQ(1, val.readFast());
+  for (int i = 0; i < 41; ++i) {
+    val.increment(1);
+  }
+  EXPECT_EQ(42, val.readFast());
+  --val;
+  EXPECT_EQ(41, val.readFast());
+}
+
+// Note: This is somewhat fragile to the implementation.  If this causes
+// problems, feel free to remove it.
+TEST(ThreadCachedInt, SingleThreadedCached) {
+  ThreadCachedInt<int64_t> val(0, 10);
+  EXPECT_EQ(0, val.readFast());
+  ++val;
+  EXPECT_EQ(0, val.readFast());
+  for (int i = 0; i < 7; ++i) {
+    val.increment(1);
+  }
+  EXPECT_EQ(0, val.readFast());
+  EXPECT_EQ(0, val.readFastAndReset());
+  EXPECT_EQ(8, val.readFull());
+  EXPECT_EQ(8, val.readFullAndReset());
+  EXPECT_EQ(0, val.readFull());
+  EXPECT_EQ(0, val.readFast());
+}
+
+ThreadCachedInt<int32_t> globalInt32(0, 11);
+ThreadCachedInt<int64_t> globalInt64(0, 11);
+int kNumInserts = 100000;
+DEFINE_int32(numThreads, 8, "Number simultaneous threads for benchmarks.");
+#define CREATE_INC_FUNC(size)                                       \
+  void incFunc ## size () {                                         \
+    const int num = kNumInserts / FLAGS_numThreads;                 \
+    for (int i = 0; i < num; ++i) {                                 \
+      ++globalInt ## size ;                                         \
+    }                                                               \
+  }
+CREATE_INC_FUNC(64);
+CREATE_INC_FUNC(32);
+
+// Confirms counts are accurate with competing threads
+TEST(ThreadCachedInt, MultiThreadedCached) {
+  kNumInserts = 100000;
+  CHECK_EQ(0, kNumInserts % FLAGS_numThreads) <<
+    "FLAGS_numThreads must evenly divide kNumInserts (" << kNumInserts << ").";
+  const int numPerThread = kNumInserts / FLAGS_numThreads;
+  ThreadCachedInt<int64_t> TCInt64(0, numPerThread - 2);
+  {
+    std::atomic<bool> run(true);
+    std::atomic<int> threadsDone(0);
+    std::vector<std::thread> threads;
+    for (int i = 0; i < FLAGS_numThreads; ++i) {
+      threads.push_back(std::thread([&] {
+        FOR_EACH_RANGE(k, 0, numPerThread) {
+          ++TCInt64;
+        }
+        std::atomic_fetch_add(&threadsDone, 1);
+        while (run.load()) { usleep(100); }
+      }));
+    }
+
+    // We create and increment another ThreadCachedInt here to make sure it
+    // doesn't interact with the other instances
+    ThreadCachedInt<int64_t> otherTCInt64(0, 10);
+    otherTCInt64.set(33);
+    ++otherTCInt64;
+
+    while (threadsDone.load() < FLAGS_numThreads) { usleep(100); }
+
+    ++otherTCInt64;
+
+    // Threads are done incrementing, but caches have not been flushed yet, so
+    // we have to readFull.
+    EXPECT_NE(kNumInserts, TCInt64.readFast());
+    EXPECT_EQ(kNumInserts, TCInt64.readFull());
+
+    run.store(false);
+    for (auto& t : threads) {
+      t.join();
+    }
+
+  }  // Caches are flushed when threads finish
+  EXPECT_EQ(kNumInserts, TCInt64.readFast());
+}
+
+#define MAKE_MT_CACHE_SIZE_BM(size)                             \
+  void BM_mt_cache_size ## size (int iters, int cacheSize) {    \
+    kNumInserts = iters;                                        \
+    globalInt ## size.set(0);                                   \
+    globalInt ## size.setCacheSize(cacheSize);                  \
+    std::vector<std::thread> threads;                           \
+    for (int i = 0; i < FLAGS_numThreads; ++i) {                \
+      threads.push_back(std::thread(incFunc ## size));          \
+    }                                                           \
+    for (auto& t : threads) {                                   \
+      t.join();                                                 \
+    }                                                           \
+  }
+MAKE_MT_CACHE_SIZE_BM(64);
+MAKE_MT_CACHE_SIZE_BM(32);
+
+#define REG_BASELINE(name, inc_stmt)                            \
+  BENCHMARK(FB_CONCATENATE(BM_mt_baseline_, name), iters) {     \
+    const int iterPerThread = iters / FLAGS_numThreads;         \
+    std::vector<std::thread> threads;                           \
+    for (int i = 0; i < FLAGS_numThreads; ++i) {                \
+      threads.push_back(std::thread([&]() {                     \
+            for (int i = 0; i < iterPerThread; ++i) {           \
+              inc_stmt;                                         \
+            }                                                   \
+          }));                                                  \
+    }                                                           \
+    for (auto& t : threads) {                                   \
+      t.join();                                                 \
+    }                                                           \
+  }
+
+ThreadLocal<int64_t> globalTL64Baseline;
+ThreadLocal<int32_t> globalTL32Baseline;
+std::atomic<int64_t> globalInt64Baseline(0);
+std::atomic<int32_t> globalInt32Baseline(0);
+__thread int64_t global__thread64;
+__thread int32_t global__thread32;
+
+// Alternate lock-free implementation.  Achieves about the same performance,
+// but uses about 20x more memory than ThreadCachedInt with 24 threads.
+struct ShardedAtomicInt {
+  static const int64_t kBuckets_ = 2048;
+  std::atomic<int64_t> ints_[kBuckets_];
+
+  inline void inc(int64_t val = 1) {
+    int bucket = hash::twang_mix64(
+      uint64_t(pthread_self())) & (kBuckets_ - 1);
+    std::atomic_fetch_add(&ints_[bucket], val);
+  }
+
+  // read the first few and extrapolate
+  int64_t readFast() {
+    int64_t ret = 0;
+    static const int numToRead = 8;
+    FOR_EACH_RANGE(i, 0, numToRead) {
+      ret += ints_[i].load(std::memory_order_relaxed);
+    }
+    return ret * (kBuckets_ / numToRead);
+  }
+
+  // readFull is lock-free, but has to do thousands of loads...
+  int64_t readFull() {
+    int64_t ret = 0;
+    for (auto& i : ints_) {
+      // Fun fact - using memory_order_consume below reduces perf 30-40% in high
+      // contention benchmarks.
+      ret += i.load(std::memory_order_relaxed);
+    }
+    return ret;
+  }
+};
+ShardedAtomicInt shd_int64;
+
+REG_BASELINE(_thread64, global__thread64 += 1);
+REG_BASELINE(_thread32, global__thread32 += 1);
+REG_BASELINE(ThreadLocal64, *globalTL64Baseline += 1);
+REG_BASELINE(ThreadLocal32, *globalTL32Baseline += 1);
+REG_BASELINE(atomic_inc64,
+             std::atomic_fetch_add(&globalInt64Baseline, int64_t(1)));
+REG_BASELINE(atomic_inc32,
+             std::atomic_fetch_add(&globalInt32Baseline, int32_t(1)));
+REG_BASELINE(ShardedAtm64, shd_int64.inc());
+
+BENCHMARK_PARAM(BM_mt_cache_size64, 0);
+BENCHMARK_PARAM(BM_mt_cache_size64, 10);
+BENCHMARK_PARAM(BM_mt_cache_size64, 100);
+BENCHMARK_PARAM(BM_mt_cache_size64, 1000);
+BENCHMARK_PARAM(BM_mt_cache_size32, 0);
+BENCHMARK_PARAM(BM_mt_cache_size32, 10);
+BENCHMARK_PARAM(BM_mt_cache_size32, 100);
+BENCHMARK_PARAM(BM_mt_cache_size32, 1000);
+BENCHMARK_DRAW_LINE();
+
+// single threaded
+BENCHMARK(Atomic_readFull) {
+  doNotOptimizeAway(globalInt64Baseline.load(std::memory_order_relaxed));
+}
+BENCHMARK(ThrCache_readFull) {
+  doNotOptimizeAway(globalInt64.readFull());
+}
+BENCHMARK(Sharded_readFull) {
+  doNotOptimizeAway(shd_int64.readFull());
+}
+BENCHMARK(ThrCache_readFast) {
+  doNotOptimizeAway(globalInt64.readFast());
+}
+BENCHMARK(Sharded_readFast) {
+  doNotOptimizeAway(shd_int64.readFast());
+}
+BENCHMARK_DRAW_LINE();
+
+// multi threaded
+REG_BASELINE(Atomic_readFull,
+      doNotOptimizeAway(globalInt64Baseline.load(std::memory_order_relaxed)));
+REG_BASELINE(ThrCache_readFull, doNotOptimizeAway(globalInt64.readFull()));
+REG_BASELINE(Sharded_readFull, doNotOptimizeAway(shd_int64.readFull()));
+REG_BASELINE(ThrCache_readFast, doNotOptimizeAway(globalInt64.readFast()));
+REG_BASELINE(Sharded_readFast, doNotOptimizeAway(shd_int64.readFast()));
+BENCHMARK_DRAW_LINE();
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  google::SetCommandLineOptionWithMode(
+    "bm_min_usec", "10000", google::SET_FLAG_IF_DEFAULT
+  );
+  if (FLAGS_benchmark) {
+    folly::runBenchmarks();
+  }
+  return RUN_ALL_TESTS();
+}
+
+/*
+ Ran with 20 threads on dual 12-core Xeon(R) X5650 @ 2.67GHz with 12-MB caches
+
+ Benchmark                               Iters   Total t    t/iter iter/sec
+ ------------------------------------------------------------------------------
+ + 103% BM_mt_baseline__thread64     10000000  13.54 ms  1.354 ns  704.4 M
+*       BM_mt_baseline__thread32     10000000  6.651 ms  665.1 ps    1.4 G
+ +50.3% BM_mt_baseline_ThreadLocal64  10000000  9.994 ms  999.4 ps  954.2 M
+ +49.9% BM_mt_baseline_ThreadLocal32  10000000  9.972 ms  997.2 ps  956.4 M
+ +2650% BM_mt_baseline_atomic_inc64  10000000  182.9 ms  18.29 ns  52.13 M
+ +2665% BM_mt_baseline_atomic_inc32  10000000  183.9 ms  18.39 ns  51.85 M
+ +75.3% BM_mt_baseline_ShardedAtm64  10000000  11.66 ms  1.166 ns  817.8 M
+ +6670% BM_mt_cache_size64/0         10000000  450.3 ms  45.03 ns  21.18 M
+ +1644% BM_mt_cache_size64/10        10000000    116 ms   11.6 ns   82.2 M
+ + 381% BM_mt_cache_size64/100       10000000  32.04 ms  3.204 ns  297.7 M
+ + 129% BM_mt_cache_size64/1000      10000000  15.24 ms  1.524 ns  625.8 M
+ +6052% BM_mt_cache_size32/0         10000000  409.2 ms  40.92 ns  23.31 M
+ +1304% BM_mt_cache_size32/10        10000000  93.39 ms  9.339 ns  102.1 M
+ + 298% BM_mt_cache_size32/100       10000000  26.52 ms  2.651 ns  359.7 M
+ +68.1% BM_mt_cache_size32/1000      10000000  11.18 ms  1.118 ns  852.9 M
+------------------------------------------------------------------------------
+ +10.4% Atomic_readFull              10000000  36.05 ms  3.605 ns  264.5 M
+ + 619% ThrCache_readFull            10000000  235.1 ms  23.51 ns  40.57 M
+ SLOW   Sharded_readFull              1981093      2 s    1.01 us  967.3 k
+*       ThrCache_readFast            10000000  32.65 ms  3.265 ns  292.1 M
+ +10.0% Sharded_readFast             10000000  35.92 ms  3.592 ns  265.5 M
+------------------------------------------------------------------------------
+ +4.54% BM_mt_baseline_Atomic_readFull  10000000  8.672 ms  867.2 ps  1.074 G
+ SLOW   BM_mt_baseline_ThrCache_readFull  10000000  996.9 ms  99.69 ns  9.567 M
+ SLOW   BM_mt_baseline_Sharded_readFull  10000000  891.5 ms  89.15 ns   10.7 M
+*       BM_mt_baseline_ThrCache_readFast  10000000  8.295 ms  829.5 ps  1.123 G
+ +12.7% BM_mt_baseline_Sharded_readFast  10000000  9.348 ms  934.8 ps   1020 M
+------------------------------------------------------------------------------
+*/
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/ThreadLocalTest.cpp
@@ -0,0 +1,579 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/ThreadLocal.h"
+
+#include <sys/types.h>
+#include <sys/wait.h>
+#include <unistd.h>
+
+#include <array>
+#include <atomic>
+#include <chrono>
+#include <condition_variable>
+#include <map>
+#include <mutex>
+#include <set>
+#include <thread>
+#include <unordered_map>
+
+#include <boost/thread/tss.hpp>
+#include <gflags/gflags.h>
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+#include "folly/Benchmark.h"
+
+using namespace folly;
+
+struct Widget {
+  static int totalVal_;
+  int val_;
+  ~Widget() {
+    totalVal_ += val_;
+  }
+
+  static void customDeleter(Widget* w, TLPDestructionMode mode) {
+    totalVal_ += (mode == TLPDestructionMode::ALL_THREADS) * 1000;
+    delete w;
+  }
+};
+int Widget::totalVal_ = 0;
+
+TEST(ThreadLocalPtr, BasicDestructor) {
+  Widget::totalVal_ = 0;
+  ThreadLocalPtr<Widget> w;
+  std::thread([&w]() {
+      w.reset(new Widget());
+      w.get()->val_ += 10;
+    }).join();
+  EXPECT_EQ(10, Widget::totalVal_);
+}
+
+TEST(ThreadLocalPtr, CustomDeleter1) {
+  Widget::totalVal_ = 0;
+  {
+    ThreadLocalPtr<Widget> w;
+    std::thread([&w]() {
+        w.reset(new Widget(), Widget::customDeleter);
+        w.get()->val_ += 10;
+      }).join();
+    EXPECT_EQ(10, Widget::totalVal_);
+  }
+  EXPECT_EQ(10, Widget::totalVal_);
+}
+
+TEST(ThreadLocalPtr, resetNull) {
+  ThreadLocalPtr<int> tl;
+  EXPECT_FALSE(tl);
+  tl.reset(new int(4));
+  EXPECT_TRUE(static_cast<bool>(tl));
+  EXPECT_EQ(*tl.get(), 4);
+  tl.reset();
+  EXPECT_FALSE(tl);
+}
+
+// Test deleting the ThreadLocalPtr object
+TEST(ThreadLocalPtr, CustomDeleter2) {
+  Widget::totalVal_ = 0;
+  std::thread t;
+  std::mutex mutex;
+  std::condition_variable cv;
+  enum class State {
+    START,
+    DONE,
+    EXIT
+  };
+  State state = State::START;
+  {
+    ThreadLocalPtr<Widget> w;
+    t = std::thread([&]() {
+        w.reset(new Widget(), Widget::customDeleter);
+        w.get()->val_ += 10;
+
+        // Notify main thread that we're done
+        {
+          std::unique_lock<std::mutex> lock(mutex);
+          state = State::DONE;
+          cv.notify_all();
+        }
+
+        // Wait for main thread to allow us to exit
+        {
+          std::unique_lock<std::mutex> lock(mutex);
+          while (state != State::EXIT) {
+            cv.wait(lock);
+          }
+        }
+    });
+
+    // Wait for main thread to start (and set w.get()->val_)
+    {
+      std::unique_lock<std::mutex> lock(mutex);
+      while (state != State::DONE) {
+        cv.wait(lock);
+      }
+    }
+
+    // Thread started but hasn't exited yet
+    EXPECT_EQ(0, Widget::totalVal_);
+
+    // Destroy ThreadLocalPtr<Widget> (by letting it go out of scope)
+  }
+
+  EXPECT_EQ(1010, Widget::totalVal_);
+
+  // Allow thread to exit
+  {
+    std::unique_lock<std::mutex> lock(mutex);
+    state = State::EXIT;
+    cv.notify_all();
+  }
+  t.join();
+
+  EXPECT_EQ(1010, Widget::totalVal_);
+}
+
+TEST(ThreadLocal, BasicDestructor) {
+  Widget::totalVal_ = 0;
+  ThreadLocal<Widget> w;
+  std::thread([&w]() { w->val_ += 10; }).join();
+  EXPECT_EQ(10, Widget::totalVal_);
+}
+
+TEST(ThreadLocal, SimpleRepeatDestructor) {
+  Widget::totalVal_ = 0;
+  {
+    ThreadLocal<Widget> w;
+    w->val_ += 10;
+  }
+  {
+    ThreadLocal<Widget> w;
+    w->val_ += 10;
+  }
+  EXPECT_EQ(20, Widget::totalVal_);
+}
+
+TEST(ThreadLocal, InterleavedDestructors) {
+  Widget::totalVal_ = 0;
+  ThreadLocal<Widget>* w = NULL;
+  int wVersion = 0;
+  const int wVersionMax = 2;
+  int thIter = 0;
+  std::mutex lock;
+  auto th = std::thread([&]() {
+    int wVersionPrev = 0;
+    while (true) {
+      while (true) {
+        std::lock_guard<std::mutex> g(lock);
+        if (wVersion > wVersionMax) {
+          return;
+        }
+        if (wVersion > wVersionPrev) {
+          // We have a new version of w, so it should be initialized to zero
+          EXPECT_EQ((*w)->val_, 0);
+          break;
+        }
+      }
+      std::lock_guard<std::mutex> g(lock);
+      wVersionPrev = wVersion;
+      (*w)->val_ += 10;
+      ++thIter;
+    }
+  });
+  FOR_EACH_RANGE(i, 0, wVersionMax) {
+    int thIterPrev = 0;
+    {
+      std::lock_guard<std::mutex> g(lock);
+      thIterPrev = thIter;
+      delete w;
+      w = new ThreadLocal<Widget>();
+      ++wVersion;
+    }
+    while (true) {
+      std::lock_guard<std::mutex> g(lock);
+      if (thIter > thIterPrev) {
+        break;
+      }
+    }
+  }
+  {
+    std::lock_guard<std::mutex> g(lock);
+    wVersion = wVersionMax + 1;
+  }
+  th.join();
+  EXPECT_EQ(wVersionMax * 10, Widget::totalVal_);
+}
+
+class SimpleThreadCachedInt {
+
+  class NewTag;
+  ThreadLocal<int,NewTag> val_;
+
+ public:
+  void add(int val) {
+    *val_ += val;
+  }
+
+  int read() {
+    int ret = 0;
+    for (const auto& i : val_.accessAllThreads()) {
+      ret += i;
+    }
+    return ret;
+  }
+};
+
+TEST(ThreadLocalPtr, AccessAllThreadsCounter) {
+  const int kNumThreads = 10;
+  SimpleThreadCachedInt stci;
+  std::atomic<bool> run(true);
+  std::atomic<int> totalAtomic(0);
+  std::vector<std::thread> threads;
+  for (int i = 0; i < kNumThreads; ++i) {
+    threads.push_back(std::thread([&,i]() {
+      stci.add(1);
+      totalAtomic.fetch_add(1);
+      while (run.load()) { usleep(100); }
+    }));
+  }
+  while (totalAtomic.load() != kNumThreads) { usleep(100); }
+  EXPECT_EQ(kNumThreads, stci.read());
+  run.store(false);
+  for (auto& t : threads) {
+    t.join();
+  }
+}
+
+TEST(ThreadLocal, resetNull) {
+  ThreadLocal<int> tl;
+  tl.reset(new int(4));
+  EXPECT_EQ(*tl.get(), 4);
+  tl.reset();
+  EXPECT_EQ(*tl.get(), 0);
+  tl.reset(new int(5));
+  EXPECT_EQ(*tl.get(), 5);
+}
+
+namespace {
+struct Tag {};
+
+struct Foo {
+  folly::ThreadLocal<int, Tag> tl;
+};
+}  // namespace
+
+TEST(ThreadLocal, Movable1) {
+  Foo a;
+  Foo b;
+  EXPECT_TRUE(a.tl.get() != b.tl.get());
+
+  a = Foo();
+  b = Foo();
+  EXPECT_TRUE(a.tl.get() != b.tl.get());
+}
+
+TEST(ThreadLocal, Movable2) {
+  std::map<int, Foo> map;
+
+  map[42];
+  map[10];
+  map[23];
+  map[100];
+
+  std::set<void*> tls;
+  for (auto& m : map) {
+    tls.insert(m.second.tl.get());
+  }
+
+  // Make sure that we have 4 different instances of *tl
+  EXPECT_EQ(4, tls.size());
+}
+
+namespace {
+
+constexpr size_t kFillObjectSize = 300;
+
+std::atomic<uint64_t> gDestroyed;
+
+/**
+ * Fill a chunk of memory with a unique-ish pattern that includes the thread id
+ * (so deleting one of these from another thread would cause a failure)
+ *
+ * Verify it explicitly and on destruction.
+ */
+class FillObject {
+ public:
+  explicit FillObject(uint64_t idx) : idx_(idx) {
+    uint64_t v = val();
+    for (size_t i = 0; i < kFillObjectSize; ++i) {
+      data_[i] = v;
+    }
+  }
+
+  void check() {
+    uint64_t v = val();
+    for (size_t i = 0; i < kFillObjectSize; ++i) {
+      CHECK_EQ(v, data_[i]);
+    }
+  }
+
+  ~FillObject() {
+    ++gDestroyed;
+  }
+
+ private:
+  uint64_t val() const {
+    return (idx_ << 40) | uint64_t(pthread_self());
+  }
+
+  uint64_t idx_;
+  uint64_t data_[kFillObjectSize];
+};
+
+}  // namespace
+
+#if FOLLY_HAVE_STD__THIS_THREAD__SLEEP_FOR
+TEST(ThreadLocal, Stress) {
+  constexpr size_t numFillObjects = 250;
+  std::array<ThreadLocalPtr<FillObject>, numFillObjects> objects;
+
+  constexpr size_t numThreads = 32;
+  constexpr size_t numReps = 20;
+
+  std::vector<std::thread> threads;
+  threads.reserve(numThreads);
+
+  for (size_t i = 0; i < numThreads; ++i) {
+    threads.emplace_back([&objects] {
+      for (size_t rep = 0; rep < numReps; ++rep) {
+        for (size_t i = 0; i < objects.size(); ++i) {
+          objects[i].reset(new FillObject(rep * objects.size() + i));
+          std::this_thread::sleep_for(std::chrono::microseconds(100));
+        }
+        for (size_t i = 0; i < objects.size(); ++i) {
+          objects[i]->check();
+        }
+      }
+    });
+  }
+
+  for (auto& t : threads) {
+    t.join();
+  }
+
+  EXPECT_EQ(numFillObjects * numThreads * numReps, gDestroyed);
+}
+#endif
+
+// Yes, threads and fork don't mix
+// (http://cppwisdom.quora.com/Why-threads-and-fork-dont-mix) but if you're
+// stupid or desperate enough to try, we shouldn't stand in your way.
+namespace {
+class HoldsOne {
+ public:
+  HoldsOne() : value_(1) { }
+  // Do an actual access to catch the buggy case where this == nullptr
+  int value() const { return value_; }
+ private:
+  int value_;
+};
+
+struct HoldsOneTag {};
+
+ThreadLocal<HoldsOne, HoldsOneTag> ptr;
+
+int totalValue() {
+  int value = 0;
+  for (auto& p : ptr.accessAllThreads()) {
+    value += p.value();
+  }
+  return value;
+}
+
+}  // namespace
+
+TEST(ThreadLocal, Fork) {
+  EXPECT_EQ(1, ptr->value());  // ensure created
+  EXPECT_EQ(1, totalValue());
+  // Spawn a new thread
+
+  std::mutex mutex;
+  bool started = false;
+  std::condition_variable startedCond;
+  bool stopped = false;
+  std::condition_variable stoppedCond;
+
+  std::thread t([&] () {
+    EXPECT_EQ(1, ptr->value());  // ensure created
+    {
+      std::unique_lock<std::mutex> lock(mutex);
+      started = true;
+      startedCond.notify_all();
+    }
+    {
+      std::unique_lock<std::mutex> lock(mutex);
+      while (!stopped) {
+        stoppedCond.wait(lock);
+      }
+    }
+  });
+
+  {
+    std::unique_lock<std::mutex> lock(mutex);
+    while (!started) {
+      startedCond.wait(lock);
+    }
+  }
+
+  EXPECT_EQ(2, totalValue());
+
+  pid_t pid = fork();
+  if (pid == 0) {
+    // in child
+    int v = totalValue();
+
+    // exit successfully if v == 1 (one thread)
+    // diagnostic error code otherwise :)
+    switch (v) {
+    case 1: _exit(0);
+    case 0: _exit(1);
+    }
+    _exit(2);
+  } else if (pid > 0) {
+    // in parent
+    int status;
+    EXPECT_EQ(pid, waitpid(pid, &status, 0));
+    EXPECT_TRUE(WIFEXITED(status));
+    EXPECT_EQ(0, WEXITSTATUS(status));
+  } else {
+    EXPECT_TRUE(false) << "fork failed";
+  }
+
+  EXPECT_EQ(2, totalValue());
+
+  {
+    std::unique_lock<std::mutex> lock(mutex);
+    stopped = true;
+    stoppedCond.notify_all();
+  }
+
+  t.join();
+
+  EXPECT_EQ(1, totalValue());
+}
+
+struct HoldsOneTag2 {};
+
+TEST(ThreadLocal, Fork2) {
+  // A thread-local tag that was used in the parent from a *different* thread
+  // (but not the forking thread) would cause the child to hang in a
+  // ThreadLocalPtr's object destructor. Yeah.
+  ThreadLocal<HoldsOne, HoldsOneTag2> p;
+  {
+    // use tag in different thread
+    std::thread t([&p] { p.get(); });
+    t.join();
+  }
+  pid_t pid = fork();
+  if (pid == 0) {
+    {
+      ThreadLocal<HoldsOne, HoldsOneTag2> q;
+      q.get();
+    }
+    _exit(0);
+  } else if (pid > 0) {
+    int status;
+    EXPECT_EQ(pid, waitpid(pid, &status, 0));
+    EXPECT_TRUE(WIFEXITED(status));
+    EXPECT_EQ(0, WEXITSTATUS(status));
+  } else {
+    EXPECT_TRUE(false) << "fork failed";
+  }
+}
+
+// Simple reference implementation using pthread_get_specific
+template<typename T>
+class PThreadGetSpecific {
+ public:
+  PThreadGetSpecific() : key_(0) {
+    pthread_key_create(&key_, OnThreadExit);
+  }
+
+  T* get() const {
+    return static_cast<T*>(pthread_getspecific(key_));
+  }
+
+  void reset(T* t) {
+    delete get();
+    pthread_setspecific(key_, t);
+  }
+  static void OnThreadExit(void* obj) {
+    delete static_cast<T*>(obj);
+  }
+ private:
+  pthread_key_t key_;
+};
+
+DEFINE_int32(numThreads, 8, "Number simultaneous threads for benchmarks.");
+
+#define REG(var)                                                \
+  BENCHMARK(FB_CONCATENATE(BM_mt_, var), iters) {               \
+    const int itersPerThread = iters / FLAGS_numThreads;        \
+    std::vector<std::thread> threads;                           \
+    for (int i = 0; i < FLAGS_numThreads; ++i) {                \
+      threads.push_back(std::thread([&]() {                     \
+        var.reset(new int(0));                                  \
+        for (int i = 0; i < itersPerThread; ++i) {              \
+          ++(*var.get());                                       \
+        }                                                       \
+      }));                                                      \
+    }                                                           \
+    for (auto& t : threads) {                                   \
+      t.join();                                                 \
+    }                                                           \
+  }
+
+ThreadLocalPtr<int> tlp;
+REG(tlp);
+PThreadGetSpecific<int> pthread_get_specific;
+REG(pthread_get_specific);
+boost::thread_specific_ptr<int> boost_tsp;
+REG(boost_tsp);
+BENCHMARK_DRAW_LINE();
+
+int main(int argc, char** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  google::SetCommandLineOptionWithMode(
+    "bm_max_iters", "100000000", google::SET_FLAG_IF_DEFAULT
+  );
+  if (FLAGS_benchmark) {
+    folly::runBenchmarks();
+  }
+  return RUN_ALL_TESTS();
+}
+
+/*
+Ran with 24 threads on dual 12-core Xeon(R) X5650 @ 2.67GHz with 12-MB caches
+
+Benchmark                               Iters   Total t    t/iter iter/sec
+------------------------------------------------------------------------------
+*       BM_mt_tlp                   100000000  39.88 ms  398.8 ps  2.335 G
+ +5.91% BM_mt_pthread_get_specific  100000000  42.23 ms  422.3 ps  2.205 G
+ + 295% BM_mt_boost_tsp             100000000  157.8 ms  1.578 ns  604.5 M
+------------------------------------------------------------------------------
+*/
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/TimeoutQueueTest.cpp
@@ -0,0 +1,113 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <gtest/gtest.h>
+#include "folly/TimeoutQueue.h"
+
+using namespace folly;
+
+TEST(TimeoutQueue, Simple) {
+  typedef std::vector<TimeoutQueue::Id> EventVec;
+  EventVec events;
+
+  TimeoutQueue q;
+  TimeoutQueue::Callback cb =
+    [&events](TimeoutQueue::Id id, int64_t now) {
+      events.push_back(id);
+    };
+
+  EXPECT_EQ(1, q.add(0, 10, cb));
+  EXPECT_EQ(2, q.add(0, 11, cb));
+  EXPECT_EQ(3, q.addRepeating(0, 9, cb));
+
+  EXPECT_TRUE(events.empty());
+  EXPECT_EQ(21, q.runOnce(12));  // now+9
+
+  bool r = (EventVec{3,1,2} == events);
+  EXPECT_TRUE(r);
+
+  events.clear();
+  EXPECT_EQ(49, q.runOnce(40));
+  r = (EventVec{3} == events);
+  EXPECT_TRUE(r);
+}
+
+TEST(TimeoutQueue, Erase) {
+  typedef std::vector<TimeoutQueue::Id> EventVec;
+  EventVec events;
+
+  TimeoutQueue q;
+  TimeoutQueue::Callback cb =
+    [&events, &q](TimeoutQueue::Id id, int64_t now) {
+      events.push_back(id);
+      if (id == 2) {
+        q.erase(1);
+      }
+    };
+
+  EXPECT_EQ(1, q.addRepeating(0, 10, cb));
+  EXPECT_EQ(2, q.add(0, 35, cb));
+
+  int64_t now = 0;
+  while (now < std::numeric_limits<int64_t>::max()) {
+    now = q.runOnce(now);
+  }
+
+  bool r = (EventVec{1,1,1,2} == events);
+  EXPECT_TRUE(r);
+}
+
+TEST(TimeoutQueue, RunOnceRepeating) {
+  int count = 0;
+  TimeoutQueue q;
+  TimeoutQueue::Callback cb =
+    [&count, &q](TimeoutQueue::Id id, int64_t now) {
+      if (++count == 100) {
+        EXPECT_TRUE(q.erase(id));
+      }
+    };
+
+  EXPECT_EQ(1, q.addRepeating(0, 0, cb));
+
+  EXPECT_EQ(0, q.runOnce(0));
+  EXPECT_EQ(1, count);
+  EXPECT_EQ(0, q.runOnce(0));
+  EXPECT_EQ(2, count);
+  EXPECT_EQ(std::numeric_limits<int64_t>::max(), q.runLoop(0));
+  EXPECT_EQ(100, count);
+}
+
+TEST(TimeoutQueue, RunOnceReschedule) {
+  int count = 0;
+  TimeoutQueue q;
+  TimeoutQueue::Callback cb;
+  cb = [&count, &q, &cb](TimeoutQueue::Id id, int64_t now) {
+      if (++count < 100) {
+        EXPECT_LT(id, q.add(now, 0, cb));
+      }
+    };
+
+  EXPECT_EQ(1, q.add(0, 0, cb));
+
+  int64_t now = 0;
+  EXPECT_EQ(0, q.runOnce(0));
+  EXPECT_EQ(1, count);
+  EXPECT_EQ(0, q.runOnce(0));
+  EXPECT_EQ(2, count);
+  EXPECT_EQ(std::numeric_limits<int64_t>::max(), q.runLoop(0));
+  EXPECT_EQ(100, count);
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/TimeseriesBenchmark.cpp
@@ -0,0 +1,77 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#include "folly/stats/BucketedTimeSeries.h"
+#include "folly/stats/BucketedTimeSeries-defs.h"
+
+#include <glog/logging.h>
+
+#include "folly/Benchmark.h"
+
+using std::chrono::seconds;
+using folly::BenchmarkSuspender;
+using folly::BucketedTimeSeries;
+
+void addValue(unsigned int iters,
+              seconds duration, size_t numBuckets,
+              size_t callsPerSecond) {
+  BenchmarkSuspender suspend;
+  BucketedTimeSeries<int64_t> ts(numBuckets, duration);
+  suspend.dismiss();
+
+  seconds currentTime(1342000000);
+  size_t timeCounter = 0;
+  for (unsigned int n = 0; n < iters; ++n, ++timeCounter) {
+    if (timeCounter >= callsPerSecond) {
+      timeCounter = 0;
+      ++currentTime;
+    }
+    ts.addValue(currentTime, n);
+  }
+}
+
+BENCHMARK_NAMED_PARAM(addValue, AllTime_1perSec, seconds(0), 60, 1);
+BENCHMARK_NAMED_PARAM(addValue, 3600x60_1perSec, seconds(3600), 60, 1);
+BENCHMARK_NAMED_PARAM(addValue, 600x60_1perSec, seconds(600), 60, 1);
+BENCHMARK_NAMED_PARAM(addValue, 60x60_1perSec, seconds(60), 60, 1);
+BENCHMARK_NAMED_PARAM(addValue, 100x10_1perSec, seconds(100), 10, 1);
+BENCHMARK_NAMED_PARAM(addValue, 71x5_1perSec, seconds(71), 5, 1);
+BENCHMARK_NAMED_PARAM(addValue, 1x1_1perSec, seconds(1), 1, 1);
+
+BENCHMARK_DRAW_LINE()
+
+BENCHMARK_NAMED_PARAM(addValue, AllTime_10perSec, seconds(0), 60, 10);
+BENCHMARK_NAMED_PARAM(addValue, 3600x60_10perSec, seconds(3600), 60, 10);
+BENCHMARK_NAMED_PARAM(addValue, 600x60_10perSec, seconds(600), 60, 10);
+BENCHMARK_NAMED_PARAM(addValue, 60x60_10perSec, seconds(60), 60, 10);
+BENCHMARK_NAMED_PARAM(addValue, 100x10_10perSec, seconds(100), 10, 10);
+BENCHMARK_NAMED_PARAM(addValue, 71x5_10perSec, seconds(71), 5, 10);
+BENCHMARK_NAMED_PARAM(addValue, 1x1_10perSec, seconds(1), 1, 10);
+
+BENCHMARK_DRAW_LINE()
+
+BENCHMARK_NAMED_PARAM(addValue, AllTime_100perSec, seconds(0), 60, 100);
+BENCHMARK_NAMED_PARAM(addValue, 3600x60_100perSec, seconds(3600), 60, 100);
+BENCHMARK_NAMED_PARAM(addValue, 600x60_100perSec, seconds(600), 60, 100);
+BENCHMARK_NAMED_PARAM(addValue, 60x60_100perSec, seconds(60), 60, 100);
+BENCHMARK_NAMED_PARAM(addValue, 100x10_100perSec, seconds(100), 10, 100);
+BENCHMARK_NAMED_PARAM(addValue, 71x5_100perSec, seconds(71), 5, 100);
+BENCHMARK_NAMED_PARAM(addValue, 1x1_100perSec, seconds(1), 1, 100);
+
+int main(int argc, char *argv[]) {
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  folly::runBenchmarks();
+  return 0;
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/TimeseriesHistogramTest.cpp
@@ -0,0 +1,487 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/stats/TimeseriesHistogram.h"
+#include "folly/stats/TimeseriesHistogram-defs.h"
+
+#include <gtest/gtest.h>
+
+using namespace std;
+using namespace folly;
+using std::chrono::seconds;
+
+namespace IntMTMHTS {
+  enum Levels {
+    MINUTE,
+    TEN_MINUTE,
+    HOUR,
+    ALLTIME,
+    NUM_LEVELS,
+  };
+
+  const seconds kDurations[] = {
+    seconds(60), seconds(600), seconds(3600), seconds(0)
+  };
+};
+
+namespace IntMHTS {
+  enum Levels {
+    MINUTE,
+    HOUR,
+    ALLTIME,
+    NUM_LEVELS,
+  };
+
+  const seconds kDurations[] = {
+    seconds(60), seconds(3600), seconds(0)
+  };
+};
+
+typedef std::mt19937 RandomInt32;
+
+TEST(TimeseriesHistogram, Percentile) {
+  RandomInt32 random(5);
+  // [10, 109], 12 buckets including above and below
+  {
+    TimeseriesHistogram<int> h(10, 10, 110,
+                               MultiLevelTimeSeries<int>(
+                                 60, IntMTMHTS::NUM_LEVELS,
+                                 IntMTMHTS::kDurations));
+
+    EXPECT_EQ(0, h.getPercentileEstimate(0, IntMTMHTS::ALLTIME));
+
+    EXPECT_EQ(12, h.getNumBuckets());
+    EXPECT_EQ(10, h.getBucketSize());
+    EXPECT_EQ(10, h.getMin());
+    EXPECT_EQ(110, h.getMax());
+
+    for (int i = 0; i < h.getNumBuckets(); ++i) {
+      EXPECT_EQ(4, h.getBucket(i).numLevels());
+    }
+
+    int maxVal = 120;
+    h.addValue(seconds(0), 0);
+    h.addValue(seconds(0), maxVal);
+    for (int i = 0; i < 98; i++) {
+      h.addValue(seconds(0), random() % maxVal);
+    }
+
+    h.update(std::chrono::duration_cast<std::chrono::seconds>(
+               std::chrono::system_clock::now().time_since_epoch()));
+    // bucket 0 stores everything below min, so its minimum
+    // is the lowest possible number
+    EXPECT_EQ(std::numeric_limits<int>::min(),
+              h.getPercentileBucketMin(1, IntMTMHTS::ALLTIME));
+    EXPECT_EQ(110, h.getPercentileBucketMin(99, IntMTMHTS::ALLTIME));
+
+    EXPECT_EQ(-2, h.getPercentileEstimate(0, IntMTMHTS::ALLTIME));
+    EXPECT_EQ(-1, h.getPercentileEstimate(1, IntMTMHTS::ALLTIME));
+    EXPECT_EQ(119, h.getPercentileEstimate(99, IntMTMHTS::ALLTIME));
+    EXPECT_EQ(120, h.getPercentileEstimate(100, IntMTMHTS::ALLTIME));
+  }
+}
+
+TEST(TimeseriesHistogram, String) {
+  RandomInt32 random(5);
+  // [10, 109], 12 buckets including above and below
+  {
+    TimeseriesHistogram<int> hist(10, 10, 110,
+                                  MultiLevelTimeSeries<int>(
+                                    60, IntMTMHTS::NUM_LEVELS,
+                                    IntMTMHTS::kDurations));
+
+    int maxVal = 120;
+    hist.addValue(seconds(0), 0);
+    hist.addValue(seconds(0), maxVal);
+    for (int i = 0; i < 98; i++) {
+      hist.addValue(seconds(0), random() % maxVal);
+    }
+
+    hist.update(seconds(0));
+
+    const char* const kStringValues1[IntMTMHTS::NUM_LEVELS] =  {
+      "-2147483648:12:4,10:8:13,20:8:24,30:6:34,40:13:46,50:8:54,60:7:64,"
+        "70:7:74,80:8:84,90:10:94,100:3:103,110:10:115",
+      "-2147483648:12:4,10:8:13,20:8:24,30:6:34,40:13:46,50:8:54,60:7:64,"
+        "70:7:74,80:8:84,90:10:94,100:3:103,110:10:115",
+      "-2147483648:12:4,10:8:13,20:8:24,30:6:34,40:13:46,50:8:54,60:7:64,"
+        "70:7:74,80:8:84,90:10:94,100:3:103,110:10:115",
+      "-2147483648:12:4,10:8:13,20:8:24,30:6:34,40:13:46,50:8:54,60:7:64,"
+        "70:7:74,80:8:84,90:10:94,100:3:103,110:10:115",
+    };
+
+    CHECK_EQ(IntMTMHTS::NUM_LEVELS, hist.getNumLevels());
+
+    for (int level = 0; level < hist.getNumLevels(); ++level) {
+      EXPECT_EQ(kStringValues1[level], hist.getString(level));
+    }
+
+    const char* const kStringValues2[IntMTMHTS::NUM_LEVELS] =  {
+      "-2147483648:12:4,10:8:13,20:8:24,30:6:34,40:13:46,50:8:54,60:7:64,"
+        "70:7:74,80:8:84,90:10:94,100:3:103,110:10:115",
+      "-2147483648:12:4,10:8:13,20:8:24,30:6:34,40:13:46,50:8:54,60:7:64,"
+        "70:7:74,80:8:84,90:10:94,100:3:103,110:10:115",
+      "-2147483648:12:4,10:8:13,20:8:24,30:6:34,40:13:46,50:8:54,60:7:64,"
+        "70:7:74,80:8:84,90:10:94,100:3:103,110:10:115",
+      "-2147483648:12:4,10:8:13,20:8:24,30:6:34,40:13:46,50:8:54,60:7:64,"
+        "70:7:74,80:8:84,90:10:94,100:3:103,110:10:115",
+    };
+
+    CHECK_EQ(IntMTMHTS::NUM_LEVELS, hist.getNumLevels());
+
+    for (int level = 0; level < hist.getNumLevels(); ++level) {
+      EXPECT_EQ(kStringValues2[level], hist.getString(level));
+    }
+  }
+}
+
+TEST(TimeseriesHistogram, Clear) {
+  {
+    TimeseriesHistogram<int> hist(10, 0, 100,
+                                  MultiLevelTimeSeries<int>(
+                                    60, IntMTMHTS::NUM_LEVELS,
+                                    IntMTMHTS::kDurations));
+
+    for (int now = 0; now < 3600; now++) {
+      for (int i = 0; i < 100; i++) {
+        hist.addValue(seconds(now), i, 2);  // adds each item 2 times
+      }
+    }
+
+    // check clearing
+    hist.clear();
+
+    for (int b = 0; b < hist.getNumBuckets(); ++b) {
+      EXPECT_EQ(0, hist.getBucket(b).count(IntMTMHTS::MINUTE));
+      EXPECT_EQ(0, hist.getBucket(b).count(IntMTMHTS::TEN_MINUTE));
+      EXPECT_EQ(0, hist.getBucket(b).count(IntMTMHTS::HOUR));
+      EXPECT_EQ(0, hist.getBucket(b).count(IntMTMHTS::ALLTIME));
+    }
+
+    for (int pct = 0; pct <= 100; pct++) {
+      EXPECT_EQ(0, hist.getPercentileBucketMin(pct, IntMTMHTS::MINUTE));
+      EXPECT_EQ(0, hist.getPercentileBucketMin(pct, IntMTMHTS::TEN_MINUTE));
+      EXPECT_EQ(0, hist.getPercentileBucketMin(pct, IntMTMHTS::HOUR));
+      EXPECT_EQ(0, hist.getPercentileBucketMin(pct, IntMTMHTS::ALLTIME));
+
+      EXPECT_EQ(0, hist.getPercentileEstimate(pct, IntMTMHTS::MINUTE));
+      EXPECT_EQ(0, hist.getPercentileEstimate(pct, IntMTMHTS::TEN_MINUTE));
+      EXPECT_EQ(0, hist.getPercentileEstimate(pct, IntMTMHTS::HOUR));
+      EXPECT_EQ(0, hist.getPercentileEstimate(pct, IntMTMHTS::ALLTIME));
+    }
+  }
+}
+
+
+TEST(TimeseriesHistogram, Basic) {
+  {
+    TimeseriesHistogram<int> hist(10, 0, 100,
+                                  MultiLevelTimeSeries<int>(
+                                    60, IntMTMHTS::NUM_LEVELS,
+                                    IntMTMHTS::kDurations));
+
+    for (int now = 0; now < 3600; now++) {
+      for (int i = 0; i < 100; i++) {
+        hist.addValue(seconds(now), i);
+      }
+    }
+
+    hist.update(seconds(3599));
+    for (int pct = 1; pct <= 100; pct++) {
+      int expected = (pct - 1) / 10 * 10;
+      EXPECT_EQ(expected, hist.getPercentileBucketMin(pct, IntMTMHTS::MINUTE));
+      EXPECT_EQ(expected, hist.getPercentileBucketMin(pct,
+                                                      IntMTMHTS::TEN_MINUTE));
+      EXPECT_EQ(expected, hist.getPercentileBucketMin(pct, IntMTMHTS::HOUR));
+      EXPECT_EQ(expected, hist.getPercentileBucketMin(pct, IntMTMHTS::ALLTIME));
+    }
+
+    for (int b = 1; (b + 1) < hist.getNumBuckets(); ++b) {
+      EXPECT_EQ(600, hist.getBucket(b).count(IntMTMHTS::MINUTE));
+      EXPECT_EQ(6000, hist.getBucket(b).count(IntMTMHTS::TEN_MINUTE));
+      EXPECT_EQ(36000, hist.getBucket(b).count(IntMTMHTS::HOUR));
+      EXPECT_EQ(36000, hist.getBucket(b).count(IntMTMHTS::ALLTIME));
+    }
+    EXPECT_EQ(0, hist.getBucket(0).count(IntMTMHTS::MINUTE));
+    EXPECT_EQ(0, hist.getBucket(hist.getNumBuckets() - 1).count(
+                IntMTMHTS::MINUTE));
+  }
+
+  // -----------------
+
+  {
+    TimeseriesHistogram<int> hist(10, 0, 100,
+                                  MultiLevelTimeSeries<int>(
+                                    60, IntMTMHTS::NUM_LEVELS,
+                                    IntMTMHTS::kDurations));
+
+    for (int now = 0; now < 3600; now++) {
+      for (int i = 0; i < 100; i++) {
+        hist.addValue(seconds(now), i, 2);  // adds each item 2 times
+      }
+    }
+
+    hist.update(seconds(3599));
+    for (int pct = 1; pct <= 100; pct++) {
+      int expected = (pct - 1) / 10 * 10;
+      EXPECT_EQ(expected, hist.getPercentileBucketMin(pct, IntMTMHTS::MINUTE));
+      EXPECT_EQ(expected, hist.getPercentileBucketMin(pct,
+                                                      IntMTMHTS::TEN_MINUTE));
+      EXPECT_EQ(expected, hist.getPercentileBucketMin(pct, IntMTMHTS::HOUR));
+      EXPECT_EQ(expected, hist.getPercentileBucketMin(pct, IntMTMHTS::ALLTIME));
+   }
+
+    for (int b = 1; (b + 1) < hist.getNumBuckets(); ++b) {
+      EXPECT_EQ(600 * 2, hist.getBucket(b).count(IntMTMHTS::MINUTE));
+      EXPECT_EQ(6000 * 2, hist.getBucket(b).count(IntMTMHTS::TEN_MINUTE));
+      EXPECT_EQ(36000 * 2, hist.getBucket(b).count(IntMTMHTS::HOUR));
+      EXPECT_EQ(36000 * 2, hist.getBucket(b).count(IntMTMHTS::ALLTIME));
+    }
+    EXPECT_EQ(0, hist.getBucket(0).count(IntMTMHTS::MINUTE));
+    EXPECT_EQ(0, hist.getBucket(hist.getNumBuckets() - 1).count(
+                IntMTMHTS::MINUTE));
+  }
+
+  // -----------------
+
+  {
+    TimeseriesHistogram<int> hist(10, 0, 100,
+                                  MultiLevelTimeSeries<int>(
+                                    60, IntMTMHTS::NUM_LEVELS,
+                                    IntMTMHTS::kDurations));
+
+    for (int now = 0; now < 3600; now++) {
+      for (int i = 0; i < 50; i++) {
+        hist.addValue(seconds(now), i * 2, 2);  // adds each item 2 times
+      }
+    }
+
+    hist.update(seconds(3599));
+    for (int pct = 1; pct <= 100; pct++) {
+      int expected = (pct - 1) / 10 * 10;
+      EXPECT_EQ(expected, hist.getPercentileBucketMin(pct, IntMTMHTS::MINUTE));
+      EXPECT_EQ(expected, hist.getPercentileBucketMin(pct,
+                                                      IntMTMHTS::TEN_MINUTE));
+      EXPECT_EQ(expected, hist.getPercentileBucketMin(pct, IntMTMHTS::HOUR));
+      EXPECT_EQ(expected, hist.getPercentileBucketMin(pct, IntMTMHTS::ALLTIME));
+    }
+
+    EXPECT_EQ(0, hist.getBucket(0).count(IntMTMHTS::MINUTE));
+    EXPECT_EQ(0, hist.getBucket(0).count(IntMTMHTS::TEN_MINUTE));
+    EXPECT_EQ(0, hist.getBucket(0).count(IntMTMHTS::HOUR));
+    EXPECT_EQ(0, hist.getBucket(0).count(IntMTMHTS::ALLTIME));
+    EXPECT_EQ(0, hist.getBucket(hist.getNumBuckets() - 1).count(
+                IntMTMHTS::MINUTE));
+    EXPECT_EQ(0,
+              hist.getBucket(hist.getNumBuckets() - 1).
+                count(IntMTMHTS::TEN_MINUTE));
+    EXPECT_EQ(0, hist.getBucket(hist.getNumBuckets() - 1).count(
+                IntMTMHTS::HOUR));
+    EXPECT_EQ(0,
+              hist.getBucket(hist.getNumBuckets() - 1).count(
+                IntMTMHTS::ALLTIME));
+
+    for (int b = 1; (b + 1) < hist.getNumBuckets(); ++b) {
+      EXPECT_EQ(600, hist.getBucket(b).count(IntMTMHTS::MINUTE));
+      EXPECT_EQ(6000, hist.getBucket(b).count(IntMTMHTS::TEN_MINUTE));
+      EXPECT_EQ(36000, hist.getBucket(b).count(IntMTMHTS::HOUR));
+      EXPECT_EQ(36000, hist.getBucket(b).count(IntMTMHTS::ALLTIME));
+    }
+
+    for (int i = 0; i < 100; ++i) {
+      hist.addValue(seconds(3599), 200 + i);
+    }
+    hist.update(seconds(3599));
+    EXPECT_EQ(100,
+              hist.getBucket(hist.getNumBuckets() - 1).count(
+                IntMTMHTS::ALLTIME));
+
+  }
+}
+
+TEST(TimeseriesHistogram, QueryByInterval) {
+  TimeseriesHistogram<int> mhts(8, 8, 120,
+                                MultiLevelTimeSeries<int>(
+                                  60, IntMHTS::NUM_LEVELS,
+                                  IntMHTS::kDurations));
+
+  mhts.update(seconds(0));
+
+  int curTime;
+  for (curTime = 0; curTime < 7200; curTime++) {
+    mhts.addValue(seconds(curTime), 1);
+  }
+  for (curTime = 7200; curTime < 7200 + 3540; curTime++) {
+    mhts.addValue(seconds(curTime), 10);
+  }
+  for (curTime = 7200 + 3540; curTime < 7200 + 3600; curTime++) {
+    mhts.addValue(seconds(curTime), 100);
+  }
+
+  mhts.update(seconds(7200 + 3600 - 1));
+
+  struct TimeInterval {
+    TimeInterval(int s, int e)
+      : start(s), end(e) {}
+
+    std::chrono::seconds start;
+    std::chrono::seconds end;
+  };
+  TimeInterval intervals[12] = {
+    { curTime - 60, curTime },
+    { curTime - 3600, curTime },
+    { curTime - 7200, curTime },
+    { curTime - 3600, curTime - 60 },
+    { curTime - 7200, curTime - 60 },
+    { curTime - 7200, curTime - 3600 },
+    { curTime - 50, curTime - 20 },
+    { curTime - 3020, curTime - 20 },
+    { curTime - 7200, curTime - 20 },
+    { curTime - 3000, curTime - 1000 },
+    { curTime - 7200, curTime - 1000 },
+    { curTime - 7200, curTime - 3600 },
+  };
+
+  int expectedSums[12] = {
+    6000, 41400, 32400, 35400, 32129, 16200, 3000, 33600, 32308, 20000, 27899,
+    16200
+  };
+
+  int expectedCounts[12] = {
+    60, 3600, 7200, 3540, 7139, 3600, 30, 3000, 7178, 2000, 6199, 3600
+  };
+
+  // The first 7200 values added all fell below the histogram minimum,
+  // and went into the bucket that tracks all of the too-small values.
+  // This bucket reports a minimum value of the smallest possible integer.
+  int belowMinBucket = std::numeric_limits<int>::min();
+
+  int expectedValues[12][3] = {
+    {96, 96, 96},
+    { 8,  8, 96},
+    { belowMinBucket,  belowMinBucket,  8}, // alltime
+    { 8,  8,  8},
+    { belowMinBucket,  belowMinBucket,  8}, // alltime
+    { belowMinBucket,  belowMinBucket,  8}, // alltime
+    {96, 96, 96},
+    { 8,  8, 96},
+    { belowMinBucket,  belowMinBucket,  8}, // alltime
+    { 8,  8,  8},
+    { belowMinBucket,  belowMinBucket,  8}, // alltime
+    { belowMinBucket,  belowMinBucket,  8}  // alltime
+  };
+
+  for (int i = 0; i < 12; i++) {
+    const auto& itv = intervals[i];
+    int s = mhts.sum(itv.start, itv.end);
+    EXPECT_EQ(expectedSums[i], s);
+
+    int c = mhts.count(itv.start, itv.end);
+    EXPECT_EQ(expectedCounts[i], c);
+  }
+
+  // 3 levels
+  for (int i = 1; i <= 100; i++) {
+    EXPECT_EQ(96, mhts.getPercentileBucketMin(i, 0));
+    EXPECT_EQ(96, mhts.getPercentileBucketMin(i, seconds(curTime - 60),
+                                              seconds(curTime)));
+    EXPECT_EQ(8, mhts.getPercentileBucketMin(i, seconds(curTime - 3540),
+                                             seconds(curTime - 60)));
+  }
+
+  EXPECT_EQ(8, mhts.getPercentileBucketMin(1, 1));
+  EXPECT_EQ(8, mhts.getPercentileBucketMin(98, 1));
+  EXPECT_EQ(96, mhts.getPercentileBucketMin(99, 1));
+  EXPECT_EQ(96, mhts.getPercentileBucketMin(100, 1));
+
+  EXPECT_EQ(belowMinBucket, mhts.getPercentileBucketMin(1, 2));
+  EXPECT_EQ(belowMinBucket, mhts.getPercentileBucketMin(66, 2));
+  EXPECT_EQ(8, mhts.getPercentileBucketMin(67, 2));
+  EXPECT_EQ(8, mhts.getPercentileBucketMin(99, 2));
+  EXPECT_EQ(96, mhts.getPercentileBucketMin(100, 2));
+
+  // 0 is currently the value for bucket 0 (below min)
+  for (int i = 0; i < 12; i++) {
+    const auto& itv = intervals[i];
+    int v = mhts.getPercentileBucketMin(1, itv.start, itv.end);
+    EXPECT_EQ(expectedValues[i][0], v);
+
+    v = mhts.getPercentileBucketMin(50, itv.start, itv.end);
+    EXPECT_EQ(expectedValues[i][1], v);
+
+    v = mhts.getPercentileBucketMin(99, itv.start, itv.end);
+    EXPECT_EQ(expectedValues[i][2], v);
+  }
+
+  for (int i = 0; i < 12; i++) {
+    const auto& itv = intervals[i];
+    int c = mhts.count(itv.start, itv.end);
+    // Some of the older intervals that fall in the alltime bucket
+    // are off by 1 or 2 in their estimated counts.
+    size_t tolerance = 0;
+    if (itv.start <= seconds(curTime - 7200)) {
+      tolerance = 2;
+    } else if (itv.start <= seconds(curTime - 3000)) {
+      tolerance = 1;
+    }
+    size_t actualCount = (itv.end - itv.start).count();
+    size_t estimatedCount = mhts.count(itv.start, itv.end);
+    EXPECT_GE(actualCount, estimatedCount);
+    EXPECT_LE(actualCount - tolerance, estimatedCount);
+  }
+}
+
+TEST(TimeseriesHistogram, SingleUniqueValue) {
+  int values[] = {-1, 0, 500, 1000, 1500};
+  for (int ii = 0; ii < 5; ++ii) {
+    int value = values[ii];
+    TimeseriesHistogram<int> h(10, 0, 1000,
+                               MultiLevelTimeSeries<int>(
+                                 60, IntMTMHTS::NUM_LEVELS,
+                                 IntMTMHTS::kDurations));
+
+    const int kNumIters = 1000;
+    for (int jj = 0; jj < kNumIters; ++jj) {
+      h.addValue(seconds(time(nullptr)), value);
+    }
+    h.update(seconds(time(nullptr)));
+    // since we've only added one unique value, all percentiles should
+    // be that value
+    EXPECT_EQ(h.getPercentileEstimate(10, 0), value);
+    EXPECT_EQ(h.getPercentileEstimate(50, 0), value);
+    EXPECT_EQ(h.getPercentileEstimate(99, 0), value);
+
+    // Things get trickier if there are multiple unique values.
+    const int kNewValue = 750;
+    for (int kk = 0; kk < 2*kNumIters; ++kk) {
+      h.addValue(seconds(time(nullptr)), kNewValue);
+    }
+    h.update(seconds(time(nullptr)));
+    EXPECT_NEAR(h.getPercentileEstimate(50, 0), kNewValue+5, 5);
+    if (value >= 0 && value <= 1000) {
+      // only do further testing if value is within our bucket range,
+      // else estimates can be wildly off
+      if (kNewValue > value) {
+        EXPECT_NEAR(h.getPercentileEstimate(10, 0), value+5, 5);
+        EXPECT_NEAR(h.getPercentileEstimate(99, 0), kNewValue+5, 5);
+      } else {
+        EXPECT_NEAR(h.getPercentileEstimate(10, 0), kNewValue+5, 5);
+        EXPECT_NEAR(h.getPercentileEstimate(99, 0), value+5, 5);
+      }
+    }
+  }
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/TimeseriesTest.cpp
@@ -0,0 +1,911 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/stats/BucketedTimeSeries.h"
+#include "folly/stats/BucketedTimeSeries-defs.h"
+#include "folly/stats/MultiLevelTimeSeries.h"
+#include "folly/stats/MultiLevelTimeSeries-defs.h"
+
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+#include "folly/Foreach.h"
+
+using std::chrono::seconds;
+using std::string;
+using std::vector;
+using folly::BucketedTimeSeries;
+
+struct TestData {
+  size_t duration;
+  size_t numBuckets;
+  vector<ssize_t> bucketStarts;
+};
+vector<TestData> testData = {
+  // 71 seconds x 4 buckets
+  { 71, 4, {0, 18, 36, 54}},
+  // 100 seconds x 10 buckets
+  { 100, 10, {0, 10, 20, 30, 40, 50, 60, 70, 80, 90}},
+  // 10 seconds x 10 buckets
+  { 10, 10, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}},
+  // 10 seconds x 1 buckets
+  { 10, 1, {0}},
+  // 1 second x 1 buckets
+  { 1, 1, {0}},
+};
+
+TEST(BucketedTimeSeries, getBucketInfo) {
+  for (const auto& data : testData) {
+    BucketedTimeSeries<int64_t> ts(data.numBuckets, seconds(data.duration));
+
+    for (uint32_t n = 0; n < 10000; n += 1234) {
+      seconds offset(n * data.duration);
+
+      for (uint32_t idx = 0; idx < data.numBuckets; ++idx) {
+        seconds bucketStart(data.bucketStarts[idx]);
+        seconds nextBucketStart;
+        if (idx + 1 < data.numBuckets) {
+            nextBucketStart = seconds(data.bucketStarts[idx + 1]);
+        } else {
+            nextBucketStart = seconds(data.duration);
+        }
+
+        seconds expectedStart = offset + bucketStart;
+        seconds expectedNextStart = offset + nextBucketStart;
+        seconds midpoint = (expectedStart + expectedNextStart) / 2;
+
+        vector<std::pair<string, seconds>> timePoints = {
+          {"expectedStart", expectedStart},
+          {"midpoint", midpoint},
+          {"expectedEnd", expectedNextStart - seconds(1)},
+        };
+
+        for (const auto& point : timePoints) {
+          // Check that getBucketIdx() returns the expected index
+          EXPECT_EQ(idx, ts.getBucketIdx(point.second)) <<
+            data.duration << "x" << data.numBuckets << ": " <<
+            point.first << "=" << point.second.count();
+
+          // Check the data returned by getBucketInfo()
+          size_t returnedIdx;
+          seconds returnedStart;
+          seconds returnedNextStart;
+          ts.getBucketInfo(expectedStart, &returnedIdx,
+                           &returnedStart, &returnedNextStart);
+          EXPECT_EQ(idx, returnedIdx) <<
+            data.duration << "x" << data.numBuckets << ": " <<
+            point.first << "=" << point.second.count();
+          EXPECT_EQ(expectedStart.count(), returnedStart.count()) <<
+            data.duration << "x" << data.numBuckets << ": " <<
+            point.first << "=" << point.second.count();
+          EXPECT_EQ(expectedNextStart.count(), returnedNextStart.count()) <<
+            data.duration << "x" << data.numBuckets << ": " <<
+            point.first << "=" << point.second.count();
+        }
+      }
+    }
+  }
+}
+
+void testUpdate100x10(size_t offset) {
+  // This test code only works when offset is a multiple of the bucket width
+  CHECK_EQ(0, offset % 10);
+
+  // Create a 100 second timeseries, with 10 buckets
+  BucketedTimeSeries<int64_t> ts(10, seconds(100));
+
+  auto setup = [&] {
+    ts.clear();
+    // Add 1 value to each bucket
+    for (int n = 5; n <= 95; n += 10) {
+      ts.addValue(seconds(n + offset), 6);
+    }
+
+    EXPECT_EQ(10, ts.count());
+    EXPECT_EQ(60, ts.sum());
+    EXPECT_EQ(6, ts.avg());
+  };
+
+  // Update 2 buckets forwards.  This should throw away 2 data points.
+  setup();
+  ts.update(seconds(110 + offset));
+  EXPECT_EQ(8, ts.count());
+  EXPECT_EQ(48, ts.sum());
+  EXPECT_EQ(6, ts.avg());
+
+  // The last time we added was 95.
+  // Try updating to 189.  This should clear everything but the last bucket.
+  setup();
+  ts.update(seconds(151 + offset));
+  EXPECT_EQ(4, ts.count());
+  //EXPECT_EQ(6, ts.sum());
+  EXPECT_EQ(6, ts.avg());
+
+  // The last time we added was 95.
+  // Try updating to 193: This is nearly one full loop around,
+  // back to the same bucket.  update() needs to clear everything
+  setup();
+  ts.update(seconds(193 + offset));
+  EXPECT_EQ(0, ts.count());
+  EXPECT_EQ(0, ts.sum());
+  EXPECT_EQ(0, ts.avg());
+
+  // The last time we added was 95.
+  // Try updating to 197: This is slightly over one full loop around,
+  // back to the same bucket.  update() needs to clear everything
+  setup();
+  ts.update(seconds(197 + offset));
+  EXPECT_EQ(0, ts.count());
+  EXPECT_EQ(0, ts.sum());
+  EXPECT_EQ(0, ts.avg());
+
+  // The last time we added was 95.
+  // Try updating to 230: This is well over one full loop around,
+  // and everything should be cleared.
+  setup();
+  ts.update(seconds(230 + offset));
+  EXPECT_EQ(0, ts.count());
+  EXPECT_EQ(0, ts.sum());
+  EXPECT_EQ(0, ts.avg());
+}
+
+TEST(BucketedTimeSeries, update100x10) {
+  // Run testUpdate100x10() multiple times, with various offsets.
+  // This makes sure the update code works regardless of which bucket it starts
+  // at in the modulo arithmetic.
+  testUpdate100x10(0);
+  testUpdate100x10(50);
+  testUpdate100x10(370);
+  testUpdate100x10(1937090);
+}
+
+TEST(BucketedTimeSeries, update71x5) {
+  // Create a 71 second timeseries, with 5 buckets
+  // This tests when the number of buckets does not divide evenly into the
+  // duration.
+  BucketedTimeSeries<int64_t> ts(5, seconds(71));
+
+  auto setup = [&] {
+    ts.clear();
+    // Add 1 value to each bucket
+    ts.addValue(seconds(11), 6);
+    ts.addValue(seconds(24), 6);
+    ts.addValue(seconds(42), 6);
+    ts.addValue(seconds(43), 6);
+    ts.addValue(seconds(66), 6);
+
+    EXPECT_EQ(5, ts.count());
+    EXPECT_EQ(30, ts.sum());
+    EXPECT_EQ(6, ts.avg());
+  };
+
+  // Update 2 buckets forwards.  This should throw away 2 data points.
+  setup();
+  ts.update(seconds(99));
+  EXPECT_EQ(3, ts.count());
+  EXPECT_EQ(18, ts.sum());
+  EXPECT_EQ(6, ts.avg());
+
+  // Update 3 buckets forwards.  This should throw away 3 data points.
+  setup();
+  ts.update(seconds(100));
+  EXPECT_EQ(2, ts.count());
+  EXPECT_EQ(12, ts.sum());
+  EXPECT_EQ(6, ts.avg());
+
+  // Update 4 buckets forwards, just under the wrap limit.
+  // This should throw everything but the last bucket away.
+  setup();
+  ts.update(seconds(127));
+  EXPECT_EQ(1, ts.count());
+  EXPECT_EQ(6, ts.sum());
+  EXPECT_EQ(6, ts.avg());
+
+  // Update 5 buckets forwards, exactly at the wrap limit.
+  // This should throw everything away.
+  setup();
+  ts.update(seconds(128));
+  EXPECT_EQ(0, ts.count());
+  EXPECT_EQ(0, ts.sum());
+  EXPECT_EQ(0, ts.avg());
+
+  // Update very far forwards, wrapping multiple times.
+  // This should throw everything away.
+  setup();
+  ts.update(seconds(1234));
+  EXPECT_EQ(0, ts.count());
+  EXPECT_EQ(0, ts.sum());
+  EXPECT_EQ(0, ts.avg());
+}
+
+TEST(BucketedTimeSeries, elapsed) {
+  BucketedTimeSeries<int64_t> ts(60, seconds(600));
+
+  // elapsed() is 0 when no data points have been added
+  EXPECT_EQ(0, ts.elapsed().count());
+
+  // With exactly 1 data point, elapsed() should report 1 second of data
+  seconds start(239218);
+  ts.addValue(start + seconds(0), 200);
+  EXPECT_EQ(1, ts.elapsed().count());
+  // Adding a data point 10 seconds later should result in an elapsed time of
+  // 11 seconds (the time range is [0, 10], inclusive).
+  ts.addValue(start + seconds(10), 200);
+  EXPECT_EQ(11, ts.elapsed().count());
+
+  // elapsed() returns to 0 after clear()
+  ts.clear();
+  EXPECT_EQ(0, ts.elapsed().count());
+
+  // Restart, with the starting point on an easier number to work with
+  ts.addValue(seconds(10), 200);
+  EXPECT_EQ(1, ts.elapsed().count());
+  ts.addValue(seconds(580), 200);
+  EXPECT_EQ(571, ts.elapsed().count());
+  ts.addValue(seconds(590), 200);
+  EXPECT_EQ(581, ts.elapsed().count());
+  ts.addValue(seconds(598), 200);
+  EXPECT_EQ(589, ts.elapsed().count());
+  ts.addValue(seconds(599), 200);
+  EXPECT_EQ(590, ts.elapsed().count());
+  ts.addValue(seconds(600), 200);
+  EXPECT_EQ(591, ts.elapsed().count());
+  ts.addValue(seconds(608), 200);
+  EXPECT_EQ(599, ts.elapsed().count());
+  ts.addValue(seconds(609), 200);
+  EXPECT_EQ(600, ts.elapsed().count());
+  // Once we reach 600 seconds worth of data, when we move on to the next
+  // second a full bucket will get thrown out.  Now we drop back down to 591
+  // seconds worth of data
+  ts.addValue(seconds(610), 200);
+  EXPECT_EQ(591, ts.elapsed().count());
+  ts.addValue(seconds(618), 200);
+  EXPECT_EQ(599, ts.elapsed().count());
+  ts.addValue(seconds(619), 200);
+  EXPECT_EQ(600, ts.elapsed().count());
+  ts.addValue(seconds(620), 200);
+  EXPECT_EQ(591, ts.elapsed().count());
+  ts.addValue(seconds(123419), 200);
+  EXPECT_EQ(600, ts.elapsed().count());
+  ts.addValue(seconds(123420), 200);
+  EXPECT_EQ(591, ts.elapsed().count());
+  ts.addValue(seconds(123425), 200);
+  EXPECT_EQ(596, ts.elapsed().count());
+
+  // Time never moves backwards.
+  // Calling update with an old timestamp will just be ignored.
+  ts.update(seconds(29));
+  EXPECT_EQ(596, ts.elapsed().count());
+}
+
+TEST(BucketedTimeSeries, rate) {
+  BucketedTimeSeries<int64_t> ts(60, seconds(600));
+
+  // Add 3 values every 2 seconds, until fill up the buckets
+  for (size_t n = 0; n < 600; n += 2) {
+    ts.addValue(seconds(n), 200, 3);
+  }
+
+  EXPECT_EQ(900, ts.count());
+  EXPECT_EQ(180000, ts.sum());
+  EXPECT_EQ(200, ts.avg());
+
+  // Really we only entered 599 seconds worth of data: [0, 598] (inclusive)
+  EXPECT_EQ(599, ts.elapsed().count());
+  EXPECT_NEAR(300.5, ts.rate(), 0.005);
+  EXPECT_NEAR(1.5, ts.countRate(), 0.005);
+
+  // If we add 1 more second, now we will have 600 seconds worth of data
+  ts.update(seconds(599));
+  EXPECT_EQ(600, ts.elapsed().count());
+  EXPECT_NEAR(300, ts.rate(), 0.005);
+  EXPECT_EQ(300, ts.rate<int>());
+  EXPECT_NEAR(1.5, ts.countRate(), 0.005);
+
+  // However, 1 more second after that and we will have filled up all the
+  // buckets, and have to drop one.
+  ts.update(seconds(600));
+  EXPECT_EQ(591, ts.elapsed().count());
+  EXPECT_NEAR(299.5, ts.rate(), 0.01);
+  EXPECT_EQ(299, ts.rate<int>());
+  EXPECT_NEAR(1.5, ts.countRate(), 0.005);
+}
+
+TEST(BucketedTimeSeries, avgTypeConversion) {
+  // Make sure the computed average values are accurate regardless
+  // of the input type and return type.
+
+  {
+    // Simple sanity tests for small positive integer values
+    BucketedTimeSeries<int64_t> ts(60, seconds(600));
+    ts.addValue(seconds(0), 4, 100);
+    ts.addValue(seconds(0), 10, 200);
+    ts.addValue(seconds(0), 16, 100);
+
+    EXPECT_DOUBLE_EQ(10.0, ts.avg());
+    EXPECT_DOUBLE_EQ(10.0, ts.avg<float>());
+    EXPECT_EQ(10, ts.avg<uint64_t>());
+    EXPECT_EQ(10, ts.avg<int64_t>());
+    EXPECT_EQ(10, ts.avg<int32_t>());
+    EXPECT_EQ(10, ts.avg<int16_t>());
+    EXPECT_EQ(10, ts.avg<int8_t>());
+    EXPECT_EQ(10, ts.avg<uint8_t>());
+  }
+
+  {
+    // Test signed integer types with negative values
+    BucketedTimeSeries<int64_t> ts(60, seconds(600));
+    ts.addValue(seconds(0), -100);
+    ts.addValue(seconds(0), -200);
+    ts.addValue(seconds(0), -300);
+    ts.addValue(seconds(0), -200, 65535);
+
+    EXPECT_DOUBLE_EQ(-200.0, ts.avg());
+    EXPECT_DOUBLE_EQ(-200.0, ts.avg<float>());
+    EXPECT_EQ(-200, ts.avg<int64_t>());
+    EXPECT_EQ(-200, ts.avg<int32_t>());
+    EXPECT_EQ(-200, ts.avg<int16_t>());
+  }
+
+  {
+    // Test uint64_t values that would overflow int64_t
+    BucketedTimeSeries<uint64_t> ts(60, seconds(600));
+    ts.addValueAggregated(seconds(0),
+                          std::numeric_limits<uint64_t>::max(),
+                          std::numeric_limits<uint64_t>::max());
+
+    EXPECT_DOUBLE_EQ(1.0, ts.avg());
+    EXPECT_DOUBLE_EQ(1.0, ts.avg<float>());
+    EXPECT_EQ(1, ts.avg<uint64_t>());
+    EXPECT_EQ(1, ts.avg<int64_t>());
+    EXPECT_EQ(1, ts.avg<int8_t>());
+  }
+
+  {
+    // Test doubles with small-ish values that will fit in integer types
+    BucketedTimeSeries<double> ts(60, seconds(600));
+    ts.addValue(seconds(0), 4.0, 100);
+    ts.addValue(seconds(0), 10.0, 200);
+    ts.addValue(seconds(0), 16.0, 100);
+
+    EXPECT_DOUBLE_EQ(10.0, ts.avg());
+    EXPECT_DOUBLE_EQ(10.0, ts.avg<float>());
+    EXPECT_EQ(10, ts.avg<uint64_t>());
+    EXPECT_EQ(10, ts.avg<int64_t>());
+    EXPECT_EQ(10, ts.avg<int32_t>());
+    EXPECT_EQ(10, ts.avg<int16_t>());
+    EXPECT_EQ(10, ts.avg<int8_t>());
+    EXPECT_EQ(10, ts.avg<uint8_t>());
+  }
+
+  {
+    // Test doubles with huge values
+    BucketedTimeSeries<double> ts(60, seconds(600));
+    ts.addValue(seconds(0), 1e19, 100);
+    ts.addValue(seconds(0), 2e19, 200);
+    ts.addValue(seconds(0), 3e19, 100);
+
+    EXPECT_DOUBLE_EQ(ts.avg(), 2e19);
+    EXPECT_NEAR(ts.avg<float>(), 2e19, 1e11);
+  }
+
+  {
+    // Test doubles where the sum adds up larger than a uint64_t,
+    // but the average fits in an int64_t
+    BucketedTimeSeries<double> ts(60, seconds(600));
+    uint64_t value = 0x3fffffffffffffff;
+    FOR_EACH_RANGE(i, 0, 16) {
+      ts.addValue(seconds(0), value);
+    }
+
+    EXPECT_DOUBLE_EQ(value, ts.avg());
+    EXPECT_DOUBLE_EQ(value, ts.avg<float>());
+    // Some precision is lost here due to the huge sum, so the
+    // integer average returned is off by one.
+    EXPECT_NEAR(value, ts.avg<uint64_t>(), 1);
+    EXPECT_NEAR(value, ts.avg<int64_t>(), 1);
+  }
+
+  {
+    // Test BucketedTimeSeries with a smaller integer type
+    BucketedTimeSeries<int16_t> ts(60, seconds(600));
+    FOR_EACH_RANGE(i, 0, 101) {
+      ts.addValue(seconds(0), i);
+    }
+
+    EXPECT_DOUBLE_EQ(50.0, ts.avg());
+    EXPECT_DOUBLE_EQ(50.0, ts.avg<float>());
+    EXPECT_EQ(50, ts.avg<uint64_t>());
+    EXPECT_EQ(50, ts.avg<int64_t>());
+    EXPECT_EQ(50, ts.avg<int16_t>());
+    EXPECT_EQ(50, ts.avg<int8_t>());
+  }
+
+  {
+    // Test BucketedTimeSeries with long double input
+    BucketedTimeSeries<long double> ts(60, seconds(600));
+    ts.addValueAggregated(seconds(0), 1000.0L, 7);
+
+    long double expected = 1000.0L / 7.0L;
+    EXPECT_DOUBLE_EQ(static_cast<double>(expected), ts.avg());
+    EXPECT_DOUBLE_EQ(static_cast<float>(expected), ts.avg<float>());
+    EXPECT_DOUBLE_EQ(expected, ts.avg<long double>());
+    EXPECT_EQ(static_cast<uint64_t>(expected), ts.avg<uint64_t>());
+    EXPECT_EQ(static_cast<int64_t>(expected), ts.avg<int64_t>());
+  }
+
+  {
+    // Test BucketedTimeSeries with int64_t values,
+    // but using an average that requires a fair amount of precision.
+    BucketedTimeSeries<int64_t> ts(60, seconds(600));
+    ts.addValueAggregated(seconds(0), 1000, 7);
+
+    long double expected = 1000.0L / 7.0L;
+    EXPECT_DOUBLE_EQ(static_cast<double>(expected), ts.avg());
+    EXPECT_DOUBLE_EQ(static_cast<float>(expected), ts.avg<float>());
+    EXPECT_DOUBLE_EQ(expected, ts.avg<long double>());
+    EXPECT_EQ(static_cast<uint64_t>(expected), ts.avg<uint64_t>());
+    EXPECT_EQ(static_cast<int64_t>(expected), ts.avg<int64_t>());
+  }
+}
+
+TEST(BucketedTimeSeries, forEachBucket) {
+  typedef BucketedTimeSeries<int64_t>::Bucket Bucket;
+  struct BucketInfo {
+    BucketInfo(const Bucket* b, seconds s, seconds ns)
+      : bucket(b), start(s), nextStart(ns) {}
+
+    const Bucket* bucket;
+    seconds start;
+    seconds nextStart;
+  };
+
+  for (const auto& data : testData) {
+    BucketedTimeSeries<int64_t> ts(data.numBuckets, seconds(data.duration));
+
+    vector<BucketInfo> info;
+    auto fn = [&](const Bucket& bucket, seconds bucketStart,
+                  seconds bucketEnd) -> bool {
+      info.emplace_back(&bucket, bucketStart, bucketEnd);
+      return true;
+    };
+
+    // If we haven't yet added any data, the current bucket will start at 0,
+    // and all data previous buckets will have negative times.
+    ts.forEachBucket(fn);
+
+    CHECK_EQ(data.numBuckets, info.size());
+
+    // Check the data passed in to the function
+    size_t infoIdx = 0;
+    size_t bucketIdx = 1;
+    ssize_t offset = -data.duration;
+    for (size_t n = 0; n < data.numBuckets; ++n) {
+      if (bucketIdx >= data.numBuckets) {
+        bucketIdx = 0;
+        offset += data.duration;
+      }
+
+      EXPECT_EQ(data.bucketStarts[bucketIdx] + offset,
+                info[infoIdx].start.count()) <<
+        data.duration << "x" << data.numBuckets << ": bucketIdx=" <<
+        bucketIdx << ", infoIdx=" << infoIdx;
+
+      size_t nextBucketIdx = bucketIdx + 1;
+      ssize_t nextOffset = offset;
+      if (nextBucketIdx >= data.numBuckets) {
+        nextBucketIdx = 0;
+        nextOffset += data.duration;
+      }
+      EXPECT_EQ(data.bucketStarts[nextBucketIdx] + nextOffset,
+                info[infoIdx].nextStart.count()) <<
+        data.duration << "x" << data.numBuckets << ": bucketIdx=" <<
+        bucketIdx << ", infoIdx=" << infoIdx;
+
+      EXPECT_EQ(&ts.getBucketByIndex(bucketIdx), info[infoIdx].bucket);
+
+      ++bucketIdx;
+      ++infoIdx;
+    }
+  }
+}
+
+TEST(BucketedTimeSeries, queryByIntervalSimple) {
+  BucketedTimeSeries<int> a(3, seconds(12));
+  for (int i = 0; i < 8; i++) {
+    a.addValue(seconds(i), 1);
+  }
+  // We added 1 at each second from 0..7
+  // Query from the time period 0..2.
+  // This is entirely in the first bucket, which has a sum of 4.
+  // The code knows only part of the bucket is covered, and correctly
+  // estimates the desired sum as 3.
+  EXPECT_EQ(2, a.sum(seconds(0), seconds(2)));
+}
+
+TEST(BucketedTimeSeries, queryByInterval) {
+  // Set up a BucketedTimeSeries tracking 6 seconds in 3 buckets
+  const int kNumBuckets = 3;
+  const int kDuration = 6;
+  BucketedTimeSeries<double> b(kNumBuckets, seconds(kDuration));
+
+  for (unsigned int i = 0; i < kDuration; ++i) {
+    // add value 'i' at time 'i'
+    b.addValue(seconds(i), i);
+  }
+
+  // Current bucket state:
+  // 0: time=[0, 2): values=(0, 1), sum=1, count=2
+  // 1: time=[2, 4): values=(2, 3), sum=5, count=1
+  // 2: time=[4, 6): values=(4, 5), sum=9, count=2
+  double expectedSums1[kDuration + 1][kDuration + 1] = {
+    {0,  4.5,   9, 11.5,  14, 14.5,  15},
+    {0,  4.5,   7,  9.5,  10, 10.5,  -1},
+    {0,  2.5,   5,  5.5,   6,   -1,  -1},
+    {0,  2.5,   3,  3.5,  -1,   -1,  -1},
+    {0,  0.5,   1,   -1,  -1,   -1,  -1},
+    {0,  0.5,  -1,   -1,  -1,   -1,  -1},
+    {0,   -1,  -1,   -1,  -1,   -1,  -1}
+  };
+  int expectedCounts1[kDuration + 1][kDuration + 1] = {
+    {0,  1,  2,  3,  4,  5,  6},
+    {0,  1,  2,  3,  4,  5, -1},
+    {0,  1,  2,  3,  4, -1, -1},
+    {0,  1,  2,  3, -1, -1, -1},
+    {0,  1,  2, -1, -1, -1, -1},
+    {0,  1, -1, -1, -1, -1, -1},
+    {0, -1, -1, -1, -1, -1, -1}
+  };
+
+  seconds currentTime = b.getLatestTime() + seconds(1);
+  for (int i = 0; i <= kDuration + 1; i++) {
+    for (int j = 0; j <= kDuration - i; j++) {
+      seconds start = currentTime - seconds(i + j);
+      seconds end = currentTime - seconds(i);
+      double expectedSum = expectedSums1[i][j];
+      EXPECT_EQ(expectedSum, b.sum(start, end)) <<
+        "i=" << i << ", j=" << j <<
+        ", interval=[" << start.count() << ", " << end.count() << ")";
+
+      uint64_t expectedCount = expectedCounts1[i][j];
+      EXPECT_EQ(expectedCount, b.count(start, end)) <<
+        "i=" << i << ", j=" << j <<
+        ", interval=[" << start.count() << ", " << end.count() << ")";
+
+      double expectedAvg = expectedCount ? expectedSum / expectedCount : 0;
+      EXPECT_EQ(expectedAvg, b.avg(start, end)) <<
+        "i=" << i << ", j=" << j <<
+        ", interval=[" << start.count() << ", " << end.count() << ")";
+
+      double expectedRate = j ? expectedSum / j : 0;
+      EXPECT_EQ(expectedRate, b.rate(start, end)) <<
+        "i=" << i << ", j=" << j <<
+        ", interval=[" << start.count() << ", " << end.count() << ")";
+    }
+  }
+
+  // Add 3 more values.
+  // This will overwrite 1 full bucket, and put us halfway through the next.
+  for (unsigned int i = kDuration; i < kDuration + 3; ++i) {
+    b.addValue(seconds(i), i);
+  }
+  EXPECT_EQ(seconds(4), b.getEarliestTime());
+
+  // Current bucket state:
+  // 0: time=[6,  8): values=(6, 7), sum=13, count=2
+  // 1: time=[8, 10): values=(8),    sum=8, count=1
+  // 2: time=[4,  6): values=(4, 5), sum=9, count=2
+  double expectedSums2[kDuration + 1][kDuration + 1] = {
+    {0,    8, 14.5,   21, 25.5,  30,  30},
+    {0,  6.5,   13, 17.5,   22,  22,  -1},
+    {0,  6.5,   11, 15.5, 15.5,  -1,  -1},
+    {0,  4.5,    9,    9,   -1,  -1,  -1},
+    {0,  4.5,  4.5,   -1,   -1,  -1,  -1},
+    {0,    0,   -1,   -1,   -1,  -1,  -1},
+    {0,   -1,   -1,   -1,   -1,  -1,  -1}
+  };
+  int expectedCounts2[kDuration + 1][kDuration + 1] = {
+    {0,  1,  2,  3,  4,  5,  5},
+    {0,  1,  2,  3,  4,  4, -1},
+    {0,  1,  2,  3,  3, -1, -1},
+    {0,  1,  2,  2, -1, -1, -1},
+    {0,  1,  1, -1, -1, -1, -1},
+    {0,  0, -1, -1, -1, -1, -1},
+    {0, -1, -1, -1, -1, -1, -1}
+  };
+
+  currentTime = b.getLatestTime() + seconds(1);
+  for (int i = 0; i <= kDuration + 1; i++) {
+    for (int j = 0; j <= kDuration - i; j++) {
+      seconds start = currentTime - seconds(i + j);
+      seconds end = currentTime - seconds(i);
+      double expectedSum = expectedSums2[i][j];
+      EXPECT_EQ(expectedSum, b.sum(start, end)) <<
+        "i=" << i << ", j=" << j <<
+        ", interval=[" << start.count() << ", " << end.count() << ")";
+
+      uint64_t expectedCount = expectedCounts2[i][j];
+      EXPECT_EQ(expectedCount, b.count(start, end)) <<
+        "i=" << i << ", j=" << j <<
+        ", interval=[" << start.count() << ", " << end.count() << ")";
+
+      double expectedAvg = expectedCount ? expectedSum / expectedCount : 0;
+      EXPECT_EQ(expectedAvg, b.avg(start, end)) <<
+        "i=" << i << ", j=" << j <<
+        ", interval=[" << start.count() << ", " << end.count() << ")";
+
+      seconds dataStart = std::max(start, b.getEarliestTime());
+      seconds dataEnd = std::max(end, dataStart);
+      seconds expectedInterval = dataEnd - dataStart;
+      EXPECT_EQ(expectedInterval, b.elapsed(start, end)) <<
+        "i=" << i << ", j=" << j <<
+        ", interval=[" << start.count() << ", " << end.count() << ")";
+
+      double expectedRate = expectedInterval.count() ?
+        expectedSum / expectedInterval.count() : 0;
+      EXPECT_EQ(expectedRate, b.rate(start, end)) <<
+        "i=" << i << ", j=" << j <<
+        ", interval=[" << start.count() << ", " << end.count() << ")";
+    }
+  }
+}
+
+TEST(BucketedTimeSeries, rateByInterval) {
+  const int kNumBuckets = 5;
+  const seconds kDuration(10);
+  BucketedTimeSeries<double> b(kNumBuckets, kDuration);
+
+  // Add data points at a constant rate of 10 per second.
+  // Start adding data points at kDuration, and fill half of the buckets for
+  // now.
+  seconds start = kDuration;
+  seconds end = kDuration + (kDuration / 2);
+  const double kFixedRate = 10.0;
+  for (seconds i = start; i < end; ++i) {
+    b.addValue(i, kFixedRate);
+  }
+
+  // Querying the rate should yield kFixedRate.
+  EXPECT_EQ(kFixedRate, b.rate());
+  EXPECT_EQ(kFixedRate, b.rate(start, end));
+  EXPECT_EQ(kFixedRate, b.rate(start, start + kDuration));
+  EXPECT_EQ(kFixedRate, b.rate(end - kDuration, end));
+  EXPECT_EQ(kFixedRate, b.rate(end - seconds(1), end));
+  // We have been adding 1 data point per second, so countRate()
+  // should be 1.
+  EXPECT_EQ(1.0, b.countRate());
+  EXPECT_EQ(1.0, b.countRate(start, end));
+  EXPECT_EQ(1.0, b.countRate(start, start + kDuration));
+  EXPECT_EQ(1.0, b.countRate(end - kDuration, end));
+  EXPECT_EQ(1.0, b.countRate(end - seconds(1), end));
+
+  // We haven't added anything before time kDuration.
+  // Querying data earlier than this should result in a rate of 0.
+  EXPECT_EQ(0.0, b.rate(seconds(0), seconds(1)));
+  EXPECT_EQ(0.0, b.countRate(seconds(0), seconds(1)));
+
+  // Fill the remainder of the timeseries from kDuration to kDuration*2
+  start = end;
+  end = kDuration * 2;
+  for (seconds i = start; i < end; ++i) {
+    b.addValue(i, kFixedRate);
+  }
+
+  EXPECT_EQ(kFixedRate, b.rate());
+  EXPECT_EQ(kFixedRate, b.rate(kDuration, kDuration * 2));
+  EXPECT_EQ(kFixedRate, b.rate(seconds(0), kDuration * 2));
+  EXPECT_EQ(kFixedRate, b.rate(seconds(0), kDuration * 10));
+  EXPECT_EQ(1.0, b.countRate());
+  EXPECT_EQ(1.0, b.countRate(kDuration, kDuration * 2));
+  EXPECT_EQ(1.0, b.countRate(seconds(0), kDuration * 2));
+  EXPECT_EQ(1.0, b.countRate(seconds(0), kDuration * 10));
+}
+
+namespace IntMHTS {
+  enum Levels {
+    MINUTE,
+    HOUR,
+    ALLTIME,
+    NUM_LEVELS,
+  };
+
+  const seconds kMinuteHourDurations[] = {
+    seconds(60), seconds(3600), seconds(0)
+  };
+};
+
+TEST(MinuteHourTimeSeries, Basic) {
+  folly::MultiLevelTimeSeries<int> mhts(60, IntMHTS::NUM_LEVELS,
+                                        IntMHTS::kMinuteHourDurations);
+  EXPECT_EQ(mhts.numLevels(), IntMHTS::NUM_LEVELS);
+  EXPECT_EQ(mhts.numLevels(), 3);
+
+  EXPECT_EQ(mhts.sum(IntMHTS::MINUTE), 0);
+  EXPECT_EQ(mhts.sum(IntMHTS::HOUR), 0);
+  EXPECT_EQ(mhts.sum(IntMHTS::ALLTIME), 0);
+
+  EXPECT_EQ(mhts.avg(IntMHTS::MINUTE), 0);
+  EXPECT_EQ(mhts.avg(IntMHTS::HOUR), 0);
+  EXPECT_EQ(mhts.avg(IntMHTS::ALLTIME), 0);
+
+  EXPECT_EQ(mhts.rate(IntMHTS::MINUTE), 0);
+  EXPECT_EQ(mhts.rate(IntMHTS::HOUR), 0);
+  EXPECT_EQ(mhts.rate(IntMHTS::ALLTIME), 0);
+
+  EXPECT_EQ(mhts.getLevel(IntMHTS::MINUTE).elapsed().count(), 0);
+  EXPECT_EQ(mhts.getLevel(IntMHTS::HOUR).elapsed().count(), 0);
+  EXPECT_EQ(mhts.getLevel(IntMHTS::ALLTIME).elapsed().count(), 0);
+
+  seconds cur_time(0);
+
+  mhts.addValue(cur_time++, 10);
+  mhts.flush();
+
+  EXPECT_EQ(mhts.getLevel(IntMHTS::MINUTE).elapsed().count(), 1);
+  EXPECT_EQ(mhts.getLevel(IntMHTS::HOUR).elapsed().count(), 1);
+  EXPECT_EQ(mhts.getLevel(IntMHTS::ALLTIME).elapsed().count(), 1);
+
+  for (int i = 0; i < 299; ++i) {
+    mhts.addValue(cur_time++, 10);
+  }
+  mhts.flush();
+
+  EXPECT_EQ(mhts.getLevel(IntMHTS::MINUTE).elapsed().count(), 60);
+  EXPECT_EQ(mhts.getLevel(IntMHTS::HOUR).elapsed().count(), 300);
+  EXPECT_EQ(mhts.getLevel(IntMHTS::ALLTIME).elapsed().count(), 300);
+
+  EXPECT_EQ(mhts.sum(IntMHTS::MINUTE), 600);
+  EXPECT_EQ(mhts.sum(IntMHTS::HOUR), 300*10);
+  EXPECT_EQ(mhts.sum(IntMHTS::ALLTIME), 300*10);
+
+  EXPECT_EQ(mhts.avg(IntMHTS::MINUTE), 10);
+  EXPECT_EQ(mhts.avg(IntMHTS::HOUR), 10);
+  EXPECT_EQ(mhts.avg(IntMHTS::ALLTIME), 10);
+
+  EXPECT_EQ(mhts.rate(IntMHTS::MINUTE), 10);
+  EXPECT_EQ(mhts.rate(IntMHTS::HOUR), 10);
+  EXPECT_EQ(mhts.rate(IntMHTS::ALLTIME), 10);
+
+  for (int i = 0; i < 3600*3 - 300; ++i) {
+    mhts.addValue(cur_time++, 10);
+  }
+  mhts.flush();
+
+  EXPECT_EQ(mhts.getLevel(IntMHTS::MINUTE).elapsed().count(), 60);
+  EXPECT_EQ(mhts.getLevel(IntMHTS::HOUR).elapsed().count(), 3600);
+  EXPECT_EQ(mhts.getLevel(IntMHTS::ALLTIME).elapsed().count(), 3600*3);
+
+  EXPECT_EQ(mhts.sum(IntMHTS::MINUTE), 600);
+  EXPECT_EQ(mhts.sum(IntMHTS::HOUR), 3600*10);
+  EXPECT_EQ(mhts.sum(IntMHTS::ALLTIME), 3600*3*10);
+
+  EXPECT_EQ(mhts.avg(IntMHTS::MINUTE), 10);
+  EXPECT_EQ(mhts.avg(IntMHTS::HOUR), 10);
+  EXPECT_EQ(mhts.avg(IntMHTS::ALLTIME), 10);
+
+  EXPECT_EQ(mhts.rate(IntMHTS::MINUTE), 10);
+  EXPECT_EQ(mhts.rate(IntMHTS::HOUR), 10);
+  EXPECT_EQ(mhts.rate(IntMHTS::ALLTIME), 10);
+
+  for (int i = 0; i < 3600; ++i) {
+    mhts.addValue(cur_time++, 100);
+  }
+  mhts.flush();
+
+  EXPECT_EQ(mhts.sum(IntMHTS::MINUTE), 60*100);
+  EXPECT_EQ(mhts.sum(IntMHTS::HOUR), 3600*100);
+  EXPECT_EQ(mhts.sum(IntMHTS::ALLTIME),
+            3600*3*10 + 3600*100);
+
+  EXPECT_EQ(mhts.avg(IntMHTS::MINUTE), 100);
+  EXPECT_EQ(mhts.avg(IntMHTS::HOUR), 100);
+  EXPECT_EQ(mhts.avg(IntMHTS::ALLTIME), 32.5);
+
+  EXPECT_EQ(mhts.rate(IntMHTS::MINUTE), 100);
+  EXPECT_EQ(mhts.rate(IntMHTS::HOUR), 100);
+  EXPECT_EQ(mhts.rate(IntMHTS::ALLTIME), 32);
+
+  for (int i = 0; i < 1800; ++i) {
+    mhts.addValue(cur_time++, 120);
+  }
+  mhts.flush();
+
+  EXPECT_EQ(mhts.sum(IntMHTS::MINUTE), 60*120);
+  EXPECT_EQ(mhts.sum(IntMHTS::HOUR),
+            1800*100 + 1800*120);
+  EXPECT_EQ(mhts.sum(IntMHTS::ALLTIME),
+            3600*3*10 + 3600*100 + 1800*120);
+
+  for (int i = 0; i < 60; ++i) {
+    mhts.addValue(cur_time++, 1000);
+  }
+  mhts.flush();
+
+  EXPECT_EQ(mhts.sum(IntMHTS::MINUTE), 60*1000);
+  EXPECT_EQ(mhts.sum(IntMHTS::HOUR),
+            1740*100 + 1800*120 + 60*1000);
+  EXPECT_EQ(mhts.sum(IntMHTS::ALLTIME),
+            3600*3*10 + 3600*100 + 1800*120 + 60*1000);
+
+  mhts.clear();
+  EXPECT_EQ(mhts.sum(IntMHTS::ALLTIME), 0);
+}
+
+TEST(MinuteHourTimeSeries, QueryByInterval) {
+  folly::MultiLevelTimeSeries<int> mhts(60, IntMHTS::NUM_LEVELS,
+                                        IntMHTS::kMinuteHourDurations);
+
+  seconds curTime(0);
+  for (curTime = seconds(0); curTime < seconds(7200); curTime++) {
+    mhts.addValue(curTime, 1);
+  }
+  for (curTime = seconds(7200); curTime < seconds(7200 + 3540); curTime++) {
+    mhts.addValue(curTime, 10);
+  }
+  for (curTime = seconds(7200 + 3540); curTime < seconds(7200 + 3600);
+       curTime++) {
+    mhts.addValue(curTime, 100);
+  }
+  mhts.flush();
+
+  struct TimeInterval {
+    seconds start;
+    seconds end;
+  };
+  TimeInterval intervals[12] = {
+    { curTime - seconds(60), curTime },
+    { curTime - seconds(3600), curTime },
+    { curTime - seconds(7200), curTime },
+    { curTime - seconds(3600), curTime - seconds(60) },
+    { curTime - seconds(7200), curTime - seconds(60) },
+    { curTime - seconds(7200), curTime - seconds(3600) },
+    { curTime - seconds(50), curTime - seconds(20) },
+    { curTime - seconds(3020), curTime - seconds(20) },
+    { curTime - seconds(7200), curTime - seconds(20) },
+    { curTime - seconds(3000), curTime - seconds(1000) },
+    { curTime - seconds(7200), curTime - seconds(1000) },
+    { curTime - seconds(7200), curTime - seconds(3600) },
+  };
+
+  int expectedSums[12] = {
+    6000, 41400, 32400, 35400, 32130, 16200, 3000, 33600, 32310, 20000, 27900,
+    16200
+  };
+
+  int expectedCounts[12] = {
+    60, 3600, 7200, 3540, 7140, 3600, 30, 3000, 7180, 2000, 6200, 3600
+  };
+
+  for (int i = 0; i < 12; ++i) {
+    TimeInterval interval = intervals[i];
+
+    int s = mhts.sum(interval.start, interval.end);
+    EXPECT_EQ(expectedSums[i], s);
+
+    int c = mhts.count(interval.start, interval.end);
+    EXPECT_EQ(expectedCounts[i], c);
+
+    int a = mhts.avg<int>(interval.start, interval.end);
+    EXPECT_EQ(expectedCounts[i] ?
+              (expectedSums[i] / expectedCounts[i]) : 0,
+              a);
+
+    int r = mhts.rate<int>(interval.start, interval.end);
+    int expectedRate =
+      expectedSums[i] / (interval.end - interval.start).count();
+    EXPECT_EQ(expectedRate, r);
+  }
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/TraitsTest.cpp
@@ -0,0 +1,129 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Benchmark.h"
+#include "folly/Traits.h"
+
+#include <gflags/gflags.h>
+#include <gtest/gtest.h>
+
+using namespace folly;
+using namespace std;
+
+struct T1 {}; // old-style IsRelocatable, below
+struct T2 {}; // old-style IsRelocatable, below
+struct T3 { typedef std::true_type IsRelocatable; };
+struct T4 { typedef std::true_type IsTriviallyCopyable; };
+struct T5 : T3 {};
+
+struct F1 {};
+struct F2 { typedef int IsRelocatable; };
+struct F3 : T3 { typedef std::false_type IsRelocatable; };
+struct F4 : T1 {};
+
+namespace folly {
+  template <> struct IsRelocatable<T1> : std::true_type {};
+  template <> FOLLY_ASSUME_RELOCATABLE(T2);
+}
+
+TEST(Traits, scalars) {
+  EXPECT_TRUE(IsRelocatable<int>::value);
+  EXPECT_TRUE(IsRelocatable<bool>::value);
+  EXPECT_TRUE(IsRelocatable<double>::value);
+  EXPECT_TRUE(IsRelocatable<void*>::value);
+}
+
+TEST(Traits, containers) {
+  EXPECT_TRUE  (IsRelocatable<vector<F1>>::value);
+  EXPECT_FALSE((IsRelocatable<pair<F1, F1>>::value));
+  EXPECT_TRUE ((IsRelocatable<pair<T1, T2>>::value));
+}
+
+TEST(Traits, original) {
+  EXPECT_TRUE(IsRelocatable<T1>::value);
+  EXPECT_TRUE(IsRelocatable<T2>::value);
+}
+
+TEST(Traits, typedefd) {
+  EXPECT_TRUE (IsRelocatable<T3>::value);
+  EXPECT_TRUE (IsRelocatable<T5>::value);
+  EXPECT_FALSE(IsRelocatable<F2>::value);
+  EXPECT_FALSE(IsRelocatable<F3>::value);
+}
+
+TEST(Traits, unset) {
+  EXPECT_FALSE(IsRelocatable<F1>::value);
+  EXPECT_FALSE(IsRelocatable<F4>::value);
+}
+
+TEST(Traits, bitprop) {
+  EXPECT_TRUE(IsTriviallyCopyable<T4>::value);
+  EXPECT_TRUE(IsRelocatable<T4>::value);
+}
+
+TEST(Traits, bitAndInit) {
+  EXPECT_TRUE (IsTriviallyCopyable<int>::value);
+  EXPECT_FALSE(IsTriviallyCopyable<vector<int>>::value);
+  EXPECT_TRUE (IsZeroInitializable<int>::value);
+  EXPECT_FALSE(IsZeroInitializable<vector<int>>::value);
+}
+
+TEST(Traits, is_negative) {
+  EXPECT_TRUE(folly::is_negative(-1));
+  EXPECT_FALSE(folly::is_negative(0));
+  EXPECT_FALSE(folly::is_negative(1));
+  EXPECT_FALSE(folly::is_negative(0u));
+  EXPECT_FALSE(folly::is_negative(1u));
+
+  EXPECT_TRUE(folly::is_non_positive(-1));
+  EXPECT_TRUE(folly::is_non_positive(0));
+  EXPECT_FALSE(folly::is_non_positive(1));
+  EXPECT_TRUE(folly::is_non_positive(0u));
+  EXPECT_FALSE(folly::is_non_positive(1u));
+}
+
+TEST(Traits, relational) {
+  // We test, especially, the edge cases to make sure we don't
+  // trip -Wtautological-comparisons
+
+  EXPECT_FALSE((folly::less_than<uint8_t, 0u,   uint8_t>(0u)));
+  EXPECT_FALSE((folly::less_than<uint8_t, 0u,   uint8_t>(254u)));
+  EXPECT_FALSE((folly::less_than<uint8_t, 255u, uint8_t>(255u)));
+  EXPECT_TRUE( (folly::less_than<uint8_t, 255u, uint8_t>(254u)));
+
+  EXPECT_FALSE((folly::greater_than<uint8_t, 0u,   uint8_t>(0u)));
+  EXPECT_TRUE( (folly::greater_than<uint8_t, 0u,   uint8_t>(254u)));
+  EXPECT_FALSE((folly::greater_than<uint8_t, 255u, uint8_t>(255u)));
+  EXPECT_FALSE((folly::greater_than<uint8_t, 255u, uint8_t>(254u)));
+}
+
+struct CompleteType {};
+struct IncompleteType;
+TEST(Traits, is_complete) {
+  EXPECT_TRUE((folly::is_complete<int>::value));
+  EXPECT_TRUE((folly::is_complete<CompleteType>::value));
+  EXPECT_FALSE((folly::is_complete<IncompleteType>::value));
+}
+
+int main(int argc, char ** argv) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  if (FLAGS_benchmark) {
+    folly::runBenchmarks();
+  }
+  return RUN_ALL_TESTS();
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/UriTest.cpp
@@ -0,0 +1,246 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Uri.h"
+
+#include <boost/algorithm/string.hpp>
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+using namespace folly;
+
+namespace {
+
+}  // namespace
+
+TEST(Uri, Simple) {
+  {
+    fbstring s("http://www.facebook.com/hello/world?query#fragment");
+    Uri u(s);
+    EXPECT_EQ("http", u.scheme());
+    EXPECT_EQ("", u.username());
+    EXPECT_EQ("", u.password());
+    EXPECT_EQ("www.facebook.com", u.host());
+    EXPECT_EQ(0, u.port());
+    EXPECT_EQ("www.facebook.com", u.authority());
+    EXPECT_EQ("/hello/world", u.path());
+    EXPECT_EQ("query", u.query());
+    EXPECT_EQ("fragment", u.fragment());
+    EXPECT_EQ(s, u.fbstr());  // canonical
+  }
+
+  {
+    fbstring s("http://www.facebook.com:8080/hello/world?query#fragment");
+    Uri u(s);
+    EXPECT_EQ("http", u.scheme());
+    EXPECT_EQ("", u.username());
+    EXPECT_EQ("", u.password());
+    EXPECT_EQ("www.facebook.com", u.host());
+    EXPECT_EQ(8080, u.port());
+    EXPECT_EQ("www.facebook.com:8080", u.authority());
+    EXPECT_EQ("/hello/world", u.path());
+    EXPECT_EQ("query", u.query());
+    EXPECT_EQ("fragment", u.fragment());
+    EXPECT_EQ(s, u.fbstr());  // canonical
+  }
+
+  {
+    fbstring s("http://127.0.0.1:8080/hello/world?query#fragment");
+    Uri u(s);
+    EXPECT_EQ("http", u.scheme());
+    EXPECT_EQ("", u.username());
+    EXPECT_EQ("", u.password());
+    EXPECT_EQ("127.0.0.1", u.host());
+    EXPECT_EQ(8080, u.port());
+    EXPECT_EQ("127.0.0.1:8080", u.authority());
+    EXPECT_EQ("/hello/world", u.path());
+    EXPECT_EQ("query", u.query());
+    EXPECT_EQ("fragment", u.fragment());
+    EXPECT_EQ(s, u.fbstr());  // canonical
+  }
+
+  {
+    fbstring s("http://[::1]:8080/hello/world?query#fragment");
+    Uri u(s);
+    EXPECT_EQ("http", u.scheme());
+    EXPECT_EQ("", u.username());
+    EXPECT_EQ("", u.password());
+    EXPECT_EQ("[::1]", u.host());
+    EXPECT_EQ(8080, u.port());
+    EXPECT_EQ("[::1]:8080", u.authority());
+    EXPECT_EQ("/hello/world", u.path());
+    EXPECT_EQ("query", u.query());
+    EXPECT_EQ("fragment", u.fragment());
+    EXPECT_EQ(s, u.fbstr());  // canonical
+  }
+
+  {
+    fbstring s("http://user:pass@host.com/");
+    Uri u(s);
+    EXPECT_EQ("http", u.scheme());
+    EXPECT_EQ("user", u.username());
+    EXPECT_EQ("pass", u.password());
+    EXPECT_EQ("host.com", u.host());
+    EXPECT_EQ(0, u.port());
+    EXPECT_EQ("user:pass@host.com", u.authority());
+    EXPECT_EQ("/", u.path());
+    EXPECT_EQ("", u.query());
+    EXPECT_EQ("", u.fragment());
+    EXPECT_EQ(s, u.fbstr());
+  }
+
+  {
+    fbstring s("http://user@host.com/");
+    Uri u(s);
+    EXPECT_EQ("http", u.scheme());
+    EXPECT_EQ("user", u.username());
+    EXPECT_EQ("", u.password());
+    EXPECT_EQ("host.com", u.host());
+    EXPECT_EQ(0, u.port());
+    EXPECT_EQ("user@host.com", u.authority());
+    EXPECT_EQ("/", u.path());
+    EXPECT_EQ("", u.query());
+    EXPECT_EQ("", u.fragment());
+    EXPECT_EQ(s, u.fbstr());
+  }
+
+  {
+    fbstring s("http://user:@host.com/");
+    Uri u(s);
+    EXPECT_EQ("http", u.scheme());
+    EXPECT_EQ("user", u.username());
+    EXPECT_EQ("", u.password());
+    EXPECT_EQ("host.com", u.host());
+    EXPECT_EQ(0, u.port());
+    EXPECT_EQ("user@host.com", u.authority());
+    EXPECT_EQ("/", u.path());
+    EXPECT_EQ("", u.query());
+    EXPECT_EQ("", u.fragment());
+    EXPECT_EQ("http://user@host.com/", u.fbstr());
+  }
+
+  {
+    fbstring s("http://:pass@host.com/");
+    Uri u(s);
+    EXPECT_EQ("http", u.scheme());
+    EXPECT_EQ("", u.username());
+    EXPECT_EQ("pass", u.password());
+    EXPECT_EQ("host.com", u.host());
+    EXPECT_EQ(0, u.port());
+    EXPECT_EQ(":pass@host.com", u.authority());
+    EXPECT_EQ("/", u.path());
+    EXPECT_EQ("", u.query());
+    EXPECT_EQ("", u.fragment());
+    EXPECT_EQ(s, u.fbstr());
+  }
+
+  {
+    fbstring s("http://@host.com/");
+    Uri u(s);
+    EXPECT_EQ("http", u.scheme());
+    EXPECT_EQ("", u.username());
+    EXPECT_EQ("", u.password());
+    EXPECT_EQ("host.com", u.host());
+    EXPECT_EQ(0, u.port());
+    EXPECT_EQ("host.com", u.authority());
+    EXPECT_EQ("/", u.path());
+    EXPECT_EQ("", u.query());
+    EXPECT_EQ("", u.fragment());
+    EXPECT_EQ("http://host.com/", u.fbstr());
+  }
+
+  {
+    fbstring s("http://:@host.com/");
+    Uri u(s);
+    EXPECT_EQ("http", u.scheme());
+    EXPECT_EQ("", u.username());
+    EXPECT_EQ("", u.password());
+    EXPECT_EQ("host.com", u.host());
+    EXPECT_EQ(0, u.port());
+    EXPECT_EQ("host.com", u.authority());
+    EXPECT_EQ("/", u.path());
+    EXPECT_EQ("", u.query());
+    EXPECT_EQ("", u.fragment());
+    EXPECT_EQ("http://host.com/", u.fbstr());
+  }
+
+  {
+    fbstring s("file:///etc/motd");
+    Uri u(s);
+    EXPECT_EQ("file", u.scheme());
+    EXPECT_EQ("", u.username());
+    EXPECT_EQ("", u.password());
+    EXPECT_EQ("", u.host());
+    EXPECT_EQ(0, u.port());
+    EXPECT_EQ("", u.authority());
+    EXPECT_EQ("/etc/motd", u.path());
+    EXPECT_EQ("", u.query());
+    EXPECT_EQ("", u.fragment());
+    EXPECT_EQ(s, u.fbstr());
+  }
+
+  {
+    fbstring s("file:/etc/motd");
+    Uri u(s);
+    EXPECT_EQ("file", u.scheme());
+    EXPECT_EQ("", u.username());
+    EXPECT_EQ("", u.password());
+    EXPECT_EQ("", u.host());
+    EXPECT_EQ(0, u.port());
+    EXPECT_EQ("", u.authority());
+    EXPECT_EQ("/etc/motd", u.path());
+    EXPECT_EQ("", u.query());
+    EXPECT_EQ("", u.fragment());
+    EXPECT_EQ("file:///etc/motd", u.fbstr());
+  }
+
+  {
+    fbstring s("file://etc/motd");
+    Uri u(s);
+    EXPECT_EQ("file", u.scheme());
+    EXPECT_EQ("", u.username());
+    EXPECT_EQ("", u.password());
+    EXPECT_EQ("etc", u.host());
+    EXPECT_EQ(0, u.port());
+    EXPECT_EQ("etc", u.authority());
+    EXPECT_EQ("/motd", u.path());
+    EXPECT_EQ("", u.query());
+    EXPECT_EQ("", u.fragment());
+    EXPECT_EQ(s, u.fbstr());
+  }
+
+  {
+    fbstring s("2http://www.facebook.com");
+
+    try {
+      Uri u(s);
+      CHECK(false) << "Control should not have reached here";
+    } catch (const std::invalid_argument& ex) {
+      EXPECT_TRUE(boost::algorithm::ends_with(ex.what(), s));
+    }
+  }
+
+  {
+    fbstring s("www[facebook]com");
+
+    try {
+      Uri u("http://" + s);
+      CHECK(false) << "Control should not have reached here";
+    } catch (const std::invalid_argument& ex) {
+      EXPECT_TRUE(boost::algorithm::ends_with(ex.what(), s));
+    }
+  }
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/test/VarintTest.cpp
@@ -0,0 +1,184 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Varint.h"
+
+#include <array>
+#include <initializer_list>
+#include <random>
+#include <vector>
+
+#include <glog/logging.h>
+#include <gtest/gtest.h>
+
+#include "folly/Benchmark.h"
+#include "folly/Random.h"
+
+DEFINE_int32(random_seed, folly::randomNumberSeed(), "random seed");
+
+namespace folly { namespace test {
+
+void testVarint(uint64_t val, std::initializer_list<uint8_t> bytes) {
+  size_t n = bytes.size();
+  ByteRange expected(&*bytes.begin(), n);
+
+  {
+    uint8_t buf[kMaxVarintLength64];
+    EXPECT_EQ(expected.size(), encodeVarint(val, buf));
+    EXPECT_TRUE(ByteRange(buf, expected.size()) == expected);
+  }
+
+  {
+    ByteRange r = expected;
+    uint64_t decoded = decodeVarint(r);
+    EXPECT_TRUE(r.empty());
+    EXPECT_EQ(val, decoded);
+  }
+
+  if (n < kMaxVarintLength64) {
+    // Try from a full buffer too, different code path
+    uint8_t buf[kMaxVarintLength64];
+    memcpy(buf, &*bytes.begin(), n);
+
+    uint8_t fills[] = {0, 0x7f, 0x80, 0xff};
+
+    for (uint8_t fill : fills) {
+      memset(buf + n, fill, kMaxVarintLength64 - n);
+      ByteRange r(buf, kMaxVarintLength64);
+      uint64_t decoded = decodeVarint(r);
+      EXPECT_EQ(val, decoded);
+      EXPECT_EQ(kMaxVarintLength64 - n, r.size());
+    }
+  }
+}
+
+TEST(Varint, Simple) {
+  testVarint(0, {0});
+  testVarint(1, {1});
+  testVarint(127, {127});
+  testVarint(128, {0x80, 0x01});
+  testVarint(300, {0xac, 0x02});
+  testVarint(16383, {0xff, 0x7f});
+  testVarint(16384, {0x80, 0x80, 0x01});
+
+  testVarint(static_cast<uint32_t>(-1),
+             {0xff, 0xff, 0xff, 0xff, 0x0f});
+  testVarint(static_cast<uint64_t>(-1),
+             {0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x01});
+}
+
+TEST(ZigZag, Simple) {
+  EXPECT_EQ(0, encodeZigZag(0));
+  EXPECT_EQ(1, encodeZigZag(-1));
+  EXPECT_EQ(2, encodeZigZag(1));
+  EXPECT_EQ(3, encodeZigZag(-2));
+  EXPECT_EQ(4, encodeZigZag(2));
+
+  EXPECT_EQ(0,  decodeZigZag(0));
+  EXPECT_EQ(-1, decodeZigZag(1));
+  EXPECT_EQ(1,  decodeZigZag(2));
+  EXPECT_EQ(-2, decodeZigZag(3));
+  EXPECT_EQ(2,  decodeZigZag(4));
+}
+
+namespace {
+
+constexpr size_t kNumValues = 1000;
+std::vector<uint64_t> gValues;
+std::vector<uint64_t> gDecodedValues;
+std::vector<uint8_t> gEncoded;
+
+void generateRandomValues() {
+  LOG(INFO) << "Random seed is " << FLAGS_random_seed;
+  std::mt19937 rng(FLAGS_random_seed);
+
+  // Approximation of power law
+  std::uniform_int_distribution<int> numBytes(1, 8);
+  std::uniform_int_distribution<int> byte(0, 255);
+
+  gValues.resize(kNumValues);
+  gDecodedValues.resize(kNumValues);
+  gEncoded.resize(kNumValues * kMaxVarintLength64);
+  for (size_t i = 0; i < kNumValues; ++i) {
+    int n = numBytes(rng);
+    uint64_t val = 0;
+    for (size_t j = 0; j < n; ++j) {
+      val = (val << 8) + byte(rng);
+    }
+    gValues[i] = val;
+  }
+}
+
+// Benchmark results (Intel(R) Xeon(R) CPU E5-2660 0 @ 2.20GHz, Linux x86_64)
+//
+// I0814 19:13:14.466256  7504 VarintTest.cpp:146] Random seed is -1216518886
+// ============================================================================
+// folly/test/VarintTest.cpp                       relative  time/iter  iters/s
+// ============================================================================
+// VarintEncoding                                               6.69us  149.37K
+// VarintDecoding                                               6.85us  145.90K
+// ============================================================================
+//
+// Disabling the "fast path" code in decodeVarint hurts performance:
+//
+// I0814 19:15:13.871467  9550 VarintTest.cpp:156] Random seed is -1216518886
+// ============================================================================
+// folly/test/VarintTest.cpp                       relative  time/iter  iters/s
+// ============================================================================
+// VarintEncoding                                               6.75us  148.26K
+// VarintDecoding                                              12.60us   79.37K
+// ============================================================================
+
+BENCHMARK(VarintEncoding, iters) {
+  uint8_t* start = &(*gEncoded.begin());
+  uint8_t* p = start;
+  bool empty = (iters == 0);
+  while (iters--) {
+    p = start;
+    for (auto& v : gValues) {
+      p += encodeVarint(v, p);
+    }
+  }
+
+  gEncoded.erase(gEncoded.begin() + (p - start), gEncoded.end());
+}
+
+BENCHMARK(VarintDecoding, iters) {
+  while (iters--) {
+    size_t i = 0;
+    ByteRange range(&(*gEncoded.begin()), &(*gEncoded.end()));
+    while (!range.empty()) {
+      gDecodedValues[i++] = decodeVarint(range);
+    }
+  }
+}
+
+}  // namespace
+
+}}  // namespaces
+
+int main(int argc, char *argv[]) {
+  testing::InitGoogleTest(&argc, argv);
+  google::ParseCommandLineFlags(&argc, &argv, true);
+  google::InitGoogleLogging(argv[0]);
+  int ret = RUN_ALL_TESTS();
+  if (ret == 0) {
+    folly::test::generateRandomValues();
+    folly::runBenchmarksOnFlag();
+  }
+  return ret;
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/ThreadCachedArena.cpp
@@ -0,0 +1,43 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/ThreadCachedArena.h"
+
+namespace folly {
+
+ThreadCachedArena::ThreadCachedArena(size_t minBlockSize)
+  : minBlockSize_(minBlockSize) {
+}
+
+SysArena* ThreadCachedArena::allocateThreadLocalArena() {
+  SysArena* arena = new SysArena(minBlockSize_);
+  auto disposer = [this] (SysArena* t, TLPDestructionMode mode) {
+    std::unique_ptr<SysArena> tp(t);  // ensure it gets deleted
+    if (mode == TLPDestructionMode::THIS_THREAD) {
+      zombify(std::move(*t));
+    }
+  };
+  arena_.reset(arena, disposer);
+  return arena;
+}
+
+void ThreadCachedArena::zombify(SysArena&& arena) {
+  std::lock_guard<std::mutex> lock(zombiesMutex_);
+  zombies_.merge(std::move(arena));
+}
+
+}  // namespace folly
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/ThreadCachedArena.h
@@ -0,0 +1,81 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_THREADCACHEDARENA_H_
+#define FOLLY_THREADCACHEDARENA_H_
+
+#include <utility>
+#include <mutex>
+#include <limits>
+#include <boost/intrusive/slist.hpp>
+
+#include "folly/Likely.h"
+#include "folly/Arena.h"
+#include "folly/ThreadLocal.h"
+
+namespace folly {
+
+/**
+ * Thread-caching arena: allocate memory which gets freed when the arena gets
+ * destroyed.
+ *
+ * The arena itself allocates memory using malloc() in blocks of
+ * at least minBlockSize bytes.
+ *
+ * For speed, each thread gets its own Arena (see Arena.h); when threads
+ * exit, the Arena gets merged into a "zombie" Arena, which will be deallocated
+ * when the ThreadCachedArena object is destroyed.
+ */
+class ThreadCachedArena {
+ public:
+  explicit ThreadCachedArena(
+      size_t minBlockSize = SysArena::kDefaultMinBlockSize);
+
+  void* allocate(size_t size) {
+    SysArena* arena = arena_.get();
+    if (UNLIKELY(!arena)) {
+      arena = allocateThreadLocalArena();
+    }
+
+    return arena->allocate(size);
+  }
+
+  void deallocate(void* p) {
+    // Deallocate? Never!
+  }
+
+ private:
+  ThreadCachedArena(const ThreadCachedArena&) = delete;
+  ThreadCachedArena(ThreadCachedArena&&) = delete;
+  ThreadCachedArena& operator=(const ThreadCachedArena&) = delete;
+  ThreadCachedArena& operator=(ThreadCachedArena&&) = delete;
+
+  SysArena* allocateThreadLocalArena();
+
+  // Zombify the blocks in arena, saving them for deallocation until
+  // the ThreadCachedArena is destroyed.
+  void zombify(SysArena&& arena);
+
+  size_t minBlockSize_;
+  SysArena zombies_;  // allocated from threads that are now dead
+  std::mutex zombiesMutex_;
+  ThreadLocalPtr<SysArena> arena_;  // per-thread arena
+};
+
+}  // namespace folly
+
+#endif /* FOLLY_THREADCACHEDARENA_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/ThreadCachedInt.h
@@ -0,0 +1,179 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Higher performance (up to 10x) atomic increment using thread caching.
+ *
+ * @author Spencer Ahrens (sahrens)
+ */
+
+#ifndef FOLLY_THREADCACHEDINT_H
+#define FOLLY_THREADCACHEDINT_H
+
+#include <atomic>
+
+#include <boost/noncopyable.hpp>
+
+#include "folly/Likely.h"
+#include "folly/ThreadLocal.h"
+
+namespace folly {
+
+
+// Note that readFull requires holding a lock and iterating through all of the
+// thread local objects with the same Tag, so if you have a lot of
+// ThreadCachedInt's you should considering breaking up the Tag space even
+// further.
+template <class IntT, class Tag=IntT>
+class ThreadCachedInt : boost::noncopyable {
+  struct IntCache;
+
+ public:
+  explicit ThreadCachedInt(IntT initialVal = 0, uint32_t cacheSize = 1000)
+    : target_(initialVal), cacheSize_(cacheSize) {
+  }
+
+  void increment(IntT inc) {
+    auto cache = cache_.get();
+    if (UNLIKELY(cache == NULL || cache->parent_ == NULL)) {
+      cache = new IntCache(*this);
+      cache_.reset(cache);
+    }
+    cache->increment(inc);
+  }
+
+  // Quickly grabs the current value which may not include some cached
+  // increments.
+  IntT readFast() const {
+    return target_.load(std::memory_order_relaxed);
+  }
+
+  // Reads the current value plus all the cached increments.  Requires grabbing
+  // a lock, so this is significantly slower than readFast().
+  IntT readFull() const {
+    IntT ret = readFast();
+    for (const auto& cache : cache_.accessAllThreads()) {
+      if (!cache.reset_.load(std::memory_order_acquire)) {
+        ret += cache.val_.load(std::memory_order_relaxed);
+      }
+    }
+    return ret;
+  }
+
+  // Quickly reads and resets current value (doesn't reset cached increments).
+  IntT readFastAndReset() {
+    return target_.exchange(0, std::memory_order_release);
+  }
+
+  // This function is designed for accumulating into another counter, where you
+  // only want to count each increment once.  It can still get the count a
+  // little off, however, but it should be much better than calling readFull()
+  // and set(0) sequentially.
+  IntT readFullAndReset() {
+    IntT ret = readFastAndReset();
+    for (auto& cache : cache_.accessAllThreads()) {
+      if (!cache.reset_.load(std::memory_order_acquire)) {
+        ret += cache.val_.load(std::memory_order_relaxed);
+        cache.reset_.store(true, std::memory_order_release);
+      }
+    }
+    return ret;
+  }
+
+  void setCacheSize(uint32_t newSize) {
+    cacheSize_.store(newSize, std::memory_order_release);
+  }
+
+  uint32_t getCacheSize() const {
+    return cacheSize_.load();
+  }
+
+  ThreadCachedInt& operator+=(IntT inc) { increment(inc); return *this; }
+  ThreadCachedInt& operator-=(IntT inc) { increment(-inc); return *this; }
+  // pre-increment (we don't support post-increment)
+  ThreadCachedInt& operator++() { increment(1); return *this; }
+  ThreadCachedInt& operator--() { increment(-1); return *this; }
+
+  // Thread-safe set function.
+  // This is a best effort implementation. In some edge cases, there could be
+  // data loss (missing counts)
+  void set(IntT newVal) {
+    for (auto& cache : cache_.accessAllThreads()) {
+      cache.reset_.store(true, std::memory_order_release);
+    }
+    target_.store(newVal, std::memory_order_release);
+  }
+
+  // This is a little tricky - it's possible that our IntCaches are still alive
+  // in another thread and will get destroyed after this destructor runs, so we
+  // need to make sure we signal that this parent is dead.
+  ~ThreadCachedInt() {
+    for (auto& cache : cache_.accessAllThreads()) {
+      cache.parent_ = NULL;
+    }
+  }
+
+ private:
+  std::atomic<IntT> target_;
+  std::atomic<uint32_t> cacheSize_;
+  ThreadLocalPtr<IntCache,Tag> cache_; // Must be last for dtor ordering
+
+  // This should only ever be modified by one thread
+  struct IntCache {
+    ThreadCachedInt* parent_;
+    mutable std::atomic<IntT> val_;
+    mutable uint32_t numUpdates_;
+    std::atomic<bool> reset_;
+
+    explicit IntCache(ThreadCachedInt& parent)
+        : parent_(&parent), val_(0), numUpdates_(0), reset_(false) {}
+
+    void increment(IntT inc) {
+      if (LIKELY(!reset_.load(std::memory_order_acquire))) {
+        // This thread is the only writer to val_, so it's fine do do
+        // a relaxed load and do the addition non-atomically.
+        val_.store(
+          val_.load(std::memory_order_relaxed) + inc,
+          std::memory_order_release
+        );
+      } else {
+        val_.store(inc, std::memory_order_relaxed);
+        reset_.store(false, std::memory_order_release);
+      }
+      ++numUpdates_;
+      if (UNLIKELY(numUpdates_ >
+                   parent_->cacheSize_.load(std::memory_order_acquire))) {
+        flush();
+      }
+    }
+
+    void flush() const {
+      parent_->target_.fetch_add(val_, std::memory_order_release);
+      val_.store(0, std::memory_order_release);
+      numUpdates_ = 0;
+    }
+
+    ~IntCache() {
+      if (parent_) {
+        flush();
+      }
+    }
+  };
+};
+
+}
+
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/ThreadLocal.h
@@ -0,0 +1,344 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Improved thread local storage for non-trivial types (similar speed as
+ * pthread_getspecific but only consumes a single pthread_key_t, and 4x faster
+ * than boost::thread_specific_ptr).
+ *
+ * Also includes an accessor interface to walk all the thread local child
+ * objects of a parent.  accessAllThreads() initializes an accessor which holds
+ * a global lock *that blocks all creation and destruction of ThreadLocal
+ * objects with the same Tag* and can be used as an iterable container.
+ *
+ * Intended use is for frequent write, infrequent read data access patterns such
+ * as counters.
+ *
+ * There are two classes here - ThreadLocal and ThreadLocalPtr.  ThreadLocalPtr
+ * has semantics similar to boost::thread_specific_ptr. ThreadLocal is a thin
+ * wrapper around ThreadLocalPtr that manages allocation automatically.
+ *
+ * @author Spencer Ahrens (sahrens)
+ */
+
+#ifndef FOLLY_THREADLOCAL_H_
+#define FOLLY_THREADLOCAL_H_
+
+#include "folly/Portability.h"
+#include <boost/iterator/iterator_facade.hpp>
+#include "folly/Likely.h"
+#include <type_traits>
+
+
+namespace folly {
+enum class TLPDestructionMode {
+  THIS_THREAD,
+  ALL_THREADS
+};
+}  // namespace
+
+#include "folly/detail/ThreadLocalDetail.h"
+
+namespace folly {
+
+template<class T, class Tag> class ThreadLocalPtr;
+
+template<class T, class Tag=void>
+class ThreadLocal {
+ public:
+  ThreadLocal() { }
+
+  T* get() const {
+    T* ptr = tlp_.get();
+    if (LIKELY(ptr != nullptr)) {
+      return ptr;
+    }
+
+    // separated new item creation out to speed up the fast path.
+    return makeTlp();
+  }
+
+  T* operator->() const {
+    return get();
+  }
+
+  T& operator*() const {
+    return *get();
+  }
+
+  void reset(T* newPtr = nullptr) {
+    tlp_.reset(newPtr);
+  }
+
+  typedef typename ThreadLocalPtr<T,Tag>::Accessor Accessor;
+  Accessor accessAllThreads() const {
+    return tlp_.accessAllThreads();
+  }
+
+  // movable
+  ThreadLocal(ThreadLocal&&) = default;
+  ThreadLocal& operator=(ThreadLocal&&) = default;
+
+ private:
+  // non-copyable
+  ThreadLocal(const ThreadLocal&) = delete;
+  ThreadLocal& operator=(const ThreadLocal&) = delete;
+
+  T* makeTlp() const {
+    T* ptr = new T();
+    tlp_.reset(ptr);
+    return ptr;
+  }
+
+  mutable ThreadLocalPtr<T,Tag> tlp_;
+};
+
+/*
+ * The idea here is that __thread is faster than pthread_getspecific, so we
+ * keep a __thread array of pointers to objects (ThreadEntry::elements) where
+ * each array has an index for each unique instance of the ThreadLocalPtr
+ * object.  Each ThreadLocalPtr object has a unique id that is an index into
+ * these arrays so we can fetch the correct object from thread local storage
+ * very efficiently.
+ *
+ * In order to prevent unbounded growth of the id space and thus huge
+ * ThreadEntry::elements, arrays, for example due to continuous creation and
+ * destruction of ThreadLocalPtr objects, we keep a set of all active
+ * instances.  When an instance is destroyed we remove it from the active
+ * set and insert the id into freeIds_ for reuse.  These operations require a
+ * global mutex, but only happen at construction and destruction time.
+ *
+ * We use a single global pthread_key_t per Tag to manage object destruction and
+ * memory cleanup upon thread exit because there is a finite number of
+ * pthread_key_t's available per machine.
+ *
+ * NOTE: Apple platforms don't support the same semantics for __thread that
+ *       Linux does (and it's only supported at all on i386). For these, use
+ *       pthread_setspecific()/pthread_getspecific() for the per-thread
+ *       storage.
+ */
+
+template<class T, class Tag=void>
+class ThreadLocalPtr {
+ public:
+  ThreadLocalPtr() : id_(threadlocal_detail::StaticMeta<Tag>::create()) { }
+
+  ThreadLocalPtr(ThreadLocalPtr&& other) : id_(other.id_) {
+    other.id_ = 0;
+  }
+
+  ThreadLocalPtr& operator=(ThreadLocalPtr&& other) {
+    assert(this != &other);
+    destroy();
+    id_ = other.id_;
+    other.id_ = 0;
+    return *this;
+  }
+
+  ~ThreadLocalPtr() {
+    destroy();
+  }
+
+  T* get() const {
+    return static_cast<T*>(threadlocal_detail::StaticMeta<Tag>::get(id_).ptr);
+  }
+
+  T* operator->() const {
+    return get();
+  }
+
+  T& operator*() const {
+    return *get();
+  }
+
+  void reset(T* newPtr = nullptr) {
+    threadlocal_detail::ElementWrapper& w =
+      threadlocal_detail::StaticMeta<Tag>::get(id_);
+    if (w.ptr != newPtr) {
+      w.dispose(TLPDestructionMode::THIS_THREAD);
+      w.set(newPtr);
+    }
+  }
+
+  explicit operator bool() const {
+    return get() != nullptr;
+  }
+
+  /**
+   * reset() with a custom deleter:
+   * deleter(T* ptr, TLPDestructionMode mode)
+   * "mode" is ALL_THREADS if we're destructing this ThreadLocalPtr (and thus
+   * deleting pointers for all threads), and THIS_THREAD if we're only deleting
+   * the member for one thread (because of thread exit or reset())
+   */
+  template <class Deleter>
+  void reset(T* newPtr, Deleter deleter) {
+    threadlocal_detail::ElementWrapper& w =
+      threadlocal_detail::StaticMeta<Tag>::get(id_);
+    if (w.ptr != newPtr) {
+      w.dispose(TLPDestructionMode::THIS_THREAD);
+      w.set(newPtr, deleter);
+    }
+  }
+
+  // Holds a global lock for iteration through all thread local child objects.
+  // Can be used as an iterable container.
+  // Use accessAllThreads() to obtain one.
+  class Accessor {
+    friend class ThreadLocalPtr<T,Tag>;
+
+    threadlocal_detail::StaticMeta<Tag>& meta_;
+    std::mutex* lock_;
+    int id_;
+
+   public:
+    class Iterator;
+    friend class Iterator;
+
+    // The iterators obtained from Accessor are bidirectional iterators.
+    class Iterator : public boost::iterator_facade<
+          Iterator,                               // Derived
+          T,                                      // value_type
+          boost::bidirectional_traversal_tag> {   // traversal
+      friend class Accessor;
+      friend class boost::iterator_core_access;
+      const Accessor* const accessor_;
+      threadlocal_detail::ThreadEntry* e_;
+
+      void increment() {
+        e_ = e_->next;
+        incrementToValid();
+      }
+
+      void decrement() {
+        e_ = e_->prev;
+        decrementToValid();
+      }
+
+      T& dereference() const {
+        return *static_cast<T*>(e_->elements[accessor_->id_].ptr);
+      }
+
+      bool equal(const Iterator& other) const {
+        return (accessor_->id_ == other.accessor_->id_ &&
+                e_ == other.e_);
+      }
+
+      explicit Iterator(const Accessor* accessor)
+        : accessor_(accessor),
+          e_(&accessor_->meta_.head_) {
+      }
+
+      bool valid() const {
+        return (e_->elements &&
+                accessor_->id_ < e_->elementsCapacity &&
+                e_->elements[accessor_->id_].ptr);
+      }
+
+      void incrementToValid() {
+        for (; e_ != &accessor_->meta_.head_ && !valid(); e_ = e_->next) { }
+      }
+
+      void decrementToValid() {
+        for (; e_ != &accessor_->meta_.head_ && !valid(); e_ = e_->prev) { }
+      }
+    };
+
+    ~Accessor() {
+      release();
+    }
+
+    Iterator begin() const {
+      return ++Iterator(this);
+    }
+
+    Iterator end() const {
+      return Iterator(this);
+    }
+
+    Accessor(const Accessor&) = delete;
+    Accessor& operator=(const Accessor&) = delete;
+
+    Accessor(Accessor&& other) noexcept
+      : meta_(other.meta_),
+        lock_(other.lock_),
+        id_(other.id_) {
+      other.id_ = 0;
+      other.lock_ = nullptr;
+    }
+
+    Accessor& operator=(Accessor&& other) noexcept {
+      // Each Tag has its own unique meta, and accessors with different Tags
+      // have different types.  So either *this is empty, or this and other
+      // have the same tag.  But if they have the same tag, they have the same
+      // meta (and lock), so they'd both hold the lock at the same time,
+      // which is impossible, which leaves only one possible scenario --
+      // *this is empty.  Assert it.
+      assert(&meta_ == &other.meta_);
+      assert(lock_ == nullptr);
+      using std::swap;
+      swap(lock_, other.lock_);
+      swap(id_, other.id_);
+    }
+
+    Accessor()
+      : meta_(threadlocal_detail::StaticMeta<Tag>::instance()),
+        lock_(nullptr),
+        id_(0) {
+    }
+
+   private:
+    explicit Accessor(int id)
+      : meta_(threadlocal_detail::StaticMeta<Tag>::instance()),
+        lock_(&meta_.lock_) {
+      lock_->lock();
+      id_ = id;
+    }
+
+    void release() {
+      if (lock_) {
+        lock_->unlock();
+        id_ = 0;
+        lock_ = nullptr;
+      }
+    }
+  };
+
+  // accessor allows a client to iterate through all thread local child
+  // elements of this ThreadLocal instance.  Holds a global lock for each <Tag>
+  Accessor accessAllThreads() const {
+    static_assert(!std::is_same<Tag, void>::value,
+                  "Must use a unique Tag to use the accessAllThreads feature");
+    return Accessor(id_);
+  }
+
+ private:
+  void destroy() {
+    if (id_) {
+      threadlocal_detail::StaticMeta<Tag>::destroy(id_);
+    }
+  }
+
+  // non-copyable
+  ThreadLocalPtr(const ThreadLocalPtr&) = delete;
+  ThreadLocalPtr& operator=(const ThreadLocalPtr&) = delete;
+
+  int id_;  // every instantiation has a unique id
+};
+
+}  // namespace folly
+
+#endif /* FOLLY_THREADLOCAL_H_ */
--- /dev/null
+++ b/hphp/submodules/folly/folly/TimeoutQueue.cpp
@@ -0,0 +1,77 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/TimeoutQueue.h"
+#include <algorithm>
+#include <vector>
+
+namespace folly {
+
+TimeoutQueue::Id TimeoutQueue::add(
+  int64_t now,
+  int64_t delay,
+  Callback callback) {
+  Id id = nextId_++;
+  timeouts_.insert({id, now + delay, -1, std::move(callback)});
+  return id;
+}
+
+TimeoutQueue::Id TimeoutQueue::addRepeating(
+  int64_t now,
+  int64_t interval,
+  Callback callback) {
+  Id id = nextId_++;
+  timeouts_.insert({id, now + interval, interval, std::move(callback)});
+  return id;
+}
+
+int64_t TimeoutQueue::nextExpiration() const {
+  return (timeouts_.empty() ? std::numeric_limits<int64_t>::max() :
+          timeouts_.get<BY_EXPIRATION>().begin()->expiration);
+}
+
+bool TimeoutQueue::erase(Id id) {
+  return timeouts_.get<BY_ID>().erase(id);
+}
+
+int64_t TimeoutQueue::runInternal(int64_t now, bool onceOnly) {
+  auto& byExpiration = timeouts_.get<BY_EXPIRATION>();
+  int64_t nextExp;
+  do {
+    auto end = byExpiration.upper_bound(now);
+    std::vector<Event> expired;
+    std::move(byExpiration.begin(), end, std::back_inserter(expired));
+    byExpiration.erase(byExpiration.begin(), end);
+    for (auto& event : expired) {
+      // Reinsert if repeating, do this before executing callbacks
+      // so the callbacks have a chance to call erase
+      if (event.repeatInterval >= 0) {
+        timeouts_.insert({event.id, now + event.repeatInterval,
+                          event.repeatInterval, event.callback});
+      }
+    }
+
+    // Call callbacks
+    for (auto& event : expired) {
+      event.callback(event.id, now);
+    }
+    nextExp = nextExpiration();
+  } while (!onceOnly && nextExp <= now);
+  return nextExp;
+}
+
+}  // namespace folly
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/TimeoutQueue.h
@@ -0,0 +1,132 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Simple timeout queue.  Call user-specified callbacks when their timeouts
+ * expire.
+ *
+ * This class assumes that "time" is an int64_t and doesn't care about time
+ * units (seconds, milliseconds, etc).  You call runOnce() / runLoop() using
+ * the same time units that you use to specify callbacks.
+ *
+ * @author Tudor Bosman (tudorb@fb.com)
+ */
+
+#ifndef FOLLY_TIMEOUTQUEUE_H_
+#define FOLLY_TIMEOUTQUEUE_H_
+
+#include <stdint.h>
+#include <functional>
+#include <boost/multi_index_container.hpp>
+#include <boost/multi_index/indexed_by.hpp>
+#include <boost/multi_index/ordered_index.hpp>
+#include <boost/multi_index/member.hpp>
+
+namespace folly {
+
+class TimeoutQueue {
+ public:
+  typedef int64_t Id;
+  typedef std::function<void(Id, int64_t)> Callback;
+
+  TimeoutQueue() : nextId_(1) { }
+
+  /**
+   * Add a one-time timeout event that will fire "delay" time units from "now"
+   * (that is, the first time that run*() is called with a time value >= now
+   * + delay).
+   */
+  Id add(int64_t now, int64_t delay, Callback callback);
+
+  /**
+   * Add a repeating timeout event that will fire every "interval" time units
+   * (it will first fire when run*() is called with a time value >=
+   * now + interval).
+   *
+   * run*() will always invoke each repeating event at most once, even if
+   * more than one "interval" period has passed.
+   */
+  Id addRepeating(int64_t now, int64_t interval, Callback callback);
+
+  /**
+   * Erase a given timeout event, returns true if the event was actually
+   * erased and false if it didn't exist in our queue.
+   */
+  bool erase(Id id);
+
+  /**
+   * Process all events that are due at times <= "now" by calling their
+   * callbacks.
+   *
+   * Callbacks are allowed to call back into the queue and add / erase events;
+   * they might create more events that are already due.  In this case,
+   * runOnce() will only go through the queue once, and return a "next
+   * expiration" time in the past or present (<= now); runLoop()
+   * will process the queue again, until there are no events already due.
+   *
+   * Note that it is then possible for runLoop to never return if
+   * callbacks re-add themselves to the queue (or if you have repeating
+   * callbacks with an interval of 0).
+   *
+   * Return the time that the next event will be due (same as
+   * nextExpiration(), below)
+   */
+  int64_t runOnce(int64_t now) { return runInternal(now, true); }
+  int64_t runLoop(int64_t now) { return runInternal(now, false); }
+
+  /**
+   * Return the time that the next event will be due.
+   */
+  int64_t nextExpiration() const;
+
+ private:
+  int64_t runInternal(int64_t now, bool runOnce);
+  // noncopyable
+  TimeoutQueue(const TimeoutQueue&) = delete;
+  TimeoutQueue& operator=(const TimeoutQueue&) = delete;
+
+  struct Event {
+    Id id;
+    int64_t expiration;
+    int64_t repeatInterval;
+    Callback callback;
+  };
+
+  typedef boost::multi_index_container<
+    Event,
+    boost::multi_index::indexed_by<
+      boost::multi_index::ordered_unique<boost::multi_index::member<
+        Event, Id, &Event::id
+      >>,
+      boost::multi_index::ordered_non_unique<boost::multi_index::member<
+        Event, int64_t, &Event::expiration
+      >>
+    >
+  > Set;
+
+  enum {
+    BY_ID=0,
+    BY_EXPIRATION=1
+  };
+
+  Set timeouts_;
+  Id nextId_;
+};
+
+}  // namespace folly
+
+#endif /* FOLLY_TIMEOUTQUEUE_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/Traits.h
@@ -0,0 +1,523 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// @author: Andrei Alexandrescu
+
+#ifndef FOLLY_BASE_TRAITS_H_
+#define FOLLY_BASE_TRAITS_H_
+
+#include <memory>
+#include <limits>
+#include <type_traits>
+
+#include "folly/Portability.h"
+
+// libc++ doesn't provide this header
+#if !FOLLY_USE_LIBCPP
+// This file appears in two locations: inside fbcode and in the
+// libstdc++ source code (when embedding fbstring as std::string).
+// To aid in this schizophrenic use, two macros are defined in
+// c++config.h:
+//   _LIBSTDCXX_FBSTRING - Set inside libstdc++.  This is useful to
+//      gate use inside fbcode v. libstdc++
+#include <bits/c++config.h>
+#endif
+
+#include <boost/type_traits.hpp>
+#include <boost/mpl/and.hpp>
+#include <boost/mpl/has_xxx.hpp>
+#include <boost/mpl/not.hpp>
+
+namespace folly {
+
+/**
+ * IsRelocatable<T>::value describes the ability of moving around
+ * memory a value of type T by using memcpy (as opposed to the
+ * conservative approach of calling the copy constructor and then
+ * destroying the old temporary. Essentially for a relocatable type,
+ * the following two sequences of code should be semantically
+ * equivalent:
+ *
+ * void move1(T * from, T * to) {
+ *   new(to) T(from);
+ *   (*from).~T();
+ * }
+ *
+ * void move2(T * from, T * to) {
+ *   memcpy(to, from, sizeof(T));
+ * }
+ *
+ * Most C++ types are relocatable; the ones that aren't would include
+ * internal pointers or (very rarely) would need to update remote
+ * pointers to pointers tracking them. All C++ primitive types and
+ * type constructors are relocatable.
+ *
+ * This property can be used in a variety of optimizations. Currently
+ * fbvector uses this property intensively.
+ *
+ * The default conservatively assumes the type is not
+ * relocatable. Several specializations are defined for known
+ * types. You may want to add your own specializations. Do so in
+ * namespace folly and make sure you keep the specialization of
+ * IsRelocatable<SomeStruct> in the same header as SomeStruct.
+ *
+ * You may also declare a type to be relocatable by including
+ *    `typedef std::true_type IsRelocatable;`
+ * in the class header.
+ *
+ * It may be unset in a base class by overriding the typedef to false_type.
+ */
+/*
+ * IsTriviallyCopyable describes the value semantics property. C++11 contains
+ * the type trait is_trivially_copyable; however, it is not yet implemented
+ * in gcc (as of 4.7.1), and the user may wish to specify otherwise.
+ */
+/*
+ * IsZeroInitializable describes the property that default construction is the
+ * same as memset(dst, 0, sizeof(T)).
+ */
+
+namespace traits_detail {
+
+#define FOLLY_HAS_TRUE_XXX(name)                          \
+  BOOST_MPL_HAS_XXX_TRAIT_DEF(name);                      \
+  template <class T> struct name ## _is_true              \
+    : std::is_same<typename T::name, std::true_type> {};  \
+  template <class T> struct has_true_ ## name             \
+    : std::conditional<                                   \
+        has_ ## name <T>::value,                          \
+        name ## _is_true<T>,                              \
+        std::false_type                                   \
+      >:: type {};
+
+FOLLY_HAS_TRUE_XXX(IsRelocatable)
+FOLLY_HAS_TRUE_XXX(IsZeroInitializable)
+FOLLY_HAS_TRUE_XXX(IsTriviallyCopyable)
+
+#undef FOLLY_HAS_TRUE_XXX
+}
+
+template <class T> struct IsTriviallyCopyable
+  : std::integral_constant<bool,
+      !std::is_class<T>::value ||
+      // TODO: add alternate clause is_trivially_copyable, when available
+      traits_detail::has_true_IsTriviallyCopyable<T>::value
+    > {};
+
+template <class T> struct IsRelocatable
+  : std::integral_constant<bool,
+      !std::is_class<T>::value ||
+      // TODO add this line (and some tests for it) when we upgrade to gcc 4.7
+      //std::is_trivially_move_constructible<T>::value ||
+      IsTriviallyCopyable<T>::value ||
+      traits_detail::has_true_IsRelocatable<T>::value
+    > {};
+
+template <class T> struct IsZeroInitializable
+  : std::integral_constant<bool,
+      !std::is_class<T>::value ||
+      traits_detail::has_true_IsZeroInitializable<T>::value
+    > {};
+
+} // namespace folly
+
+/**
+ * Use this macro ONLY inside namespace folly. When using it with a
+ * regular type, use it like this:
+ *
+ * // Make sure you're at namespace ::folly scope
+ * template<> FOLLY_ASSUME_RELOCATABLE(MyType)
+ *
+ * When using it with a template type, use it like this:
+ *
+ * // Make sure you're at namespace ::folly scope
+ * template<class T1, class T2>
+ * FOLLY_ASSUME_RELOCATABLE(MyType<T1, T2>)
+ */
+#define FOLLY_ASSUME_RELOCATABLE(...) \
+  struct IsRelocatable<  __VA_ARGS__ > : std::true_type {};
+
+/**
+ * Use this macro ONLY inside namespace boost. When using it with a
+ * regular type, use it like this:
+ *
+ * // Make sure you're at namespace ::boost scope
+ * template<> FOLLY_ASSUME_HAS_NOTHROW_CONSTRUCTOR(MyType)
+ *
+ * When using it with a template type, use it like this:
+ *
+ * // Make sure you're at namespace ::boost scope
+ * template<class T1, class T2>
+ * FOLLY_ASSUME_HAS_NOTHROW_CONSTRUCTOR(MyType<T1, T2>)
+ */
+#define FOLLY_ASSUME_HAS_NOTHROW_CONSTRUCTOR(...) \
+  struct has_nothrow_constructor<  __VA_ARGS__ > : ::boost::true_type {};
+
+/**
+ * The FOLLY_ASSUME_FBVECTOR_COMPATIBLE* macros below encode two
+ * assumptions: first, that the type is relocatable per IsRelocatable
+ * above, and that it has a nothrow constructor. Most types can be
+ * assumed to satisfy both conditions, but it is the responsibility of
+ * the user to state that assumption. User-defined classes will not
+ * work with fbvector (see FBVector.h) unless they state this
+ * combination of properties.
+ *
+ * Use FOLLY_ASSUME_FBVECTOR_COMPATIBLE with regular types like this:
+ *
+ * FOLLY_ASSUME_FBVECTOR_COMPATIBLE(MyType)
+ *
+ * The versions FOLLY_ASSUME_FBVECTOR_COMPATIBLE_1, _2, _3, and _4
+ * allow using the macro for describing templatized classes with 1, 2,
+ * 3, and 4 template parameters respectively. For template classes
+ * just use the macro with the appropriate number and pass the name of
+ * the template to it. Example:
+ *
+ * template <class T1, class T2> class MyType { ... };
+ * ...
+ * // Make sure you're at global scope
+ * FOLLY_ASSUME_FBVECTOR_COMPATIBLE_2(MyType)
+ */
+
+// Use this macro ONLY at global level (no namespace)
+#define FOLLY_ASSUME_FBVECTOR_COMPATIBLE(...)                           \
+  namespace folly { template<> FOLLY_ASSUME_RELOCATABLE(__VA_ARGS__) }   \
+  namespace boost { \
+  template<> FOLLY_ASSUME_HAS_NOTHROW_CONSTRUCTOR(__VA_ARGS__) }
+// Use this macro ONLY at global level (no namespace)
+#define FOLLY_ASSUME_FBVECTOR_COMPATIBLE_1(...)                         \
+  namespace folly {                                                     \
+  template <class T1> FOLLY_ASSUME_RELOCATABLE(__VA_ARGS__<T1>) }       \
+    namespace boost {                                                   \
+    template <class T1> FOLLY_ASSUME_HAS_NOTHROW_CONSTRUCTOR(__VA_ARGS__<T1>) }
+// Use this macro ONLY at global level (no namespace)
+#define FOLLY_ASSUME_FBVECTOR_COMPATIBLE_2(...)                 \
+  namespace folly {                                             \
+  template <class T1, class T2>                                 \
+  FOLLY_ASSUME_RELOCATABLE(__VA_ARGS__<T1, T2>) }               \
+    namespace boost {                                           \
+    template <class T1, class T2>                               \
+    FOLLY_ASSUME_HAS_NOTHROW_CONSTRUCTOR(__VA_ARGS__<T1, T2>) }
+// Use this macro ONLY at global level (no namespace)
+#define FOLLY_ASSUME_FBVECTOR_COMPATIBLE_3(...)                         \
+  namespace folly {                                                     \
+  template <class T1, class T2, class T3>                               \
+  FOLLY_ASSUME_RELOCATABLE(__VA_ARGS__<T1, T2, T3>) }                   \
+    namespace boost {                                                   \
+    template <class T1, class T2, class T3>                             \
+    FOLLY_ASSUME_HAS_NOTHROW_CONSTRUCTOR(__VA_ARGS__<T1, T2, T3>) }
+// Use this macro ONLY at global level (no namespace)
+#define FOLLY_ASSUME_FBVECTOR_COMPATIBLE_4(...)                         \
+  namespace folly {                                                     \
+  template <class T1, class T2, class T3, class T4>                     \
+  FOLLY_ASSUME_RELOCATABLE(__VA_ARGS__<T1, T2, T3, T4>) }               \
+    namespace boost {                                                   \
+    template <class T1, class T2, class T3, class T4>                   \
+    FOLLY_ASSUME_HAS_NOTHROW_CONSTRUCTOR(__VA_ARGS__<T1, T2, T3, T4>) }
+
+/**
+ * Instantiate FOLLY_ASSUME_FBVECTOR_COMPATIBLE for a few types. It is
+ * safe to assume that pair is compatible if both of its components
+ * are. Furthermore, all STL containers can be assumed to comply,
+ * although that is not guaranteed by the standard.
+ */
+
+FOLLY_NAMESPACE_STD_BEGIN
+
+template <class T, class U>
+  struct pair;
+#ifndef _GLIBCXX_USE_FB
+template <class T, class R, class A>
+  class basic_string;
+#else
+template <class T, class R, class A, class S>
+  class basic_string;
+#endif
+template <class T, class A>
+  class vector;
+template <class T, class A>
+  class deque;
+template <class T, class A>
+  class list;
+template <class T, class C, class A>
+  class set;
+template <class K, class V, class C, class A>
+  class map;
+template <class T>
+  class shared_ptr;
+
+FOLLY_NAMESPACE_STD_END
+
+namespace boost {
+
+template <class T> class shared_ptr;
+
+template <class T, class U>
+struct has_nothrow_constructor< std::pair<T, U> >
+    : ::boost::mpl::and_< has_nothrow_constructor<T>,
+                          has_nothrow_constructor<U> > {};
+
+} // namespace boost
+
+namespace folly {
+
+// STL commonly-used types
+template <class T, class U>
+struct IsRelocatable<  std::pair<T, U> >
+    : ::boost::mpl::and_< IsRelocatable<T>, IsRelocatable<U> > {};
+
+// Is T one of T1, T2, ..., Tn?
+template <class T, class... Ts>
+struct IsOneOf {
+  enum { value = false };
+};
+
+template <class T, class T1, class... Ts>
+struct IsOneOf<T, T1, Ts...> {
+  enum { value = std::is_same<T, T1>::value || IsOneOf<T, Ts...>::value };
+};
+
+/**
+ * A traits class to check for incomplete types.
+ *
+ * Example:
+ *
+ *  struct FullyDeclared {}; // complete type
+ *  struct ForwardDeclared; // incomplete type
+ *
+ *  is_complete<int>::value // evaluates to true
+ *  is_complete<FullyDeclared>::value // evaluates to true
+ *  is_complete<ForwardDeclared>::value // evaluates to false
+ *
+ *  struct ForwardDeclared {}; // declared, at last
+ *
+ *  is_complete<ForwardDeclared>::value // now it evaluates to true
+ *
+ * @author: Marcelo Juchem <marcelo@fb.com>
+ */
+template <typename T>
+class is_complete {
+  template <unsigned long long> struct sfinae {};
+  template <typename U>
+  constexpr static bool test(sfinae<sizeof(U)>*) { return true; }
+  template <typename> constexpr static bool test(...) { return false; }
+public:
+  constexpr static bool value = test<T>(nullptr);
+};
+
+/*
+ * Complementary type traits for integral comparisons.
+ *
+ * For instance, `if(x < 0)` yields an error in clang for unsigned types
+ *  when -Werror is used due to -Wtautological-compare
+ *
+ *
+ * @author: Marcelo Juchem <marcelo@fb.com>
+ */
+
+namespace detail {
+
+template <typename T, bool>
+struct is_negative_impl {
+  constexpr static bool check(T x) { return x < 0; }
+};
+
+template <typename T>
+struct is_negative_impl<T, false> {
+  constexpr static bool check(T x) { return false; }
+};
+
+template <typename RHS, RHS rhs, typename LHS>
+bool less_than_impl(
+  typename std::enable_if<
+    (rhs <= std::numeric_limits<LHS>::max()
+      && rhs > std::numeric_limits<LHS>::min()),
+    LHS
+  >::type const lhs
+) {
+  return lhs < rhs;
+}
+
+template <typename RHS, RHS rhs, typename LHS>
+bool less_than_impl(
+  typename std::enable_if<
+    (rhs > std::numeric_limits<LHS>::max()),
+    LHS
+  >::type const
+) {
+  return true;
+}
+
+template <typename RHS, RHS rhs, typename LHS>
+bool less_than_impl(
+  typename std::enable_if<
+    (rhs <= std::numeric_limits<LHS>::min()),
+    LHS
+  >::type const
+) {
+  return false;
+}
+
+template <typename RHS, RHS rhs, typename LHS>
+bool greater_than_impl(
+  typename std::enable_if<
+    (rhs <= std::numeric_limits<LHS>::max()
+      && rhs >= std::numeric_limits<LHS>::min()),
+    LHS
+  >::type const lhs
+) {
+  return lhs > rhs;
+}
+
+template <typename RHS, RHS rhs, typename LHS>
+bool greater_than_impl(
+  typename std::enable_if<
+    (rhs > std::numeric_limits<LHS>::max()),
+    LHS
+  >::type const
+) {
+  return false;
+}
+
+template <typename RHS, RHS rhs, typename LHS>
+bool greater_than_impl(
+  typename std::enable_if<
+    (rhs < std::numeric_limits<LHS>::min()),
+    LHS
+  >::type const
+) {
+  return true;
+}
+
+} // namespace detail {
+
+// same as `x < 0`
+template <typename T>
+constexpr bool is_negative(T x) {
+  return folly::detail::is_negative_impl<T, std::is_signed<T>::value>::check(x);
+}
+
+// same as `x <= 0`
+template <typename T>
+constexpr bool is_non_positive(T x) { return !x || folly::is_negative(x); }
+
+// same as `x > 0`
+template <typename T>
+constexpr bool is_positive(T x) { return !is_non_positive(x); }
+
+// same as `x >= 0`
+template <typename T>
+constexpr bool is_non_negative(T x) {
+  return !x || is_positive(x);
+}
+
+template <typename RHS, RHS rhs, typename LHS>
+bool less_than(LHS const lhs) {
+  return detail::less_than_impl<
+    RHS, rhs, typename std::remove_reference<LHS>::type
+  >(lhs);
+}
+
+template <typename RHS, RHS rhs, typename LHS>
+bool greater_than(LHS const lhs) {
+  return detail::greater_than_impl<
+    RHS, rhs, typename std::remove_reference<LHS>::type
+  >(lhs);
+}
+
+} // namespace folly
+
+FOLLY_ASSUME_FBVECTOR_COMPATIBLE_3(std::basic_string);
+FOLLY_ASSUME_FBVECTOR_COMPATIBLE_2(std::vector);
+FOLLY_ASSUME_FBVECTOR_COMPATIBLE_2(std::list);
+FOLLY_ASSUME_FBVECTOR_COMPATIBLE_2(std::deque);
+FOLLY_ASSUME_FBVECTOR_COMPATIBLE_2(std::unique_ptr);
+FOLLY_ASSUME_FBVECTOR_COMPATIBLE_1(std::shared_ptr);
+FOLLY_ASSUME_FBVECTOR_COMPATIBLE_1(std::function);
+
+// Boost
+FOLLY_ASSUME_FBVECTOR_COMPATIBLE_1(boost::shared_ptr);
+
+#define FOLLY_CREATE_HAS_MEMBER_FN_TRAITS_IMPL(classname, func_name, cv_qual) \
+  template <typename TTheClass_, typename RTheReturn_, typename... TTheArgs_> \
+  class classname<TTheClass_, RTheReturn_(TTheArgs_...) cv_qual> { \
+    template < \
+      typename UTheClass_, RTheReturn_ (UTheClass_::*)(TTheArgs_...) cv_qual \
+    > struct sfinae {}; \
+    template <typename UTheClass_> \
+    constexpr static bool test(sfinae<UTheClass_, &UTheClass_::func_name>*) \
+    { return true; } \
+    template <typename> \
+    constexpr static bool test(...) { return false; } \
+  public: \
+    constexpr static bool value = test<TTheClass_>(nullptr); \
+  }
+
+/*
+ * The FOLLY_CREATE_HAS_MEMBER_FN_TRAITS is used to create traits
+ * classes that check for the existence of a member function with
+ * a given name and signature. It currently does not support
+ * checking for inherited members.
+ *
+ * Such classes receive two template parameters: the class to be checked
+ * and the signature of the member function. A static boolean field
+ * named `value` (which is also constexpr) tells whether such member
+ * function exists.
+ *
+ * Each traits class created is bound only to the member name, not to
+ * its signature nor to the type of the class containing it.
+ *
+ * Say you need to know if a given class has a member function named
+ * `test` with the following signature:
+ *
+ *    int test() const;
+ *
+ * You'd need this macro to create a traits class to check for a member
+ * named `test`, and then use this traits class to check for the signature:
+ *
+ * namespace {
+ *
+ * FOLLY_CREATE_HAS_MEMBER_FN_TRAITS(has_test_traits, test);
+ *
+ * } // unnamed-namespace
+ *
+ * void some_func() {
+ *   cout << "Does class Foo have a member int test() const? "
+ *     << boolalpha << has_test_traits<Foo, int() const>::value;
+ * }
+ *
+ * You can use the same traits class to test for a completely different
+ * signature, on a completely different class, as long as the member name
+ * is the same:
+ *
+ * void some_func() {
+ *   cout << "Does class Foo have a member int test()? "
+ *     << boolalpha << has_test_traits<Foo, int()>::value;
+ *   cout << "Does class Foo have a member int test() const? "
+ *     << boolalpha << has_test_traits<Foo, int() const>::value;
+ *   cout << "Does class Bar have a member double test(const string&, long)? "
+ *     << boolalpha << has_test_traits<Bar, double(const string&, long)>::value;
+ * }
+ *
+ * @author: Marcelo Juchem <marcelo@fb.com>
+ */
+#define FOLLY_CREATE_HAS_MEMBER_FN_TRAITS(classname, func_name) \
+  template <typename, typename> class classname; \
+  FOLLY_CREATE_HAS_MEMBER_FN_TRAITS_IMPL(classname, func_name, ); \
+  FOLLY_CREATE_HAS_MEMBER_FN_TRAITS_IMPL(classname, func_name, const); \
+  FOLLY_CREATE_HAS_MEMBER_FN_TRAITS_IMPL(classname, func_name, volatile); \
+  FOLLY_CREATE_HAS_MEMBER_FN_TRAITS_IMPL(classname, func_name, volatile const)
+
+#endif //FOLLY_BASE_TRAITS_H_
--- /dev/null
+++ b/hphp/submodules/folly/folly/Unicode.cpp
@@ -0,0 +1,54 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Unicode.h"
+
+namespace folly {
+
+//////////////////////////////////////////////////////////////////////
+
+fbstring codePointToUtf8(char32_t cp) {
+  fbstring result;
+
+  // Based on description from http://en.wikipedia.org/wiki/UTF-8.
+
+  if (cp <= 0x7f) {
+    result.resize(1);
+    result[0] = static_cast<char>(cp);
+  } else if (cp <= 0x7FF) {
+    result.resize(2);
+    result[1] = static_cast<char>(0x80 | (0x3f & cp));
+    result[0] = static_cast<char>(0xC0 | (cp >> 6));
+  } else if (cp <= 0xFFFF) {
+    result.resize(3);
+    result[2] = static_cast<char>(0x80 | (0x3f & cp));
+    result[1] = (0x80 | static_cast<char>((0x3f & (cp >> 6))));
+    result[0] = (0xE0 | static_cast<char>(cp >> 12));
+  } else if (cp <= 0x10FFFF) {
+    result.resize(4);
+    result[3] = static_cast<char>(0x80 | (0x3f & cp));
+    result[2] = static_cast<char>(0x80 | (0x3f & (cp >> 6)));
+    result[1] = static_cast<char>(0x80 | (0x3f & (cp >> 12)));
+    result[0] = static_cast<char>(0xF0 | (cp >> 18));
+  }
+
+  return result;
+}
+
+//////////////////////////////////////////////////////////////////////
+
+}
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/Unicode.h
@@ -0,0 +1,39 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// Some utility routines relating to unicode.
+
+#ifndef FOLLY_UNICODE_H_
+#define FOLLY_UNICODE_H_
+
+#include "folly/FBString.h"
+
+namespace folly {
+
+//////////////////////////////////////////////////////////////////////
+
+/*
+ * Encode a single unicode code point into a UTF-8 byte sequence.
+ *
+ * Return value is undefined if `cp' is an invalid code point.
+ */
+fbstring codePointToUtf8(char32_t cp);
+
+//////////////////////////////////////////////////////////////////////
+
+}
+
+#endif
--- /dev/null
+++ b/hphp/submodules/folly/folly/Uri.cpp
@@ -0,0 +1,124 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "folly/Uri.h"
+
+#include <ctype.h>
+#include <boost/regex.hpp>
+
+namespace folly {
+
+namespace {
+
+fbstring submatch(const boost::cmatch& m, size_t idx) {
+  auto& sub = m[idx];
+  return fbstring(sub.first, sub.second);
+}
+
+template <class String>
+void toLower(String& s) {
+  for (auto& c : s) {
+    c = tolower(c);
+  }
+}
+
+}  // namespace
+
+Uri::Uri(StringPiece str) : port_(0) {
+  static const boost::regex uriRegex(
+      "([a-zA-Z][a-zA-Z0-9+.-]*):"  // scheme:
+      "([^?#]*)"                    // authority and path
+      "(?:\\?([^#]*))?"             // ?query
+      "(?:#(.*))?");                // #fragment
+  static const boost::regex authorityAndPathRegex("//([^/]*)(/.*)?");
+
+  boost::cmatch match;
+  if (UNLIKELY(!boost::regex_match(str.begin(), str.end(), match, uriRegex))) {
+    throw std::invalid_argument(to<std::string>("invalid URI ", str));
+  }
+
+  scheme_ = submatch(match, 1);
+  toLower(scheme_);
+
+  StringPiece authorityAndPath(match[2].first, match[2].second);
+  boost::cmatch authorityAndPathMatch;
+  if (!boost::regex_match(authorityAndPath.begin(),
+                          authorityAndPath.end(),
+                          authorityAndPathMatch,
+                          authorityAndPathRegex)) {
+    // Does not start with //, doesn't have authority
+    path_ = authorityAndPath.fbstr();
+  } else {
+    static const boost::regex authorityRegex(
+        "(?:([^@:]*)(?::([^@]*))?@)?"  // username, password
+        "(\\[[^\\]]*\\]|[^\\[:]*)"     // host (IP-literal, dotted-IPv4, or
+                                       // named host)
+        "(?::(\\d*))?");               // port
+
+    auto authority = authorityAndPathMatch[1];
+    boost::cmatch authorityMatch;
+    if (!boost::regex_match(authority.first,
+                            authority.second,
+                            authorityMatch,
+                            authorityRegex)) {
+      throw std::invalid_argument(
+          to<std::string>("invalid URI authority ",
+                          StringPiece(authority.first, authority.second)));
+    }
+
+    StringPiece port(authorityMatch[4].first, authorityMatch[4].second);
+    if (!port.empty()) {
+      port_ = to<uint16_t>(port);
+    }
+
+    username_ = submatch(authorityMatch, 1);
+    password_ = submatch(authorityMatch, 2);
+    host_ = submatch(authorityMatch, 3);
+    path_ = submatch(authorityAndPathMatch, 2);
+  }
+
+  query_ = submatch(match, 3);
+  fragment_ = submatch(match, 4);
+}
+
+fbstring Uri::authority() const {
+  fbstring result;
+
+  // Port is 5 characters max and we have up to 3 delimiters.
+  result.reserve(host().size() + username().size() + password().size() + 8);
+
+  if (!username().empty() || !password().empty()) {
+    result.append(username());
+
+    if (!password().empty()) {
+      result.push_back(':');
+      result.append(password());
+    }
+
+    result.push_back('@');
+  }
+
+  result.append(host());
+
+  if (port() != 0) {
+    result.push_back(':');
+    toAppend(port(), &result);
+  }
+
+  return result;
+}
+
+}  // namespace folly
--- /dev/null
+++ b/hphp/submodules/folly/folly/Uri.h
@@ -0,0 +1,81 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_URI_H_
+#define FOLLY_URI_H_
+
+#include "folly/String.h"
+
+namespace folly {
+
+/**
+ * Class representing a URI.
+ *
+ * Consider http://www.facebook.com/foo/bar?key=foo#anchor
+ *
+ * The URI is broken down into its parts: scheme ("http"), authority
+ * (ie. host and port, in most cases: "www.facebook.com"), path
+ * ("/foo/bar"), query ("key=foo") and fragment ("anchor").  The scheme is
+ * lower-cased.
+ *
+ * If this Uri represents a URL, note that, to prevent ambiguity, the component
+ * parts are NOT percent-decoded; you should do this yourself with
+ * uriUnescape() (for the authority and path) and uriUnescape(...,
+ * UriEscapeMode::QUERY) (for the query, but probably only after splitting at
+ * '&' to identify the individual parameters).
+ */
+class Uri {
+ public:
+  /**
+   * Parse a Uri from a string.  Throws std::invalid_argument on parse error.
+   */
+  explicit Uri(StringPiece str);
+
+  const fbstring& scheme() const { return scheme_; }
+  const fbstring& username() const { return username_; }
+  const fbstring& password() const { return password_; }
+  const fbstring& host() const { return host_; }
+  uint16_t port() const { return port_; }
+  const fbstring& path() const { return path_; }
+  const fbstring& query() const { return query_; }
+  const fbstring& fragment() const { return fragment_; }
+
+  fbstring authority() const;
+
+  template <class String>
+  String toString() const;
+
+  std::string str() const { return toString<std::string>(); }
+  fbstring fbstr() const { return toString<fbstring>(); }
+
+  void setPort(uint16_t port) {port_ = port;}
+
+ private:
+  fbstring scheme_;
+  fbstring username_;
+  fbstring password_;
+  fbstring host_;
+  uint16_t port_;
+  fbstring path_;
+  fbstring query_;
+  fbstring fragment_;
+};
+
+}  // namespace folly
+
+#include "folly/Uri-inl.h"
+
+#endif /* FOLLY_URI_H_ */
--- /dev/null
+++ b/hphp/submodules/folly/folly/Uri-inl.h
@@ -0,0 +1,49 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_URI_H_
+#error This file may only be included from folly/Uri.h
+#endif
+
+#include "folly/Conv.h"
+
+namespace folly {
+
+template <class String>
+String Uri::toString() const {
+  String str;
+  toAppend(scheme_, "://", &str);
+  if (!password_.empty()) {
+    toAppend(username_, ":", password_, "@", &str);
+  } else if (!username_.empty()) {
+    toAppend(username_, "@", &str);
+  }
+  toAppend(host_, &str);
+  if (port_ != 0) {
+    toAppend(":", port_, &str);
+  }
+  toAppend(path_, &str);
+  if (!query_.empty()) {
+    toAppend("?", query_, &str);
+  }
+  if (!fragment_.empty()) {
+    toAppend("#", fragment_, &str);
+  }
+  return str;
+}
+
+}  // namespace folly
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/Varint.h
@@ -0,0 +1,129 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef FOLLY_VARINT_H_
+#define FOLLY_VARINT_H_
+
+#include "folly/Range.h"
+
+namespace folly {
+
+/**
+ * Variable-length integer encoding, using a little-endian, base-128
+ * representation.
+ *
+ * The MSb is set on all bytes except the last.
+ *
+ * Details:
+ * https://developers.google.com/protocol-buffers/docs/encoding#varints
+ *
+ * If you want to encode multiple values, GroupVarint (in GroupVarint.h)
+ * is faster and likely smaller.
+ */
+
+/**
+ * Maximum length (in bytes) of the varint encoding of a 32-bit value.
+ */
+constexpr size_t kMaxVarintLength32 = 5;
+
+/**
+ * Maximum length (in bytes) of the varint encoding of a 64-bit value.
+ */
+constexpr size_t kMaxVarintLength64 = 10;
+
+/**
+ * Encode a value in the given buffer, returning the number of bytes used
+ * for encoding.
+ * buf must have enough space to represent the value (at least
+ * kMaxVarintLength64 bytes to encode arbitrary 64-bit values)
+ */
+size_t encodeVarint(uint64_t val, uint8_t* buf);
+
+/**
+ * Decode a value from a given buffer, advances data past the returned value.
+ */
+uint64_t decodeVarint(ByteRange& data);
+
+/**
+ * ZigZag encoding that maps signed integers with a small absolute value
+ * to unsigned integers with a small (positive) values. Without this,
+ * encoding negative values using Varint would use up 9 or 10 bytes.
+ *
+ * if x >= 0, encodeZigZag(x) == 2*x
+ * if x <  0, encodeZigZag(x) == -2*x + 1
+ */
+
+inline uint64_t encodeZigZag(int64_t val) {
+  // Bit-twiddling magic stolen from the Google protocol buffer document;
+  // val >> 63 is an arithmetic shift because val is signed
+  return static_cast<uint64_t>((val << 1) ^ (val >> 63));
+}
+
+inline int64_t decodeZigZag(uint64_t val) {
+  return static_cast<int64_t>((val >> 1) ^ -(val & 1));
+}
+
+// Implementation below
+
+inline size_t encodeVarint(uint64_t val, uint8_t* buf) {
+  uint8_t* p = buf;
+  while (val >= 128) {
+    *p++ = 0x80 | (val & 0x7f);
+    val >>= 7;
+  }
+  *p++ = val;
+  return p - buf;
+}
+
+inline uint64_t decodeVarint(ByteRange& data) {
+  const int8_t* begin = reinterpret_cast<const int8_t*>(data.begin());
+  const int8_t* end = reinterpret_cast<const int8_t*>(data.end());
+  const int8_t* p = begin;
+  uint64_t val = 0;
+
+  if (LIKELY(end - begin >= kMaxVarintLength64)) {  // fast path
+    int64_t b;
+    do {
+      b = *p++; val  = (b & 0x7f)      ; if (b >= 0) break;
+      b = *p++; val |= (b & 0x7f) <<  7; if (b >= 0) break;
+      b = *p++; val |= (b & 0x7f) << 14; if (b >= 0) break;
+      b = *p++; val |= (b & 0x7f) << 21; if (b >= 0) break;
+      b = *p++; val |= (b & 0x7f) << 28; if (b >= 0) break;
+      b = *p++; val |= (b & 0x7f) << 35; if (b >= 0) break;
+      b = *p++; val |= (b & 0x7f) << 42; if (b >= 0) break;
+      b = *p++; val |= (b & 0x7f) << 49; if (b >= 0) break;
+      b = *p++; val |= (b & 0x7f) << 56; if (b >= 0) break;
+      b = *p++; val |= (b & 0x7f) << 63; if (b >= 0) break;
+      throw std::invalid_argument("Invalid varint value");  // too big
+    } while (false);
+  } else {
+    int shift = 0;
+    while (p != end && *p < 0) {
+      val |= static_cast<uint64_t>(*p++ & 0x7f) << shift;
+      shift += 7;
+    }
+    if (p == end) throw std::invalid_argument("Invalid varint value");
+    val |= static_cast<uint64_t>(*p++) << shift;
+  }
+
+  data.advance(p - begin);
+  return val;
+}
+
+}  // namespaces
+
+#endif /* FOLLY_VARINT_H_ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/wangle/detail.h
@@ -0,0 +1,153 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <folly/Optional.h>
+#include <stdexcept>
+#include <atomic>
+
+#include "Try.h"
+#include "Promise.h"
+#include "Future.h"
+
+namespace folly { namespace wangle { namespace detail {
+
+/** The shared state object for Future and Promise. */
+template<typename T>
+class FutureObject {
+ public:
+  FutureObject() = default;
+
+  // not copyable
+  FutureObject(FutureObject const&) = delete;
+  FutureObject& operator=(FutureObject const&) = delete;
+
+  // not movable (see comment in the implementation of Future::then)
+  FutureObject(FutureObject&&) = delete;
+  FutureObject& operator=(FutureObject&&) = delete;
+
+  Try<T>& getTry() {
+    return *value_;
+  }
+
+  template <typename F>
+  void setContinuation(F func) {
+    if (continuation_) {
+      throw std::logic_error("setContinuation called twice");
+    }
+
+    if (value_.hasValue()) {
+      func(std::move(*value_));
+      delete this;
+    } else {
+      continuation_ = std::move(func);
+    }
+  }
+
+  void fulfil(Try<T>&& t) {
+    if (value_.hasValue()) {
+      throw std::logic_error("fulfil called twice");
+    }
+
+    if (continuation_) {
+      continuation_(std::move(t));
+      delete this;
+    } else {
+      value_ = std::move(t);
+    }
+  }
+
+  void setException(std::exception_ptr const& e) {
+    fulfil(Try<T>(e));
+  }
+
+  template <class E> void setException(E const& e) {
+    fulfil(Try<T>(std::make_exception_ptr<E>(e)));
+  }
+
+  bool ready() const {
+    return value_.hasValue();
+  }
+
+  typename std::add_lvalue_reference<T>::type value() {
+    return value_->value();
+  }
+
+ private:
+  folly::Optional<Try<T>> value_;
+  std::function<void(Try<T>&&)> continuation_;
+};
+
+template <typename... Ts>
+struct VariadicContext {
+  VariadicContext() : total(0), count(0) {}
+  Promise<std::tuple<Try<Ts>... > > p;
+  std::tuple<Try<Ts>... > results;
+  size_t total;
+  std::atomic<size_t> count;
+  typedef Future<std::tuple<Try<Ts>...>> type;
+};
+
+template <typename... Ts, typename THead, typename... Fs>
+typename std::enable_if<sizeof...(Fs) == 0, void>::type
+whenAllVariadicHelper(VariadicContext<Ts...> *ctx, THead&& head, Fs&&... tail) {
+  head.setContinuation([ctx](Try<typename THead::value_type>&& t) {
+    std::get<sizeof...(Ts) - sizeof...(Fs) - 1>(ctx->results) = std::move(t);
+    if (++ctx->count == ctx->total) {
+      ctx->p.setValue(std::move(ctx->results));
+      delete ctx;
+    }
+  });
+}
+
+template <typename... Ts, typename THead, typename... Fs>
+typename std::enable_if<sizeof...(Fs) != 0, void>::type
+whenAllVariadicHelper(VariadicContext<Ts...> *ctx, THead&& head, Fs&&... tail) {
+  head.setContinuation([ctx](Try<typename THead::value_type>&& t) {
+    std::get<sizeof...(Ts) - sizeof...(Fs) - 1>(ctx->results) = std::move(t);
+    if (++ctx->count == ctx->total) {
+      ctx->p.setValue(std::move(ctx->results));
+      delete ctx;
+    }
+  });
+  // template tail-recursion
+  whenAllVariadicHelper(ctx, std::forward<Fs>(tail)...);
+}
+
+template <typename T>
+struct WhenAllContext {
+  explicit WhenAllContext() : count(0), total(0) {}
+  Promise<std::vector<Try<T> > > p;
+  std::vector<Try<T> > results;
+  std::atomic<size_t> count;
+  size_t total;
+};
+
+template <typename T>
+struct WhenAnyContext {
+  explicit WhenAnyContext(size_t n) : done(false), ref_count(n) {};
+  Promise<std::pair<size_t, Try<T>>> p;
+  std::atomic<bool> done;
+  std::atomic<size_t> ref_count;
+  void decref() {
+    if (--ref_count == 0) {
+      delete this;
+    }
+  }
+};
+
+}}} // namespace
--- /dev/null
+++ b/hphp/submodules/folly/folly/wangle/Executor.h
@@ -0,0 +1,28 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <boost/noncopyable.hpp>
+#include <functional>
+
+namespace folly { namespace wangle {
+  class Executor : boost::noncopyable {
+   public:
+     virtual ~Executor() = default;
+     virtual void add(std::function<void()>&&) = 0;
+  };
+}}
--- /dev/null
+++ b/hphp/submodules/folly/folly/wangle/Future.h
@@ -0,0 +1,296 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <algorithm>
+#include <exception>
+#include <functional>
+#include <memory>
+#include <type_traits>
+
+#include "folly/MoveWrapper.h"
+#include "Promise.h"
+#include "Try.h"
+
+namespace folly { namespace wangle {
+
+template <typename T> struct isFuture;
+
+template <class T>
+class Future {
+ public:
+  typedef T value_type;
+
+  // not copyable
+  Future(Future const&) = delete;
+  Future& operator=(Future const&) = delete;
+
+  // movable
+  Future(Future&&);
+  Future& operator=(Future&&);
+
+  ~Future();
+
+  /** Return the reference to result. Should not be called if !isReady().
+    Will rethrow the exception if an exception has been
+    captured.
+
+    This function is not thread safe - the returned Future can only
+    be executed from the thread that the executor runs it in.
+    See below for a thread safe version
+    */
+  typename std::add_lvalue_reference<T>::type
+  value();
+  typename std::add_lvalue_reference<const T>::type
+  value() const;
+
+  template <typename Executor>
+  Future<T> executeWithSameThread(Executor* executor);
+
+  /**
+     Thread-safe version of executeWith
+
+     Since an executor would likely start executing the Future chain
+     right away, it would be a race condition to call:
+     Future.executeWith(...).then(...), as there would be race
+     condition between the then and the running Future.
+     Instead, you may pass in a Promise so that we can set up
+     the rest of the chain in advance, without any racey
+     modifications of the continuation
+   */
+  template <typename Executor>
+  void executeWith(Executor* executor, Promise<T>&& cont_promise);
+
+  /** True when the result (or exception) is ready. */
+  bool isReady() const;
+
+  /** A reference to the Try of the value */
+  Try<T>& getTry();
+
+  /** When this Future has completed, execute func which is a function that
+    takes a Try<T>&&. A Future for the return type of func is
+    returned. e.g.
+
+    Future<string> f2 = f1.then([](Try<T>&&) { return string("foo"); });
+
+    The Future given to the functor is ready, and the functor may call
+    value(), which may rethrow if this has captured an exception. If func
+    throws, the exception will be captured in the Future that is returned.
+    */
+  /* TODO n3428 and other async frameworks have something like then(scheduler,
+     Future), we probably want to support a similar API (instead of
+     executeWith). */
+  template <class F>
+  typename std::enable_if<
+    !isFuture<typename std::result_of<F(Try<T>&&)>::type>::value,
+    Future<typename std::result_of<F(Try<T>&&)>::type> >::type
+  then(F&& func);
+
+  /// Variant where func returns a Future<T> instead of a T. e.g.
+  ///
+  ///   Future<string> f2 = f1.then(
+  ///     [](Try<T>&&) { return makeFuture<string>("foo"); });
+  template <class F>
+  typename std::enable_if<
+    isFuture<typename std::result_of<F(Try<T>&&)>::type>::value,
+    Future<typename std::result_of<F(Try<T>&&)>::type::value_type> >::type
+  then(F&& func);
+
+  /// Variant where func is an ordinary function (static method, method)
+  ///
+  ///   R doWork(Try<T>&&);
+  ///
+  ///   Future<R> f2 = f1.then(doWork);
+  ///
+  /// or
+  ///
+  ///   struct Worker {
+  ///     static R doWork(Try<T>&&); }
+  ///
+  ///   Future<R> f2 = f1.then(&Worker::doWork);
+  template <class = T, class R = std::nullptr_t>
+  typename std::enable_if<!isFuture<R>::value, Future<R>>::type
+  inline then(R(*func)(Try<T>&&)) {
+    return then([func](Try<T>&& t) {
+      return (*func)(std::move(t));
+    });
+  }
+
+  /// Variant where func returns a Future<R> instead of a R. e.g.
+  ///
+  ///   struct Worker {
+  ///     Future<R> doWork(Try<T>&&); }
+  ///
+  ///   Future<R> f2 = f1.then(&Worker::doWork);
+  template <class = T, class R = std::nullptr_t>
+  typename std::enable_if<isFuture<R>::value, R>::type
+  inline then(R(*func)(Try<T>&&)) {
+    return then([func](Try<T>&& t) {
+      return (*func)(std::move(t));
+    });
+  }
+
+  /// Variant where func is an member function
+  ///
+  ///   struct Worker {
+  ///     R doWork(Try<T>&&); }
+  ///
+  ///   Worker *w;
+  ///   Future<R> f2 = f1.then(w, &Worker::doWork);
+  template <class = T, class R = std::nullptr_t, class Caller = std::nullptr_t>
+  typename std::enable_if<!isFuture<R>::value, Future<R>>::type
+  inline then(Caller *instance, R(Caller::*func)(Try<T>&&)) {
+    return then([instance, func](Try<T>&& t) {
+      return (instance->*func)(std::move(t));
+    });
+  }
+
+  /// Variant where func returns a Future<R> instead of a R. e.g.
+  ///
+  ///   struct Worker {
+  ///     Future<R> doWork(Try<T>&&); }
+  ///
+  ///   Worker *w;
+  ///   Future<R> f2 = f1.then(w, &Worker::doWork);
+  template <class = T, class R = std::nullptr_t, class Caller = std::nullptr_t>
+  typename std::enable_if<isFuture<R>::value, R>::type
+  inline then(Caller *instance, R(Caller::*func)(Try<T>&&)) {
+    return then([instance, func](Try<T>&& t) {
+      return (instance->*func)(std::move(t));
+    });
+  }
+
+  /// Convenience method for ignoring the value and creating a Future<void>.
+  /// Exceptions still propagate.
+  Future<void> then();
+
+  /// Use of this method is advanced wizardry.
+  /// XXX should this be protected?
+  template <class F>
+  void setContinuation(F&& func);
+
+ private:
+  typedef detail::FutureObject<T>* objPtr;
+
+  // shared state object
+  objPtr obj_;
+
+  explicit
+  Future(objPtr obj) : obj_(obj) {}
+
+  void throwIfInvalid() const;
+
+  friend class Promise<T>;
+};
+
+/**
+  Make a completed Future by moving in a value. e.g.
+
+    string foo = "foo";
+    auto f = makeFuture(std::move(foo));
+
+  or
+
+    auto f = makeFuture<string>("foo");
+*/
+template <class T>
+Future<typename std::decay<T>::type> makeFuture(T&& t);
+
+/** Make a completed void Future. */
+Future<void> makeFuture();
+
+/** Make a completed Future by executing a function. If the function throws
+  we capture the exception, otherwise we capture the result. */
+template <class F>
+auto makeFutureTry(
+  F&& func,
+  typename std::enable_if<
+    !std::is_reference<F>::value, bool>::type sdf = false)
+  -> Future<decltype(func())>;
+
+template <class F>
+auto makeFutureTry(
+  F const& func)
+  -> Future<decltype(func())>;
+
+/// Make a failed Future from an exception_ptr.
+/// Because the Future's type cannot be inferred you have to specify it, e.g.
+///
+///   auto f = makeFuture<string>(std::current_exception());
+template <class T>
+Future<T> makeFuture(std::exception_ptr const& e);
+
+/** Make a Future from an exception type E that can be passed to
+  std::make_exception_ptr(). */
+template <class T, class E>
+typename std::enable_if<std::is_base_of<std::exception, E>::value, Future<T>>::type
+makeFuture(E const& e);
+
+/** When all the input Futures complete, the returned Future will complete.
+  Errors do not cause early termination; this Future will always succeed
+  after all its Futures have finished (whether successfully or with an
+  error).
+
+  The Futures are moved in, so your copies are invalid. If you need to
+  chain further from these Futures, use the variant with an output iterator.
+
+  XXX is this still true?
+  This function is thread-safe for Futures running on different threads.
+
+  The return type for Future<T> input is a Future<std::vector<Try<T>>>
+  */
+template <class InputIterator>
+Future<std::vector<Try<
+  typename std::iterator_traits<InputIterator>::value_type::value_type>>>
+whenAll(InputIterator first, InputIterator last);
+
+/// This version takes a varying number of Futures instead of an iterator.
+/// The return type for (Future<T1>, Future<T2>, ...) input
+/// is a Future<std::tuple<Try<T1>, Try<T2>, ...>>.
+/// The Futures are moved in, so your copies are invalid.
+template <typename... Fs>
+typename detail::VariadicContext<
+  typename std::decay<Fs>::type::value_type...>::type
+whenAll(Fs&&... fs);
+
+/** The result is a pair of the index of the first Future to complete and
+  the Try. If multiple Futures complete at the same time (or are already
+  complete when passed in), the "winner" is chosen non-deterministically.
+
+  This function is thread-safe for Futures running on different threads.
+  */
+template <class InputIterator>
+Future<std::pair<
+  size_t,
+  Try<typename std::iterator_traits<InputIterator>::value_type::value_type>>>
+whenAny(InputIterator first, InputIterator last);
+
+/** when n Futures have completed, the Future completes with a vector of
+  the index and Try of those n Futures (the indices refer to the original
+  order, but the result vector will be in an arbitrary order)
+
+  Not thread safe.
+  */
+template <class InputIterator>
+Future<std::vector<std::pair<
+  size_t,
+  Try<typename std::iterator_traits<InputIterator>::value_type::value_type>>>>
+whenN(InputIterator first, InputIterator last, size_t n);
+
+}} // folly::wangle
+
+#include "Future-inl.h"
--- /dev/null
+++ b/hphp/submodules/folly/folly/wangle/Future-inl.h
@@ -0,0 +1,410 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "detail.h"
+
+namespace folly { namespace wangle {
+
+template <typename T>
+struct isFuture {
+  static const bool value = false;
+};
+
+template <typename T>
+struct isFuture<Future<T> > {
+  static const bool value = true;
+};
+
+template <class T>
+Future<T>::Future(Future<T>&& other) : obj_(other.obj_) {
+  other.obj_ = nullptr;
+}
+
+template <class T>
+Future<T>& Future<T>::operator=(Future<T>&& other) {
+  std::swap(obj_, other.obj_);
+  return *this;
+}
+
+template <class T>
+Future<T>::~Future() {
+  if (obj_) {
+    if (obj_->ready()) {
+      delete obj_;
+    } else {
+      setContinuation([](Try<T>&&) {}); // detach
+    }
+  }
+}
+
+template <class T>
+void Future<T>::throwIfInvalid() const {
+  if (!obj_)
+    throw NoState();
+}
+
+template <class T>
+template <class F>
+void Future<T>::setContinuation(F&& func) {
+  throwIfInvalid();
+  obj_->setContinuation(std::move(func));
+  obj_ = nullptr;
+}
+
+template <class T>
+template <class F>
+typename std::enable_if<
+  !isFuture<typename std::result_of<F(Try<T>&&)>::type>::value,
+  Future<typename std::result_of<F(Try<T>&&)>::type> >::type
+Future<T>::then(F&& func) {
+  typedef typename std::result_of<F(Try<T>&&)>::type B;
+
+  throwIfInvalid();
+
+  // wrap these so we can move them into the lambda
+  folly::MoveWrapper<Promise<B>> p;
+  folly::MoveWrapper<F> funcm(std::forward<F>(func));
+
+  // grab the Future now before we lose our handle on the Promise
+  auto f = p->getFuture();
+
+  /* This is a bit tricky.
+
+     We can't just close over *this in case this Future gets moved. So we
+     make a new dummy Future. We could figure out something more
+     sophisticated that avoids making a new Future object when it can, as an
+     optimization. But this is correct.
+
+     obj_ can't be moved, it is explicitly disallowed (as is copying). But
+     if there's ever a reason to allow it, this is one place that makes that
+     assumption and would need to be fixed. We use a standard shared pointer
+     for obj_ (by copying it in), which means in essence obj holds a shared
+     pointer to itself.  But this shouldn't leak because Promise will not
+     outlive the continuation, because Promise will setException() with a
+     broken Promise if it is destructed before completed. We could use a
+     weak pointer but it would have to be converted to a shared pointer when
+     func is executed (because the Future returned by func may possibly
+     persist beyond the callback, if it gets moved), and so it is an
+     optimization to just make it shared from the get-go.
+
+     We have to move in the Promise and func using the MoveWrapper
+     hack. (func could be copied but it's a big drag on perf).
+
+     Two subtle but important points about this design. FutureObject has no
+     back pointers to Future or Promise, so if Future or Promise get moved
+     (and they will be moved in performant code) we don't have to do
+     anything fancy. And because we store the continuation in the
+     FutureObject, not in the Future, we can execute the continuation even
+     after the Future has gone out of scope. This is an intentional design
+     decision. It is likely we will want to be able to cancel a continuation
+     in some circumstances, but I think it should be explicit not implicit
+     in the destruction of the Future used to create it.
+     */
+  setContinuation(
+    [p, funcm](Try<T>&& t) mutable {
+      p->fulfil([&]() {
+          return (*funcm)(std::move(t));
+        });
+    });
+
+  return std::move(f);
+}
+
+template <class T>
+template <class F>
+typename std::enable_if<
+  isFuture<typename std::result_of<F(Try<T>&&)>::type>::value,
+  Future<typename std::result_of<F(Try<T>&&)>::type::value_type> >::type
+Future<T>::then(F&& func) {
+  typedef typename std::result_of<F(Try<T>&&)>::type::value_type B;
+
+  throwIfInvalid();
+
+  // wrap these so we can move them into the lambda
+  folly::MoveWrapper<Promise<B>> p;
+  folly::MoveWrapper<F> funcm(std::forward<F>(func));
+
+  // grab the Future now before we lose our handle on the Promise
+  auto f = p->getFuture();
+
+  setContinuation(
+    [p, funcm](Try<T>&& t) mutable {
+      try {
+        auto f2 = (*funcm)(std::move(t));
+        // that didn't throw, now we can steal p
+        f2.setContinuation([p](Try<B>&& b) mutable {
+            p->fulfilTry(std::move(b));
+          });
+      } catch (...) {
+        p->setException(std::current_exception());
+      }
+    });
+
+  return std::move(f);
+}
+
+template <class T>
+Future<void> Future<T>::then() {
+  return then([] (Try<T>&& t) {});
+}
+
+template <class T>
+typename std::add_lvalue_reference<T>::type Future<T>::value() {
+  throwIfInvalid();
+
+  return obj_->value();
+}
+
+template <class T>
+typename std::add_lvalue_reference<const T>::type Future<T>::value() const {
+  throwIfInvalid();
+
+  return obj_->value();
+}
+
+template <class T>
+Try<T>& Future<T>::getTry() {
+  throwIfInvalid();
+
+  return obj_->getTry();
+}
+
+template <class T>
+template <typename Executor>
+inline Future<T> Future<T>::executeWithSameThread(Executor* executor) {
+  throwIfInvalid();
+
+  folly::MoveWrapper<Promise<T>> p;
+  auto f = p->getFuture();
+
+  setContinuation([executor, p](Try<T>&& t) mutable {
+      folly::MoveWrapper<Try<T>> tt(std::move(t));
+      executor->add([p, tt]() mutable {
+          p->fulfilTry(std::move(*tt));
+        });
+    });
+
+  return f;
+}
+
+template <class T>
+template <typename Executor>
+inline void Future<T>::executeWith(
+    Executor* executor, Promise<T>&& cont_promise) {
+  throwIfInvalid();
+
+  folly::MoveWrapper<Promise<T>> p(std::move(cont_promise));
+
+  setContinuation([executor, p](Try<T>&& t) mutable {
+      folly::MoveWrapper<Try<T>> tt(std::move(t));
+      executor->add([p, tt]() mutable {
+          p->fulfilTry(std::move(*tt));
+        });
+    });
+}
+
+template <class T>
+bool Future<T>::isReady() const {
+  throwIfInvalid();
+  return obj_->ready();
+}
+
+// makeFuture
+
+template <class T>
+Future<typename std::decay<T>::type> makeFuture(T&& t) {
+  Promise<typename std::decay<T>::type> p;
+  auto f = p.getFuture();
+  p.setValue(std::forward<T>(t));
+  return std::move(f);
+}
+
+inline // for multiple translation units
+Future<void> makeFuture() {
+  Promise<void> p;
+  auto f = p.getFuture();
+  p.setValue();
+  return std::move(f);
+}
+
+template <class F>
+auto makeFutureTry(
+    F&& func,
+    typename std::enable_if<!std::is_reference<F>::value, bool>::type sdf)
+    -> Future<decltype(func())> {
+  Promise<decltype(func())> p;
+  auto f = p.getFuture();
+  p.fulfil(
+    [&func]() {
+      return (func)();
+    });
+  return std::move(f);
+}
+
+template <class F>
+auto makeFutureTry(F const& func) -> Future<decltype(func())> {
+  F copy = func;
+  return makeFutureTry(std::move(copy));
+}
+
+template <class T>
+Future<T> makeFuture(std::exception_ptr const& e) {
+  Promise<T> p;
+  auto f = p.getFuture();
+  p.setException(e);
+  return std::move(f);
+}
+
+template <class T, class E>
+typename std::enable_if<std::is_base_of<std::exception, E>::value, Future<T>>::type
+makeFuture(E const& e) {
+  Promise<T> p;
+  auto f = p.getFuture();
+  p.fulfil([&]() -> T { throw e; });
+  return std::move(f);
+}
+
+// when (variadic)
+
+template <typename... Fs>
+typename detail::VariadicContext<
+  typename std::decay<Fs>::type::value_type...>::type
+whenAll(Fs&&... fs)
+{
+  auto ctx =
+    new detail::VariadicContext<typename std::decay<Fs>::type::value_type...>();
+  ctx->total = sizeof...(fs);
+  auto f_saved = ctx->p.getFuture();
+  detail::whenAllVariadicHelper(ctx,
+    std::forward<typename std::decay<Fs>::type>(fs)...);
+  return std::move(f_saved);
+}
+
+// when (iterator)
+
+template <class InputIterator>
+Future<
+  std::vector<
+  Try<typename std::iterator_traits<InputIterator>::value_type::value_type>>>
+whenAll(InputIterator first, InputIterator last)
+{
+  typedef
+    typename std::iterator_traits<InputIterator>::value_type::value_type T;
+
+  auto n = std::distance(first, last);
+  if (n == 0)
+    return makeFuture<std::vector<Try<T>>>({});
+
+  auto ctx = new detail::WhenAllContext<T>();
+
+  ctx->total = n;
+  ctx->results.resize(ctx->total);
+
+  auto f_saved = ctx->p.getFuture();
+
+  for (size_t i = 0; first != last; ++first, ++i) {
+     auto& f = *first;
+     f.setContinuation([ctx, i](Try<T>&& t) {
+         ctx->results[i] = std::move(t);
+         if (++ctx->count == ctx->total) {
+           ctx->p.setValue(std::move(ctx->results));
+           delete ctx;
+         }
+       });
+  }
+
+  return std::move(f_saved);
+}
+
+template <class InputIterator>
+Future<
+  std::pair<size_t,
+            Try<
+              typename
+              std::iterator_traits<InputIterator>::value_type::value_type> > >
+whenAny(InputIterator first, InputIterator last) {
+  typedef
+    typename std::iterator_traits<InputIterator>::value_type::value_type T;
+
+  auto ctx = new detail::WhenAnyContext<T>(std::distance(first, last));
+  auto f_saved = ctx->p.getFuture();
+
+  for (size_t i = 0; first != last; first++, i++) {
+    auto& f = *first;
+    f.setContinuation([i, ctx](Try<T>&& t) {
+      if (!ctx->done.exchange(true)) {
+        ctx->p.setValue(std::make_pair(i, std::move(t)));
+      }
+      ctx->decref();
+    });
+  }
+
+  return std::move(f_saved);
+}
+
+template <class InputIterator>
+Future<std::vector<std::pair<size_t, Try<typename
+  std::iterator_traits<InputIterator>::value_type::value_type>>>>
+whenN(InputIterator first, InputIterator last, size_t n) {
+  typedef typename
+    std::iterator_traits<InputIterator>::value_type::value_type T;
+  typedef std::vector<std::pair<size_t, Try<T>>> V;
+
+  struct ctx_t {
+    V v;
+    size_t completed;
+    Promise<V> p;
+  };
+  auto ctx = std::make_shared<ctx_t>();
+  ctx->completed = 0;
+
+  // for each completed Future, increase count and add to vector, until we
+  // have n completed futures at which point we fulfil our Promise with the
+  // vector
+  auto it = first;
+  size_t i = 0;
+  while (it != last) {
+    it->then([ctx, n, i](Try<T>&& t) {
+      auto& v = ctx->v;
+      auto c = ++ctx->completed;
+      if (c <= n) {
+        assert(ctx->v.size() < n);
+        v.push_back(std::make_pair(i, std::move(t)));
+        if (c == n) {
+          ctx->p.fulfilTry(Try<V>(std::move(v)));
+        }
+      }
+    });
+
+    it++;
+    i++;
+  }
+
+  if (i < n) {
+    ctx->p.setException(std::runtime_error("Not enough futures"));
+  }
+
+  return ctx->p.getFuture();
+}
+
+}}
+
+// I haven't included a Future<T&> specialization because I don't forsee us
+// using it, however it is not difficult to add when needed. Refer to
+// Future<void> for guidance. std::future and boost::future code would also be
+// instructive.
--- /dev/null
+++ b/hphp/submodules/folly/folly/wangle/GenericThreadGate.h
@@ -0,0 +1,71 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+#include "ThreadGate.h"
+#include "Executor.h"
+#include <type_traits>
+
+namespace folly { namespace wangle {
+
+/// This generic threadgate takes two executors and an optional waiter (if you
+/// need to support waiting). Hint: use executors that inherit from Executor
+/// (in Executor.h), then you just do
+///
+///   GenericThreadGate tg(westExecutor, eastExecutor, waiter);
+template <
+  class WestExecutorPtr = Executor*,
+  class EastExecutorPtr = Executor*,
+  class WaiterPtr = void*>
+class GenericThreadGate : public ThreadGate {
+public:
+  /**
+    EastExecutor and WestExecutor respond threadsafely to
+    `add(std::function<void()>&&)`
+
+    Waiter responds to `makeProgress()`. It may block, as long as progress
+    will be made on the west front.
+    */
+  GenericThreadGate(WestExecutorPtr west,
+                    EastExecutorPtr east,
+                    WaiterPtr waiter = nullptr) :
+    westExecutor(west),
+    eastExecutor(east),
+    waiter(waiter)
+  {}
+
+  void addWest(std::function<void()>&& fn) { westExecutor->add(std::move(fn)); }
+  void addEast(std::function<void()>&& fn) { eastExecutor->add(std::move(fn)); }
+
+  virtual void makeProgress() {
+    makeProgress_(std::is_same<WaiterPtr, void*>());
+  }
+
+  WestExecutorPtr westExecutor;
+  EastExecutorPtr eastExecutor;
+  WaiterPtr waiter;
+
+private:
+  void makeProgress_(std::true_type const&) {
+    throw std::logic_error("No waiter.");
+  }
+
+  void makeProgress_(std::false_type const&) {
+    waiter->makeProgress();
+  }
+};
+
+}} // executor
--- /dev/null
+++ b/hphp/submodules/folly/folly/wangle/InlineExecutor.cpp
@@ -0,0 +1,16 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
--- /dev/null
+++ b/hphp/submodules/folly/folly/wangle/InlineExecutor.h
@@ -0,0 +1,29 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+#include "folly/wangle/Executor.h"
+
+namespace folly { namespace wangle {
+
+  class InlineExecutor : public Executor {
+   public:
+    void add(std::function<void()>&& f) override {
+      f();
+    }
+  };
+
+}}
--- /dev/null
+++ b/hphp/submodules/folly/folly/wangle/Later.h
@@ -0,0 +1,179 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "folly/wangle/Executor.h"
+#include "folly/wangle/Future.h"
+#include "folly/Optional.h"
+
+namespace folly { namespace wangle {
+
+template <typename T> struct isLaterOrFuture;
+template <typename T> struct isLater;
+
+/*
+ * Since wangle primitives (promise/future) are not thread safe, it is difficult
+ * to build complex asynchronous workflows. A Later allows you to build such a
+ * workflow before actually launching it so that continuations can be set in a
+ * threadsafe manner.
+ *
+ * The interface to add additional work is the same as future: a then() method
+ * that takes a function that can return either a type T, a Future<T>, or a
+ * Later<T>
+ *
+ * Thread transitions are done by using executors and calling the via() method.
+ *
+ * Here is an example of a workflow:
+ *
+ * Later<ClientRequest> later(std::move(request));
+ *
+ * auto future = later.
+ *   .via(cpuExecutor)
+ *   .then([=](Try<ClientRequest>&& t) { return doCpuWork(t.value()); })
+ *   .via(diskExecutor)
+ *   .then([=](Try<CpuResponse>&& t) { return doDiskWork(t.value()); })
+ *   .via(serverExecutor)
+ *   .then([=]Try<DiskResponse>&& t) { return sendClientResponse(t.value()); })
+ *   .launch();
+ *
+ * Although this workflow traverses many threads, we are able to string
+ * continuations together in a threadsafe manner.
+ *
+ * Laters can also be used to wrap preexisting asynchronous modules that were
+ * not built with wangle in mind. You can create a Later with a function that
+ * takes a callback as input. The function will not actually be called until
+ * launch(), allowing you to string then() statements on top of the callback.
+ */
+template <class T>
+class Later {
+ public:
+  typedef T value_type;
+
+  /*
+   * This default constructor is used to build an asynchronous workflow that
+   * takes no input.
+   */
+  template <class U = void,
+            class = typename std::enable_if<std::is_void<U>::value>::type,
+            class = typename std::enable_if<std::is_same<T, U>::value>::type>
+  Later();
+
+  /*
+   * This constructor is used to build an asynchronous workflow that takes a
+   * value as input, and that value is passed in.
+   */
+  template <class U,
+            class = typename std::enable_if<!std::is_void<U>::value>::type,
+            class = typename std::enable_if<std::is_same<T, U>::value>::type>
+  explicit Later(U&& input);
+
+  /*
+   * This constructor is used to wrap a pre-existing cob-style asynchronous api
+   * so that it can be used in wangle in a threadsafe manner. wangle provides
+   * the callback to this pre-existing api, and this callback will fulfill a
+   * promise so as to incorporate this api into the workflow.
+   *
+   * Example usage:
+   *
+   * // This adds two ints asynchronously. cob is called in another thread.
+   * void addAsync(int a, int b, std::function<void(int&&)>&& cob);
+   *
+   * Later<int> asyncWrapper([=](std::function<void(int&&)>&& fn) {
+   *   addAsync(1, 2, std::move(fn));
+   * });
+   */
+  template <class U,
+            class = typename std::enable_if<!std::is_void<U>::value>::type,
+            class = typename std::enable_if<std::is_same<T, U>::value>::type>
+  explicit Later(std::function<void(std::function<void(U&&)>&&)>&& fn);
+
+  /*
+   * then() adds additional work to the end of the workflow. If the lambda
+   * provided to then() returns a future, that future must be fulfilled in the
+   * same thread of the last set executor (either at constructor or from a call
+   * to via()).
+   */
+  template <class F>
+  typename std::enable_if<
+    !isLaterOrFuture<typename std::result_of<F(Try<T>&&)>::type>::value,
+    Later<typename std::result_of<F(Try<T>&&)>::type> >::type
+  then(F&& fn);
+
+  template <class F>
+  typename std::enable_if<
+    isFuture<typename std::result_of<F(Try<T>&&)>::type>::value,
+    Later<typename std::result_of<F(Try<T>&&)>::type::value_type> >::type
+  then(F&& fn);
+
+  /*
+   * If the function passed to then() returns a Later<T>, calls to then() will
+   * be chained to the new Later before launching the new Later.
+   *
+   * This can be used to build asynchronous modules that can be called from a
+   * user thread and completed in a callback thread. Callbacks can be set up
+   * ahead of time without thread safety issues.
+   *
+   * Using the Later(std::function<void(std::function<void(T&&)>)>&& fn)
+   * constructor, you can wrap existing asynchronous modules with a Later and
+   * can chain it to wangle asynchronous workflows via this call.
+   */
+  template <class F>
+  typename std::enable_if<
+    isLater<typename std::result_of<F(Try<T>&&)>::type>::value,
+    Later<typename std::result_of<F(Try<T>&&)>::type::value_type> >::type
+  then(F&& fn);
+
+  /*
+   * Resets the executor - all then() calls made after the call to via() will be
+   * made in the new executor. The Executor must outlive.
+   */
+  Later<T> via(Executor* executor);
+
+  /*
+   * Starts the workflow. The function provided in the constructor will be
+   * called in the executor provided in the constructor. Subsequent then()
+   * calls will be made, potentially changing threads if a via() call is made.
+   * The future returned will be fulfilled in the last executor.
+   *
+   * Thread safety issues of Futures still apply. If you want to wait on the
+   * Future, it must be done in the thread that will fulfil it. If you do not
+   * plan to use the result of the Future, use fireAndForget()
+   */
+  Future<T> launch();
+
+  /*
+   * Same as launch, only no Future is returned. This guarantees thread safe
+   * cleanup of the internal Futures, even if the Later completes in a different
+   * thread than the thread that calls fireAndForget().
+   */
+  void fireAndForget();
+
+ private:
+  Promise<void> starter_;
+  folly::Optional<Future<T>> future_;
+
+  struct hide { };
+
+  explicit Later(Promise<void>&& starter);
+
+  template <class U>
+  friend class Later;
+};
+
+}}
+
+#include "Later-inl.h"
--- /dev/null
+++ b/hphp/submodules/folly/folly/wangle/Later-inl.h
@@ -0,0 +1,152 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "folly/wangle/Executor.h"
+#include "folly/wangle/Future.h"
+#include "folly/Optional.h"
+
+namespace folly { namespace wangle {
+
+template <typename T>
+struct isLater {
+  static const bool value = false;
+};
+
+template <typename T>
+struct isLater<Later<T> > {
+  static const bool value = true;
+};
+
+template <typename T>
+struct isLaterOrFuture {
+  static const bool value = false;
+};
+
+template <typename T>
+struct isLaterOrFuture<Later<T>> {
+  static const bool value = true;
+};
+
+template <typename T>
+struct isLaterOrFuture<Future<T>> {
+  static const bool value = true;
+};
+
+template <typename T>
+template <class U, class Unused, class Unused2>
+Later<T>::Later() {
+  future_ = starter_.getFuture();
+}
+
+template <typename T>
+Later<T>::Later(Promise<void>&& starter)
+  : starter_(std::forward<Promise<void>>(starter)) { }
+
+template <class T>
+template <class U, class Unused, class Unused2>
+Later<T>::Later(U&& input) {
+  folly::MoveWrapper<Promise<U>> promise;
+  folly::MoveWrapper<U> inputm(std::forward<U>(input));
+  future_ = promise->getFuture();
+  starter_.getFuture().then([=](Try<void>&& t) mutable {
+    promise->setValue(std::move(*inputm));
+  });
+}
+
+template <class T>
+template <class U, class Unused, class Unused2>
+Later<T>::Later(std::function<void(std::function<void(U&&)>&&)>&& fn) {
+  folly::MoveWrapper<Promise<U>> promise;
+  future_ = promise->getFuture();
+  starter_.getFuture().then([=](Try<void>&& t) mutable {
+    fn([=](U&& output) mutable {
+      promise->setValue(std::move(output));
+    });
+  });
+}
+
+template <class T>
+template <class F>
+typename std::enable_if<
+  !isLaterOrFuture<typename std::result_of<F(Try<T>&&)>::type>::value,
+  Later<typename std::result_of<F(Try<T>&&)>::type> >::type
+Later<T>::then(F&& fn) {
+  typedef typename std::result_of<F(Try<T>&&)>::type B;
+
+  Later<B> later(std::move(starter_));
+  later.future_ = future_->then(std::forward<F>(fn));
+  return later;
+}
+
+template <class T>
+template <class F>
+typename std::enable_if<
+  isFuture<typename std::result_of<F(Try<T>&&)>::type>::value,
+  Later<typename std::result_of<F(Try<T>&&)>::type::value_type> >::type
+Later<T>::then(F&& fn) {
+  typedef typename std::result_of<F(Try<T>&&)>::type::value_type B;
+
+  Later<B> later(std::move(starter_));
+  later.future_ = future_->then(std::move(fn));
+  return later;
+}
+
+template <class T>
+template <class F>
+typename std::enable_if<
+  isLater<typename std::result_of<F(Try<T>&&)>::type>::value,
+  Later<typename std::result_of<F(Try<T>&&)>::type::value_type> >::type
+Later<T>::then(F&& fn) {
+  typedef typename std::result_of<F(Try<T>&&)>::type::value_type B;
+
+  folly::MoveWrapper<Promise<B>> promise;
+  folly::MoveWrapper<F> fnm(std::move(fn));
+  Later<B> later(std::move(starter_));
+  later.future_ = promise->getFuture();
+  future_->then([=](Try<T>&& t) mutable {
+    (*fnm)(std::move(t))
+    .then([=](Try<B>&& t2) mutable {
+      promise->fulfilTry(std::move(t2));
+    })
+    .launch();
+  });
+  return later;
+}
+
+template <class T>
+Later<T> Later<T>::via(Executor* executor) {
+  Promise<T> promise;
+  Later<T> later(std::move(starter_));
+  later.future_ = promise.getFuture();
+  future_->executeWith(executor, std::move(promise));
+  return later;
+}
+
+template <class T>
+Future<T> Later<T>::launch() {
+  starter_.setValue();
+  return std::move(*future_);
+}
+
+template <class T>
+void Later<T>::fireAndForget() {
+  future_->setContinuation([] (Try<T>&& t) {}); // detach
+  starter_.setValue();
+}
+
+}}
--- /dev/null
+++ b/hphp/submodules/folly/folly/wangle/ManualExecutor.cpp
@@ -0,0 +1,86 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "ManualExecutor.h"
+
+#include <string.h>
+
+#include <stdexcept>
+
+namespace folly { namespace wangle {
+
+ManualExecutor::ManualExecutor() {
+  if (sem_init(&sem_, 0, 0) == -1) {
+    throw std::runtime_error(std::string("sem_init: ") + strerror(errno));
+  }
+}
+
+void ManualExecutor::add(std::function<void()>&& callback) {
+  std::lock_guard<std::mutex> lock(lock_);
+  runnables_.push(callback);
+  sem_post(&sem_);
+}
+
+size_t ManualExecutor::run() {
+  size_t count;
+  size_t n;
+  std::function<void()> runnable;
+
+  {
+    std::lock_guard<std::mutex> lock(lock_);
+    n = runnables_.size();
+  }
+
+  for (count = 0; count < n; count++) {
+    {
+      std::lock_guard<std::mutex> lock(lock_);
+      if (runnables_.empty()) {
+        break;
+      }
+
+      // Balance the semaphore so it doesn't grow without bound
+      // if nobody is calling wait().
+      // This may fail (with EAGAIN), that's fine.
+      sem_trywait(&sem_);
+
+      runnable = std::move(runnables_.front());
+      runnables_.pop();
+    }
+    runnable();
+  }
+
+  return count;
+}
+
+void ManualExecutor::wait() {
+  while (true) {
+    {
+      std::lock_guard<std::mutex> lock(lock_);
+      if (!runnables_.empty())
+        break;
+    }
+
+    auto ret = sem_wait(&sem_);
+    if (ret == 0) {
+      break;
+    }
+    if (errno != EINVAL) {
+      throw std::runtime_error(std::string("sem_wait: ") + strerror(errno));
+    }
+  }
+}
+
+}} // namespace
--- /dev/null
+++ b/hphp/submodules/folly/folly/wangle/ManualExecutor.h
@@ -0,0 +1,51 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+#include "folly/wangle/Executor.h"
+#include <semaphore.h>
+#include <memory>
+#include <mutex>
+#include <queue>
+
+namespace folly { namespace wangle {
+
+  class ManualExecutor : public Executor {
+   public:
+    ManualExecutor();
+
+    void add(std::function<void()>&&) override;
+
+    /// Do work. Returns the number of runnables that were executed (maybe 0).
+    /// Non-blocking.
+    size_t run();
+
+    /// Wait for work to do.
+    void wait();
+
+    /// Wait for work to do, and do it.
+    void makeProgress() {
+      wait();
+      run();
+    }
+
+   private:
+    std::mutex lock_;
+    std::queue<std::function<void()>> runnables_;
+    sem_t sem_;
+  };
+
+}}
--- /dev/null
+++ b/hphp/submodules/folly/folly/wangle/Promise.h
@@ -0,0 +1,105 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "Try.h"
+#include "Future.h"
+
+namespace folly { namespace wangle {
+
+// forward declaration
+template <class T> class Future;
+
+template <class T>
+class Promise {
+public:
+  Promise();
+  ~Promise();
+
+  // not copyable
+  Promise(Promise const&) = delete;
+  Promise& operator=(Promise const&) = delete;
+
+  // movable
+  Promise(Promise<T>&&);
+  Promise& operator=(Promise<T>&&);
+
+  /** Return a Future tied to the shared state. This can be called only
+    once, thereafter Future already retrieved exception will be raised. */
+  Future<T> getFuture();
+
+  /** Fulfil the Promise with an exception_ptr, e.g.
+    try {
+      ...
+    } catch (...) {
+      p.setException(std::current_exception());
+    }
+    */
+  void setException(std::exception_ptr const&);
+
+  /** Fulfil the Promise with an exception type E, which can be passed to
+    std::make_exception_ptr(). Useful for originating exceptions. If you
+    caught an exception the exception_ptr form is more appropriate.
+    */
+  template <class E> void setException(E const&);
+
+  /** Fulfil this Promise (only for Promise<void>) */
+  void setValue();
+
+  /** Set the value (use perfect forwarding for both move and copy) */
+  template <class M>
+  void setValue(M&& value);
+
+  void fulfilTry(Try<T>&& t);
+
+  /** Fulfil this Promise with the result of a function that takes no
+    arguments and returns something implicitly convertible to T.
+    Captures exceptions. e.g.
+
+    p.fulfil([] { do something that may throw; return a T; });
+  */
+  template <class F>
+  void fulfil(const F& func);
+
+private:
+  typedef typename Future<T>::objPtr objPtr;
+
+  // Whether the Future has been retrieved (a one-time operation).
+  bool retrieved_;
+
+  // shared state object
+  objPtr obj_;
+
+  void throwIfFulfilled();
+  void throwIfRetrieved();
+
+  template <class F>
+  typename std::enable_if<
+    std::is_convertible<typename std::result_of<F()>::type, T>::value &&
+    !std::is_same<T, void>::value>::type
+  fulfilHelper(const F& func);
+
+  template <class F>
+  typename std::enable_if<
+    std::is_same<typename std::result_of<F()>::type, void>::value &&
+    std::is_same<T, void>::value>::type
+  fulfilHelper(const F& func);
+};
+
+}}
+
+#include "Promise-inl.h"
--- /dev/null
+++ b/hphp/submodules/folly/folly/wangle/Promise-inl.h
@@ -0,0 +1,160 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <atomic>
+#include <thread>
+
+#include "WangleException.h"
+#include "detail.h"
+
+namespace folly { namespace wangle {
+
+template <class T>
+Promise<T>::Promise() : retrieved_(false), obj_(new detail::FutureObject<T>())
+{}
+
+template <class T>
+Promise<T>::Promise(Promise<T>&& other) :
+retrieved_(other.retrieved_), obj_(other.obj_) {
+  other.obj_ = nullptr;
+}
+
+template <class T>
+Promise<T>& Promise<T>::operator=(Promise<T>&& other) {
+  std::swap(obj_, other.obj_);
+  std::swap(retrieved_, other.retrieved_);
+  return *this;
+}
+
+template <class T>
+void Promise<T>::throwIfFulfilled() {
+  if (!obj_)
+    throw PromiseAlreadySatisfied();
+}
+
+template <class T>
+void Promise<T>::throwIfRetrieved() {
+  if (retrieved_)
+    throw FutureAlreadyRetrieved();
+}
+
+template <class T>
+Promise<T>::~Promise() {
+  if (obj_) {
+    setException(BrokenPromise());
+  }
+}
+
+template <class T>
+Future<T> Promise<T>::getFuture() {
+  throwIfRetrieved();
+  throwIfFulfilled();
+  retrieved_ = true;
+  return Future<T>(obj_);
+}
+
+template <class T>
+template <class E>
+void Promise<T>::setException(E const& e) {
+  throwIfFulfilled();
+  setException(std::make_exception_ptr<E>(e));
+}
+
+template <class T>
+void Promise<T>::setException(std::exception_ptr const& e) {
+  throwIfFulfilled();
+  obj_->setException(e);
+  if (!retrieved_) {
+    delete obj_;
+  }
+  obj_ = nullptr;
+}
+
+template <class T>
+void Promise<T>::fulfilTry(Try<T>&& t) {
+  throwIfFulfilled();
+  obj_->fulfil(std::move(t));
+  if (!retrieved_) {
+    delete obj_;
+  }
+  obj_ = nullptr;
+}
+
+template <class T>
+template <class M>
+void Promise<T>::setValue(M&& v) {
+  static_assert(!std::is_same<T, void>::value,
+                "Use setValue() instead");
+
+  throwIfFulfilled();
+  obj_->fulfil(Try<T>(std::forward<M>(v)));
+  if (!retrieved_) {
+    delete obj_;
+  }
+  obj_ = nullptr;
+}
+
+template <class T>
+void Promise<T>::setValue() {
+  static_assert(std::is_same<T, void>::value,
+                "Use setValue(value) instead");
+
+  throwIfFulfilled();
+  obj_->fulfil(Try<void>());
+  if (!retrieved_) {
+    delete obj_;
+  }
+  obj_ = nullptr;
+}
+
+template <class T>
+template <class F>
+void Promise<T>::fulfil(const F& func) {
+  fulfilHelper(func);
+}
+
+template <class T>
+template <class F>
+typename std::enable_if<
+  std::is_convertible<typename std::result_of<F()>::type, T>::value &&
+  !std::is_same<T, void>::value>::type
+inline Promise<T>::fulfilHelper(const F& func) {
+  throwIfFulfilled();
+  try {
+    setValue(func());
+  } catch (...) {
+    setException(std::current_exception());
+  }
+}
+
+template <class T>
+template <class F>
+typename std::enable_if<
+  std::is_same<typename std::result_of<F()>::type, void>::value &&
+  std::is_same<T, void>::value>::type
+inline Promise<T>::fulfilHelper(const F& func) {
+  throwIfFulfilled();
+  try {
+    func();
+    setValue();
+  } catch (...) {
+    setException(std::current_exception());
+  }
+}
+
+}}
--- /dev/null
+++ b/hphp/submodules/folly/folly/wangle/README.md
@@ -0,0 +1,272 @@
+# Wangle
+Wangle is a framework for expressing asynchronous code in C++ using the Future pattern.
+
+**wan•gle** |ˈwaNGgəl| informal  
+*verb*  
+Obtain (something that is desired) by persuading others to comply or by manipulating events.
+
+*noun*  
+A framework for expressing asynchronous control flow in C++, that is composable and easily translated to/from synchronous code.
+
+*synonyms*  
+[Finagle](http://twitter.github.io/finagle/)
+
+Wangle is a futures-based async framework inspired by [Twitter's Finagle](http://twitter.github.io/finagle/) (which is in scala), and (loosely) building upon the existing (but anemic) Futures code found in the C++11 standard ([`std::future`](http://en.cppreference.com/w/cpp/thread/future)) and [`boost::future`](http://www.boost.org/doc/libs/1_53_0/boost/thread/future.hpp) (especially >= 1.53.0). Although inspired by the std::future interface, it is not syntactically drop-in compatible because some ideas didn't translate well enough and we decided to break from the API. But semantically, it should be straightforward to translate from existing std::future code to Wangle.
+
+The primary semantic differences are that Wangle Futures and Promises are not threadsafe; and as does `boost::future`, Wangle supports continuations (`then()`) and there are helper methods `whenAll()` and `whenAny()` which are important compositional building blocks.
+
+## Brief Synopsis
+
+```C++
+#include <folly/wangle/Future.h>
+using namespace folly::wangle;
+using namespace std;
+
+void foo(int x) {
+  // do something with x
+  cout << "foo(" << x << ")" << endl;
+}
+
+// ...
+
+  cout << "making Promise" << endl;
+  Promise<int> p;
+  Future<int> f = p.getFuture();
+  f.then(
+    [](Try<int>&& t) {
+      foo(t.value());
+    });
+  cout << "Future chain made" << endl;
+
+// ... now perhaps in another event callback
+
+  cout << "fulfilling Promise" << endl;
+  p.setValue(42);
+  cout << "Promise fulfilled" << endl;
+```
+
+This would print:
+  
+```
+making Promise
+Future chain made
+fulfilling Promise
+foo(42)
+Promise fulfilled
+```
+
+## User Guide
+
+Let's begin with an example. Consider a simplified Memcache client class with this interface:
+
+```C++
+class MemcacheClient {
+ public:
+  struct GetReply {
+    enum class Result {
+      FOUND,
+      NOT_FOUND,
+      SERVER_ERROR,
+    };
+
+    Result result;
+    // The value when result is FOUND,
+    // The error message when result is SERVER_ERROR or CLIENT_ERROR
+    // undefined otherwise
+    std::string value;
+  };
+
+  GetReply get(std::string key);
+};
+```
+
+This API is synchronous, i.e. when you call `get()` you have to wait for the result. This is very simple, but unfortunately it is also very easy to write very slow code using synchronous APIs.
+
+Now, consider this traditional asynchronous signature for `get()`:
+
+```C++
+int get(std::string key, std::function<void(GetReply)> callback);
+```
+
+When you call `get()`, your asynchronous operation begins and when it finishes your callback will be called with the result. (Unless something goes drastically wrong and you get an error code from `get()`.) Very performant code can be written with an API like this, but for nontrivial applications the code descends into a special kind of spaghetti code affectionately referred to as "callback hell".
+
+The Future-based API looks like this:
+
+```C++
+Future<GetReply> get(std::string key);
+```
+
+A `Future<GetReply>` is a placeholder for the `GetReply` that we will eventually get. A Future usually starts life out "unfulfilled", or incomplete, i.e.:
+
+```C++
+fut.isReady() == false
+fut.value()  // will throw an exception because the Future is not ready
+```
+
+At some point in the future, the Future will have been fulfilled, and we can access its value.
+
+```C++
+fut.isReady() == true
+GetReply& reply = fut.value();
+```
+
+Futures support exceptions. If something exceptional happened, your Future may represent an exception instead of a value. In that case:
+
+```C++
+fut.isReady() == true
+fut.value() // will rethrow the exception
+```
+
+Just what is exceptional depends on the API. In our example we have chosen not to raise exceptions for `SERVER_ERROR`, but represent this explicitly in the `GetReply` object. On the other hand, an astute Memcache veteran would notice that we left `CLIENT_ERROR` out of `GetReply::Result`, and perhaps a `CLIENT_ERROR` would have been raised as an exception, because `CLIENT_ERROR` means there's a bug in the library and this would be truly exceptional. These decisions are judgement calls by the API designer. The important thing is that exceptional conditions (including and especially spurious exceptions that nobody expects) get captured and can be handled higher up the "stack".
+
+So far we have described a way to initiate an asynchronous operation via an API that returns a Future, and then sometime later after it is fulfilled, we get its value. This is slightly more useful than a synchronous API, but it's not yet ideal. There are two more very important pieces to the puzzle.
+
+First, we can aggregate Futures, to define a new Future that completes after some or all of the aggregated Futures complete.  Consider two examples: fetching a batch of requests and waiting for all of them, and fetching a group of requests and waiting for only one of them.
+
+```C++
+vector<Future<GetReply>> futs;
+for (auto& key : keys) {
+  futs.push_back(mc.get(key));
+}
+auto all = whenAll(futs.begin(), futs.end());
+
+vector<Future<GetReply>> futs;
+for (auto& key : keys) {
+  futs.push_back(mc.get(key));
+}
+auto any = whenAny(futs.begin(), futs.end());
+```
+
+`all` and `any` are Futures (for the exact type and usage see the header files).  They will be complete when all/one of `futs` are complete, respectively. (There is also `whenN()` for when you need *some*.)
+
+Second, we can attach continuations (aka callbacks) to a Future, and chain them together monadically. An example will clarify:
+
+```C++
+Future<GetReply> fut1 = mc.get("foo");
+
+Future<string> fut2 = fut1.then(
+  [](Try<GetReply>&& t) {
+    if (t.value().result == MemcacheClient::GetReply::Result::FOUND)
+      return t.value().value;
+    throw SomeException("No value");
+  });
+
+Future<void> fut3 = fut2.then(
+  [](Try<string>&& t) {
+    try {
+      cout << t.value() << endl;
+    } catch (std::exception const& e) {
+      cerr << e.what() << endl;
+    }
+  });
+```
+
+That example is a little contrived but the idea is that you can transform a result from one type to another, potentially in a chain, and unhandled errors propagate. Of course, the intermediate variables are optional. `Try<T>` is the object wrapper that supports both value and exception.
+
+Using `then` to add continuations is idiomatic. It brings all the code into one place, which avoids callback hell.
+
+Up to this point we have skirted around the matter of waiting for Futures. You may never need to wait for a Future, because your code is event-driven and all follow-up action happens in a then-block. But if want to have a batch workflow, where you initiate a batch of asynchronous operations and then wait for them all to finish at a synchronization point, then you will want to wait for a Future.
+
+Other future frameworks like Finagle and std::future/boost::future, give you the ability to wait directly on a Future, by calling `fut.wait()` (naturally enough). Wangle has diverged from this pattern for performance reasons. It turns out, making things threadsafe slows them down.  Whodathunkit? So Wangle Futures (and Promises, for you API developers) are not threadsafe. Yes, you heard right, and it should give you pause—what good is an *asynchronous* framework that isn't threadsafe? Well, actually, in an event-driven architecture there's still quite a bit of value, but that doesn't really answer the question. Wangle is, indeed, meant to be used in multithreaded environments. It's just that we move synchronization out of the Future/Promise pair and instead require explicit synchronization or (preferably) crossing thread boundaries with a form of message passing. It turns out that `then()` chaining is really handy and there are often many Futures chained together. But there is often only one thread boundary to cross.  We choose to pay the threadsafety overhead only at that thread boundary.
+
+Wangle provides a mechanism to make this easy, called a `ThreadGate`. ThreadGates stand between two threads and pass messages (actually, functors) back and forth in a threadsafe manner. Let's work an example. Assume that `MemcacheClient::get()` is not thread-aware. It registers with libevent and tries to send a request, and then later in your event loop when it has successfully sent and received a reply it will complete the Future. But it doesn't even consider that there might be other threads in your program.  Now consider that `get()` calls should happen in an IO thread (the *east* thread) and user code is happening in a user thread (the *west* thread).  A ThreadGate will allow us to do this:
+
+```C++
+Future<GetReply> threadsafeGet(std::string key) {
+  std::function<Future<GetReply>()> doEast = [=]() {
+    return mc_->get(key);
+  };
+  auto westFuture = gate_.add(doEast);
+  return westFuture;
+}
+```
+
+Think of the ThreadGate as a pair of queues: from west to east we queue some functor that returns a Future, and then the ThreadGate conveys the result back from east to west and eventually fulfils the west Future. But when? The ThreadGate has to be *driven* from both sides—the IO thread has to pull work off the west-to-east queue, and the user thread has to drive the east-to-west queue. Sometimes this happens in the course of an event loop, as it would in a libevent architecture. Other times it has to be explicit, in which case you would call the gate's `makeProgress()` method. Or, if you know which Future you want to wait for you can use:
+
+```C++
+gate_.waitFor(fut);
+```
+
+The ThreadGate interface is simple, so any kind of threadsafe functor conveyance you can dream up that is perfect for your application can be implemented. Or, you can use `GenericThreadGate` and `Executor`s to make one out of existing pieces. The `ManualThreadGate` is worth a look as well, especially for unit tests.
+
+In practice, the API will probably do the gating for you, e.g. `MemcacheClient::get()` would return an already-gated Future and provide a `waitFor()` proxy that lets you wait for Futures it has returned. Then as a user you never have to worry about it. But this exposition was to explain the probably-surprising design decision to make Futures not threadsafe and not support direct waiting.
+
+`Later` is another approach to crossing thread boundaries that can be more flexible than ThreadGates.
+(TODO document `Later` here.)
+
+## You make me Promises, Promises
+
+If you are wrapping an asynchronous operation, or providing an asynchronous API to users, then you will want to make Promises. Every Future has a corresponding Promise (except Futures that spring into existence already completed, with `makeFuture()`). Promises are simple, you make one, you extract the Future, and you fulfil it with a value or an exception. Example:
+
+```C++
+Promise<int> p;
+Future<int> f = p.getFuture();
+
+f.isReady() == false
+
+p.setValue(42);
+
+f.isReady() == true
+f.value() == 42
+```
+
+and an exception example:
+
+```C++
+Promise<int> p;
+Future<int> f = p.getFuture();
+
+f.isReady() == false
+
+p.setException(std::runtime_error("Fail"));
+
+f.isReady() == true
+f.value() // throws the exception
+```
+
+It's good practice to use fulfil which takes a function and automatically captures exceptions, e.g.
+
+```C++
+Promise<int> p;
+p.fulfil([]{
+  try {
+    // do stuff that may throw
+    return 42;
+  } catch (MySpecialException const& e) {
+    // handle it
+    return 7;
+  }
+  // Any exceptions that we didn't catch, will be caught for us
+});
+```
+
+## FAQ
+
+### Why not use std::future?
+No callback or continuation support.
+See also http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2012/n3428.pdf
+
+### Why not use boost::future?
+- 1.53 is brand new, and not in fbcode
+- It's still a bit buggy/bleeding-edge
+- They haven't fleshed out the threading model very well yet, e.g. every single `then` currently spawns a new thread unless you explicitly ask it to work on this thread only, and there is no support for executors yet.
+- boost::future has locking which isn't necessary in our cooperative-multithreaded use case, and assumed to be very expensive.
+
+### Why use heap-allocated shared state? Why is Promise not a subclass of Future?
+C++. It boils down to wanting to return a Future by value for performance (move semantics and compiler optimizations), and programmer sanity, and needing a reference to the shared state by both the user (which holds the Future) and the asynchronous operation (which holds the Promise), and allowing either to go out of scope.
+
+### What about proper continuations? Futures suck.
+People mean two things here, they either mean using continuations or they mean using generators which require continuations. It's important to know those are two distinct questions, but in our context the answer is the same because continuations are a prerequisite for generators.
+
+C++ doesn't directly support continuations very well. But there are some ways to do them in C/C++ that rely on some rather low-level facilities like `setjmp` and `longjmp` (among others). So yes, they are possible (cf. [Mordor](https://github.com/ccutrer/mordor)).
+
+The tradeoff is memory. Each continuation has a stack, and that stack is usually fixed-size and has to be big enough to support whatever ordinary computation you might want to do on it. So each living continuation requires a relatively large amount of memory. If you know the number of continuations will be small, this might be a good fit. In particular, it might be faster and the code might read cleaner.
+
+Wangle takes the middle road between callback hell and continuations, one which has been trodden and proved useful in other languages. It doesn't claim to be the best model for all situations. Use your tools wisely.
+
+### It's so @!#?'n hard to get the thread safety right
+That's not a question.
+
+Yes, it is hard and so you should use a ThreadGate or Later if you need to do any crossing of thread boundaries. Otherwise you need to be very careful. Especially because in most cases the naïve approach is not threadsafe and naïve testing doesn't expose the race condition.
+
+The careful reader will note that we have only assumed locks to be a terrible performance penalty. We are planning on thoroughly benchmarking locks (and the alternative code patterns that having locks would enable), and if we find locks are Not That Bad™ we might reverse this decision (and fold ThreadGate and Later into Future/Promise).
--- /dev/null
+++ b/hphp/submodules/folly/folly/wangle/test/FutureTest.cpp
@@ -0,0 +1,617 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <algorithm>
+#include <folly/small_vector.h>
+#include <gtest/gtest.h>
+#include <memory>
+#include <string>
+#include <type_traits>
+#include <unistd.h>
+#include "folly/wangle/Executor.h"
+#include "folly/wangle/Future.h"
+
+using namespace folly::wangle;
+using std::pair;
+using std::string;
+using std::unique_ptr;
+using std::vector;
+
+#define EXPECT_TYPE(x, T) \
+  EXPECT_TRUE((std::is_same<decltype(x), T>::value))
+
+typedef WangleException eggs_t;
+static eggs_t eggs("eggs");
+
+// Future
+
+TEST(Future, try) {
+  class A {
+   public:
+    A(int x) : x_(x) {}
+
+    int x() const {
+      return x_;
+    }
+   private:
+    int x_;
+  };
+
+  A a(5);
+  Try<A> t_a(std::move(a));
+
+  Try<void> t_void;
+
+  EXPECT_EQ(5, t_a.value().x());
+}
+
+TEST(Future, special) {
+  EXPECT_FALSE(std::is_copy_constructible<Future<int>>::value);
+  EXPECT_FALSE(std::is_copy_assignable<Future<int>>::value);
+  EXPECT_TRUE(std::is_move_constructible<Future<int>>::value);
+  EXPECT_TRUE(std::is_move_assignable<Future<int>>::value);
+}
+
+TEST(Future, then) {
+  bool flag = false;
+
+  makeFuture<int>(42).then([&](Try<int>&& t) {
+                              flag = true;
+                              EXPECT_EQ(42, t.value());
+                            });
+  EXPECT_TRUE(flag); flag = false;
+
+  makeFuture<int>(42)
+    .then([](Try<int>&& t) { return t.value(); })
+    .then([&](Try<int>&& t) { flag = true; EXPECT_EQ(42, t.value()); });
+  EXPECT_TRUE(flag); flag = false;
+
+  makeFuture().then([&](Try<void>&& t) { flag = true; t.value(); });
+  EXPECT_TRUE(flag); flag = false;
+
+  Promise<void> p;
+  auto f = p.getFuture().then([&](Try<void>&& t) { flag = true; });
+  EXPECT_FALSE(flag);
+  EXPECT_FALSE(f.isReady());
+  p.setValue();
+  EXPECT_TRUE(flag);
+  EXPECT_TRUE(f.isReady());
+}
+
+static string doWorkStatic(Try<string>&& t) {
+  return t.value() + ";static";
+}
+
+TEST(Future, thenFunction) {
+  struct Worker {
+    string doWork(Try<string>&& t) {
+      return t.value() + ";class";
+    }
+    static string doWorkStatic(Try<string>&& t) {
+      return t.value() + ";class-static";
+    }
+  } w;
+
+  auto f = makeFuture<string>("start")
+    .then(doWorkStatic)
+    .then(Worker::doWorkStatic)
+    .then(&w, &Worker::doWork);
+
+  EXPECT_EQ(f.value(), "start;static;class-static;class");
+}
+
+static Future<string> doWorkStaticFuture(Try<string>&& t) {
+  return makeFuture(t.value() + ";static");
+}
+
+TEST(Future, thenFunctionFuture) {
+  struct Worker {
+    Future<string> doWorkFuture(Try<string>&& t) {
+      return makeFuture(t.value() + ";class");
+    }
+    static Future<string> doWorkStaticFuture(Try<string>&& t) {
+      return makeFuture(t.value() + ";class-static");
+    }
+  } w;
+
+  auto f = makeFuture<string>("start")
+    .then(doWorkStaticFuture)
+    .then(Worker::doWorkStaticFuture)
+    .then(&w, &Worker::doWorkFuture);
+
+  EXPECT_EQ(f.value(), "start;static;class-static;class");
+}
+
+TEST(Future, value) {
+  auto f = makeFuture(unique_ptr<int>(new int(42)));
+  auto up = std::move(f.value());
+  EXPECT_EQ(42, *up);
+
+  EXPECT_THROW(makeFuture<int>(eggs).value(), eggs_t);
+}
+
+TEST(Future, isReady) {
+  Promise<int> p;
+  auto f = p.getFuture();
+  EXPECT_FALSE(f.isReady());
+  p.setValue(42);
+  EXPECT_TRUE(f.isReady());
+  }
+
+TEST(Future, hasException) {
+  EXPECT_TRUE(makeFuture<int>(eggs).getTry().hasException());
+  EXPECT_FALSE(makeFuture(42).getTry().hasException());
+}
+
+TEST(Future, hasValue) {
+  EXPECT_TRUE(makeFuture(42).getTry().hasValue());
+  EXPECT_FALSE(makeFuture<int>(eggs).getTry().hasValue());
+}
+
+TEST(Future, makeFuture) {
+  EXPECT_TYPE(makeFuture(42), Future<int>);
+  EXPECT_EQ(42, makeFuture(42).value());
+
+  EXPECT_TYPE(makeFuture<float>(42), Future<float>);
+  EXPECT_EQ(42, makeFuture<float>(42).value());
+
+  auto fun = [] { return 42; };
+  EXPECT_TYPE(makeFutureTry(fun), Future<int>);
+  EXPECT_EQ(42, makeFutureTry(fun).value());
+
+  auto failfun = []() -> int { throw eggs; };
+  EXPECT_TYPE(makeFutureTry(failfun), Future<int>);
+  EXPECT_THROW(makeFutureTry(failfun).value(), eggs_t);
+
+  EXPECT_TYPE(makeFuture(), Future<void>);
+}
+
+// Promise
+
+TEST(Promise, special) {
+  EXPECT_FALSE(std::is_copy_constructible<Promise<int>>::value);
+  EXPECT_FALSE(std::is_copy_assignable<Promise<int>>::value);
+  EXPECT_TRUE(std::is_move_constructible<Promise<int>>::value);
+  EXPECT_TRUE(std::is_move_assignable<Promise<int>>::value);
+}
+
+TEST(Promise, getFuture) {
+  Promise<int> p;
+  Future<int> f = p.getFuture();
+  EXPECT_FALSE(f.isReady());
+}
+
+TEST(Promise, setValue) {
+  Promise<int> fund;
+  auto ffund = fund.getFuture();
+  fund.setValue(42);
+  EXPECT_EQ(42, ffund.value());
+
+  struct Foo {
+    string name;
+    int value;
+  };
+
+  Promise<Foo> pod;
+  auto fpod = pod.getFuture();
+  Foo f = {"the answer", 42};
+  pod.setValue(f);
+  Foo f2 = fpod.value();
+  EXPECT_EQ(f.name, f2.name);
+  EXPECT_EQ(f.value, f2.value);
+
+  pod = Promise<Foo>();
+  fpod = pod.getFuture();
+  pod.setValue(std::move(f2));
+  Foo f3 = fpod.value();
+  EXPECT_EQ(f.name, f3.name);
+  EXPECT_EQ(f.value, f3.value);
+
+  Promise<unique_ptr<int>> mov;
+  auto fmov = mov.getFuture();
+  mov.setValue(unique_ptr<int>(new int(42)));
+  unique_ptr<int> ptr = std::move(fmov.value());
+  EXPECT_EQ(42, *ptr);
+
+  Promise<void> v;
+  auto fv = v.getFuture();
+  v.setValue();
+  EXPECT_TRUE(fv.isReady());
+}
+
+TEST(Promise, setException) {
+  {
+    Promise<void> p;
+    auto f = p.getFuture();
+    p.setException(eggs);
+    EXPECT_THROW(f.value(), eggs_t);
+  }
+  {
+    Promise<void> p;
+    auto f = p.getFuture();
+    try {
+      throw eggs;
+    } catch (...) {
+      p.setException(std::current_exception());
+    }
+    EXPECT_THROW(f.value(), eggs_t);
+  }
+}
+
+TEST(Promise, fulfil) {
+  {
+    Promise<int> p;
+    auto f = p.getFuture();
+    p.fulfil([] { return 42; });
+    EXPECT_EQ(42, f.value());
+  }
+  {
+    Promise<int> p;
+    auto f = p.getFuture();
+    p.fulfil([]() -> int { throw eggs; });
+    EXPECT_THROW(f.value(), eggs_t);
+  }
+}
+
+TEST(Future, finish) {
+  auto x = std::make_shared<int>(0);
+  Promise<int> p;
+  auto f = p.getFuture().then([x](Try<int>&& t) { *x = t.value(); });
+
+  // The continuation hasn't executed
+  EXPECT_EQ(0, *x);
+
+  // The continuation has a reference to x
+  EXPECT_EQ(2, x.use_count());
+
+  p.setValue(42);
+
+  // the continuation has executed
+  EXPECT_EQ(42, *x);
+
+  // the continuation has been destructed
+  // and has released its reference to x
+  EXPECT_EQ(1, x.use_count());
+}
+
+TEST(Future, unwrap) {
+  Promise<int> a;
+  Promise<int> b;
+
+  auto fa = a.getFuture();
+  auto fb = b.getFuture();
+
+  bool flag1 = false;
+  bool flag2 = false;
+
+  // do a, then do b, and get the result of a + b.
+  Future<int> f = fa.then([&](Try<int>&& ta) {
+    auto va = ta.value();
+    flag1 = true;
+    return fb.then([va, &flag2](Try<int>&& tb) {
+      flag2 = true;
+      return va + tb.value();
+    });
+  });
+
+  EXPECT_FALSE(flag1);
+  EXPECT_FALSE(flag2);
+  EXPECT_FALSE(f.isReady());
+
+  a.setValue(3);
+  EXPECT_TRUE(flag1);
+  EXPECT_FALSE(flag2);
+  EXPECT_FALSE(f.isReady());
+
+  b.setValue(4);
+  EXPECT_TRUE(flag1);
+  EXPECT_TRUE(flag2);
+  EXPECT_EQ(7, f.value());
+}
+
+TEST(Future, whenAll) {
+  // returns a vector variant
+  {
+    vector<Promise<int>> promises(10);
+    vector<Future<int>> futures;
+
+    for (auto& p : promises)
+      futures.push_back(p.getFuture());
+
+    auto allf = whenAll(futures.begin(), futures.end());
+
+    random_shuffle(promises.begin(), promises.end());
+    for (auto& p : promises) {
+      EXPECT_FALSE(allf.isReady());
+      p.setValue(42);
+    }
+
+    EXPECT_TRUE(allf.isReady());
+    auto& results = allf.value();
+    for (auto& t : results) {
+      EXPECT_EQ(42, t.value());
+    }
+  }
+
+  // check error semantics
+  {
+    vector<Promise<int>> promises(4);
+    vector<Future<int>> futures;
+
+    for (auto& p : promises)
+      futures.push_back(p.getFuture());
+
+    auto allf = whenAll(futures.begin(), futures.end());
+
+
+    promises[0].setValue(42);
+    promises[1].setException(eggs);
+
+    EXPECT_FALSE(allf.isReady());
+
+    promises[2].setValue(42);
+
+    EXPECT_FALSE(allf.isReady());
+
+    promises[3].setException(eggs);
+
+    EXPECT_TRUE(allf.isReady());
+    EXPECT_FALSE(allf.getTry().hasException());
+
+    auto& results = allf.value();
+    EXPECT_EQ(42, results[0].value());
+    EXPECT_TRUE(results[1].hasException());
+    EXPECT_EQ(42, results[2].value());
+    EXPECT_TRUE(results[3].hasException());
+  }
+
+  // check that futures are ready in then()
+  {
+    vector<Promise<void>> promises(10);
+    vector<Future<void>> futures;
+
+    for (auto& p : promises)
+      futures.push_back(p.getFuture());
+
+    auto allf = whenAll(futures.begin(), futures.end())
+      .then([](Try<vector<Try<void>>>&& ts) {
+        for (auto& f : ts.value())
+          f.value();
+      });
+
+    random_shuffle(promises.begin(), promises.end());
+    for (auto& p : promises)
+      p.setValue();
+    EXPECT_TRUE(allf.isReady());
+  }
+}
+
+
+TEST(Future, whenAny) {
+  {
+    vector<Promise<int>> promises(10);
+    vector<Future<int>> futures;
+
+    for (auto& p : promises)
+      futures.push_back(p.getFuture());
+
+    for (auto& f : futures) {
+      EXPECT_FALSE(f.isReady());
+    }
+
+    auto anyf = whenAny(futures.begin(), futures.end());
+
+    /* futures were moved in, so these are invalid now */
+    EXPECT_FALSE(anyf.isReady());
+
+    promises[7].setValue(42);
+    EXPECT_TRUE(anyf.isReady());
+    auto& idx_fut = anyf.value();
+
+    auto i = idx_fut.first;
+    EXPECT_EQ(7, i);
+
+    auto& f = idx_fut.second;
+    EXPECT_EQ(42, f.value());
+  }
+
+  // error
+  {
+    vector<Promise<void>> promises(10);
+    vector<Future<void>> futures;
+
+    for (auto& p : promises)
+      futures.push_back(p.getFuture());
+
+    for (auto& f : futures) {
+      EXPECT_FALSE(f.isReady());
+    }
+
+    auto anyf = whenAny(futures.begin(), futures.end());
+
+    EXPECT_FALSE(anyf.isReady());
+
+    promises[3].setException(eggs);
+    EXPECT_TRUE(anyf.isReady());
+    EXPECT_TRUE(anyf.value().second.hasException());
+  }
+
+  // then()
+  {
+    vector<Promise<int>> promises(10);
+    vector<Future<int>> futures;
+
+    for (auto& p : promises)
+      futures.push_back(p.getFuture());
+
+    auto anyf = whenAny(futures.begin(), futures.end())
+      .then([](Try<pair<size_t, Try<int>>>&& f) {
+        EXPECT_EQ(42, f.value().second.value());
+      });
+
+    promises[3].setValue(42);
+    EXPECT_TRUE(anyf.isReady());
+  }
+}
+
+
+TEST(when, already_completed) {
+  {
+    vector<Future<void>> fs;
+    for (int i = 0; i < 10; i++)
+      fs.push_back(makeFuture());
+
+    whenAll(fs.begin(), fs.end())
+      .then([&](Try<vector<Try<void>>>&& t) {
+        EXPECT_EQ(fs.size(), t.value().size());
+      });
+  }
+  {
+    vector<Future<int>> fs;
+    for (int i = 0; i < 10; i++)
+      fs.push_back(makeFuture(i));
+
+    whenAny(fs.begin(), fs.end())
+      .then([&](Try<pair<size_t, Try<int>>>&& t) {
+        auto& p = t.value();
+        EXPECT_EQ(p.first, p.second.value());
+      });
+  }
+}
+
+TEST(when, whenN) {
+  vector<Promise<void>> promises(10);
+  vector<Future<void>> futures;
+
+  for (auto& p : promises)
+    futures.push_back(p.getFuture());
+
+  bool flag = false;
+  size_t n = 3;
+  whenN(futures.begin(), futures.end(), n)
+    .then([&](Try<vector<pair<size_t, Try<void>>>>&& t) {
+      flag = true;
+      auto v = t.value();
+      EXPECT_EQ(n, v.size());
+      for (auto& tt : v)
+        EXPECT_TRUE(tt.second.hasValue());
+    });
+
+  promises[0].setValue();
+  EXPECT_FALSE(flag);
+  promises[1].setValue();
+  EXPECT_FALSE(flag);
+  promises[2].setValue();
+  EXPECT_TRUE(flag);
+}
+
+/* Ensure that we can compile when_{all,any} with folly::small_vector */
+TEST(when, small_vector) {
+  using folly::small_vector;
+  {
+    small_vector<Future<void>> futures;
+
+    for (int i = 0; i < 10; i++)
+      futures.push_back(makeFuture());
+
+    auto anyf = whenAny(futures.begin(), futures.end());
+  }
+
+  {
+    small_vector<Future<void>> futures;
+
+    for (int i = 0; i < 10; i++)
+      futures.push_back(makeFuture());
+
+    auto allf = whenAll(futures.begin(), futures.end());
+  }
+}
+
+TEST(Future, whenAllVariadic) {
+  Promise<bool> pb;
+  Promise<int> pi;
+  Future<bool> fb = pb.getFuture();
+  Future<int> fi = pi.getFuture();
+  bool flag = false;
+  whenAll(std::move(fb), std::move(fi))
+    .then([&](Try<std::tuple<Try<bool>, Try<int>>>&& t) {
+      flag = true;
+      EXPECT_TRUE(t.hasValue());
+      EXPECT_TRUE(std::get<0>(t.value()).hasValue());
+      EXPECT_EQ(std::get<0>(t.value()).value(), true);
+      EXPECT_TRUE(std::get<1>(t.value()).hasValue());
+      EXPECT_EQ(std::get<1>(t.value()).value(), 42);
+    });
+  pb.setValue(true);
+  EXPECT_FALSE(flag);
+  pi.setValue(42);
+  EXPECT_TRUE(flag);
+}
+
+TEST(Future, whenAllVariadicReferences) {
+  Promise<bool> pb;
+  Promise<int> pi;
+  Future<bool> fb = pb.getFuture();
+  Future<int> fi = pi.getFuture();
+  bool flag = false;
+  whenAll(fb, fi)
+    .then([&](Try<std::tuple<Try<bool>, Try<int>>>&& t) {
+      flag = true;
+      EXPECT_TRUE(t.hasValue());
+      EXPECT_TRUE(std::get<0>(t.value()).hasValue());
+      EXPECT_EQ(std::get<0>(t.value()).value(), true);
+      EXPECT_TRUE(std::get<1>(t.value()).hasValue());
+      EXPECT_EQ(std::get<1>(t.value()).value(), 42);
+    });
+  pb.setValue(true);
+  EXPECT_FALSE(flag);
+  pi.setValue(42);
+  EXPECT_TRUE(flag);
+}
+
+TEST(Future, whenAll_none) {
+  vector<Future<int>> fs;
+  auto f = whenAll(fs.begin(), fs.end());
+  EXPECT_TRUE(f.isReady());
+}
+
+TEST(Future, throwCaughtInImmediateThen) {
+  // Neither of these should throw "Promise already satisfied"
+  makeFuture().then(
+    [=](Try<void>&&) -> int { throw std::exception(); });
+  makeFuture().then(
+    [=](Try<void>&&) -> Future<int> { throw std::exception(); });
+}
+
+TEST(Future, throwIfFailed) {
+  makeFuture<void>(eggs)
+    .then([=](Try<void>&& t) {
+      EXPECT_THROW(t.throwIfFailed(), eggs_t);
+    });
+  makeFuture()
+    .then([=](Try<void>&& t) {
+      EXPECT_NO_THROW(t.throwIfFailed());
+    });
+
+  makeFuture<int>(eggs)
+    .then([=](Try<int>&& t) {
+      EXPECT_THROW(t.throwIfFailed(), eggs_t);
+    });
+  makeFuture<int>(42)
+    .then([=](Try<int>&& t) {
+      EXPECT_NO_THROW(t.throwIfFailed());
+    });
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/wangle/test/LaterTest.cpp
@@ -0,0 +1,169 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <gtest/gtest.h>
+#include <thread>
+
+#include "folly/wangle/ManualExecutor.h"
+#include "folly/wangle/InlineExecutor.h"
+#include "folly/wangle/Later.h"
+
+using namespace folly::wangle;
+
+struct ManualWaiter {
+  explicit ManualWaiter(std::shared_ptr<ManualExecutor> ex) : ex(ex) {}
+
+  void makeProgress() {
+    ex->wait();
+    ex->run();
+  }
+
+  std::shared_ptr<ManualExecutor> ex;
+};
+
+struct LaterFixture : public testing::Test {
+  LaterFixture() :
+    westExecutor(new ManualExecutor),
+    eastExecutor(new ManualExecutor),
+    waiter(new ManualWaiter(westExecutor)),
+    done(false)
+  {
+    t = std::thread([=] {
+      ManualWaiter eastWaiter(eastExecutor);
+      while (!done)
+        eastWaiter.makeProgress();
+      });
+  }
+
+  ~LaterFixture() {
+    done = true;
+    eastExecutor->add([=]() { });
+    t.join();
+  }
+
+  void addAsync(int a, int b, std::function<void(int&&)>&& cob) {
+    eastExecutor->add([=]() {
+      cob(a + b);
+    });
+  }
+
+  Later<void> later;
+  std::shared_ptr<ManualExecutor> westExecutor;
+  std::shared_ptr<ManualExecutor> eastExecutor;
+  std::shared_ptr<ManualWaiter> waiter;
+  InlineExecutor inlineExecutor;
+  bool done;
+  std::thread t;
+};
+
+TEST(Later, construct_and_launch) {
+  bool fulfilled = false;
+  auto later = Later<void>().then([&](Try<void>&& t) {
+    fulfilled = true;
+    return makeFuture<int>(1);
+  });
+
+  // has not started yet.
+  EXPECT_FALSE(fulfilled);
+
+  EXPECT_EQ(later.launch().value(), 1);
+  EXPECT_TRUE(fulfilled);
+}
+
+TEST(Later, then_value) {
+  auto future = Later<int>(std::move(1))
+    .then([](Try<int>&& t) {
+      return t.value() == 1;
+    })
+    .launch();
+
+  EXPECT_TRUE(future.value());
+}
+
+TEST(Later, then_future) {
+  auto future = Later<int>(1)
+    .then([](Try<int>&& t) {
+      return makeFuture(t.value() == 1);
+    })
+    .launch();
+  EXPECT_TRUE(future.value());
+}
+
+TEST_F(LaterFixture, thread_hops) {
+  auto westThreadId = std::this_thread::get_id();
+  auto future = later.via(eastExecutor.get()).then([=](Try<void>&& t) {
+    EXPECT_NE(std::this_thread::get_id(), westThreadId);
+    return makeFuture<int>(1);
+  }).via(westExecutor.get()
+  ).then([=](Try<int>&& t) {
+    EXPECT_EQ(std::this_thread::get_id(), westThreadId);
+    return t.value();
+  }).launch();
+  while (!future.isReady()) {
+    waiter->makeProgress();
+  }
+  EXPECT_EQ(future.value(), 1);
+}
+
+TEST_F(LaterFixture, wrapping_preexisting_async_modules) {
+  auto westThreadId = std::this_thread::get_id();
+  std::function<void(std::function<void(int&&)>&&)> wrapper =
+    [=](std::function<void(int&&)>&& fn) {
+      addAsync(2, 2, std::move(fn));
+    };
+  auto future = Later<int>(std::move(wrapper))
+  .via(westExecutor.get())
+  .then([=](Try<int>&& t) {
+    EXPECT_EQ(std::this_thread::get_id(), westThreadId);
+    return t.value();
+  })
+  .launch();
+  while (!future.isReady()) {
+    waiter->makeProgress();
+  }
+  EXPECT_EQ(future.value(), 4);
+}
+
+TEST_F(LaterFixture, chain_laters) {
+  auto westThreadId = std::this_thread::get_id();
+  auto future = later.via(eastExecutor.get()).then([=](Try<void>&& t) {
+    EXPECT_NE(std::this_thread::get_id(), westThreadId);
+    return makeFuture<int>(1);
+  }).then([=](Try<int>&& t) {
+    int val = t.value();
+    return Later<int>(std::move(val)).via(westExecutor.get())
+      .then([=](Try<int>&& t) mutable {
+        EXPECT_EQ(std::this_thread::get_id(), westThreadId);
+        return t.value();
+      });
+  }).then([=](Try<int>&& t) {
+    EXPECT_EQ(std::this_thread::get_id(), westThreadId);
+    return t.value();
+  }).launch();
+
+  while (!future.isReady()) {
+    waiter->makeProgress();
+  }
+  EXPECT_EQ(future.value(), 1);
+}
+
+TEST_F(LaterFixture, fire_and_forget) {
+  auto west = westExecutor.get();
+  later.via(eastExecutor.get()).then([=](Try<void>&& t) {
+    west->add([]() {});
+  }).fireAndForget();
+  waiter->makeProgress();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/wangle/test/main.cpp
@@ -0,0 +1,22 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <gtest/gtest.h>
+
+int main(int argc, char** argv) {
+  ::testing::InitGoogleTest(&argc, argv);
+  return RUN_ALL_TESTS();
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/wangle/test/ThreadGateTest.cpp
@@ -0,0 +1,165 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <gtest/gtest.h>
+#include <thread>
+#include <future>
+
+#include "folly/wangle/Executor.h"
+#include "folly/wangle/ManualExecutor.h"
+#include "folly/wangle/ThreadGate.h"
+#include "folly/wangle/GenericThreadGate.h"
+
+using namespace folly::wangle;
+using std::make_shared;
+using std::shared_ptr;
+using std::thread;
+using std::vector;
+
+struct ManualWaiter {
+  explicit ManualWaiter(shared_ptr<ManualExecutor> ex) : ex(ex) {}
+
+  void makeProgress() {
+    ex->wait();
+    ex->run();
+  }
+
+  shared_ptr<ManualExecutor> ex;
+};
+
+struct GenericThreadGateFixture : public testing::Test {
+  GenericThreadGateFixture() :
+    westExecutor(new ManualExecutor),
+    eastExecutor(new ManualExecutor),
+    waiter(new ManualWaiter(westExecutor)),
+    tg(westExecutor, eastExecutor, waiter),
+    done(false)
+  {
+    t = thread([=] {
+      ManualWaiter eastWaiter(eastExecutor);
+      while (!done)
+        eastWaiter.makeProgress();
+    });
+  }
+
+  ~GenericThreadGateFixture() {
+    done = true;
+    tg.gate<void>([] { return makeFuture(); });
+    t.join();
+  }
+
+  shared_ptr<ManualExecutor> westExecutor;
+  shared_ptr<ManualExecutor> eastExecutor;
+  shared_ptr<ManualWaiter> waiter;
+  GenericThreadGate<
+    shared_ptr<ManualExecutor>,
+    shared_ptr<ManualExecutor>,
+    shared_ptr<ManualWaiter>> tg;
+  bool done;
+  thread t;
+};
+
+TEST_F(GenericThreadGateFixture, gate_and_wait) {
+  auto f = tg.gate<void>([] { return makeFuture(); });
+  EXPECT_FALSE(f.isReady());
+
+  tg.waitFor(f);
+  EXPECT_TRUE(f.isReady());
+}
+
+TEST_F(GenericThreadGateFixture, gate_many) {
+  vector<Future<void>> fs;
+  int n = 10;
+
+  for (int i = 0; i < n; i++)
+    fs.push_back(tg.gate<void>([&] { return makeFuture(); }));
+
+  for (auto& f : fs)
+    EXPECT_FALSE(f.isReady());
+
+  auto all = whenAll(fs.begin(), fs.end());
+  tg.waitFor(all);
+}
+
+TEST_F(GenericThreadGateFixture, gate_alternating) {
+  vector<Promise<void>> ps(10);
+  vector<Future<void>> fs;
+  size_t count = 0;
+
+  for (auto& p : ps) {
+    auto* pp = &p;
+    auto f = tg.gate<void>([=] { return pp->getFuture(); });
+
+    // abuse the thread gate to do our dirty work in the other thread
+    tg.gate<void>([=] { pp->setValue(); return makeFuture(); });
+
+    fs.push_back(f.then([&](Try<void>&&) { count++; }));
+  }
+
+  for (auto& f : fs)
+    EXPECT_FALSE(f.isReady());
+  EXPECT_EQ(0, count);
+
+  auto all = whenAll(fs.begin(), fs.end());
+  tg.waitFor(all);
+
+  EXPECT_EQ(ps.size(), count);
+}
+
+TEST(GenericThreadGate, noWaiter) {
+  auto west = make_shared<ManualExecutor>();
+  auto east = make_shared<ManualExecutor>();
+  Promise<void> p;
+  auto dummyFuture = p.getFuture();
+
+  GenericThreadGate<ManualExecutor*, ManualExecutor*>
+    tg(west.get(), east.get());
+
+  EXPECT_THROW(tg.waitFor(dummyFuture), std::logic_error);
+}
+
+TEST_F(GenericThreadGateFixture, gate_with_promise) {
+  Promise<int> p;
+
+  auto westId = std::this_thread::get_id();
+  bool westThenCalled = false;
+  auto f = p.getFuture().then(
+    [westId, &westThenCalled](Try<int>&& t) {
+      EXPECT_EQ(t.value(), 1);
+      EXPECT_EQ(std::this_thread::get_id(), westId);
+      westThenCalled = true;
+      return t.value();
+    });
+
+  bool eastPromiseMade = false;
+  std::async(std::launch::async, [&p, &eastPromiseMade, this]() {
+    // South thread != west thread. p gets set in west thread.
+    tg.gate<int>([&p, &eastPromiseMade, this] {
+                   EXPECT_EQ(t.get_id(), std::this_thread::get_id());
+                   Promise<int> eastPromise;
+                   auto eastFuture = eastPromise.getFuture();
+                   eastPromise.setValue(1);
+                   eastPromiseMade = true;
+                   return eastFuture;
+                 },
+                 std::move(p));
+    });
+
+  tg.waitFor(f);
+  EXPECT_TRUE(westThenCalled);
+  EXPECT_TRUE(eastPromiseMade);
+  EXPECT_EQ(f.value(), 1);
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/wangle/test/Try.cpp
@@ -0,0 +1,61 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <gtest/gtest.h>
+
+#include "folly/Memory.h"
+#include "folly/wangle/Try.h"
+
+using namespace folly::wangle;
+
+TEST(Try, makeTryFunction) {
+  auto func = []() {
+    return folly::make_unique<int>(1);
+  };
+
+  auto result = makeTryFunction(func);
+  EXPECT_TRUE(result.hasValue());
+  EXPECT_EQ(*result.value(), 1);
+}
+
+TEST(Try, makeTryFunctionThrow) {
+  auto func = []() {
+    throw std::runtime_error("Runtime");
+    return folly::make_unique<int>(1);
+  };
+
+  auto result = makeTryFunction(func);
+  EXPECT_TRUE(result.hasException());
+}
+
+TEST(Try, makeTryFunctionVoid) {
+  auto func = []() {
+    return;
+  };
+
+  auto result = makeTryFunction(func);
+  EXPECT_TRUE(result.hasValue());
+}
+
+TEST(Try, makeTryFunctionVoidThrow) {
+  auto func = []() {
+    throw std::runtime_error("Runtime");
+    return;
+  };
+
+  auto result = makeTryFunction(func);
+  EXPECT_TRUE(result.hasException());
+}
--- /dev/null
+++ b/hphp/submodules/folly/folly/wangle/ThreadGate.cpp
@@ -0,0 +1,28 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "ThreadGate.h"
+#include <stdexcept>
+
+namespace folly { namespace wangle {
+
+void ThreadGate::makeProgress()
+{
+  throw std::logic_error("This ThreadGate doesn't know how to "
+                         "make progress.");
+}
+
+}} // namespace
--- /dev/null
+++ b/hphp/submodules/folly/folly/wangle/ThreadGate.h
@@ -0,0 +1,193 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+#include <memory>
+#include "Future.h"
+
+namespace folly { namespace wangle {
+
+/**
+  Yo dawg, I heard you like asynchrony so I put asynchrony in your asynchronous
+  framework.
+
+  Wangle's futures and promises are not thread safe. Counterintuitive as this
+  may seem at first, this is very intentional. Making futures and promises
+  threadsafe drastically reduces their performance.
+
+  On the other hand, an asynchronous framework isn't much use if you can't do
+  asynchronous things in other threads. So we use the ThreadGate strategy to
+  decouple the threads and their futures with a form of message passing.
+
+  There are two actors, the east thread which does the asynchronous operation
+  (the server) and the west thread that wants the asynchronous operation done
+  (the client).
+
+  The client calls gate<T>(fn), which returns a Future<T>. Practically speaking
+  the returned Future<T> is the same as the Future<T> returned by fn. But
+  there are actually two futures involved - the original Future which will be
+  generated by fn (called the east Future), and the Future actually returned
+  by gate<T>(fn) (called the west Future).
+
+  These two futures are decoupled, and although the fulfilment of the east
+  Future eventually causes fulfilment of the west Future, those fulfilments
+  happen in their own threads.
+
+  In order to make and use a ThreadGate, you need to provide a strategy for
+  executing code in the east and west threads. These strategies may be
+  different. The only requirement is a threadsafe method
+  `void add(function<void()>&&)`.
+
+  In order for your ThreadGate to do anything, you need to drive those
+  executors somehow. An event loop is a natural fit. A thread pool might be
+  made to work. You could use a busy loop to make a very expensive space
+  heater. 0MQ would be pleasant.
+
+  Another pattern supported by the ThreadGate is the single-thread pattern. In
+  this pattern, non-blocking I/O drives the asynchronous operation, and
+  futures are fulfilled in an event loop callback. In this scenario,
+  ThreadGate is largely superfluous, and the executors would likely just
+  execute code immediately and inline (and therefore not need to be driven, or
+  threadsafe). But a Waiter strategy that makes progress by driving the event
+  loop one iteration would allow for gate-and-wait code which is agnostic to
+  the small detail that everything happens in one thread. It would also make
+  Future change toward a multithreaded architecture easier, as you need only
+  change the components of the ThreadGate which your client code is already
+  using.
+
+  Later (in Later.h) is an alternative mechanism for thread-traversing
+  asynchronous workflows.
+  */
+class ThreadGate {
+public:
+  virtual ~ThreadGate() {}
+
+  /**
+    Returns a Future that will be fulfilled after the Future that will be
+    returned by fn() has been fulfilled, with the same value or exception
+    (moved).
+
+    There's a lot of nuance in that sentence. Let's break it down.
+
+    fn kicks off the asynchronous operation (makes the east Promise), and must
+    be executed in the east thread because the east thread is where the east
+    Promise will be fulfilled. Since gate is being called from the west
+    thread, we must gate fn using the east executor. fn is not executed
+    immediately, it is queued up and will be executed by the east thread as it
+    drives the executor.
+
+    We create the west Promise and return its Future.
+
+    When the east thread executes its task, fn is called and the resulting
+    Future gets a callback that will gate another task back to the west.
+
+    Sometime later, the asynchronous operation completes and the east Promise
+    is fulfilled. Then the east Future executes its callback, which adds a
+    task to the west executor that task is to fulfil the west Promise with the
+    same Try<T>, and it will execute in the west thread.
+
+    At this point, the west Future is still unfulfilled, even though the east
+    Future has been fulfilled and its callback has finished executing. Only
+    when the west executor is driven to execute that task, the west Future
+    will be completed and its callbacks called.
+
+    In summary, both east and west need to have plans to drive their
+    executors, or nothing will actually happen. When the executors are driven,
+    then everything flows. */
+  template <class T>
+  Future<T> gate(std::function<Future<T>()>&& fn) {
+    Promise<T> pWest;
+    Future<T> fWest = pWest.getFuture();
+
+    gate(std::move(fn), std::move(pWest));
+    return fWest;
+  }
+
+  /**
+   * This version of gate is to support use cases where the calling thread is
+   * not the west thread. Here is an example use case.
+   *
+   *  Promise<T> pWest;
+   *  Future<T> fWest = pWest.getFuture();
+   *
+   *  // Set up callbacks for west from a thread that is not west.
+   *  fWest.then(...).then(...);
+   *
+   *  threadGate.gate(..., std::move(pWest));
+   *
+   * This function assumes that it is safe to call addEast from a thread that is
+   * not the west thread.
+   */
+  template <class T>
+  void gate(std::function<Future<T>()>&& fn,
+            Promise<T>&& p) {
+    folly::MoveWrapper<Promise<T>> pWest(std::move(p));
+    folly::MoveWrapper<std::function<Future<T>()>> fnm(std::move(fn));
+    this->addEast([pWest, fnm, this]() mutable {
+      (*fnm)().then([pWest, this](Try<T>&& t) mutable {
+        folly::MoveWrapper<Try<T>> tm(std::move(t));
+        this->addWest([pWest, tm]() mutable {
+          pWest->fulfilTry(std::move(*tm));
+        });
+      });
+    });
+  }
+
+  /**
+    If your workflow calls for synchronizing with a
+    west Future, then you may call waitFor, but if your west thread is
+    event-driven you will probably not need to call waitFor.
+
+    In order for waitFor to behave properly, you must ensure that the Waiter's
+    makeProgress method causes some progress to be made on the west thread,
+    i.e. drives the west executor either directly or indirectly.
+
+    (Naturally, progress needs to be made on the east thread as well. i.e. the
+    east executor is driven, the asynchronous operation happens, and its
+    Promise is fulfilled. It is likely that none of this concerns the consumer
+    of waitFor.)
+
+    This is the only function that uses the Waiter. It is never called
+    internally. Therefore, if you never use waitFor you can safely provide a
+    DummyWaiter.
+    */
+  template <class T>
+  void waitFor(Future<T> const& f) {
+    while (!f.isReady()) {
+      this->makeProgress();
+    }
+  }
+
+  template <class T>
+  typename std::add_lvalue_reference<T>::type
+  value(Future<T>& f) {
+    waitFor<T>(f);
+    return f.value();
+  }
+
+  template <class T>
+  typename std::add_lvalue_reference<const T>::type
+  value(Future<T> const& f) {
+    waitFor<T>(f);
+    return f.value();
+  }
+
+  virtual void addEast(std::function<void()>&&) = 0;
+  virtual void addWest(std::function<void()>&&) = 0;
+  virtual void makeProgress();
+};
+
+}} // namespace
--- /dev/null
+++ b/hphp/submodules/folly/folly/wangle/Try.h
@@ -0,0 +1,124 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+namespace folly { namespace wangle {
+
+template <class T>
+class Try {
+  static_assert(!std::is_reference<T>::value,
+                "Try may not be used with reference types");
+
+  enum class Contains {
+    VALUE,
+    EXCEPTION,
+    NOTHING,
+  };
+
+ public:
+  typedef T element_type;
+
+  Try() : contains_(Contains::NOTHING) {}
+  explicit Try(const T& v) : contains_(Contains::VALUE), value_(v) {}
+  explicit Try(T&& v) : contains_(Contains::VALUE), value_(std::move(v)) {}
+  explicit Try(std::exception_ptr e) : contains_(Contains::EXCEPTION), e_(e) {}
+
+  // move
+  Try(Try<T>&& t);
+  Try& operator=(Try<T>&& t);
+
+  // no copy
+  Try(const Try<T>& t) = delete;
+  Try& operator=(const Try<T>& t) = delete;
+
+  ~Try();
+
+  T& value();
+  const T& value() const;
+
+  void throwIfFailed() const;
+
+  const T& operator*() const { return value(); }
+        T& operator*()       { return value(); }
+
+  const T* operator->() const { return &value(); }
+        T* operator->()       { return &value(); }
+
+  bool hasValue() const { return contains_ == Contains::VALUE; }
+  bool hasException() const { return contains_ == Contains::EXCEPTION; }
+
+ private:
+  Contains contains_;
+  union {
+    T value_;
+    std::exception_ptr e_;
+  };
+};
+
+template <>
+class Try<void> {
+ public:
+  Try() : hasValue_(true) {}
+  explicit Try(std::exception_ptr e) : hasValue_(false), e_(e) {}
+
+  void value() const { throwIfFailed(); }
+  void operator*() const { return value(); }
+
+  inline void throwIfFailed() const;
+
+  bool hasValue() const { return hasValue_; }
+  bool hasException() const { return !hasValue_; }
+
+ private:
+  bool hasValue_;
+  std::exception_ptr e_;
+};
+
+/**
+ * Extracts value from try and returns it. Throws if try contained an exception.
+ */
+template <typename T>
+T moveFromTry(wangle::Try<T>&& t);
+
+/**
+ * Throws if try contained an exception.
+ */
+void moveFromTry(wangle::Try<void>&& t);
+
+/**
+ * Constructs Try based on the result of execution of function f (e.g. result
+ * or exception).
+ */
+template <typename F>
+typename std::enable_if<
+  !std::is_same<typename std::result_of<F()>::type, void>::value,
+  Try<typename std::result_of<F()>::type>>::type
+makeTryFunction(F&& f);
+
+/**
+ * makeTryFunction specialization for void functions.
+ */
+template <typename F>
+typename std::enable_if<
+  std::is_same<typename std::result_of<F()>::type, void>::value,
+  Try<void>>::type
+makeTryFunction(F&& f);
+
+
+}}
+
+#include "Try-inl.h"
--- /dev/null
+++ b/hphp/submodules/folly/folly/wangle/Try-inl.h
@@ -0,0 +1,120 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <stdexcept>
+
+#include "WangleException.h"
+
+namespace folly { namespace wangle {
+
+template <class T>
+Try<T>::Try(Try<T>&& t) : contains_(t.contains_) {
+  if (contains_ == Contains::VALUE) {
+    new (&value_)T(std::move(t.value_));
+  } else if (contains_ == Contains::EXCEPTION) {
+    new (&e_)std::exception_ptr(t.e_);
+  }
+}
+
+template <class T>
+Try<T>& Try<T>::operator=(Try<T>&& t) {
+  this->~Try();
+  contains_ = t.contains_;
+  if (contains_ == Contains::VALUE) {
+    new (&value_)T(std::move(t.value_));
+  } else if (contains_ == Contains::EXCEPTION) {
+    new (&e_)std::exception_ptr(t.e_);
+  }
+  return *this;
+}
+
+template <class T>
+Try<T>::~Try() {
+  if (contains_ == Contains::VALUE) {
+    value_.~T();
+  } else if (contains_ == Contains::EXCEPTION) {
+    e_.~exception_ptr();
+  }
+}
+
+template <class T>
+T& Try<T>::value() {
+  throwIfFailed();
+  return value_;
+}
+
+template <class T>
+const T& Try<T>::value() const {
+  throwIfFailed();
+  return value_;
+}
+
+template <class T>
+void Try<T>::throwIfFailed() const {
+  if (contains_ != Contains::VALUE) {
+    if (contains_ == Contains::EXCEPTION) {
+      std::rethrow_exception(e_);
+    } else {
+      throw UsingUninitializedTry();
+    }
+  }
+}
+
+void Try<void>::throwIfFailed() const {
+  if (!hasValue_) {
+    std::rethrow_exception(e_);
+  }
+}
+
+template <typename T>
+inline T moveFromTry(wangle::Try<T>&& t) {
+  return std::move(t.value());
+}
+
+inline void moveFromTry(wangle::Try<void>&& t) {
+  return t.value();
+}
+
+template <typename F>
+typename std::enable_if<
+  !std::is_same<typename std::result_of<F()>::type, void>::value,
+  Try<typename std::result_of<F()>::type>>::type
+makeTryFunction(F&& f) {
+  typedef typename std::result_of<F()>::type ResultType;
+  try {
+    auto value = f();
+    return Try<ResultType>(std::move(value));
+  } catch (...) {
+    return Try<ResultType>(std::current_exception());
+  }
+}
+
+template <typename F>
+typename std::enable_if<
+  std::is_same<typename std::result_of<F()>::type, void>::value,
+  Try<void>>::type
+makeTryFunction(F&& f) {
+  try {
+    f();
+    return Try<void>();
+  } catch (...) {
+    return Try<void>(std::current_exception());
+  }
+}
+
+}}
--- /dev/null
+++ b/hphp/submodules/folly/folly/wangle/WangleException.h
@@ -0,0 +1,77 @@
+/*
+ * Copyright 2014 Facebook, Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <exception>
+
+namespace folly { namespace wangle {
+
+class WangleException : public std::exception {
+
+public:
+
+  explicit WangleException(std::string message_arg)
+    : message(message_arg) {}
+
+  ~WangleException() throw(){}
+
+  virtual const char *what() const throw() {
+    return message.c_str();
+  }
+
+  bool operator==(const WangleException &other) const{
+    return other.message == this->message;
+  }
+
+  bool operator!=(const WangleException &other) const{
+    return !(*this == other);
+  }
+
+  protected:
+    std::string message;
+};
+
+class BrokenPromise : public WangleException {
+  public:
+    explicit BrokenPromise() :
+      WangleException("Broken promise") { }
+};
+
+class NoState : public WangleException {
+  public:
+    explicit NoState() : WangleException("No state") { }
+};
+
+class PromiseAlreadySatisfied : public WangleException {
+  public:
+    explicit PromiseAlreadySatisfied() :
+      WangleException("Promise already satisfied") { }
+};
+
+class FutureAlreadyRetrieved : public WangleException {
+  public:
+    explicit FutureAlreadyRetrieved () :
+      WangleException("Future already retrieved") { }
+};
+
+class UsingUninitializedTry : public WangleException {
+  public:
+    explicit UsingUninitializedTry() :
+      WangleException("Using unitialized try") { }
+};
+
+}}
--- /dev/null
+++ b/hphp/submodules/folly/.gitignore
@@ -0,0 +1,32 @@
+*.o
+*.lo
+*.la
+Makefile
+Makefile.in
+.libs
+.deps
+stamp-h1
+folly-config.h
+_configs.sed
+aclocal.m4
+autom4te.cache
+build-aux
+libtool
+folly/test/gtest-1.6.0
+folly/folly-config.h
+folly/test/*_benchmark
+folly/test/*_test
+folly/test/*_test_using_jemalloc
+folly/config.*
+folly/configure
+folly/m4/libtool.m4
+folly/m4/ltoptions.m4
+folly/m4/ltsugar.m4
+folly/m4/ltversion.m4
+folly/m4/lt~obsolete.m4
+folly/generate_fingerprint_tables
+folly/FormatTables.cpp
+folly/EscapeTables.cpp
+folly/GroupVarintTables.cpp
+folly/FingerprintTables.cpp
+
--- /dev/null
+++ b/hphp/submodules/folly/LICENSE
@@ -0,0 +1,177 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
--- /dev/null
+++ b/hphp/submodules/folly/README
@@ -0,0 +1,60 @@
+Folly: Facebook Open-source LibrarY
+-----------------------------------
+
+Folly is an open-source C++ library developed and used at Facebook.
+
+For details, see folly/docs/Overview.md.
+
+Folly is published on Github at https://github.com/facebook/folly; for
+discussions, there is a Google group at
+https://groups.google.com/d/forum/facebook-folly.
+
+Dependencies
+------------
+
+- double-conversion (http://code.google.com/p/double-conversion/)
+
+    By default, the build tooling for double-conversion does not build
+    any libraries, which folly requires.  To build the necessary libraries
+    copy folly/SConstruct.double-conversion to your double-conversion
+    source directory before building:
+
+      [double-conversion/] scons -f SConstruct.double-conversion
+
+    Then set CPPFLAGS/LDFLAGS so that folly can find your double-conversion
+    build:
+
+      [folly/] LDFLAGS=-L<double-conversion>/ CPPFLAGS=-I<double-conversion>/src/
+        configure ...
+
+- googletest (Google C++ Testing Framework)
+
+  Grab gtest 1.6.0 from:
+  http://googletest.googlecode.com/files/gtest-1.6.0.zip
+
+  Unzip it inside of the test/ subdirectory.
+
+- additional platform specific dependencies:
+
+  Ubuntu 12.10 64-bit
+    - g++
+    - automake
+    - autoconf
+    - autoconf-archive
+    - libtool
+    - libboost1.46-all-dev
+    - libgoogle-glog-dev
+    - libgflags-dev
+    - scons (for double-conversion)
+
+  Fedora 17 64-bit
+    - gcc
+    - gcc-c++
+    - autoconf
+    - autoconf-archive
+    - automake
+    - boost-devel
+    - libtool
+    - glog-devel
+    - gflags-devel
+    - scons (for double-conversion)
